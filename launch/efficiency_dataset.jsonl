{"repo":"apache/airflow","pull_number":51510,"instance_id":"apache__airflow-51510","issue_numbers":[47600],"base_commit":"695fef9a3cafb8d62c9d0063926fd9e5c3aa9b1e","patch":"diff --git a/airflow-core/src/airflow/cli/cli_config.py b/airflow-core/src/airflow/cli/cli_config.py\nindex a6b4fe40d90cf..725fbe46a4330 100644\n--- a/airflow-core/src/airflow/cli/cli_config.py\n+++ b/airflow-core/src/airflow/cli/cli_config.py\n@@ -483,6 +483,15 @@ def string_lower_type(val):\n     type=positive_int(allow_zero=False),\n     help=\"Wait time between retries in seconds\",\n )\n+ARG_DB_BATCH_SIZE = Arg(\n+    (\"--batch-size\",),\n+    default=None,\n+    type=positive_int(allow_zero=False),\n+    help=(\n+        \"Maximum number of rows to delete or archive in a single transaction.\\n\"\n+        \"Lower values reduce long-running locks but increase the number of batches.\"\n+    ),\n+)\n \n # pool\n ARG_POOL_NAME = Arg((\"pool\",), metavar=\"NAME\", help=\"Pool name\")\n@@ -1452,6 +1461,7 @@ class GroupCommand(NamedTuple):\n             ARG_VERBOSE,\n             ARG_YES,\n             ARG_DB_SKIP_ARCHIVE,\n+            ARG_DB_BATCH_SIZE,\n         ),\n     ),\n     ActionCommand(\ndiff --git a/airflow-core/src/airflow/cli/commands/db_command.py b/airflow-core/src/airflow/cli/commands/db_command.py\nindex 5a4f70cc60472..0681639ad2130 100644\n--- a/airflow-core/src/airflow/cli/commands/db_command.py\n+++ b/airflow-core/src/airflow/cli/commands/db_command.py\n@@ -283,6 +283,7 @@ def cleanup_tables(args):\n         verbose=args.verbose,\n         confirm=not args.yes,\n         skip_archive=args.skip_archive,\n+        batch_size=args.batch_size,\n     )\n \n \ndiff --git a/airflow-core/src/airflow/utils/db_cleanup.py b/airflow-core/src/airflow/utils/db_cleanup.py\nindex 77fa2a2147382..4c8e0a0dbc74b 100644\n--- a/airflow-core/src/airflow/utils/db_cleanup.py\n+++ b/airflow-core/src/airflow/utils/db_cleanup.py\n@@ -169,61 +169,80 @@ def _dump_table_to_file(*, target_table: str, file_path: str, export_format: str\n         raise AirflowException(f\"Export format {export_format} is not supported.\")\n \n \n-def _do_delete(*, query: Query, orm_model: Base, skip_archive: bool, session: Session) -> None:\n+def _do_delete(\n+    *, query: Query, orm_model: Base, skip_archive: bool, session: Session, batch_size: int | None\n+) -> None:\n+    import itertools\n     import re\n \n-    print(\"Performing Delete...\")\n-    # using bulk delete\n-    # create a new table and copy the rows there\n-    timestamp_str = re.sub(r\"[^\\d]\", \"\", timezone.utcnow().isoformat())[:14]\n-    target_table_name = f\"{ARCHIVE_TABLE_PREFIX}{orm_model.name}__{timestamp_str}\"\n-    print(f\"Moving data to table {target_table_name}\")\n     bind = session.get_bind()\n     dialect_name = bind.dialect.name\n-    target_table = None\n-    try:\n-        if dialect_name == \"mysql\":\n-            # MySQL with replication needs this split into two queries, so just do it for all MySQL\n-            # ERROR 1786 (HY000): Statement violates GTID consistency: CREATE TABLE ... SELECT.\n-            session.execute(text(f\"CREATE TABLE {target_table_name} LIKE {orm_model.name}\"))\n-            metadata = reflect_tables([target_table_name], session)\n-            target_table = metadata.tables[target_table_name]\n-            insert_stm = target_table.insert().from_select(target_table.c, query)\n-            logger.debug(\"insert statement:\\n%s\", insert_stm.compile())\n-            session.execute(insert_stm)\n+    batch_counter = itertools.count(1)\n+\n+    while True:\n+        limited_query = query.limit(batch_size) if batch_size else query\n+        if limited_query.count() == 0:  # nothing left to delete\n+            break\n+\n+        batch_no = next(batch_counter)\n+        suffix = f\"__b{batch_no}\" if batch_size else \"\"\n+\n+        if batch_size:\n+            print(f\"Performing Delete (batch {batch_no}, max {batch_size} rows)...\")\n         else:\n-            stmt = CreateTableAs(target_table_name, query.selectable)\n-            logger.debug(\"ctas query:\\n%s\", stmt.compile())\n-            session.execute(stmt)\n-        session.commit()\n-\n-        # delete the rows from the old table\n-        metadata = reflect_tables([orm_model.name, target_table_name], session)\n-        source_table = metadata.tables[orm_model.name]\n-        target_table = metadata.tables[target_table_name]\n-        logger.debug(\"rows moved; purging from %s\", source_table.name)\n-        if dialect_name == \"sqlite\":\n-            pk_cols = source_table.primary_key.columns\n-            delete = source_table.delete().where(\n-                tuple_(*pk_cols).in_(\n-                    select(*[target_table.c[x.name] for x in source_table.primary_key.columns])\n+            print(\"Performing Delete...\")\n+\n+        # using bulk delete\n+        # create a new table and copy the rows there\n+        timestamp_str = re.sub(r\"[^\\d]\", \"\", timezone.utcnow().isoformat())[:14]\n+        target_table_name = f\"{ARCHIVE_TABLE_PREFIX}{orm_model.name}__{timestamp_str}{suffix}\"\n+        print(f\"Moving data to table {target_table_name}\")\n+        target_table = None\n+\n+        try:\n+            if dialect_name == \"mysql\":\n+                # MySQL with replication needs this split into two queries, so just do it for all MySQL\n+                # ERROR 1786 (HY000): Statement violates GTID consistency: CREATE TABLE ... SELECT.\n+                session.execute(text(f\"CREATE TABLE {target_table_name} LIKE {orm_model.name}\"))\n+                metadata = reflect_tables([target_table_name], session)\n+                target_table = metadata.tables[target_table_name]\n+                insert_stm = target_table.insert().from_select(target_table.c, limited_query)\n+                logger.debug(\"insert statement:\\n%s\", insert_stm.compile())\n+                session.execute(insert_stm)\n+            else:\n+                stmt = CreateTableAs(target_table_name, limited_query.selectable)\n+                logger.debug(\"ctas query:\\n%s\", stmt.compile())\n+                session.execute(stmt)\n+            session.commit()\n+\n+            # delete the rows from the old table\n+            metadata = reflect_tables([orm_model.name, target_table_name], session)\n+            source_table = metadata.tables[orm_model.name]\n+            target_table = metadata.tables[target_table_name]\n+            logger.debug(\"rows moved; purging from %s\", source_table.name)\n+            if dialect_name == \"sqlite\":\n+                pk_cols = source_table.primary_key.columns\n+                delete = source_table.delete().where(\n+                    tuple_(*pk_cols).in_(\n+                        select(*[target_table.c[x.name] for x in source_table.primary_key.columns])\n+                    )\n                 )\n-            )\n-        else:\n-            delete = source_table.delete().where(\n-                and_(col == target_table.c[col.name] for col in source_table.primary_key.columns)\n-            )\n-        logger.debug(\"delete statement:\\n%s\", delete.compile())\n-        session.execute(delete)\n-        session.commit()\n-    except BaseException as e:\n-        raise e\n-    finally:\n-        if target_table is not None and skip_archive:\n-            bind = session.get_bind()\n-            target_table.drop(bind=bind)\n+            else:\n+                delete = source_table.delete().where(\n+                    and_(col == target_table.c[col.name] for col in source_table.primary_key.columns)\n+                )\n+            logger.debug(\"delete statement:\\n%s\", delete.compile())\n+            session.execute(delete)\n             session.commit()\n-            print(\"Finished Performing Delete\")\n+        except BaseException as e:\n+            raise e\n+        finally:\n+            if target_table is not None and skip_archive:\n+                bind = session.get_bind()\n+                target_table.drop(bind=bind)\n+                session.commit()\n+\n+    print(\"Finished Performing Delete\")\n \n \n def _subquery_keep_last(\n@@ -306,6 +325,7 @@ def _cleanup_table(\n     verbose: bool = False,\n     skip_archive: bool = False,\n     session: Session,\n+    batch_size: int | None = None,\n     **kwargs,\n ) -> None:\n     print()\n@@ -325,7 +345,13 @@ def _cleanup_table(\n     num_rows = _check_for_rows(query=query, print_rows=False)\n \n     if num_rows and not dry_run:\n-        _do_delete(query=query, orm_model=orm_model, skip_archive=skip_archive, session=session)\n+        _do_delete(\n+            query=query,\n+            orm_model=orm_model,\n+            skip_archive=skip_archive,\n+            session=session,\n+            batch_size=batch_size,\n+        )\n \n     session.commit()\n \n@@ -432,6 +458,7 @@ def run_cleanup(\n     confirm: bool = True,\n     skip_archive: bool = False,\n     session: Session = NEW_SESSION,\n+    batch_size: int | None = None,\n ) -> None:\n     \"\"\"\n     Purges old records in airflow metadata database.\n@@ -474,6 +501,7 @@ def run_cleanup(\n                     **table_config.__dict__,\n                     skip_archive=skip_archive,\n                     session=session,\n+                    batch_size=batch_size,\n                 )\n                 session.commit()\n         else:\n","test_patch":"diff --git a/airflow-core/tests/unit/cli/commands/test_db_command.py b/airflow-core/tests/unit/cli/commands/test_db_command.py\nindex e6d1b9bd6d94a..e910cc1e3148b 100644\n--- a/airflow-core/tests/unit/cli/commands/test_db_command.py\n+++ b/airflow-core/tests/unit/cli/commands/test_db_command.py\n@@ -376,6 +376,7 @@ def test_date_timezone_omitted(self, run_cleanup_mock, timezone):\n             verbose=False,\n             confirm=False,\n             skip_archive=False,\n+            batch_size=None,\n         )\n \n     @pytest.mark.parametrize(\"timezone\", [\"UTC\", \"Europe/Berlin\", \"America/Los_Angeles\"])\n@@ -396,6 +397,7 @@ def test_date_timezone_supplied(self, run_cleanup_mock, timezone):\n             verbose=False,\n             confirm=False,\n             skip_archive=False,\n+            batch_size=None,\n         )\n \n     @pytest.mark.parametrize(\"confirm_arg, expected\", [([\"-y\"], False), ([], True)])\n@@ -422,6 +424,7 @@ def test_confirm(self, run_cleanup_mock, confirm_arg, expected):\n             verbose=False,\n             confirm=expected,\n             skip_archive=False,\n+            batch_size=None,\n         )\n \n     @pytest.mark.parametrize(\"extra_arg, expected\", [([\"--skip-archive\"], True), ([], False)])\n@@ -448,6 +451,7 @@ def test_skip_archive(self, run_cleanup_mock, extra_arg, expected):\n             verbose=False,\n             confirm=True,\n             skip_archive=expected,\n+            batch_size=None,\n         )\n \n     @pytest.mark.parametrize(\"dry_run_arg, expected\", [([\"--dry-run\"], True), ([], False)])\n@@ -474,6 +478,7 @@ def test_dry_run(self, run_cleanup_mock, dry_run_arg, expected):\n             verbose=False,\n             confirm=True,\n             skip_archive=False,\n+            batch_size=None,\n         )\n \n     @pytest.mark.parametrize(\n@@ -502,6 +507,7 @@ def test_tables(self, run_cleanup_mock, extra_args, expected):\n             verbose=False,\n             confirm=True,\n             skip_archive=False,\n+            batch_size=None,\n         )\n \n     @pytest.mark.parametrize(\"extra_args, expected\", [([\"--verbose\"], True), ([], False)])\n@@ -528,6 +534,34 @@ def test_verbose(self, run_cleanup_mock, extra_args, expected):\n             verbose=expected,\n             confirm=True,\n             skip_archive=False,\n+            batch_size=None,\n+        )\n+\n+    @pytest.mark.parametrize(\"extra_args, expected\", [([\"--batch-size\", \"1234\"], 1234), ([], None)])\n+    @patch(\"airflow.cli.commands.db_command.run_cleanup\")\n+    def test_batch_size(self, run_cleanup_mock, extra_args, expected):\n+        \"\"\"\n+        batch_size should be forwarded to run_cleanup with correct type.\n+        \"\"\"\n+        args = self.parser.parse_args(\n+            [\n+                \"db\",\n+                \"clean\",\n+                \"--clean-before-timestamp\",\n+                \"2021-01-01\",\n+                *extra_args,\n+            ]\n+        )\n+        db_command.cleanup_tables(args)\n+\n+        run_cleanup_mock.assert_called_once_with(\n+            table_names=None,\n+            dry_run=False,\n+            clean_before_timestamp=pendulum.parse(\"2021-01-01 00:00:00Z\"),\n+            verbose=False,\n+            confirm=True,\n+            skip_archive=False,\n+            batch_size=expected,\n         )\n \n     @patch(\"airflow.cli.commands.db_command.export_archived_records\")\ndiff --git a/airflow-core/tests/unit/utils/test_db_cleanup.py b/airflow-core/tests/unit/utils/test_db_cleanup.py\nindex e479d621c1759..00c1cf4efd442 100644\n--- a/airflow-core/tests/unit/utils/test_db_cleanup.py\n+++ b/airflow-core/tests/unit/utils/test_db_cleanup.py\n@@ -123,6 +123,20 @@ def test_run_cleanup_skip_archive(self, cleanup_table_mock, kwargs, should_skip)\n         )\n         assert cleanup_table_mock.call_args.kwargs[\"skip_archive\"] is should_skip\n \n+    @patch(\"airflow.utils.db_cleanup._cleanup_table\")\n+    def test_run_cleanup_batch_size_propagation(self, cleanup_table_mock):\n+        \"\"\"Ensure batch_size is forwarded from run_cleanup to _cleanup_table.\"\"\"\n+        run_cleanup(\n+            clean_before_timestamp=None,\n+            table_names=[\"log\"],\n+            dry_run=None,\n+            verbose=None,\n+            confirm=False,\n+            batch_size=1234,\n+        )\n+        cleanup_table_mock.assert_called_once()\n+        assert cleanup_table_mock.call_args.kwargs[\"batch_size\"] == 1234\n+\n     @pytest.mark.parametrize(\n         \"table_names\",\n         [\n","problem_statement":"Allow to set batch size for db clean CLI\n### Body\n\nThe db clean CLI was added in https://github.com/apache/airflow/pull/20838\nThere is issue of timeout if the running the CLI on a very large table(s), ideally we should allow running the command with some kind of batch size flag that will split the work and execute it batch after batch.\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.\n","hints_text":"Hi @eladkal, can you assign this issue to me?\n@eladkal  I see that this is assigned to @Riddhikshah21.  If collaboration is possible, I'd love to help!\n> [@eladkal](https://github.com/eladkal) I see that this is assigned to [@Riddhikshah21](https://github.com/Riddhikshah21). If collaboration is possible, I'd love to help!\n\n@shubham-pyc : I suggest you pick another [open unassigned issue](https://github.com/apache/airflow/issues?q=is%3Aissue%20state%3Aopen%20no%3Aassignee). You should probably keep an eye on this Issue's PR and share feedback if you have any. \n\n\n@Riddhikshah21 , Do you have any questions or need help with this issue?\nI can work on this if no progress is being made. @rawwar \nThe [Astronomer example](https://www.astronomer.io/docs/learn/cleanup-dag-tutorial) provides a practical solution by finding the first candidate timestamp for cleanup, and using it to generate batches. But it is not doable with the public interface, it relies on internal APIs.\nWould be nice to have a similar batch-size as CLI parameter.\n\n","all_hints_text":"Hi @eladkal, can you assign this issue to me?\n@eladkal  I see that this is assigned to @Riddhikshah21.  If collaboration is possible, I'd love to help!\n> [@eladkal](https://github.com/eladkal) I see that this is assigned to [@Riddhikshah21](https://github.com/Riddhikshah21). If collaboration is possible, I'd love to help!\n\n@shubham-pyc : I suggest you pick another [open unassigned issue](https://github.com/apache/airflow/issues?q=is%3Aissue%20state%3Aopen%20no%3Aassignee). You should probably keep an eye on this Issue's PR and share feedback if you have any. \n\n\n@Riddhikshah21 , Do you have any questions or need help with this issue?\nI can work on this if no progress is being made. @rawwar \nThe [Astronomer example](https://www.astronomer.io/docs/learn/cleanup-dag-tutorial) provides a practical solution by finding the first candidate timestamp for cleanup, and using it to generate batches. But it is not doable with the public interface, it relies on internal APIs.\nWould be nice to have a similar batch-size as CLI parameter.\n\n","commit_urls":["https://github.com/apache/airflow/commit/f94cf06a86a297d2799815f6954314ebaf8318d8","https://github.com/apache/airflow/commit/42c290fcff6ebd55680ac4c8c50c0a2dc38c1191","https://github.com/apache/airflow/commit/d34eb3c3c9adcc5174eef611a051a4c50701cbc8"],"created_at":"2025-06-08T00:47:15Z","classification":"Efficiency"}
{"repo":"apache/airflow","pull_number":52840,"instance_id":"apache__airflow-52840","issue_numbers":[49001],"base_commit":"d2fc75339e3ef8a62712758a444387e15500c48d","patch":"diff --git a/chart/values.schema.json b/chart/values.schema.json\nindex e2943aa0975ca..37d8041a1fc2b 100644\n--- a/chart/values.schema.json\n+++ b/chart/values.schema.json\n@@ -1820,7 +1820,7 @@\n                         \"query\": {\n                             \"description\": \"Query to use for KEDA autoscaling. Must return a single integer.\",\n                             \"type\": \"string\",\n-                            \"default\": \"SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }}) FROM task_instance WHERE (state='running' OR state='queued') {{- if or (contains \\\"CeleryKubernetesExecutor\\\" .Values.executor) (contains \\\"KubernetesExecutor\\\" .Values.executor) }} AND queue != '{{ .Values.config.celery_kubernetes_executor.kubernetes_queue }}' {{- end }}\"\n+                            \"default\": \"SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }}) FROM task_instance WHERE (state='running' OR state='queued') {{- if contains \\\"CeleryKubernetesExecutor\\\" .Values.executor }} AND queue != '{{ .Values.config.celery_kubernetes_executor.kubernetes_queue }}' {{- else if contains \\\"KubernetesExecutor\\\" .Values.executor }} AND executor != 'KubernetesExecutor' {{- end }}\"\n                         },\n                         \"usePgbouncer\": {\n                             \"description\": \"Weather to use PGBouncer to connect to the database or not when it is enabled. This configuration will be ignored if PGBouncer is not enabled.\",\ndiff --git a/chart/values.yaml b/chart/values.yaml\nindex cff157e6d50ae..b9dcd651d2d3d 100644\n--- a/chart/values.yaml\n+++ b/chart/values.yaml\n@@ -732,9 +732,10 @@ workers:\n       SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }})\n       FROM task_instance\n       WHERE (state='running' OR state='queued')\n-      {{- if or (contains \"CeleryKubernetesExecutor\" .Values.executor)\n-      (contains \"KubernetesExecutor\" .Values.executor) }}\n+      {{- if contains \"CeleryKubernetesExecutor\" .Values.executor }}\n       AND queue != '{{ .Values.config.celery_kubernetes_executor.kubernetes_queue }}'\n+      {{- else if contains \"KubernetesExecutor\" .Values.executor }}\n+      AND executor != 'KubernetesExecutor'\n       {{- end }}\n \n     # Weather to use PGBouncer to connect to the database or not when it is enabled\n","test_patch":"diff --git a/helm-tests/tests/helm_tests/other/test_keda.py b/helm-tests/tests/helm_tests/other/test_keda.py\nindex 40b319a267848..4aa29cda30b05 100644\n--- a/helm-tests/tests/helm_tests/other/test_keda.py\n+++ b/helm-tests/tests/helm_tests/other/test_keda.py\n@@ -103,8 +103,11 @@ def build_query(executor, concurrency=16, queue=None):\n             f\"SELECT ceil(COUNT(*)::decimal / {concurrency}) \"\n             \"FROM task_instance WHERE (state='running' OR state='queued')\"\n         )\n-        if \"CeleryKubernetesExecutor\" in executor or \"KubernetesExecutor\" in executor:\n-            query += f\" AND queue != '{queue or 'kubernetes'}'\"\n+        if \"CeleryKubernetesExecutor\" in executor:\n+            queue_value = queue or \"kubernetes\"\n+            query += f\" AND queue != '{queue_value}'\"\n+        elif \"KubernetesExecutor\" in executor:\n+            query += \" AND executor != 'KubernetesExecutor'\"\n         return query\n \n     @pytest.mark.parametrize(\n","problem_statement":"KEDA Query for multiple executors should use \"executor\" field instead of \"queue\"\n### Official Helm Chart version\n\n1.16.0 (latest released)\n\n### Apache Airflow version\n\n2.10.5\n\n### Kubernetes Version\n\n1.30.10\n\n### Helm Chart configuration\n\nHello!\nWhen using [multiple executors](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/index.html#using-multiple-executors-concurrently), tasks are routed to the appropriate executor using the `executor` attribute.\nHowever, the default KEDA query in the chart uses the `queue` column in the database, which was relevant for the hybrid CeleryKubernetesExecutor.\n\nKEDA Query:\n```\nSELECT ceil(COUNT(*)::decimal / 4) FROM task_instance WHERE (state='running' OR state='queued') AND queue != 'kubernetes'\n```\n\n### Docker Image customizations\n\nnot relevant\n\n### What happened\n\nCelery workers scale up even when the task is configured to use KubernetesExecutor, since its queue attribute is `default`, while its executor attribute is `KubernetesExecutor`\n\n### What you think should happen instead\n\nWhen using the hybrid executor, the query should use the `queue`.\nOtherwise, the query should use `executor`.\n\n### How to reproduce\n\nDeploy Airflow with keda enabled, using multiple executors, with celery being the default:\n\n```\nexecutor: CeleryExecutor,KubernetesExecutor\nworkers:\n  keda:\n    enabled: true\n```\n\nCreate a DAG to launch multiple tasks using `KubernetesExecutor` with default queue.\n\nThe KEDA query will cause scale up of celery workers.\n\n### Anything else\n\nThank you and please let me know if any additional details are needed, or if I missed something to address this issue :)\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n","hints_text":"Hi @noamst-monday,\nBased on your description, I have reproduced the issue where Celery worker pods scale up unnecessarily when using multiple executors (`CeleryExecutor,KubernetesExecutor`), due to the KEDA query relying on the `queue` field instead of the `executor` field.\n\n\n---\n### Verify Unnecessary Scaling:\n- Notice that `airflow-dev-worker-0` (Celery worker pods) scale up (e.g., from 0/2 to 2/2) even though the tasks are configured with `executor='KubernetesExecutor'` and `queue='default'`.\n- The KEDA query interprets tasks with `queue='default'` as eligible for Celery worker scaling, despite their executor setting, leading to unnecessary pod creation and termination cycles.\n\n![Image](https://github.com/user-attachments/assets/94404dfc-5bfc-4b5f-8dc6-9c230a6ecf84)\n\n![Image](https://github.com/user-attachments/assets/56eb8f04-3541-4ea9-b5da-f4ea40aeddf5)\n\n---\n### Next Steps\nApply the corrected KEDA query from the PR (using `executor != 'KubernetesExecutor'`) and repeat the test to verify that Celery workers no longer scale up for `KubernetesExecutor` tasks.\n\n","all_hints_text":"Hi @noamst-monday,\nBased on your description, I have reproduced the issue where Celery worker pods scale up unnecessarily when using multiple executors (`CeleryExecutor,KubernetesExecutor`), due to the KEDA query relying on the `queue` field instead of the `executor` field.\n\n\n---\n### Verify Unnecessary Scaling:\n- Notice that `airflow-dev-worker-0` (Celery worker pods) scale up (e.g., from 0/2 to 2/2) even though the tasks are configured with `executor='KubernetesExecutor'` and `queue='default'`.\n- The KEDA query interprets tasks with `queue='default'` as eligible for Celery worker scaling, despite their executor setting, leading to unnecessary pod creation and termination cycles.\n\n![Image](https://github.com/user-attachments/assets/94404dfc-5bfc-4b5f-8dc6-9c230a6ecf84)\n\n![Image](https://github.com/user-attachments/assets/56eb8f04-3541-4ea9-b5da-f4ea40aeddf5)\n\n---\n### Next Steps\nApply the corrected KEDA query from the PR (using `executor != 'KubernetesExecutor'`) and repeat the test to verify that Celery workers no longer scale up for `KubernetesExecutor` tasks.\n\n","commit_urls":["https://github.com/apache/airflow/commit/45adbb3bfce057459882e98d7afe73e54755590b","https://github.com/apache/airflow/commit/15b0e67e3ed727c5f777050c839422762438cc2c"],"created_at":"2025-07-04T05:36:38Z","classification":"Efficiency"}
{"repo":"apache/airflow","pull_number":51699,"instance_id":"apache__airflow-51699","issue_numbers":[50185,51213],"base_commit":"915c2d7eefed55ea464fa192da93791fbf75f738","patch":"diff --git a/airflow-core/src/airflow/dag_processing/manager.py b/airflow-core/src/airflow/dag_processing/manager.py\nindex 272e086d90903..8af35af62b7ef 100644\n--- a/airflow-core/src/airflow/dag_processing/manager.py\n+++ b/airflow-core/src/airflow/dag_processing/manager.py\n@@ -77,6 +77,8 @@\n from airflow.utils.sqlalchemy import prohibit_commit, with_row_locks\n \n if TYPE_CHECKING:\n+    from socket import socket\n+\n     from sqlalchemy.orm import Session\n \n     from airflow.callbacks.callback_requests import CallbackRequest\n@@ -388,7 +390,7 @@ def _service_processor_sockets(self, timeout: float | None = 1.0):\n         \"\"\"\n         events = self.selector.select(timeout=timeout)\n         for key, _ in events:\n-            socket_handler = key.data\n+            socket_handler, on_close = key.data\n \n             # BrokenPipeError should be caught and treated as if the handler returned false, similar\n             # to EOF case\n@@ -397,8 +399,9 @@ def _service_processor_sockets(self, timeout: float | None = 1.0):\n             except BrokenPipeError:\n                 need_more = False\n             if not need_more:\n-                self.selector.unregister(key.fileobj)\n-                key.fileobj.close()  # type: ignore[union-attr]\n+                sock: socket = key.fileobj  # type: ignore[assignment]\n+                on_close(sock)\n+                sock.close()\n \n     def _queue_requested_files_for_parsing(self) -> None:\n         \"\"\"Queue any files requested for parsing as requested by users via UI/API.\"\"\"\ndiff --git a/airflow-core/src/airflow/dag_processing/processor.py b/airflow-core/src/airflow/dag_processing/processor.py\nindex 5f69082c758aa..011393f22c886 100644\n--- a/airflow-core/src/airflow/dag_processing/processor.py\n+++ b/airflow-core/src/airflow/dag_processing/processor.py\n@@ -68,7 +68,6 @@ class DagFileParseRequest(BaseModel):\n     bundle_path: Path\n     \"\"\"Passing bundle path around lets us figure out relative file path.\"\"\"\n \n-    requests_fd: int\n     callback_requests: list[CallbackRequest] = Field(default_factory=list)\n     type: Literal[\"DagFileParseRequest\"] = \"DagFileParseRequest\"\n \n@@ -102,18 +101,16 @@ class DagFileParsingResult(BaseModel):\n def _parse_file_entrypoint():\n     import structlog\n \n-    from airflow.sdk.execution_time import task_runner\n+    from airflow.sdk.execution_time import comms, task_runner\n \n     # Parse DAG file, send JSON back up!\n-    comms_decoder = task_runner.CommsDecoder[ToDagProcessor, ToManager](\n-        input=sys.stdin,\n-        decoder=TypeAdapter[ToDagProcessor](ToDagProcessor),\n+    comms_decoder = comms.CommsDecoder[ToDagProcessor, ToManager](\n+        body_decoder=TypeAdapter[ToDagProcessor](ToDagProcessor),\n     )\n \n-    msg = comms_decoder.get_message()\n+    msg = comms_decoder._get_response()\n     if not isinstance(msg, DagFileParseRequest):\n         raise RuntimeError(f\"Required first message to be a DagFileParseRequest, it was {msg}\")\n-    comms_decoder.request_socket = os.fdopen(msg.requests_fd, \"wb\", buffering=0)\n \n     task_runner.SUPERVISOR_COMMS = comms_decoder\n     log = structlog.get_logger(logger_name=\"task\")\n@@ -125,7 +122,7 @@ def _parse_file_entrypoint():\n \n     result = _parse_file(msg, log)\n     if result is not None:\n-        comms_decoder.send_request(log, result)\n+        comms_decoder.send(result)\n \n \n def _parse_file(msg: DagFileParseRequest, log: FilteringBoundLogger) -> DagFileParsingResult | None:\n@@ -266,20 +263,18 @@ def _on_child_started(\n         msg = DagFileParseRequest(\n             file=os.fspath(path),\n             bundle_path=bundle_path,\n-            requests_fd=self._requests_fd,\n             callback_requests=callbacks,\n         )\n-        self.send_msg(msg)\n+        self.send_msg(msg, request_id=0)\n \n-    def _handle_request(self, msg: ToManager, log: FilteringBoundLogger) -> None:  # type: ignore[override]\n+    def _handle_request(self, msg: ToManager, log: FilteringBoundLogger, req_id: int) -> None:  # type: ignore[override]\n         from airflow.sdk.api.datamodels._generated import ConnectionResponse, VariableResponse\n \n         resp: BaseModel | None = None\n         dump_opts = {}\n         if isinstance(msg, DagFileParsingResult):\n             self.parsing_result = msg\n-            return\n-        if isinstance(msg, GetConnection):\n+        elif isinstance(msg, GetConnection):\n             conn = self.client.connections.get(msg.conn_id)\n             if isinstance(conn, ConnectionResponse):\n                 conn_result = ConnectionResult.from_conn_response(conn)\n@@ -301,10 +296,16 @@ def _handle_request(self, msg: ToManager, log: FilteringBoundLogger) -> None:  #\n             resp = self.client.variables.delete(msg.key)\n         else:\n             log.error(\"Unhandled request\", msg=msg)\n+            self.send_msg(\n+                None,\n+                request_id=req_id,\n+                error=ErrorResponse(\n+                    detail={\"status_code\": 400, \"message\": \"Unhandled request\"},\n+                ),\n+            )\n             return\n \n-        if resp:\n-            self.send_msg(resp, **dump_opts)\n+        self.send_msg(resp, request_id=req_id, error=None, **dump_opts)\n \n     @property\n     def is_ready(self) -> bool:\n@@ -312,7 +313,7 @@ def is_ready(self) -> bool:\n             # Process still alive, def can't be finished yet\n             return False\n \n-        return self._num_open_sockets == 0\n+        return not self._open_sockets\n \n     def wait(self) -> int:\n         raise NotImplementedError(f\"Don't call wait on {type(self).__name__} objects\")\ndiff --git a/airflow-core/src/airflow/jobs/triggerer_job_runner.py b/airflow-core/src/airflow/jobs/triggerer_job_runner.py\nindex 5df6a03261523..adc87e802fa6f 100644\n--- a/airflow-core/src/airflow/jobs/triggerer_job_runner.py\n+++ b/airflow-core/src/airflow/jobs/triggerer_job_runner.py\n@@ -28,6 +28,7 @@\n from collections.abc import Generator, Iterable\n from contextlib import suppress\n from datetime import datetime\n+from socket import socket\n from traceback import format_exception\n from typing import TYPE_CHECKING, Annotated, Any, ClassVar, Literal, TypedDict, Union\n \n@@ -43,6 +44,7 @@\n from airflow.jobs.job import perform_heartbeat\n from airflow.models.trigger import Trigger\n from airflow.sdk.execution_time.comms import (\n+    CommsDecoder,\n     ConnectionResult,\n     DagRunStateResult,\n     DRCount,\n@@ -58,6 +60,7 @@\n     TICount,\n     VariableResult,\n     XComResult,\n+    _RequestFrame,\n )\n from airflow.sdk.execution_time.supervisor import WatchedSubprocess, make_buffered_socket_reader\n from airflow.stats import Stats\n@@ -70,8 +73,6 @@\n from airflow.utils.session import provide_session\n \n if TYPE_CHECKING:\n-    from socket import socket\n-\n     from sqlalchemy.orm import Session\n     from structlog.typing import FilteringBoundLogger, WrappedLogger\n \n@@ -181,7 +182,6 @@ class messages:\n     class StartTriggerer(BaseModel):\n         \"\"\"Tell the async trigger runner process to start, and where to send status update messages.\"\"\"\n \n-        requests_fd: int\n         type: Literal[\"StartTriggerer\"] = \"StartTriggerer\"\n \n     class TriggerStateChanges(BaseModel):\n@@ -295,7 +295,7 @@ class TriggerRunnerSupervisor(WatchedSubprocess):\n     \"\"\"\n     TriggerRunnerSupervisor is responsible for monitoring the subprocess and marshalling DB access.\n \n-    This class (which runs in the main process) is responsible for querying the DB, sending RunTrigger\n+    This class (which runs in the main/sync process) is responsible for querying the DB, sending RunTrigger\n     workload messages to the subprocess, and collecting results and updating them in the DB.\n     \"\"\"\n \n@@ -342,8 +342,8 @@ def start(  # type: ignore[override]\n     ):\n         proc = super().start(id=job.id, job=job, target=cls.run_in_process, logger=logger, **kwargs)\n \n-        msg = messages.StartTriggerer(requests_fd=proc._requests_fd)\n-        proc.send_msg(msg)\n+        msg = messages.StartTriggerer()\n+        proc.send_msg(msg, request_id=0)\n         return proc\n \n     @functools.cached_property\n@@ -355,7 +355,7 @@ def client(self) -> Client:\n         client.base_url = \"http://in-process.invalid./\"  # type: ignore[assignment]\n         return client\n \n-    def _handle_request(self, msg: ToTriggerSupervisor, log: FilteringBoundLogger) -> None:  # type: ignore[override]\n+    def _handle_request(self, msg: ToTriggerSupervisor, log: FilteringBoundLogger, req_id: int) -> None:  # type: ignore[override]\n         from airflow.sdk.api.datamodels._generated import (\n             ConnectionResponse,\n             TaskStatesResponse,\n@@ -454,8 +454,7 @@ def _handle_request(self, msg: ToTriggerSupervisor, log: FilteringBoundLogger) -\n         else:\n             raise ValueError(f\"Unknown message type {type(msg)}\")\n \n-        if resp:\n-            self.send_msg(resp, **dump_opts)\n+        self.send_msg(resp, request_id=req_id, error=None, **dump_opts)\n \n     def run(self) -> None:\n         \"\"\"Run synchronously and handle all database reads/writes.\"\"\"\n@@ -628,7 +627,7 @@ def _register_pipe_readers(self, stdout: socket, stderr: socket, requests: socke\n             ),\n         )\n \n-    def _process_log_messages_from_subprocess(self) -> Generator[None, bytes, None]:\n+    def _process_log_messages_from_subprocess(self) -> Generator[None, bytes | bytearray, None]:\n         import msgspec\n         from structlog.stdlib import NAME_TO_LEVEL\n \n@@ -691,14 +690,60 @@ class TriggerDetails(TypedDict):\n     events: int\n \n \n+@attrs.define(kw_only=True)\n+class TriggerCommsDecoder(CommsDecoder[ToTriggerRunner, ToTriggerSupervisor]):\n+    _async_writer: asyncio.StreamWriter = attrs.field(alias=\"async_writer\")\n+    _async_reader: asyncio.StreamReader = attrs.field(alias=\"async_reader\")\n+\n+    body_decoder: TypeAdapter[ToTriggerRunner] = attrs.field(\n+        factory=lambda: TypeAdapter(ToTriggerRunner), repr=False\n+    )\n+\n+    _lock: asyncio.Lock = attrs.field(factory=asyncio.Lock, repr=False)\n+\n+    def _read_frame(self):\n+        from asgiref.sync import async_to_sync\n+\n+        return async_to_sync(self._aread_frame)()\n+\n+    def send(self, msg: ToTriggerSupervisor) -> ToTriggerRunner | None:\n+        from asgiref.sync import async_to_sync\n+\n+        return async_to_sync(self.asend)(msg)\n+\n+    async def _aread_frame(self):\n+        len_bytes = await self._async_reader.readexactly(4)\n+        len = int.from_bytes(len_bytes, byteorder=\"big\")\n+        if len >= 2**32:\n+            raise OverflowError(f\"Refusing to receive messages larger than 4GiB {len=}\")\n+\n+        buffer = await self._async_reader.readexactly(len)\n+        return self.resp_decoder.decode(buffer)\n+\n+    async def _aget_response(self, expect_id: int) -> ToTriggerRunner | None:\n+        frame = await self._aread_frame()\n+        if frame.id != expect_id:\n+            # Given the lock we take out in `asend`, this _shouldn't_ be possible, but I'd rather fail with\n+            # this explicit error return the wrong type of message back to a Trigger\n+            raise RuntimeError(f\"Response read out of order! Got {frame.id=}, {expect_id=}\")\n+        return self._from_frame(frame)\n+\n+    async def asend(self, msg: ToTriggerSupervisor) -> ToTriggerRunner | None:\n+        frame = _RequestFrame(id=next(self.id_counter), body=msg.model_dump())\n+        bytes = frame.as_bytes()\n+\n+        async with self._lock:\n+            self._async_writer.write(bytes)\n+\n+            return await self._aget_response(frame.id)\n+\n+\n class TriggerRunner:\n     \"\"\"\n     Runtime environment for all triggers.\n \n-    Mainly runs inside its own thread, where it hands control off to an asyncio\n-    event loop, but is also sometimes interacted with from the main thread\n-    (where all the DB queries are done). All communication between threads is\n-    done via Deques.\n+    Mainly runs inside its own process, where it hands control off to an asyncio\n+    event loop. All communication between this and it's (sync) supervisor is done via sockets\n     \"\"\"\n \n     # Maps trigger IDs to their running tasks and other info\n@@ -726,10 +771,7 @@ class TriggerRunner:\n     # TODO: connect this to the parent process\n     log: FilteringBoundLogger = structlog.get_logger()\n \n-    requests_sock: asyncio.StreamWriter\n-    response_sock: asyncio.StreamReader\n-\n-    decoder: TypeAdapter[ToTriggerRunner]\n+    comms_decoder: TriggerCommsDecoder\n \n     def __init__(self):\n         super().__init__()\n@@ -740,7 +782,6 @@ def __init__(self):\n         self.events = deque()\n         self.failed_triggers = deque()\n         self.job_id = None\n-        self.decoder = TypeAdapter(ToTriggerRunner)\n \n     def run(self):\n         \"\"\"Sync entrypoint - just run a run in an async loop.\"\"\"\n@@ -796,36 +837,21 @@ async def init_comms(self):\n         \"\"\"\n         from airflow.sdk.execution_time import task_runner\n \n-        loop = asyncio.get_event_loop()\n+        # Yes, we read and write to stdin! It's a socket, not a normal stdin.\n+        reader, writer = await asyncio.open_connection(sock=socket(fileno=0))\n \n-        comms_decoder = task_runner.CommsDecoder[ToTriggerRunner, ToTriggerSupervisor](\n-            input=sys.stdin,\n-            decoder=self.decoder,\n+        self.comms_decoder = TriggerCommsDecoder(\n+            async_writer=writer,\n+            async_reader=reader,\n         )\n \n-        task_runner.SUPERVISOR_COMMS = comms_decoder\n-\n-        async def connect_stdin() -> asyncio.StreamReader:\n-            reader = asyncio.StreamReader()\n-            protocol = asyncio.StreamReaderProtocol(reader)\n-            await loop.connect_read_pipe(lambda: protocol, sys.stdin)\n-            return reader\n-\n-        self.response_sock = await connect_stdin()\n+        task_runner.SUPERVISOR_COMMS = self.comms_decoder\n \n-        line = await self.response_sock.readline()\n+        msg = await self.comms_decoder._aget_response(expect_id=0)\n \n-        msg = self.decoder.validate_json(line)\n         if not isinstance(msg, messages.StartTriggerer):\n             raise RuntimeError(f\"Required first message to be a messages.StartTriggerer, it was {msg}\")\n \n-        comms_decoder.request_socket = os.fdopen(msg.requests_fd, \"wb\", buffering=0)\n-        writer_transport, writer_protocol = await loop.connect_write_pipe(\n-            lambda: asyncio.streams.FlowControlMixin(loop=loop),\n-            comms_decoder.request_socket,\n-        )\n-        self.requests_sock = asyncio.streams.StreamWriter(writer_transport, writer_protocol, None, loop)\n-\n     async def create_triggers(self):\n         \"\"\"Drain the to_create queue and create all new triggers that have been requested in the DB.\"\"\"\n         while self.to_create:\n@@ -934,8 +960,6 @@ async def cleanup_finished_triggers(self) -> list[int]:\n         return finished_ids\n \n     async def sync_state_to_supervisor(self, finished_ids: list[int]):\n-        from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n-\n         # Copy out of our deques in threadsafe manner to sync state with parent\n         events_to_send = []\n         while self.events:\n@@ -961,19 +985,17 @@ async def sync_state_to_supervisor(self, finished_ids: list[int]):\n         if not finished_ids:\n             msg.finished = None\n \n-        # Block triggers from making any requests for the duration of this\n-        async with SUPERVISOR_COMMS.lock:\n-            # Tell the monitor that we've finished triggers so it can update things\n-            self.requests_sock.write(msg.model_dump_json(exclude_none=True).encode() + b\"\\n\")\n-            line = await self.response_sock.readline()\n-\n-        if line == b\"\":  # EoF received!\n+        # Tell the monitor that we've finished triggers so it can update things\n+        try:\n+            resp = await self.comms_decoder.asend(msg)\n+        except asyncio.IncompleteReadError:\n             if task := asyncio.current_task():\n                 task.cancel(\"EOF - shutting down\")\n+                return\n+            raise\n \n-        resp = self.decoder.validate_json(line)\n         if not isinstance(resp, messages.TriggerStateSync):\n-            raise RuntimeError(f\"Expected to get a TriggerStateSync message, instead we got f{type(msg)}\")\n+            raise RuntimeError(f\"Expected to get a TriggerStateSync message, instead we got {type(msg)}\")\n         self.to_create.extend(resp.to_create)\n         self.to_cancel.extend(resp.to_cancel)\n \ndiff --git a/task-sdk/pyproject.toml b/task-sdk/pyproject.toml\nindex a3fa147910e3a..468a6308e6032 100644\n--- a/task-sdk/pyproject.toml\n+++ b/task-sdk/pyproject.toml\n@@ -47,7 +47,6 @@ classifiers = [\n \n dependencies = [\n     \"apache-airflow-core<3.2.0,>=3.1.0\",\n-    \"aiologic>=0.14.0\",\n     \"attrs>=24.2.0, !=25.2.0\",\n     \"fsspec>=2023.10.0\",\n     \"httpx>=0.27.0\",\ndiff --git a/task-sdk/src/airflow/sdk/bases/xcom.py b/task-sdk/src/airflow/sdk/bases/xcom.py\nindex c9b777daca32e..770dbf53df769 100644\n--- a/task-sdk/src/airflow/sdk/bases/xcom.py\n+++ b/task-sdk/src/airflow/sdk/bases/xcom.py\n@@ -77,9 +77,8 @@ def set(\n             map_index=map_index,\n         )\n \n-        SUPERVISOR_COMMS.send_request(\n-            log=log,\n-            msg=SetXCom(\n+        SUPERVISOR_COMMS.send(\n+            SetXCom(\n                 key=key,\n                 value=value,\n                 dag_id=dag_id,\n@@ -114,9 +113,8 @@ def _set_xcom_in_db(\n         \"\"\"\n         from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n \n-        SUPERVISOR_COMMS.send_request(\n-            log=log,\n-            msg=SetXCom(\n+        SUPERVISOR_COMMS.send(\n+            SetXCom(\n                 key=key,\n                 value=value,\n                 dag_id=dag_id,\n@@ -188,23 +186,16 @@ def _get_xcom_db_ref(\n         \"\"\"\n         from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n \n-        # Since Triggers can hit this code path via `sync_to_async` (which uses threads internally)\n-        # we need to make sure that we \"atomically\" send a request and get the response to that\n-        # back so that two triggers don't end up interleaving requests and create a possible\n-        # race condition where the wrong trigger reads the response.\n-        with SUPERVISOR_COMMS.lock:\n-            SUPERVISOR_COMMS.send_request(\n-                log=log,\n-                msg=GetXCom(\n-                    key=key,\n-                    dag_id=dag_id,\n-                    task_id=task_id,\n-                    run_id=run_id,\n-                    map_index=map_index,\n-                ),\n-            )\n-\n-            msg = SUPERVISOR_COMMS.get_message()\n+        msg = SUPERVISOR_COMMS.send(\n+            GetXCom(\n+                key=key,\n+                dag_id=dag_id,\n+                task_id=task_id,\n+                run_id=run_id,\n+                map_index=map_index,\n+            ),\n+        )\n+\n         if not isinstance(msg, XComResult):\n             raise TypeError(f\"Expected XComResult, received: {type(msg)} {msg}\")\n \n@@ -248,23 +239,16 @@ def get_one(\n         \"\"\"\n         from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n \n-        # Since Triggers can hit this code path via `sync_to_async` (which uses threads internally)\n-        # we need to make sure that we \"atomically\" send a request and get the response to that\n-        # back so that two triggers don't end up interleaving requests and create a possible\n-        # race condition where the wrong trigger reads the response.\n-        with SUPERVISOR_COMMS.lock:\n-            SUPERVISOR_COMMS.send_request(\n-                log=log,\n-                msg=GetXCom(\n-                    key=key,\n-                    dag_id=dag_id,\n-                    task_id=task_id,\n-                    run_id=run_id,\n-                    map_index=map_index,\n-                    include_prior_dates=include_prior_dates,\n-                ),\n-            )\n-            msg = SUPERVISOR_COMMS.get_message()\n+        msg = SUPERVISOR_COMMS.send(\n+            GetXCom(\n+                key=key,\n+                dag_id=dag_id,\n+                task_id=task_id,\n+                run_id=run_id,\n+                map_index=map_index,\n+                include_prior_dates=include_prior_dates,\n+            ),\n+        )\n \n         if not isinstance(msg, XComResult):\n             raise TypeError(f\"Expected XComResult, received: {type(msg)} {msg}\")\n@@ -307,24 +291,17 @@ def get_all(\n         \"\"\"\n         from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n \n-        # Since Triggers can hit this code path via `sync_to_async` (which uses threads internally)\n-        # we need to make sure that we \"atomically\" send a request and get the response to that\n-        # back so that two triggers don't end up interleaving requests and create a possible\n-        # race condition where the wrong trigger reads the response.\n-        with SUPERVISOR_COMMS.lock:\n-            SUPERVISOR_COMMS.send_request(\n-                log=log,\n-                msg=GetXComSequenceSlice(\n-                    key=key,\n-                    dag_id=dag_id,\n-                    task_id=task_id,\n-                    run_id=run_id,\n-                    start=None,\n-                    stop=None,\n-                    step=None,\n-                ),\n-            )\n-            msg = SUPERVISOR_COMMS.get_message()\n+        msg = SUPERVISOR_COMMS.send(\n+            msg=GetXComSequenceSlice(\n+                key=key,\n+                dag_id=dag_id,\n+                task_id=task_id,\n+                run_id=run_id,\n+                start=None,\n+                stop=None,\n+                step=None,\n+            ),\n+        )\n \n         if not isinstance(msg, XComSequenceSliceResult):\n             raise TypeError(f\"Expected XComSequenceSliceResult, received: {type(msg)} {msg}\")\n@@ -379,9 +356,8 @@ def delete(\n             map_index=map_index,\n         )\n         cls.purge(xcom_result)  # type: ignore[call-arg]\n-        SUPERVISOR_COMMS.send_request(\n-            log=log,\n-            msg=DeleteXCom(\n+        SUPERVISOR_COMMS.send(\n+            DeleteXCom(\n                 key=key,\n                 dag_id=dag_id,\n                 task_id=task_id,\ndiff --git a/task-sdk/src/airflow/sdk/definitions/asset/decorators.py b/task-sdk/src/airflow/sdk/definitions/asset/decorators.py\nindex d899dd3718336..44479fbb9cd42 100644\n--- a/task-sdk/src/airflow/sdk/definitions/asset/decorators.py\n+++ b/task-sdk/src/airflow/sdk/definitions/asset/decorators.py\n@@ -69,18 +69,16 @@ def from_definition(cls, definition: AssetDefinition | MultiAssetDefinition) ->\n         )\n \n     def _iter_kwargs(self, context: Mapping[str, Any]) -> Iterator[tuple[str, Any]]:\n-        import structlog\n-\n         from airflow.sdk.execution_time.comms import ErrorResponse, GetAssetByName\n         from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n \n-        log = structlog.get_logger(logger_name=self.__class__.__qualname__)\n-\n         def _fetch_asset(name: str) -> Asset:\n-            SUPERVISOR_COMMS.send_request(log, GetAssetByName(name=name))\n-            if isinstance(msg := SUPERVISOR_COMMS.get_message(), ErrorResponse):\n-                raise AirflowRuntimeError(msg)\n-            return Asset(**msg.model_dump(exclude={\"type\"}))\n+            resp = SUPERVISOR_COMMS.send(GetAssetByName(name=name))\n+            if resp is None:\n+                raise RuntimeError(\"Empty non-error response received\")\n+            if isinstance(resp, ErrorResponse):\n+                raise AirflowRuntimeError(resp)\n+            return Asset(**resp.model_dump(exclude={\"type\"}))\n \n         value: Any\n         for key, param in inspect.signature(self.python_callable).parameters.items():\ndiff --git a/task-sdk/src/airflow/sdk/execution_time/comms.py b/task-sdk/src/airflow/sdk/execution_time/comms.py\nindex d0622cf6ffc86..be2f4f8eacd46 100644\n--- a/task-sdk/src/airflow/sdk/execution_time/comms.py\n+++ b/task-sdk/src/airflow/sdk/execution_time/comms.py\n@@ -19,12 +19,17 @@\n Communication protocol between the Supervisor and the task process\n ==================================================================\n \n-* All communication is done over stdout/stdin in the form of \"JSON lines\" (each\n-  message is a single JSON document terminated by `\\n` character)\n-* Messages from the subprocess are all log messages and are sent directly to the log\n+* All communication is done over the subprocesses stdin in the form of a binary length-prefixed msgpack frame\n+  (4 byte, big-endian length, followed by the msgpack-encoded _RequestFrame.) Each side uses this same\n+  encoding\n+* Log Messages from the subprocess are sent over the dedicated logs socket (which is line-based JSON)\n * No messages are sent to task process except in response to a request. (This is because the task process will\n   be running user's code, so we can't read from stdin until we enter our code, such as when requesting an XCom\n   value etc.)\n+* Every request returns a response, even if the frame is otherwise empty.\n+* Requests are written by the subprocess to fd0/stdin. This is making use of the fact that stdin is a\n+  bi-directional socket, and thus we can write to it and don't need a dedicated extra socket for sending\n+  requests.\n \n The reason this communication protocol exists, rather than the task process speaking directly to the Task\n Execution API server is because:\n@@ -43,15 +48,20 @@\n \n from __future__ import annotations\n \n+import itertools\n from collections.abc import Iterator\n from datetime import datetime\n from functools import cached_property\n-from typing import Annotated, Any, Literal, Union\n+from pathlib import Path\n+from socket import socket\n+from typing import TYPE_CHECKING, Annotated, Any, ClassVar, Generic, Literal, TypeVar, Union\n from uuid import UUID\n \n import attrs\n+import msgspec\n+import structlog\n from fastapi import Body\n-from pydantic import AwareDatetime, BaseModel, ConfigDict, Field, JsonValue, field_serializer\n+from pydantic import AwareDatetime, BaseModel, ConfigDict, Field, JsonValue, TypeAdapter, field_serializer\n \n from airflow.sdk.api.datamodels._generated import (\n     AssetEventDagRunReference,\n@@ -80,6 +90,145 @@\n )\n from airflow.sdk.exceptions import ErrorType\n \n+if TYPE_CHECKING:\n+    from structlog.typing import FilteringBoundLogger as Logger\n+\n+SendMsgType = TypeVar(\"SendMsgType\", bound=BaseModel)\n+ReceiveMsgType = TypeVar(\"ReceiveMsgType\", bound=BaseModel)\n+\n+\n+def _msgpack_enc_hook(obj: Any) -> Any:\n+    import pendulum\n+\n+    if isinstance(obj, pendulum.DateTime):\n+        # convert the pendulm Datetime subclass into a raw datetime so that msgspec can use it's native\n+        # encoding\n+        return datetime(\n+            obj.year, obj.month, obj.day, obj.hour, obj.minute, obj.second, obj.microsecond, tzinfo=obj.tzinfo\n+        )\n+    if isinstance(obj, Path):\n+        return str(obj)\n+    if isinstance(obj, BaseModel):\n+        return obj.model_dump(exclude_unset=True)\n+\n+    # Raise a NotImplementedError for other types\n+    raise NotImplementedError(f\"Objects of type {type(obj)} are not supported\")\n+\n+\n+def _new_encoder() -> msgspec.msgpack.Encoder:\n+    return msgspec.msgpack.Encoder(enc_hook=_msgpack_enc_hook)\n+\n+\n+class _RequestFrame(msgspec.Struct, array_like=True, frozen=True, omit_defaults=True):\n+    id: int\n+    \"\"\"\n+    The request id, set by the sender.\n+\n+    This is used to allow \"pipeling\" of requests and to be able to tie response to requests, which is\n+    particularly useful in the Triggerer where multiple async tasks can send a requests concurrently.\n+    \"\"\"\n+    body: dict[str, Any] | None\n+\n+    req_encoder: ClassVar[msgspec.msgpack.Encoder] = _new_encoder()\n+\n+    def as_bytes(self) -> bytearray:\n+        # https://jcristharif.com/msgspec/perf-tips.html#length-prefix-framing for inspiration\n+        buffer = bytearray(256)\n+\n+        self.req_encoder.encode_into(self, buffer, 4)\n+\n+        n = len(buffer) - 4\n+        if n >= 2**32:\n+            raise OverflowError(f\"Cannot send messages larger than 4GiB {n=}\")\n+        buffer[:4] = n.to_bytes(4, byteorder=\"big\")\n+\n+        return buffer\n+\n+\n+class _ResponseFrame(_RequestFrame, frozen=True):\n+    id: int\n+    \"\"\"\n+    The id of the request this is a response to\n+    \"\"\"\n+    body: dict[str, Any] | None = None\n+    error: dict[str, Any] | None = None\n+\n+\n+@attrs.define()\n+class CommsDecoder(Generic[ReceiveMsgType, SendMsgType]):\n+    \"\"\"Handle communication between the task in this process and the supervisor parent process.\"\"\"\n+\n+    log: Logger = attrs.field(repr=False, factory=structlog.get_logger)\n+    socket: socket = attrs.field(factory=lambda: socket(fileno=0))\n+\n+    resp_decoder: msgspec.msgpack.Decoder[_ResponseFrame] = attrs.field(\n+        factory=lambda: msgspec.msgpack.Decoder(_ResponseFrame), repr=False\n+    )\n+\n+    id_counter: Iterator[int] = attrs.field(factory=itertools.count)\n+\n+    # We could be \"clever\" here and set the default to this based type parameters and a custom\n+    # `__class_getitem__`, but that's a lot of code the one subclass we've got currently. So we'll just use a\n+    # \"sort of wrong default\"\n+    body_decoder: TypeAdapter[ReceiveMsgType] = attrs.field(factory=lambda: TypeAdapter(ToTask), repr=False)\n+\n+    err_decoder: TypeAdapter[ErrorResponse] = attrs.field(factory=lambda: TypeAdapter(ToTask), repr=False)\n+\n+    def send(self, msg: SendMsgType) -> ReceiveMsgType | None:\n+        \"\"\"Send a request to the parent and block until the response is received.\"\"\"\n+        frame = _RequestFrame(id=next(self.id_counter), body=msg.model_dump())\n+        bytes = frame.as_bytes()\n+\n+        self.socket.sendall(bytes)\n+\n+        return self._get_response()\n+\n+    def _read_frame(self):\n+        \"\"\"\n+        Get a message from the parent.\n+\n+        This will block until the message has been received.\n+        \"\"\"\n+        if self.socket:\n+            self.socket.setblocking(True)\n+        len_bytes = self.socket.recv(4)\n+\n+        if len_bytes == b\"\":\n+            raise EOFError(\"Request socket closed before length\")\n+\n+        len = int.from_bytes(len_bytes, byteorder=\"big\")\n+\n+        buffer = bytearray(len)\n+        nread = self.socket.recv_into(buffer)\n+        if nread != len:\n+            raise RuntimeError(\n+                f\"unable to read full response in child. (We read {nread}, but expected {len})\"\n+            )\n+        if nread == 0:\n+            raise EOFError(f\"Request socket closed before response was complete ({self.id_counter=})\")\n+\n+        return self.resp_decoder.decode(buffer)\n+\n+    def _from_frame(self, frame) -> ReceiveMsgType | None:\n+        from airflow.sdk.exceptions import AirflowRuntimeError\n+\n+        if frame.error is not None:\n+            err = self.err_decoder.validate_python(frame.error)\n+            raise AirflowRuntimeError(error=err)\n+\n+        if frame.body is None:\n+            return None\n+\n+        try:\n+            return self.body_decoder.validate_python(frame.body)\n+        except Exception:\n+            self.log.exception(\"Unable to decode message\")\n+            raise\n+\n+    def _get_response(self) -> ReceiveMsgType | None:\n+        frame = self._read_frame()\n+        return self._from_frame(frame)\n+\n \n class StartupDetails(BaseModel):\n     model_config = ConfigDict(arbitrary_types_allowed=True)\n@@ -87,13 +236,7 @@ class StartupDetails(BaseModel):\n     ti: TaskInstance\n     dag_rel_path: str\n     bundle_info: BundleInfo\n-    requests_fd: int\n     start_date: datetime\n-    \"\"\"\n-    The channel for the task to send requests over.\n-\n-    Responses will come back on stdin\n-    \"\"\"\n     ti_context: TIRunContext\n     type: Literal[\"StartupDetails\"] = \"StartupDetails\"\n \ndiff --git a/task-sdk/src/airflow/sdk/execution_time/context.py b/task-sdk/src/airflow/sdk/execution_time/context.py\nindex 6621b9c61edcc..c76994995ebab 100644\n--- a/task-sdk/src/airflow/sdk/execution_time/context.py\n+++ b/task-sdk/src/airflow/sdk/execution_time/context.py\n@@ -152,13 +152,7 @@ def _get_connection(conn_id: str) -> Connection:\n     from airflow.sdk.execution_time.comms import ErrorResponse, GetConnection\n     from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n \n-    # Since Triggers can hit this code path via `sync_to_async` (which uses threads internally)\n-    # we need to make sure that we \"atomically\" send a request and get the response to that\n-    # back so that two triggers don't end up interleaving requests and create a possible\n-    # race condition where the wrong trigger reads the response.\n-    with SUPERVISOR_COMMS.lock:\n-        SUPERVISOR_COMMS.send_request(log=log, msg=GetConnection(conn_id=conn_id))\n-        msg = SUPERVISOR_COMMS.get_message()\n+    msg = SUPERVISOR_COMMS.send(GetConnection(conn_id=conn_id))\n \n     if isinstance(msg, ErrorResponse):\n         raise AirflowRuntimeError(msg)\n@@ -207,13 +201,7 @@ def _get_variable(key: str, deserialize_json: bool) -> Any:\n     from airflow.sdk.execution_time.comms import ErrorResponse, GetVariable\n     from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n \n-    # Since Triggers can hit this code path via `sync_to_async` (which uses threads internally)\n-    # we need to make sure that we \"atomically\" send a request and get the response to that\n-    # back so that two triggers don't end up interleaving requests and create a possible\n-    # race condition where the wrong trigger reads the response.\n-    with SUPERVISOR_COMMS.lock:\n-        SUPERVISOR_COMMS.send_request(log=log, msg=GetVariable(key=key))\n-        msg = SUPERVISOR_COMMS.get_message()\n+    msg = SUPERVISOR_COMMS.send(GetVariable(key=key))\n \n     if isinstance(msg, ErrorResponse):\n         raise AirflowRuntimeError(msg)\n@@ -263,11 +251,7 @@ def _set_variable(key: str, value: Any, description: str | None = None, serializ\n     except Exception as e:\n         log.exception(e)\n \n-    # It is best to have lock everywhere or nowhere on the SUPERVISOR_COMMS, lock was\n-    # primarily added for triggers but it doesn't make sense to have it in some places\n-    # and not in the rest. A lot of this will be simplified by https://github.com/apache/airflow/issues/46426\n-    with SUPERVISOR_COMMS.lock:\n-        SUPERVISOR_COMMS.send_request(log=log, msg=PutVariable(key=key, value=value, description=description))\n+    SUPERVISOR_COMMS.send(PutVariable(key=key, value=value, description=description))\n \n \n def _delete_variable(key: str) -> None:\n@@ -279,12 +263,7 @@ def _delete_variable(key: str) -> None:\n     from airflow.sdk.execution_time.comms import DeleteVariable\n     from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n \n-    # It is best to have lock everywhere or nowhere on the SUPERVISOR_COMMS, lock was\n-    # primarily added for triggers but it doesn't make sense to have it in some places\n-    # and not in the rest. A lot of this will be simplified by https://github.com/apache/airflow/issues/46426\n-    with SUPERVISOR_COMMS.lock:\n-        SUPERVISOR_COMMS.send_request(log=log, msg=DeleteVariable(key=key))\n-        msg = SUPERVISOR_COMMS.get_message()\n+    msg = SUPERVISOR_COMMS.send(DeleteVariable(key=key))\n     if TYPE_CHECKING:\n         assert isinstance(msg, OKResponse)\n \n@@ -387,23 +366,29 @@ def _resolve_asset_ref(self, ref: AssetRef) -> AssetUniqueKey:\n     @staticmethod\n     def _get_asset_from_db(name: str | None = None, uri: str | None = None) -> Asset:\n         from airflow.sdk.definitions.asset import Asset\n-        from airflow.sdk.execution_time.comms import ErrorResponse, GetAssetByName, GetAssetByUri\n+        from airflow.sdk.execution_time.comms import (\n+            ErrorResponse,\n+            GetAssetByName,\n+            GetAssetByUri,\n+            ToSupervisor,\n+        )\n         from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n \n+        msg: ToSupervisor\n         if name:\n-            SUPERVISOR_COMMS.send_request(log=log, msg=GetAssetByName(name=name))\n+            msg = GetAssetByName(name=name)\n         elif uri:\n-            SUPERVISOR_COMMS.send_request(log=log, msg=GetAssetByUri(uri=uri))\n+            msg = GetAssetByUri(uri=uri)\n         else:\n             raise ValueError(\"Either name or uri must be provided\")\n \n-        msg = SUPERVISOR_COMMS.get_message()\n-        if isinstance(msg, ErrorResponse):\n-            raise AirflowRuntimeError(msg)\n+        resp = SUPERVISOR_COMMS.send(msg)\n+        if isinstance(resp, ErrorResponse):\n+            raise AirflowRuntimeError(resp)\n \n         if TYPE_CHECKING:\n-            assert isinstance(msg, AssetResult)\n-        return Asset(**msg.model_dump(exclude={\"type\"}))\n+            assert isinstance(resp, AssetResult)\n+        return Asset(**resp.model_dump(exclude={\"type\"}))\n \n \n @attrs.define\n@@ -537,9 +522,11 @@ def __getitem__(self, key: int | Asset | AssetAlias | AssetRef) -> list[AssetEve\n             ErrorResponse,\n             GetAssetEventByAsset,\n             GetAssetEventByAssetAlias,\n+            ToSupervisor,\n         )\n         from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n \n+        msg: ToSupervisor\n         if isinstance(key, int):  # Support index access; it's easier for trivial cases.\n             obj = self._inlets[key]\n             if not isinstance(obj, (Asset, AssetAlias, AssetRef)):\n@@ -549,31 +536,33 @@ def __getitem__(self, key: int | Asset | AssetAlias | AssetRef) -> list[AssetEve\n \n         if isinstance(obj, Asset):\n             asset = self._assets[AssetUniqueKey.from_asset(obj)]\n-            SUPERVISOR_COMMS.send_request(log=log, msg=GetAssetEventByAsset(name=asset.name, uri=asset.uri))\n+            msg = GetAssetEventByAsset(name=asset.name, uri=asset.uri)\n         elif isinstance(obj, AssetNameRef):\n             try:\n                 asset = next(a for k, a in self._assets.items() if k.name == obj.name)\n             except StopIteration:\n                 raise KeyError(obj) from None\n-            SUPERVISOR_COMMS.send_request(log=log, msg=GetAssetEventByAsset(name=asset.name, uri=None))\n+            msg = GetAssetEventByAsset(name=asset.name, uri=None)\n         elif isinstance(obj, AssetUriRef):\n             try:\n                 asset = next(a for k, a in self._assets.items() if k.uri == obj.uri)\n             except StopIteration:\n                 raise KeyError(obj) from None\n-            SUPERVISOR_COMMS.send_request(log=log, msg=GetAssetEventByAsset(name=None, uri=asset.uri))\n+            msg = GetAssetEventByAsset(name=None, uri=asset.uri)\n         elif isinstance(obj, AssetAlias):\n             asset_alias = self._asset_aliases[AssetAliasUniqueKey.from_asset_alias(obj)]\n-            SUPERVISOR_COMMS.send_request(log=log, msg=GetAssetEventByAssetAlias(alias_name=asset_alias.name))\n+            msg = GetAssetEventByAssetAlias(alias_name=asset_alias.name)\n+        else:\n+            raise TypeError(f\"`key` is of unknown type ({type(key).__name__})\")\n \n-        msg = SUPERVISOR_COMMS.get_message()\n-        if isinstance(msg, ErrorResponse):\n-            raise AirflowRuntimeError(msg)\n+        resp = SUPERVISOR_COMMS.send(msg)\n+        if isinstance(resp, ErrorResponse):\n+            raise AirflowRuntimeError(resp)\n \n         if TYPE_CHECKING:\n-            assert isinstance(msg, AssetEventsResult)\n+            assert isinstance(resp, AssetEventsResult)\n \n-        return list(msg.iter_asset_event_results())\n+        return list(resp.iter_asset_event_results())\n \n \n @attrs.define\n@@ -630,8 +619,7 @@ def get_previous_dagrun_success(ti_id: UUID) -> PrevSuccessfulDagRunResponse:\n     )\n     from airflow.sdk.execution_time.task_runner import SUPERVISOR_COMMS\n \n-    SUPERVISOR_COMMS.send_request(log=log, msg=GetPrevSuccessfulDagRun(ti_id=ti_id))\n-    msg = SUPERVISOR_COMMS.get_message()\n+    msg = SUPERVISOR_COMMS.send(GetPrevSuccessfulDagRun(ti_id=ti_id))\n \n     if TYPE_CHECKING:\n         assert isinstance(msg, PrevSuccessfulDagRunResult)\ndiff --git a/task-sdk/src/airflow/sdk/execution_time/lazy_sequence.py b/task-sdk/src/airflow/sdk/execution_time/lazy_sequence.py\nindex 9cf9acfac81bb..d8b356870c7aa 100644\n--- a/task-sdk/src/airflow/sdk/execution_time/lazy_sequence.py\n+++ b/task-sdk/src/airflow/sdk/execution_time/lazy_sequence.py\n@@ -91,16 +91,14 @@ def __len__(self) -> int:\n \n             task = self._xcom_arg.operator\n \n-            SUPERVISOR_COMMS.send_request(\n-                log=log,\n-                msg=GetXComCount(\n+            msg = SUPERVISOR_COMMS.send(\n+                GetXComCount(\n                     key=self._xcom_arg.key,\n                     dag_id=task.dag_id,\n                     run_id=self._ti.run_id,\n                     task_id=task.task_id,\n                 ),\n             )\n-            msg = SUPERVISOR_COMMS.get_message()\n             if isinstance(msg, ErrorResponse):\n                 raise RuntimeError(msg)\n             if not isinstance(msg, XComCountResponse):\n@@ -127,43 +125,37 @@ def __getitem__(self, key: int | slice) -> T | Sequence[T]:\n \n         if isinstance(key, slice):\n             start, stop, step = _coerce_slice(key)\n-            with SUPERVISOR_COMMS.lock:\n-                source = (xcom_arg := self._xcom_arg).operator\n-                SUPERVISOR_COMMS.send_request(\n-                    log=log,\n-                    msg=GetXComSequenceSlice(\n-                        key=xcom_arg.key,\n-                        dag_id=source.dag_id,\n-                        task_id=source.task_id,\n-                        run_id=self._ti.run_id,\n-                        start=start,\n-                        stop=stop,\n-                        step=step,\n-                    ),\n-                )\n-                msg = SUPERVISOR_COMMS.get_message()\n-                if not isinstance(msg, XComSequenceSliceResult):\n-                    raise TypeError(f\"Got unexpected response to GetXComSequenceSlice: {msg!r}\")\n-            return [XCom.deserialize_value(_XComWrapper(value)) for value in msg.root]\n-\n-        if not isinstance(key, int):\n-            if (index := getattr(key, \"__index__\", None)) is not None:\n-                key = index()\n-            raise TypeError(f\"Sequence indices must be integers or slices not {type(key).__name__}\")\n-\n-        with SUPERVISOR_COMMS.lock:\n             source = (xcom_arg := self._xcom_arg).operator\n-            SUPERVISOR_COMMS.send_request(\n-                log=log,\n-                msg=GetXComSequenceItem(\n+            msg = SUPERVISOR_COMMS.send(\n+                GetXComSequenceSlice(\n                     key=xcom_arg.key,\n                     dag_id=source.dag_id,\n                     task_id=source.task_id,\n                     run_id=self._ti.run_id,\n-                    offset=key,\n+                    start=start,\n+                    stop=stop,\n+                    step=step,\n                 ),\n             )\n-            msg = SUPERVISOR_COMMS.get_message()\n+            if not isinstance(msg, XComSequenceSliceResult):\n+                raise TypeError(f\"Got unexpected response to GetXComSequenceSlice: {msg!r}\")\n+            return [XCom.deserialize_value(_XComWrapper(value)) for value in msg.root]\n+\n+        if not isinstance(key, int):\n+            if (index := getattr(key, \"__index__\", None)) is not None:\n+                key = index()\n+            raise TypeError(f\"Sequence indices must be integers or slices not {type(key).__name__}\")\n+\n+        source = (xcom_arg := self._xcom_arg).operator\n+        msg = SUPERVISOR_COMMS.send(\n+            GetXComSequenceItem(\n+                key=xcom_arg.key,\n+                dag_id=source.dag_id,\n+                task_id=source.task_id,\n+                run_id=self._ti.run_id,\n+                offset=key,\n+            ),\n+        )\n         if isinstance(msg, ErrorResponse):\n             raise IndexError(key)\n         if not isinstance(msg, XComSequenceIndexResult):\ndiff --git a/task-sdk/src/airflow/sdk/execution_time/supervisor.py b/task-sdk/src/airflow/sdk/execution_time/supervisor.py\nindex dc63daeef7bb7..2ae554cace1a5 100644\n--- a/task-sdk/src/airflow/sdk/execution_time/supervisor.py\n+++ b/task-sdk/src/airflow/sdk/execution_time/supervisor.py\n@@ -27,12 +27,13 @@\n import signal\n import sys\n import time\n+import weakref\n from collections import deque\n from collections.abc import Generator\n from contextlib import contextmanager, suppress\n from datetime import datetime, timezone\n from http import HTTPStatus\n-from socket import SO_SNDBUF, SOL_SOCKET, SocketIO, socket, socketpair\n+from socket import socket, socketpair\n from typing import (\n     TYPE_CHECKING,\n     BinaryIO,\n@@ -44,7 +45,6 @@\n )\n from uuid import UUID\n \n-import aiologic\n import attrs\n import httpx\n import msgspec\n@@ -64,6 +64,7 @@\n     XComSequenceIndexResponse,\n )\n from airflow.sdk.exceptions import ErrorType\n+from airflow.sdk.execution_time import comms\n from airflow.sdk.execution_time.comms import (\n     AssetEventsResult,\n     AssetResult,\n@@ -109,6 +110,8 @@\n     XComResult,\n     XComSequenceIndexResult,\n     XComSequenceSliceResult,\n+    _RequestFrame,\n+    _ResponseFrame,\n )\n from airflow.sdk.execution_time.secrets_masker import mask_secret\n \n@@ -180,23 +183,6 @@\n ********************************************************************************************************\"\"\"\n \n \n-def mkpipe(\n-    remote_read: bool = False,\n-) -> tuple[socket, socket]:\n-    \"\"\"Create a pair of connected sockets.\"\"\"\n-    rsock, wsock = socketpair()\n-    local, remote = (wsock, rsock) if remote_read else (rsock, wsock)\n-\n-    if remote_read:\n-        # Setting a 4KB buffer here if possible, if not, it still works, so we will suppress all exceptions\n-        with suppress(Exception):\n-            local.setsockopt(SO_SNDBUF, SOL_SOCKET, BUFFER_SIZE)\n-        # set nonblocking to True so that send or sendall waits till all data is sent\n-        local.setblocking(True)\n-\n-    return remote, local\n-\n-\n def _subprocess_main():\n     from airflow.sdk.execution_time.task_runner import main\n \n@@ -224,14 +210,13 @@ def _configure_logs_over_json_channel(log_fd: int):\n def _reopen_std_io_handles(child_stdin, child_stdout, child_stderr):\n     # Ensure that sys.stdout et al (and the underlying filehandles for C libraries etc) are connected to the\n     # pipes from the supervisor\n-\n     for handle_name, fd, sock, mode in (\n-        (\"stdin\", 0, child_stdin, \"r\"),\n+        # Yes, we want to re-open stdin in write mode! This is cause it is a bi-directional socket, so we can\n+        # read and write to it.\n+        (\"stdin\", 0, child_stdin, \"w\"),\n         (\"stdout\", 1, child_stdout, \"w\"),\n         (\"stderr\", 2, child_stderr, \"w\"),\n     ):\n-        handle = getattr(sys, handle_name)\n-        handle.close()\n         os.dup2(sock.fileno(), fd)\n         del sock\n \n@@ -319,7 +304,7 @@ def __getattr__(name: str):\n \n \n def _fork_main(\n-    child_stdin: socket,\n+    requests: socket,\n     child_stdout: socket,\n     child_stderr: socket,\n     log_fd: int,\n@@ -349,7 +334,7 @@ def _fork_main(\n     _reset_signals()\n     if log_fd:\n         _configure_logs_over_json_channel(log_fd)\n-    _reopen_std_io_handles(child_stdin, child_stdout, child_stderr)\n+    _reopen_std_io_handles(requests, child_stdout, child_stderr)\n \n     def exit(n: int) -> NoReturn:\n         with suppress(ValueError, OSError):\n@@ -360,11 +345,11 @@ def exit(n: int) -> NoReturn:\n             last_chance_stderr.flush()\n \n         # Explicitly close the child-end of our supervisor sockets so\n-        # the parent sees EOF on both \"requests\" and \"logs\" channels.\n+        # the parent sees EOF on \"logs\" channel.\n         with suppress(OSError):\n             os.close(log_fd)\n         with suppress(OSError):\n-            os.close(child_stdin.fileno())\n+            os.close(requests.fileno())\n         os._exit(n)\n \n     if hasattr(atexit, \"_clear\"):\n@@ -432,16 +417,18 @@ class WatchedSubprocess:\n     \"\"\"The decoder to use for incoming messages from the child process.\"\"\"\n \n     _process: psutil.Process = attrs.field(repr=False)\n-    _requests_fd: int\n     \"\"\"File descriptor for request handling.\"\"\"\n \n-    _num_open_sockets: int = 4\n     _exit_code: int | None = attrs.field(default=None, init=False)\n     _process_exit_monotonic: float | None = attrs.field(default=None, init=False)\n-    _fd_to_socket_type: dict[int, str] = attrs.field(factory=dict, init=False)\n+    _open_sockets: weakref.WeakKeyDictionary[socket, str] = attrs.field(\n+        factory=weakref.WeakKeyDictionary, init=False\n+    )\n \n     selector: selectors.BaseSelector = attrs.field(factory=selectors.DefaultSelector, repr=False)\n \n+    _frame_encoder: msgspec.msgpack.Encoder = attrs.field(factory=comms._new_encoder, repr=False)\n+\n     process_log: FilteringBoundLogger = attrs.field(repr=False)\n \n     subprocess_logs_to_stdout: bool = False\n@@ -460,18 +447,19 @@ def start(\n     ) -> Self:\n         \"\"\"Fork and start a new subprocess with the specified target function.\"\"\"\n         # Create socketpairs/\"pipes\" to connect to the stdin and out from the subprocess\n-        child_stdin, feed_stdin = mkpipe(remote_read=True)\n-        child_stdout, read_stdout = mkpipe()\n-        child_stderr, read_stderr = mkpipe()\n+        child_stdout, read_stdout = socketpair()\n+        child_stderr, read_stderr = socketpair()\n+\n+        # Place for child to send requests/read responses, and the server side to read/respond\n+        child_requests, read_requests = socketpair()\n \n-        # Open these socketpair before forking off the child, so that it is open when we fork.\n-        child_comms, read_msgs = mkpipe()\n-        child_logs, read_logs = mkpipe()\n+        # Open the socketpair before forking off the child, so that it is open when we fork.\n+        child_logs, read_logs = socketpair()\n \n         pid = os.fork()\n         if pid == 0:\n             # Close and delete of the parent end of the sockets.\n-            cls._close_unused_sockets(feed_stdin, read_stdout, read_stderr, read_msgs, read_logs)\n+            cls._close_unused_sockets(read_requests, read_stdout, read_stderr, read_logs)\n \n             # Python GC should delete these for us, but lets make double sure that we don't keep anything\n             # around in the forked processes, especially things that might involve open files or sockets!\n@@ -480,28 +468,28 @@ def start(\n \n             try:\n                 # Run the child entrypoint\n-                _fork_main(child_stdin, child_stdout, child_stderr, child_logs.fileno(), target)\n+                _fork_main(child_requests, child_stdout, child_stderr, child_logs.fileno(), target)\n             except BaseException as e:\n+                import traceback\n+\n                 with suppress(BaseException):\n                     # We can't use log here, as if we except out of _fork_main something _weird_ went on.\n-                    print(\"Exception in _fork_main, exiting with code 124\", e, file=sys.stderr)\n+                    print(\"Exception in _fork_main, exiting with code 124\", file=sys.stderr)\n+                    traceback.print_exception(type(e), e, e.__traceback__, file=sys.stderr)\n \n             # It's really super super important we never exit this block. We are in the forked child, and if we\n             # do then _THINGS GET WEIRD_.. (Normally `_fork_main` itself will `_exit()` so we never get here)\n             os._exit(124)\n \n-        requests_fd = child_comms.fileno()\n-\n         # Close the remaining parent-end of the sockets we've passed to the child via fork. We still have the\n         # other end of the pair open\n-        cls._close_unused_sockets(child_stdin, child_stdout, child_stderr, child_comms, child_logs)\n+        cls._close_unused_sockets(child_stdout, child_stderr, child_logs)\n \n         logger = logger or cast(\"FilteringBoundLogger\", structlog.get_logger(logger_name=\"task\").bind())\n         proc = cls(\n             pid=pid,\n-            stdin=feed_stdin,\n+            stdin=read_requests,\n             process=psutil.Process(pid),\n-            requests_fd=requests_fd,\n             process_log=logger,\n             start_time=time.monotonic(),\n             **constructor_kwargs,\n@@ -510,7 +498,7 @@ def start(\n         proc._register_pipe_readers(\n             stdout=read_stdout,\n             stderr=read_stderr,\n-            requests=read_msgs,\n+            requests=read_requests,\n             logs=read_logs,\n         )\n \n@@ -523,24 +511,26 @@ def _register_pipe_readers(self, stdout: socket, stderr: socket, requests: socke\n         # alternatives are used automatically) -- this is a way of having \"event-based\" code, but without\n         # needing full async, to read and process output from each socket as it is received.\n \n-        # Track socket types for debugging\n-        self._fd_to_socket_type = {\n-            stdout.fileno(): \"stdout\",\n-            stderr.fileno(): \"stderr\",\n-            requests.fileno(): \"requests\",\n-            logs.fileno(): \"logs\",\n-        }\n+        # Track the open sockets, and for debugging what type each one is\n+        self._open_sockets.update(\n+            (\n+                (stdout, \"stdout\"),\n+                (stderr, \"stderr\"),\n+                (logs, \"logs\"),\n+                (requests, \"requests\"),\n+            )\n+        )\n \n         target_loggers: tuple[FilteringBoundLogger, ...] = (self.process_log,)\n         if self.subprocess_logs_to_stdout:\n             target_loggers += (log,)\n         self.selector.register(\n-            stdout, selectors.EVENT_READ, self._create_socket_handler(target_loggers, channel=\"stdout\")\n+            stdout, selectors.EVENT_READ, self._create_log_forwarder(target_loggers, channel=\"stdout\")\n         )\n         self.selector.register(\n             stderr,\n             selectors.EVENT_READ,\n-            self._create_socket_handler(target_loggers, channel=\"stderr\", log_level=logging.ERROR),\n+            self._create_log_forwarder(target_loggers, channel=\"stderr\", log_level=logging.ERROR),\n         )\n         self.selector.register(\n             logs,\n@@ -552,37 +542,52 @@ def _register_pipe_readers(self, stdout: socket, stderr: socket, requests: socke\n         self.selector.register(\n             requests,\n             selectors.EVENT_READ,\n-            make_buffered_socket_reader(self.handle_requests(log), on_close=self._on_socket_closed),\n+            length_prefixed_frame_reader(self.handle_requests(log), on_close=self._on_socket_closed),\n         )\n \n-    def _create_socket_handler(self, loggers, channel, log_level=logging.INFO) -> Callable[[socket], bool]:\n+    def _create_log_forwarder(self, loggers, channel, log_level=logging.INFO) -> Callable[[socket], bool]:\n         \"\"\"Create a socket handler that forwards logs to a logger.\"\"\"\n         return make_buffered_socket_reader(\n             forward_to_log(loggers, chan=channel, level=log_level), on_close=self._on_socket_closed\n         )\n \n-    def _on_socket_closed(self):\n+    def _on_socket_closed(self, sock: socket):\n         # We want to keep servicing this process until we've read up to EOF from all the sockets.\n-        self._num_open_sockets -= 1\n \n-    def send_msg(self, msg: BaseModel, **dump_opts):\n-        \"\"\"Send the given pydantic message to the subprocess at once by encoding it and adding a line break.\"\"\"\n-        b = msg.model_dump_json(**dump_opts).encode() + b\"\\n\"\n-        self.stdin.sendall(b)\n+        with suppress(KeyError):\n+            self.selector.unregister(sock)\n+            del self._open_sockets[sock]\n+\n+    def send_msg(\n+        self, msg: BaseModel | None, request_id: int, error: ErrorResponse | None = None, **dump_opts\n+    ):\n+        \"\"\"\n+        Send the msg as a length-prefixed response frame.\n+\n+        ``request_id`` is the ID that the client sent in it's request, and has no meaning to the server\n+\n+        \"\"\"\n+        if msg:\n+            frame = _ResponseFrame(id=request_id, body=msg.model_dump(**dump_opts))\n+        else:\n+            err_resp = error.model_dump() if error else None\n+            frame = _ResponseFrame(id=request_id, error=err_resp)\n+\n+        self.stdin.sendall(frame.as_bytes())\n \n-    def handle_requests(self, log: FilteringBoundLogger) -> Generator[None, bytes, None]:\n+    def handle_requests(self, log: FilteringBoundLogger) -> Generator[None, _RequestFrame, None]:\n         \"\"\"Handle incoming requests from the task process, respond with the appropriate data.\"\"\"\n         while True:\n-            line = yield\n+            request = yield\n \n             try:\n-                msg = self.decoder.validate_json(line)\n+                msg = self.decoder.validate_python(request.body)\n             except Exception:\n-                log.exception(\"Unable to decode message\", line=line)\n+                log.exception(\"Unable to decode message\", body=request.body)\n                 continue\n \n             try:\n-                self._handle_request(msg, log)\n+                self._handle_request(msg, log, request.id)\n             except ServerResponseError as e:\n                 error_details = e.response.json() if e.response else None\n                 log.error(\n@@ -594,27 +599,25 @@ def handle_requests(self, log: FilteringBoundLogger) -> Generator[None, bytes, N\n \n                 # Send error response back to task so that the error appears in the task logs\n                 self.send_msg(\n-                    ErrorResponse(\n+                    msg=None,\n+                    error=ErrorResponse(\n                         error=ErrorType.API_SERVER_ERROR,\n                         detail={\n                             \"status_code\": e.response.status_code,\n                             \"message\": str(e),\n                             \"detail\": error_details,\n                         },\n-                    )\n+                    ),\n+                    request_id=request.id,\n                 )\n \n-    def _handle_request(self, msg, log: FilteringBoundLogger) -> None:\n+    def _handle_request(self, msg, log: FilteringBoundLogger, req_id: int) -> None:\n         raise NotImplementedError()\n \n     @staticmethod\n     def _close_unused_sockets(*sockets):\n         \"\"\"Close unused ends of sockets after fork.\"\"\"\n         for sock in sockets:\n-            if isinstance(sock, SocketIO):\n-                # If we have the socket IO object, we need to close the underlying socket foricebly here too,\n-                # else we get unclosed socket warnings, and likely leaking FDs too\n-                sock._sock.close()\n             sock.close()\n \n     def _cleanup_open_sockets(self):\n@@ -624,20 +627,18 @@ def _cleanup_open_sockets(self):\n         # sockets the supervisor would wait forever thinking they are still\n         # active. This cleanup ensures we always release resources and exit.\n         stuck_sockets = []\n-        for key in list(self.selector.get_map().values()):\n-            socket_type = self._fd_to_socket_type.get(key.fd, f\"unknown-{key.fd}\")\n-            stuck_sockets.append(f\"{socket_type}({key.fd})\")\n+        for sock, socket_type in self._open_sockets.items():\n+            fileno = \"unknown\"\n             with suppress(Exception):\n-                self.selector.unregister(key.fileobj)\n-            with suppress(Exception):\n-                key.fileobj.close()  # type: ignore[union-attr]\n+                fileno = sock.fileno()\n+                sock.close()\n+            stuck_sockets.append(f\"{socket_type}(fd={fileno})\")\n \n         if stuck_sockets:\n             log.warning(\"Force-closed stuck sockets\", pid=self.pid, sockets=stuck_sockets)\n \n         self.selector.close()\n-        self._close_unused_sockets(self.stdin)\n-        self._num_open_sockets = 0\n+        self.stdin.close()\n \n     def kill(\n         self,\n@@ -736,7 +737,7 @@ def _service_subprocess(\n         events = self.selector.select(timeout=timeout)\n         for key, _ in events:\n             # Retrieve the handler responsible for processing this file object (e.g., stdout, stderr)\n-            socket_handler = key.data\n+            socket_handler, on_close = key.data\n \n             # Example of handler behavior:\n             # If the subprocess writes \"Hello, World!\" to stdout:\n@@ -746,15 +747,16 @@ def _service_subprocess(\n             # to EOF case\n             try:\n                 need_more = socket_handler(key.fileobj)\n-            except BrokenPipeError:\n+            except (BrokenPipeError, ConnectionResetError):\n                 need_more = False\n \n             # If the handler signals that the file object is no longer needed (EOF, closed, etc.)\n             # unregister it from the selector to stop monitoring; `wait()` blocks until all selectors\n             # are removed.\n             if not need_more:\n-                self.selector.unregister(key.fileobj)\n-                key.fileobj.close()  # type: ignore[union-attr]\n+                sock: socket = key.fileobj  # type: ignore[assignment]\n+                on_close(sock)\n+                sock.close()\n \n         # Check if the subprocess has exited\n         return self._check_subprocess_exit(raise_on_timeout=raise_on_timeout, expect_signal=expect_signal)\n@@ -773,16 +775,16 @@ def _check_subprocess_exit(\n                 raise\n         else:\n             self._process_exit_monotonic = time.monotonic()\n-            self._close_unused_sockets(self.stdin)\n-            # Put a message in the viewable task logs\n \n             if expect_signal is not None and self._exit_code == -expect_signal:\n                 # Bypass logging, the caller expected us to exit with this\n                 return self._exit_code\n \n-            # psutil turns signal exit codes into an enum for us. Handy. (Otherwise it's a plain integer) if exit_code and (name := getattr(exit_code, \"name\")):\n+            # Put a message in the viewable task logs\n+\n             if self._exit_code == -signal.SIGSEGV:\n                 self.process_log.critical(SIGSEGV_MESSAGE)\n+            # psutil turns signal exit codes into an enum for us. Handy. (Otherwise it's a plain integer) if exit_code and (name := getattr(exit_code, \"name\")):\n             elif name := getattr(self._exit_code, \"name\", None):\n                 message = \"Process terminated by signal\"\n                 level = logging.ERROR\n@@ -809,7 +811,7 @@ class ActivitySubprocess(WatchedSubprocess):\n     _last_heartbeat_attempt: float = attrs.field(default=0, init=False)\n \n     # After the failure of a heartbeat, we'll increment this counter. If it reaches `MAX_FAILED_HEARTBEATS`, we\n-    # will kill the process. This is to handle temporary network issues etc. ensuring that the process\n+    # will kill theprocess. This is to handle temporary network issues etc. ensuring that the process\n     # does not hang around forever.\n     failed_heartbeats: int = attrs.field(default=0, init=False)\n \n@@ -861,7 +863,6 @@ def _on_child_started(self, ti: TaskInstance, dag_rel_path: str | os.PathLike[st\n             ti=ti,\n             dag_rel_path=os.fspath(dag_rel_path),\n             bundle_info=bundle_info,\n-            requests_fd=self._requests_fd,\n             ti_context=ti_context,\n             start_date=start_date,\n         )\n@@ -870,8 +871,8 @@ def _on_child_started(self, ti: TaskInstance, dag_rel_path: str | os.PathLike[st\n         log.debug(\"Sending\", msg=msg)\n \n         try:\n-            self.send_msg(msg)\n-        except BrokenPipeError:\n+            self.send_msg(msg, request_id=0)\n+        except (BrokenPipeError, ConnectionResetError):\n             # Debug is fine, the process will have shown _something_ in it's last_chance exception handler\n             log.debug(\"Couldn't send startup message to Subprocess - it died very early\", pid=self.pid)\n \n@@ -930,7 +931,7 @@ def _monitor_subprocess(self):\n         - Processes events triggered on the monitored file objects, such as data availability or EOF.\n         - Sends heartbeats to ensure the process is alive and checks if the subprocess has exited.\n         \"\"\"\n-        while self._exit_code is None or self._num_open_sockets > 0:\n+        while self._exit_code is None or self._open_sockets:\n             last_heartbeat_ago = time.monotonic() - self._last_successful_heartbeat\n             # Monitor the task to see if it's done. Wait in a syscall (`select`) for as long as possible\n             # so we notice the subprocess finishing as quick as we can.\n@@ -946,16 +947,11 @@ def _monitor_subprocess(self):\n             # This listens for activity (e.g., subprocess output) on registered file objects\n             alive = self._service_subprocess(max_wait_time=max_wait_time) is None\n \n-            if self._exit_code is not None and self._num_open_sockets > 0:\n+            if self._exit_code is not None and self._open_sockets:\n                 if (\n                     self._process_exit_monotonic\n                     and time.monotonic() - self._process_exit_monotonic > SOCKET_CLEANUP_TIMEOUT\n                 ):\n-                    log.debug(\n-                        \"Forcefully closing remaining sockets\",\n-                        open_sockets=self._num_open_sockets,\n-                        pid=self.pid,\n-                    )\n                     self._cleanup_open_sockets()\n \n             if alive:\n@@ -1051,7 +1047,7 @@ def final_state(self):\n             return SERVER_TERMINATED\n         return TaskInstanceState.FAILED\n \n-    def _handle_request(self, msg: ToSupervisor, log: FilteringBoundLogger):\n+    def _handle_request(self, msg: ToSupervisor, log: FilteringBoundLogger, req_id: int):\n         log.debug(\"Received message from task runner\", msg=msg)\n         resp: BaseModel | None = None\n         dump_opts = {}\n@@ -1224,10 +1220,17 @@ def _handle_request(self, msg: ToSupervisor, log: FilteringBoundLogger):\n             dump_opts = {\"exclude_unset\": True}\n         else:\n             log.error(\"Unhandled request\", msg=msg)\n+            self.send_msg(\n+                None,\n+                request_id=req_id,\n+                error=ErrorResponse(\n+                    error=ErrorType.API_SERVER_ERROR,\n+                    detail={\"status_code\": 400, \"message\": \"Unhandled request\"},\n+                ),\n+            )\n             return\n \n-        if resp:\n-            self.send_msg(resp, **dump_opts)\n+        self.send_msg(resp, request_id=req_id, error=None, **dump_opts)\n \n \n def in_process_api_server():\n@@ -1237,24 +1240,26 @@ def in_process_api_server():\n     return api\n \n \n-@attrs.define\n+@attrs.define(kw_only=True)\n class InProcessSupervisorComms:\n     \"\"\"In-process communication handler that uses deques instead of sockets.\"\"\"\n \n+    log: FilteringBoundLogger = attrs.field(repr=False, factory=structlog.get_logger)\n     supervisor: InProcessTestSupervisor\n-    messages: deque[BaseModel] = attrs.field(factory=deque)\n-    lock: aiologic.Lock = attrs.field(factory=aiologic.Lock)\n+    messages: deque[BaseModel | None] = attrs.field(factory=deque)\n \n-    def get_message(self) -> BaseModel:\n+    def _get_response(self) -> BaseModel | None:\n         \"\"\"Get a message from the supervisor. Blocks until a message is available.\"\"\"\n         return self.messages.popleft()\n \n-    def send_request(self, log, msg: BaseModel):\n+    def send(self, msg: BaseModel):\n         \"\"\"Send a request to the supervisor.\"\"\"\n-        log.debug(\"Sending request\", msg=msg)\n+        self.log.debug(\"Sending request\", msg=msg)\n \n         with set_supervisor_comms(None):\n-            self.supervisor._handle_request(msg, log)  # type: ignore[arg-type]\n+            self.supervisor._handle_request(msg, log, 0)  # type: ignore[arg-type]\n+\n+        return self._get_response()\n \n \n @attrs.define\n@@ -1272,7 +1277,8 @@ class InProcessTestSupervisor(ActivitySubprocess):\n     \"\"\"A supervisor that runs tasks in-process for easier testing.\"\"\"\n \n     comms: InProcessSupervisorComms = attrs.field(init=False)\n-    stdin = attrs.field(init=False)\n+\n+    stdin: socket = attrs.field(init=False)\n \n     @classmethod\n     def start(  # type: ignore[override]\n@@ -1298,7 +1304,6 @@ def start(  # type: ignore[override]\n             id=what.id,\n             pid=os.getpid(),  # Use current process\n             process=psutil.Process(),  # Current process\n-            requests_fd=-1,  # Not used in in-process mode\n             process_log=logger or structlog.get_logger(logger_name=\"task\").bind(),\n             client=cls._api_client(task.dag),\n             **kwargs,\n@@ -1363,7 +1368,9 @@ def _api_client(dag=None):\n         client.base_url = \"http://in-process.invalid./\"  # type: ignore[assignment]\n         return client\n \n-    def send_msg(self, msg: BaseModel, **dump_opts):\n+    def send_msg(\n+        self, msg: BaseModel | None, request_id: int, error: ErrorResponse | None = None, **dump_opts\n+    ):\n         \"\"\"Override to use in-process comms.\"\"\"\n         self.comms.messages.append(msg)\n \n@@ -1421,9 +1428,9 @@ def run_task_in_process(ti: TaskInstance, task) -> TaskRunResult:\n # to a (sync) generator\n def make_buffered_socket_reader(\n     gen: Generator[None, bytes | bytearray, None],\n-    on_close: Callable,\n+    on_close: Callable[[socket], None],\n     buffer_size: int = 4096,\n-) -> Callable[[socket], bool]:\n+):\n     buffer = bytearray()  # This will hold our accumulated binary data\n     read_buffer = bytearray(buffer_size)  # Temporary buffer for each read\n \n@@ -1440,8 +1447,6 @@ def cb(sock: socket):\n             if len(buffer):\n                 with suppress(StopIteration):\n                     gen.send(buffer)\n-            # Tell loop to close this selector\n-            on_close()\n             return False\n \n         buffer.extend(read_buffer[:n_received])\n@@ -1452,18 +1457,62 @@ def cb(sock: socket):\n             try:\n                 gen.send(line)\n             except StopIteration:\n-                on_close()\n                 return False\n             buffer = buffer[newline_pos + 1 :]  # Update the buffer with remaining data\n \n         return True\n \n-    return cb\n+    return cb, on_close\n+\n+\n+def length_prefixed_frame_reader(\n+    gen: Generator[None, _RequestFrame, None], on_close: Callable[[socket], None]\n+):\n+    length_needed: int | None = None\n+    # This will hold our accumulated/partial binary frame if it doesn't come in a single read\n+    buffer: memoryview | None = None\n+    # position in the buffer to store next read\n+    pos = 0\n+    decoder = msgspec.msgpack.Decoder[_RequestFrame](_RequestFrame)\n+\n+    # We need to start up the generator to get it to the point it's at waiting on the yield\n+    next(gen)\n+\n+    def cb(sock: socket):\n+        nonlocal buffer, length_needed, pos\n+\n+        if length_needed is None:\n+            # Read the 32bit length of the frame\n+            bytes = sock.recv(4)\n+            if bytes == b\"\":\n+                return False\n+\n+            length_needed = int.from_bytes(bytes, byteorder=\"big\")\n+            buffer = memoryview(bytearray(length_needed))\n+        if length_needed and buffer:\n+            n = sock.recv_into(buffer[pos:])\n+            if n == 0:\n+                # EOF\n+                return False\n+            pos += n\n+\n+            if pos >= length_needed:\n+                request = decoder.decode(buffer)\n+                buffer = None\n+                pos = 0\n+                length_needed = None\n+                try:\n+                    gen.send(request)\n+                except StopIteration:\n+                    return False\n+        return True\n+\n+    return cb, on_close\n \n \n def process_log_messages_from_subprocess(\n     loggers: tuple[FilteringBoundLogger, ...],\n-) -> Generator[None, bytes, None]:\n+) -> Generator[None, bytes | bytearray, None]:\n     from structlog.stdlib import NAME_TO_LEVEL\n \n     while True:\n@@ -1499,10 +1548,9 @@ def process_log_messages_from_subprocess(\n \n def forward_to_log(\n     target_loggers: tuple[FilteringBoundLogger, ...], chan: str, level: int\n-) -> Generator[None, bytes, None]:\n+) -> Generator[None, bytes | bytearray, None]:\n     while True:\n-        buf = yield\n-        line = bytes(buf)\n+        line = yield\n         # Strip off new line\n         line = line.rstrip()\n         try:\ndiff --git a/task-sdk/src/airflow/sdk/execution_time/task_runner.py b/task-sdk/src/airflow/sdk/execution_time/task_runner.py\nindex d2cc479d5f4e2..5ad5d4df45feb 100644\n--- a/task-sdk/src/airflow/sdk/execution_time/task_runner.py\n+++ b/task-sdk/src/airflow/sdk/execution_time/task_runner.py\n@@ -28,16 +28,14 @@\n from collections.abc import Callable, Iterable, Iterator, Mapping\n from contextlib import suppress\n from datetime import datetime, timezone\n-from io import FileIO\n from itertools import product\n from pathlib import Path\n-from typing import TYPE_CHECKING, Annotated, Any, Generic, Literal, TextIO, TypeVar\n+from typing import TYPE_CHECKING, Annotated, Any, Literal\n \n-import aiologic\n import attrs\n import lazy_object_proxy\n import structlog\n-from pydantic import AwareDatetime, BaseModel, ConfigDict, Field, JsonValue, TypeAdapter\n+from pydantic import AwareDatetime, ConfigDict, Field, JsonValue\n \n from airflow.dag_processing.bundles.base import BaseDagBundle, BundleVersionLock\n from airflow.dag_processing.bundles.manager import DagBundlesManager\n@@ -59,6 +57,7 @@\n from airflow.sdk.execution_time.callback_runner import create_executable_runner\n from airflow.sdk.execution_time.comms import (\n     AssetEventDagRunReferenceResult,\n+    CommsDecoder,\n     DagRunStateResult,\n     DeferTask,\n     DRCount,\n@@ -424,10 +423,9 @@ def get_first_reschedule_date(self, context: Context) -> AwareDatetime | None:\n \n         log.debug(\"Requesting first reschedule date from supervisor\")\n \n-        SUPERVISOR_COMMS.send_request(\n-            log=log, msg=GetTaskRescheduleStartDate(ti_id=self.id, try_number=first_try_number)\n+        response = SUPERVISOR_COMMS.send(\n+            msg=GetTaskRescheduleStartDate(ti_id=self.id, try_number=first_try_number)\n         )\n-        response = SUPERVISOR_COMMS.get_message()\n \n         if TYPE_CHECKING:\n             assert isinstance(response, TaskRescheduleStartDate)\n@@ -445,22 +443,17 @@ def get_ti_count(\n         states: list[str] | None = None,\n     ) -> int:\n         \"\"\"Return the number of task instances matching the given criteria.\"\"\"\n-        log = structlog.get_logger(logger_name=\"task\")\n-\n-        with SUPERVISOR_COMMS.lock:\n-            SUPERVISOR_COMMS.send_request(\n-                log=log,\n-                msg=GetTICount(\n-                    dag_id=dag_id,\n-                    map_index=map_index,\n-                    task_ids=task_ids,\n-                    task_group_id=task_group_id,\n-                    logical_dates=logical_dates,\n-                    run_ids=run_ids,\n-                    states=states,\n-                ),\n-            )\n-            response = SUPERVISOR_COMMS.get_message()\n+        response = SUPERVISOR_COMMS.send(\n+            GetTICount(\n+                dag_id=dag_id,\n+                map_index=map_index,\n+                task_ids=task_ids,\n+                task_group_id=task_group_id,\n+                logical_dates=logical_dates,\n+                run_ids=run_ids,\n+                states=states,\n+            ),\n+        )\n \n         if TYPE_CHECKING:\n             assert isinstance(response, TICount)\n@@ -477,21 +470,16 @@ def get_task_states(\n         run_ids: list[str] | None = None,\n     ) -> dict[str, Any]:\n         \"\"\"Return the task states matching the given criteria.\"\"\"\n-        log = structlog.get_logger(logger_name=\"task\")\n-\n-        with SUPERVISOR_COMMS.lock:\n-            SUPERVISOR_COMMS.send_request(\n-                log=log,\n-                msg=GetTaskStates(\n-                    dag_id=dag_id,\n-                    map_index=map_index,\n-                    task_ids=task_ids,\n-                    task_group_id=task_group_id,\n-                    logical_dates=logical_dates,\n-                    run_ids=run_ids,\n-                ),\n-            )\n-            response = SUPERVISOR_COMMS.get_message()\n+        response = SUPERVISOR_COMMS.send(\n+            GetTaskStates(\n+                dag_id=dag_id,\n+                map_index=map_index,\n+                task_ids=task_ids,\n+                task_group_id=task_group_id,\n+                logical_dates=logical_dates,\n+                run_ids=run_ids,\n+            ),\n+        )\n \n         if TYPE_CHECKING:\n             assert isinstance(response, TaskStatesResult)\n@@ -506,19 +494,14 @@ def get_dr_count(\n         states: list[str] | None = None,\n     ) -> int:\n         \"\"\"Return the number of DAG runs matching the given criteria.\"\"\"\n-        log = structlog.get_logger(logger_name=\"task\")\n-\n-        with SUPERVISOR_COMMS.lock:\n-            SUPERVISOR_COMMS.send_request(\n-                log=log,\n-                msg=GetDRCount(\n-                    dag_id=dag_id,\n-                    logical_dates=logical_dates,\n-                    run_ids=run_ids,\n-                    states=states,\n-                ),\n-            )\n-            response = SUPERVISOR_COMMS.get_message()\n+        response = SUPERVISOR_COMMS.send(\n+            GetDRCount(\n+                dag_id=dag_id,\n+                logical_dates=logical_dates,\n+                run_ids=run_ids,\n+                states=states,\n+            ),\n+        )\n \n         if TYPE_CHECKING:\n             assert isinstance(response, DRCount)\n@@ -528,10 +511,7 @@ def get_dr_count(\n     @staticmethod\n     def get_dagrun_state(dag_id: str, run_id: str) -> str:\n         \"\"\"Return the state of the DAG run with the given Run ID.\"\"\"\n-        log = structlog.get_logger(logger_name=\"task\")\n-        with SUPERVISOR_COMMS.lock:\n-            SUPERVISOR_COMMS.send_request(log=log, msg=GetDagRunState(dag_id=dag_id, run_id=run_id))\n-            response = SUPERVISOR_COMMS.get_message()\n+        response = SUPERVISOR_COMMS.send(msg=GetDagRunState(dag_id=dag_id, run_id=run_id))\n \n         if TYPE_CHECKING:\n             assert isinstance(response, DagRunStateResult)\n@@ -650,62 +630,6 @@ def parse(what: StartupDetails, log: Logger) -> RuntimeTaskInstance:\n     )\n \n \n-SendMsgType = TypeVar(\"SendMsgType\", bound=BaseModel)\n-ReceiveMsgType = TypeVar(\"ReceiveMsgType\", bound=BaseModel)\n-\n-\n-@attrs.define()\n-class CommsDecoder(Generic[ReceiveMsgType, SendMsgType]):\n-    \"\"\"Handle communication between the task in this process and the supervisor parent process.\"\"\"\n-\n-    input: TextIO\n-\n-    request_socket: FileIO = attrs.field(init=False, default=None)\n-\n-    # We could be \"clever\" here and set the default to this based type parameters and a custom\n-    # `__class_getitem__`, but that's a lot of code the one subclass we've got currently. So we'll just use a\n-    # \"sort of wrong default\"\n-    decoder: TypeAdapter[ReceiveMsgType] = attrs.field(factory=lambda: TypeAdapter(ToTask), repr=False)\n-\n-    lock: aiologic.Lock = attrs.field(factory=aiologic.Lock, repr=False)\n-\n-    def get_message(self) -> ReceiveMsgType:\n-        \"\"\"\n-        Get a message from the parent.\n-\n-        This will block until the message has been received.\n-        \"\"\"\n-        line = None\n-\n-        # TODO: Investigate why some empty lines are sent to the processes stdin.\n-        #   That was highlighted when working on https://github.com/apache/airflow/issues/48183\n-        #   and is maybe related to deferred/triggerer only context.\n-        while not line:\n-            line = self.input.readline()\n-\n-        try:\n-            msg = self.decoder.validate_json(line)\n-        except Exception:\n-            structlog.get_logger(logger_name=\"CommsDecoder\").exception(\"Unable to decode message\", line=line)\n-            raise\n-\n-        if isinstance(msg, StartupDetails):\n-            # If we read a startup message, pull out the FDs we care about!\n-            if msg.requests_fd > 0:\n-                self.request_socket = os.fdopen(msg.requests_fd, \"wb\", buffering=0)\n-        elif isinstance(msg, ErrorResponse) and msg.error == ErrorType.API_SERVER_ERROR:\n-            structlog.get_logger(logger_name=\"task\").error(\"Error response from the API Server\")\n-            raise AirflowRuntimeError(error=msg)\n-\n-        return msg\n-\n-    def send_request(self, log: Logger, msg: SendMsgType):\n-        encoded_msg = msg.model_dump_json().encode() + b\"\\n\"\n-\n-        log.debug(\"Sending request\", json=encoded_msg)\n-        self.request_socket.write(encoded_msg)\n-\n-\n # This global variable will be used by Connection/Variable/XCom classes, or other parts of the task's execution,\n # to send requests back to the supervisor process.\n #\n@@ -725,31 +649,33 @@ def send_request(self, log: Logger, msg: SendMsgType):\n \n \n def startup() -> tuple[RuntimeTaskInstance, Context, Logger]:\n-    msg = SUPERVISOR_COMMS.get_message()\n+    # The parent sends us a StartupDetails message un-prompted. After this, every single message is only sent\n+    # in response to us sending a request.\n+    msg = SUPERVISOR_COMMS._get_response()\n+\n+    if not isinstance(msg, StartupDetails):\n+        raise RuntimeError(f\"Unhandled startup message {type(msg)} {msg}\")\n \n     log = structlog.get_logger(logger_name=\"task\")\n \n+    # setproctitle causes issue on Mac OS: https://github.com/benoitc/gunicorn/issues/3021\n+    os_type = sys.platform\n+    if os_type == \"darwin\":\n+        log.debug(\"Mac OS detected, skipping setproctitle\")\n+    else:\n+        from setproctitle import setproctitle\n+\n+        setproctitle(f\"airflow worker -- {msg.ti.id}\")\n+\n     try:\n         get_listener_manager().hook.on_starting(component=TaskRunnerMarker())\n     except Exception:\n         log.exception(\"error calling listener\")\n \n-    if isinstance(msg, StartupDetails):\n-        # setproctitle causes issue on Mac OS: https://github.com/benoitc/gunicorn/issues/3021\n-        os_type = sys.platform\n-        if os_type == \"darwin\":\n-            log.debug(\"Mac OS detected, skipping setproctitle\")\n-        else:\n-            from setproctitle import setproctitle\n-\n-            setproctitle(f\"airflow worker -- {msg.ti.id}\")\n-\n-        with _airflow_parsing_context_manager(dag_id=msg.ti.dag_id, task_id=msg.ti.task_id):\n-            ti = parse(msg, log)\n-            ti.log_url = get_log_url_from_ti(ti)\n-        log.debug(\"DAG file parsed\", file=msg.dag_rel_path)\n-    else:\n-        raise RuntimeError(f\"Unhandled startup message {type(msg)} {msg}\")\n+    with _airflow_parsing_context_manager(dag_id=msg.ti.dag_id, task_id=msg.ti.task_id):\n+        ti = parse(msg, log)\n+        ti.log_url = get_log_url_from_ti(ti)\n+    log.debug(\"DAG file parsed\", file=msg.dag_rel_path)\n \n     return ti, ti.get_template_context(), log\n \n@@ -797,7 +723,7 @@ def _prepare(ti: RuntimeTaskInstance, log: Logger, context: Context) -> ToSuperv\n \n     if rendered_fields := _serialize_rendered_fields(ti.task):\n         # so that we do not call the API unnecessarily\n-        SUPERVISOR_COMMS.send_request(log=log, msg=SetRenderedFields(rendered_fields=rendered_fields))\n+        SUPERVISOR_COMMS.send(msg=SetRenderedFields(rendered_fields=rendered_fields))\n \n     _validate_task_inlets_and_outlets(ti=ti, log=log)\n \n@@ -817,8 +743,7 @@ def _validate_task_inlets_and_outlets(*, ti: RuntimeTaskInstance, log: Logger) -\n     if not ti.task.inlets and not ti.task.outlets:\n         return\n \n-    SUPERVISOR_COMMS.send_request(msg=ValidateInletsAndOutlets(ti_id=ti.id), log=log)\n-    inactive_assets_resp = SUPERVISOR_COMMS.get_message()\n+    inactive_assets_resp = SUPERVISOR_COMMS.send(msg=ValidateInletsAndOutlets(ti_id=ti.id))\n     if TYPE_CHECKING:\n         assert isinstance(inactive_assets_resp, InactiveAssetsResult)\n     if inactive_assets := inactive_assets_resp.inactive_assets:\n@@ -914,7 +839,7 @@ def run(\n     except DownstreamTasksSkipped as skip:\n         log.info(\"Skipping downstream tasks.\")\n         tasks_to_skip = skip.tasks if isinstance(skip.tasks, list) else [skip.tasks]\n-        SUPERVISOR_COMMS.send_request(log=log, msg=SkipDownstreamTasks(tasks=tasks_to_skip))\n+        SUPERVISOR_COMMS.send(msg=SkipDownstreamTasks(tasks=tasks_to_skip))\n         msg, state = _handle_current_task_success(context, ti)\n     except DagRunTriggerException as drte:\n         msg, state = _handle_trigger_dag_run(drte, context, ti, log)\n@@ -974,7 +899,7 @@ def run(\n         error = e\n     finally:\n         if msg:\n-            SUPERVISOR_COMMS.send_request(msg=msg, log=log)\n+            SUPERVISOR_COMMS.send(msg=msg)\n \n     # Return the message to make unit tests easier too\n     ti.state = state\n@@ -1012,9 +937,8 @@ def _handle_trigger_dag_run(\n ) -> tuple[ToSupervisor, TaskInstanceState]:\n     \"\"\"Handle exception from TriggerDagRunOperator.\"\"\"\n     log.info(\"Triggering Dag Run.\", trigger_dag_id=drte.trigger_dag_id)\n-    SUPERVISOR_COMMS.send_request(\n-        log=log,\n-        msg=TriggerDagRun(\n+    comms_msg = SUPERVISOR_COMMS.send(\n+        TriggerDagRun(\n             dag_id=drte.trigger_dag_id,\n             run_id=drte.dag_run_id,\n             logical_date=drte.logical_date,\n@@ -1023,7 +947,6 @@ def _handle_trigger_dag_run(\n         ),\n     )\n \n-    comms_msg = SUPERVISOR_COMMS.get_message()\n     if isinstance(comms_msg, ErrorResponse) and comms_msg.error == ErrorType.DAGRUN_ALREADY_EXISTS:\n         if drte.skip_when_already_exists:\n             log.info(\n@@ -1078,10 +1001,9 @@ def _handle_trigger_dag_run(\n             )\n             time.sleep(drte.poke_interval)\n \n-            SUPERVISOR_COMMS.send_request(\n-                log=log, msg=GetDagRunState(dag_id=drte.trigger_dag_id, run_id=drte.dag_run_id)\n+            comms_msg = SUPERVISOR_COMMS.send(\n+                GetDagRunState(dag_id=drte.trigger_dag_id, run_id=drte.dag_run_id)\n             )\n-            comms_msg = SUPERVISOR_COMMS.get_message()\n             if TYPE_CHECKING:\n                 assert isinstance(comms_msg, DagRunStateResult)\n             if comms_msg.state in drte.failed_states:\n@@ -1270,10 +1192,7 @@ def finalize(\n     if getattr(ti.task, \"overwrite_rtif_after_execution\", False):\n         log.debug(\"Overwriting Rendered template fields.\")\n         if ti.task.template_fields:\n-            SUPERVISOR_COMMS.send_request(\n-                log=log,\n-                msg=SetRenderedFields(rendered_fields=_serialize_rendered_fields(ti.task)),\n-            )\n+            SUPERVISOR_COMMS.send(SetRenderedFields(rendered_fields=_serialize_rendered_fields(ti.task)))\n \n     log.debug(\"Running finalizers\", ti=ti)\n     if state == TaskInstanceState.SUCCESS:\n@@ -1314,9 +1233,11 @@ def finalize(\n \n \n def main():\n-    # TODO: add an exception here, it causes an oof of a stack trace!\n+    # TODO: add an exception here, it causes an oof of a stack trace if it happens to early!\n+    log = structlog.get_logger(logger_name=\"task\")\n+\n     global SUPERVISOR_COMMS\n-    SUPERVISOR_COMMS = CommsDecoder[ToTask, ToSupervisor](input=sys.stdin)\n+    SUPERVISOR_COMMS = CommsDecoder[ToTask, ToSupervisor](log=log)\n \n     try:\n         ti, context, log = startup()\n@@ -1327,19 +1248,17 @@ def main():\n             state, msg, error = run(ti, context, log)\n             finalize(ti, state, context, log, error)\n     except KeyboardInterrupt:\n-        log = structlog.get_logger(logger_name=\"task\")\n         log.exception(\"Ctrl-c hit\")\n         exit(2)\n     except Exception:\n-        log = structlog.get_logger(logger_name=\"task\")\n         log.exception(\"Top level error\")\n         exit(1)\n     finally:\n         # Ensure the request socket is closed on the child side in all circumstances\n         # before the process fully terminates.\n-        if SUPERVISOR_COMMS and SUPERVISOR_COMMS.request_socket:\n+        if SUPERVISOR_COMMS and SUPERVISOR_COMMS.socket:\n             with suppress(Exception):\n-                SUPERVISOR_COMMS.request_socket.close()\n+                SUPERVISOR_COMMS.socket.close()\n \n \n if __name__ == \"__main__\":\n","test_patch":"diff --git a/airflow-core/tests/unit/dag_processing/test_manager.py b/airflow-core/tests/unit/dag_processing/test_manager.py\nindex c9e974b8cdb9a..02121f0194e82 100644\n--- a/airflow-core/tests/unit/dag_processing/test_manager.py\n+++ b/airflow-core/tests/unit/dag_processing/test_manager.py\n@@ -30,10 +30,11 @@\n from datetime import datetime, timedelta\n from logging.config import dictConfig\n from pathlib import Path\n-from socket import socket\n+from socket import socket, socketpair\n from unittest import mock\n from unittest.mock import MagicMock\n \n+import msgspec\n import pytest\n import time_machine\n from sqlalchemy import func, select\n@@ -54,7 +55,6 @@\n from airflow.models.dagbundle import DagBundleModel\n from airflow.models.dagcode import DagCode\n from airflow.models.serialized_dag import SerializedDagModel\n-from airflow.sdk.execution_time.supervisor import mkpipe\n from airflow.utils import timezone\n from airflow.utils.net import get_hostname\n from airflow.utils.session import create_session\n@@ -138,20 +138,19 @@ def mock_processor(self, start_time: float | None = None) -> tuple[DagFileProces\n         logger_filehandle = MagicMock()\n         proc.create_time.return_value = time.time()\n         proc.wait.return_value = 0\n-        read_end, write_end = mkpipe(remote_read=True)\n+        read_end, write_end = socketpair()\n         ret = DagFileProcessorProcess(\n             process_log=MagicMock(),\n             id=uuid7(),\n             pid=1234,\n             process=proc,\n             stdin=write_end,\n-            requests_fd=123,\n             logger_filehandle=logger_filehandle,\n             client=MagicMock(),\n         )\n         if start_time:\n             ret.start_time = start_time\n-        ret._num_open_sockets = 0\n+        ret._open_sockets.clear()\n         return ret, read_end\n \n     @pytest.fixture\n@@ -552,18 +551,17 @@ def test_kill_timed_out_processors_no_kill(self):\n \n     @pytest.mark.usefixtures(\"testing_dag_bundle\")\n     @pytest.mark.parametrize(\n-        [\"callbacks\", \"path\", \"expected_buffer\"],\n+        [\"callbacks\", \"path\", \"expected_body\"],\n         [\n             pytest.param(\n                 [],\n                 \"/opt/airflow/dags/test_dag.py\",\n-                b\"{\"\n-                b'\"file\":\"/opt/airflow/dags/test_dag.py\",'\n-                b'\"bundle_path\":\"/opt/airflow/dags\",'\n-                b'\"requests_fd\":123,'\n-                b'\"callback_requests\":[],'\n-                b'\"type\":\"DagFileParseRequest\"'\n-                b\"}\\n\",\n+                {\n+                    \"file\": \"/opt/airflow/dags/test_dag.py\",\n+                    \"bundle_path\": \"/opt/airflow/dags\",\n+                    \"callback_requests\": [],\n+                    \"type\": \"DagFileParseRequest\",\n+                },\n             ),\n             pytest.param(\n                 [\n@@ -577,44 +575,39 @@ def test_kill_timed_out_processors_no_kill(self):\n                     )\n                 ],\n                 \"/opt/airflow/dags/dag_callback_dag.py\",\n-                b\"{\"\n-                b'\"file\":\"/opt/airflow/dags/dag_callback_dag.py\",'\n-                b'\"bundle_path\":\"/opt/airflow/dags\",'\n-                b'\"requests_fd\":123,\"callback_requests\":'\n-                b\"[\"\n-                b\"{\"\n-                b'\"filepath\":\"dag_callback_dag.py\",'\n-                b'\"bundle_name\":\"testing\",'\n-                b'\"bundle_version\":null,'\n-                b'\"msg\":null,'\n-                b'\"dag_id\":\"dag_id\",'\n-                b'\"run_id\":\"run_id\",'\n-                b'\"is_failure_callback\":false,'\n-                b'\"type\":\"DagCallbackRequest\"'\n-                b\"}\"\n-                b\"],\"\n-                b'\"type\":\"DagFileParseRequest\"'\n-                b\"}\\n\",\n+                {\n+                    \"file\": \"/opt/airflow/dags/dag_callback_dag.py\",\n+                    \"bundle_path\": \"/opt/airflow/dags\",\n+                    \"callback_requests\": [\n+                        {\n+                            \"filepath\": \"dag_callback_dag.py\",\n+                            \"bundle_name\": \"testing\",\n+                            \"bundle_version\": None,\n+                            \"msg\": None,\n+                            \"dag_id\": \"dag_id\",\n+                            \"run_id\": \"run_id\",\n+                            \"is_failure_callback\": False,\n+                            \"type\": \"DagCallbackRequest\",\n+                        }\n+                    ],\n+                    \"type\": \"DagFileParseRequest\",\n+                },\n             ),\n         ],\n     )\n-    def test_serialize_callback_requests(self, callbacks, path, expected_buffer):\n+    def test_serialize_callback_requests(self, callbacks, path, expected_body):\n+        from airflow.sdk.execution_time.comms import _ResponseFrame\n+\n         processor, read_socket = self.mock_processor()\n         processor._on_child_started(callbacks, path, bundle_path=Path(\"/opt/airflow/dags\"))\n \n         read_socket.settimeout(0.1)\n-        val = b\"\"\n-        try:\n-            while not val.endswith(b\"\\n\"):\n-                chunk = read_socket.recv(4096)\n-                if not chunk:\n-                    break\n-                val += chunk\n-        except (BlockingIOError, TimeoutError):\n-            # no response written, valid for some message types.\n-            pass\n-\n-        assert val == expected_buffer\n+        # Read response from the read end of the socket\n+        frame_len = int.from_bytes(read_socket.recv(4), \"big\")\n+        bytes = read_socket.recv(frame_len)\n+        frame = msgspec.msgpack.Decoder(_ResponseFrame).decode(bytes)\n+\n+        assert frame.body == expected_body\n \n     @conf_vars({(\"core\", \"load_examples\"): \"False\"})\n     @pytest.mark.execution_timeout(10)\ndiff --git a/airflow-core/tests/unit/dag_processing/test_processor.py b/airflow-core/tests/unit/dag_processing/test_processor.py\nindex 28ce7a8c23fe6..8d77da61cafb9 100644\n--- a/airflow-core/tests/unit/dag_processing/test_processor.py\n+++ b/airflow-core/tests/unit/dag_processing/test_processor.py\n@@ -42,7 +42,7 @@\n from airflow.models.baseoperator import BaseOperator\n from airflow.models.serialized_dag import SerializedDagModel\n from airflow.sdk.api.client import Client\n-from airflow.sdk.execution_time.task_runner import CommsDecoder\n+from airflow.sdk.execution_time import comms\n from airflow.utils import timezone\n from airflow.utils.session import create_session\n from airflow.utils.state import DagRunState, TaskInstanceState\n@@ -87,7 +87,6 @@ def _process_file(\n             DagFileParseRequest(\n                 file=file_path,\n                 bundle_path=TEST_DAG_FOLDER,\n-                requests_fd=1,\n                 callback_requests=callback_requests or [],\n             ),\n             log=structlog.get_logger(),\n@@ -393,26 +392,38 @@ def disable_capturing():\n \n @pytest.mark.usefixtures(\"testing_dag_bundle\")\n @pytest.mark.usefixtures(\"disable_capturing\")\n-def test_parse_file_entrypoint_parses_dag_callbacks(spy_agency):\n+def test_parse_file_entrypoint_parses_dag_callbacks(mocker):\n     r, w = socketpair()\n-    # Create a valid FD for the decoder to open\n-    _, w2 = socketpair()\n-\n-    w.makefile(\"wb\").write(\n-        b'{\"file\":\"/files/dags/wait.py\",\"bundle_path\":\"/files/dags\",\"requests_fd\":'\n-        + str(w2.fileno()).encode(\"ascii\")\n-        + b',\"callback_requests\": [{\"filepath\": \"wait.py\", \"bundle_name\": \"testing\", \"bundle_version\": null, '\n-        b'\"msg\": \"task_failure\", \"dag_id\": \"wait_to_fail\", \"run_id\": '\n-        b'\"manual__2024-12-30T21:02:55.203691+00:00\", '\n-        b'\"is_failure_callback\": true, \"type\": \"DagCallbackRequest\"}], \"type\": \"DagFileParseRequest\"}\\n'\n+\n+    frame = comms._ResponseFrame(\n+        id=1,\n+        body={\n+            \"file\": \"/files/dags/wait.py\",\n+            \"bundle_path\": \"/files/dags\",\n+            \"callback_requests\": [\n+                {\n+                    \"filepath\": \"wait.py\",\n+                    \"bundle_name\": \"testing\",\n+                    \"bundle_version\": None,\n+                    \"msg\": \"task_failure\",\n+                    \"dag_id\": \"wait_to_fail\",\n+                    \"run_id\": \"manual__2024-12-30T21:02:55.203691+00:00\",\n+                    \"is_failure_callback\": True,\n+                    \"type\": \"DagCallbackRequest\",\n+                }\n+            ],\n+            \"type\": \"DagFileParseRequest\",\n+        },\n     )\n+    bytes = frame.as_bytes()\n+    w.sendall(bytes)\n \n-    decoder = CommsDecoder[DagFileParseRequest, DagFileParsingResult](\n-        input=r.makefile(\"r\"),\n-        decoder=TypeAdapter[DagFileParseRequest](DagFileParseRequest),\n+    decoder = comms.CommsDecoder[DagFileParseRequest, DagFileParsingResult](\n+        socket=r,\n+        body_decoder=TypeAdapter[DagFileParseRequest](DagFileParseRequest),\n     )\n \n-    msg = decoder.get_message()\n+    msg = decoder._get_response()\n     assert isinstance(msg, DagFileParseRequest)\n     assert msg.file == \"/files/dags/wait.py\"\n     assert msg.callback_requests == [\n@@ -455,7 +466,7 @@ def fake_collect_dags(self, *args, **kwargs):\n         )\n     ]\n     _parse_file(\n-        DagFileParseRequest(file=\"A\", bundle_path=\"no matter\", requests_fd=1, callback_requests=requests),\n+        DagFileParseRequest(file=\"A\", bundle_path=\"no matter\", callback_requests=requests),\n         log=structlog.get_logger(),\n     )\n \n@@ -489,8 +500,6 @@ def fake_collect_dags(self, *args, **kwargs):\n             bundle_version=None,\n         )\n     ]\n-    _parse_file(\n-        DagFileParseRequest(file=\"A\", requests_fd=1, callback_requests=requests), log=structlog.get_logger()\n-    )\n+    _parse_file(DagFileParseRequest(file=\"A\", callback_requests=requests), log=structlog.get_logger())\n \n     assert called is True\ndiff --git a/airflow-core/tests/unit/hooks/test_base.py b/airflow-core/tests/unit/hooks/test_base.py\nindex 8c2d44f84854b..6d54827e67ced 100644\n--- a/airflow-core/tests/unit/hooks/test_base.py\n+++ b/airflow-core/tests/unit/hooks/test_base.py\n@@ -17,8 +17,6 @@\n # under the License.\n from __future__ import annotations\n \n-from unittest import mock\n-\n import pytest\n \n from airflow.exceptions import AirflowNotFoundException\n@@ -54,18 +52,18 @@ def test_get_connection(self, mock_supervisor_comms):\n             extra='{\"extra_key\": \"extra_value\"}',\n         )\n \n-        mock_supervisor_comms.get_message.return_value = conn\n+        mock_supervisor_comms.send.return_value = conn\n \n         hook = BaseHook(logger_name=\"\")\n         hook.get_connection(conn_id=\"test_conn\")\n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            msg=GetConnection(conn_id=\"test_conn\"), log=mock.ANY\n+        mock_supervisor_comms.send.assert_called_once_with(\n+            msg=GetConnection(conn_id=\"test_conn\"),\n         )\n \n     def test_get_connection_not_found(self, mock_supervisor_comms):\n         conn_id = \"test_conn\"\n         hook = BaseHook()\n-        mock_supervisor_comms.get_message.return_value = ErrorResponse(error=ErrorType.CONNECTION_NOT_FOUND)\n+        mock_supervisor_comms.send.return_value = ErrorResponse(error=ErrorType.CONNECTION_NOT_FOUND)\n \n         with pytest.raises(AirflowNotFoundException, match=rf\".*{conn_id}.*\"):\n             hook.get_connection(conn_id=conn_id)\n@@ -85,5 +83,4 @@ def test_get_connection_secrets_backend_configured(self, mock_supervisor_comms,\n \n             assert retrieved_conn.conn_id == \"CONN_A\"\n \n-            mock_supervisor_comms.send_request.assert_not_called()\n-            mock_supervisor_comms.get_message.assert_not_called()\n+            mock_supervisor_comms.send.assert_not_called()\ndiff --git a/airflow-core/tests/unit/jobs/test_triggerer_job.py b/airflow-core/tests/unit/jobs/test_triggerer_job.py\nindex d3c9ae6f27dc9..d3cecb5fc94e6 100644\n--- a/airflow-core/tests/unit/jobs/test_triggerer_job.py\n+++ b/airflow-core/tests/unit/jobs/test_triggerer_job.py\n@@ -34,6 +34,7 @@\n from airflow.hooks.base import BaseHook\n from airflow.jobs.job import Job\n from airflow.jobs.triggerer_job_runner import (\n+    TriggerCommsDecoder,\n     TriggererJobRunner,\n     TriggerRunner,\n     TriggerRunnerSupervisor,\n@@ -172,7 +173,6 @@ def builder(job=None):\n             pid=process.pid,\n             stdin=mocker.Mock(),\n             process=process,\n-            requests_fd=-1,\n             capacity=10,\n         )\n         # Mock the selector\n@@ -302,10 +302,9 @@ async def test_invalid_trigger(self, supervisor_builder):\n             id=1, ti=None, classpath=\"fake.classpath\", encrypted_kwargs={}\n         )\n         trigger_runner = TriggerRunner()\n-        trigger_runner.requests_sock = MagicMock()\n-        trigger_runner.response_sock = AsyncMock()\n-        trigger_runner.response_sock.readline.return_value = (\n-            b'{\"type\": \"TriggerStateSync\", \"to_create\": [], \"to_cancel\": []}\\n'\n+        trigger_runner.comms_decoder = AsyncMock(spec=TriggerCommsDecoder)\n+        trigger_runner.comms_decoder.asend.return_value = messages.TriggerStateSync(\n+            to_create=[], to_cancel=[]\n         )\n \n         trigger_runner.to_create.append(workload)\n@@ -316,9 +315,9 @@ async def test_invalid_trigger(self, supervisor_builder):\n         await trigger_runner.sync_state_to_supervisor(ids)\n \n         # Check that we sent the right info in the failure message\n-        assert trigger_runner.requests_sock.write.call_count == 1\n-        blob = trigger_runner.requests_sock.write.mock_calls[0].args[0]\n-        msg = messages.TriggerStateChanges.model_validate_json(blob)\n+        assert trigger_runner.comms_decoder.asend.call_count == 1\n+        msg = trigger_runner.comms_decoder.asend.mock_calls[0].args[0]\n+        assert isinstance(msg, messages.TriggerStateChanges)\n \n         assert msg.events is None\n         assert msg.failures is not None\n@@ -552,6 +551,7 @@ def test_failed_trigger(session, dag_maker, supervisor_builder):\n                 )\n             ],\n         ),\n+        req_id=1,\n         log=MagicMock(),\n     )\n \n@@ -622,10 +622,6 @@ def handle_events(self):\n         super().handle_events()\n \n \n-@pytest.mark.xfail(\n-    reason=\"We know that test is flaky and have no time to fix it before 3.0. \"\n-    \"We should fix it later. TODO: AIP-72\"\n-)\n @pytest.mark.asyncio\n @pytest.mark.execution_timeout(20)\n async def test_trigger_can_access_variables_connections_and_xcoms(session, dag_maker):\n@@ -726,13 +722,8 @@ async def run(self, **args) -> AsyncIterator[TriggerEvent]:\n         yield TriggerEvent({\"count\": dag_run_states_count, \"dag_run_state\": dag_run_state})\n \n \n-@pytest.mark.xfail(\n-    reason=\"We know that test is flaky and have no time to fix it before 3.0. \"\n-    \"We should fix it later. TODO: AIP-72\"\n-)\n @pytest.mark.asyncio\n-@pytest.mark.flaky(reruns=2, reruns_delay=10)\n-@pytest.mark.execution_timeout(30)\n+@pytest.mark.execution_timeout(10)\n async def test_trigger_can_fetch_trigger_dag_run_count_and_state_in_deferrable(session, dag_maker):\n     \"\"\"Checks that the trigger will successfully fetch the count of trigger DAG runs.\"\"\"\n     # Create the test DAG and task\n@@ -822,13 +813,8 @@ async def run(self, **args) -> AsyncIterator[TriggerEvent]:\n         yield TriggerEvent({\"ti_count\": ti_count, \"dr_count\": dr_count, \"task_states\": task_states})\n \n \n-@pytest.mark.xfail(\n-    reason=\"We know that test is flaky and have no time to fix it before 3.0. \"\n-    \"We should fix it later. TODO: AIP-72\"\n-)\n @pytest.mark.asyncio\n-@pytest.mark.flaky(reruns=2, reruns_delay=10)\n-@pytest.mark.execution_timeout(30)\n+@pytest.mark.execution_timeout(10)\n async def test_trigger_can_fetch_dag_run_count_ti_count_in_deferrable(session, dag_maker):\n     \"\"\"Checks that the trigger will successfully fetch the count of DAG runs, Task count and task states.\"\"\"\n     # Create the test DAG and task\ndiff --git a/airflow-core/tests/unit/models/test_taskinstance.py b/airflow-core/tests/unit/models/test_taskinstance.py\nindex 401302e64eea4..5c6e4f69a5256 100644\n--- a/airflow-core/tests/unit/models/test_taskinstance.py\n+++ b/airflow-core/tests/unit/models/test_taskinstance.py\n@@ -1886,7 +1886,7 @@ def producer_with_inactive(*, outlet_events):\n     def test_inlet_asset_extra(self, dag_maker, session, mock_supervisor_comms):\n         from airflow.sdk.definitions.asset import Asset\n \n-        mock_supervisor_comms.get_message.return_value = AssetEventsResult(\n+        mock_supervisor_comms.send.return_value = AssetEventsResult(\n             asset_events=[\n                 AssetEventResponse(\n                     id=1,\n@@ -1960,7 +1960,7 @@ def read(*, inlet_events):\n     @pytest.mark.need_serialized_dag\n     def test_inlet_unresolved_asset_alias(self, dag_maker, session, mock_supervisor_comms):\n         asset_alias_name = \"test_inlet_asset_extra_asset_alias\"\n-        mock_supervisor_comms.get_message.return_value = AssetEventsResult(asset_events=[])\n+        mock_supervisor_comms.send.return_value = AssetEventsResult(asset_events=[])\n \n         asset_alias_model = AssetAliasModel(name=asset_alias_name)\n         session.add(asset_alias_model)\ndiff --git a/devel-common/src/tests_common/pytest_plugin.py b/devel-common/src/tests_common/pytest_plugin.py\nindex f01ed6ca3657b..41bb7390ce74c 100644\n--- a/devel-common/src/tests_common/pytest_plugin.py\n+++ b/devel-common/src/tests_common/pytest_plugin.py\n@@ -1956,17 +1956,27 @@ def override_caplog(request):\n \n \n @pytest.fixture\n-def mock_supervisor_comms():\n+def mock_supervisor_comms(monkeypatch):\n     # for back-compat\n     from tests_common.test_utils.version_compat import AIRFLOW_V_3_0_PLUS\n \n     if not AIRFLOW_V_3_0_PLUS:\n         yield None\n         return\n-    with mock.patch(\n-        \"airflow.sdk.execution_time.task_runner.SUPERVISOR_COMMS\", create=True\n-    ) as supervisor_comms:\n-        yield supervisor_comms\n+\n+    from airflow.sdk.execution_time import comms, task_runner\n+\n+    # Deal with TaskSDK 1.0/1.1 vs 1.2+. Annoying, and shouldn't need to exist once the separation between\n+    # core and TaskSDK is finished\n+    if CommsDecoder := getattr(comms, \"CommsDecoder\", None):\n+        comms = mock.create_autospec(CommsDecoder)\n+        monkeypatch.setattr(task_runner, \"SUPERVISOR_COMMS\", comms, raising=False)\n+    else:\n+        CommsDecoder = getattr(task_runner, \"CommsDecoder\")\n+        comms = mock.create_autospec(CommsDecoder)\n+        comms.send = comms.get_message\n+        monkeypatch.setattr(task_runner, \"SUPERVISOR_COMMS\", comms, raising=False)\n+    yield comms\n \n \n @pytest.fixture\n@@ -1991,7 +2001,6 @@ def mocked_parse(spy_agency):\n                         id=uuid7(), task_id=\"hello\", dag_id=\"super_basic_run\", run_id=\"c\", try_number=1\n                     ),\n                     file=\"\",\n-                    requests_fd=0,\n                 ),\n                 \"example_dag_id\",\n                 CustomOperator(task_id=\"hello\"),\n@@ -2200,9 +2209,10 @@ def _create_task_instance(\n             ),\n             dag_rel_path=\"\",\n             bundle_info=BundleInfo(name=\"anything\", version=\"any\"),\n-            requests_fd=0,\n             ti_context=ti_context,\n             start_date=start_date,  # type: ignore\n+            # Back-compat of task-sdk. Only affects us when we manually create these objects in tests.\n+            **({\"requests_fd\": 0} if \"requests_fd\" in StartupDetails.model_fields else {}),  # type: ignore\n         )\n \n         ti = mocked_parse(startup_details, dag_id, task)\ndiff --git a/providers/amazon/tests/unit/amazon/aws/links/test_athena.py b/providers/amazon/tests/unit/amazon/aws/links/test_athena.py\nindex 8abe65cf2cb8c..99a3536d17838 100644\n--- a/providers/amazon/tests/unit/amazon/aws/links/test_athena.py\n+++ b/providers/amazon/tests/unit/amazon/aws/links/test_athena.py\n@@ -30,7 +30,7 @@ class TestAthenaQueryResultsLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=AthenaQueryResultsLink.key,\n                 value={\n                     \"region_name\": \"eu-west-1\",\ndiff --git a/providers/amazon/tests/unit/amazon/aws/links/test_batch.py b/providers/amazon/tests/unit/amazon/aws/links/test_batch.py\nindex 8ecf7022f2162..70cd65655bfec 100644\n--- a/providers/amazon/tests/unit/amazon/aws/links/test_batch.py\n+++ b/providers/amazon/tests/unit/amazon/aws/links/test_batch.py\n@@ -34,7 +34,7 @@ class TestBatchJobDefinitionLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"eu-west-1\",\n@@ -58,7 +58,7 @@ class TestBatchJobDetailsLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"cn-north-1\",\n@@ -80,7 +80,7 @@ class TestBatchJobQueueLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"us-east-1\",\ndiff --git a/providers/amazon/tests/unit/amazon/aws/links/test_comprehend.py b/providers/amazon/tests/unit/amazon/aws/links/test_comprehend.py\nindex 2c60aa27d5085..9b88270b5bd30 100644\n--- a/providers/amazon/tests/unit/amazon/aws/links/test_comprehend.py\n+++ b/providers/amazon/tests/unit/amazon/aws/links/test_comprehend.py\n@@ -34,7 +34,7 @@ class TestComprehendPiiEntitiesDetectionLink(BaseAwsLinksTestCase):\n     def test_extra_link(self, mock_supervisor_comms):\n         test_job_id = \"123-345-678\"\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"eu-west-1\",\n@@ -61,7 +61,7 @@ def test_extra_link(self, mock_supervisor_comms):\n             \"arn:aws:comprehend:us-east-1:0123456789:document-classifier/test-custom-document-classifier\"\n         )\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"us-east-1\",\ndiff --git a/providers/amazon/tests/unit/amazon/aws/links/test_datasync.py b/providers/amazon/tests/unit/amazon/aws/links/test_datasync.py\nindex cd7f5be5cbe63..79c8469b701f7 100644\n--- a/providers/amazon/tests/unit/amazon/aws/links/test_datasync.py\n+++ b/providers/amazon/tests/unit/amazon/aws/links/test_datasync.py\n@@ -34,7 +34,7 @@ class TestDataSyncTaskLink(BaseAwsLinksTestCase):\n     def test_extra_link(self, mock_supervisor_comms):\n         task_id = TASK_ID\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"us-east-1\",\n@@ -56,7 +56,7 @@ class TestDataSyncTaskExecutionLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"us-east-1\",\ndiff --git a/providers/amazon/tests/unit/amazon/aws/links/test_ec2.py b/providers/amazon/tests/unit/amazon/aws/links/test_ec2.py\nindex 79680d3caa136..f451c910058cf 100644\n--- a/providers/amazon/tests/unit/amazon/aws/links/test_ec2.py\n+++ b/providers/amazon/tests/unit/amazon/aws/links/test_ec2.py\n@@ -32,7 +32,7 @@ class TestEC2InstanceLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"eu-west-1\",\n@@ -66,7 +66,7 @@ def test_instance_id_filter(self):\n     def test_extra_link(self, mock_supervisor_comms):\n         instance_list = \",:\".join(self.INSTANCE_IDS)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"eu-west-1\",\ndiff --git a/providers/amazon/tests/unit/amazon/aws/links/test_emr.py b/providers/amazon/tests/unit/amazon/aws/links/test_emr.py\nindex cbe2544bd86db..feda067f7cc7a 100644\n--- a/providers/amazon/tests/unit/amazon/aws/links/test_emr.py\n+++ b/providers/amazon/tests/unit/amazon/aws/links/test_emr.py\n@@ -45,7 +45,7 @@ class TestEmrClusterLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"us-west-1\",\n@@ -82,7 +82,7 @@ class TestEmrLogsLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"eu-west-2\",\n@@ -127,7 +127,7 @@ def test_extra_link(self, mocked_emr_serverless_hook, mock_supervisor_comms):\n         mocked_client.get_dashboard_for_job_run.return_value = {\"url\": \"https://example.com/?authToken=1234\"}\n \n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"conn_id\": \"aws-test\",\n@@ -160,7 +160,7 @@ def test_extra_link(self, mocked_emr_serverless_hook, mock_supervisor_comms):\n         mocked_client.get_dashboard_for_job_run.return_value = {\"url\": \"https://example.com/?authToken=1234\"}\n \n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"conn_id\": \"aws-test\",\n@@ -254,7 +254,7 @@ class TestEmrServerlessS3LogsLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"us-west-1\",\n@@ -282,7 +282,7 @@ class TestEmrServerlessCloudWatchLogsLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"us-west-1\",\ndiff --git a/providers/amazon/tests/unit/amazon/aws/links/test_glue.py b/providers/amazon/tests/unit/amazon/aws/links/test_glue.py\nindex b73c65182f662..2b1f076e149df 100644\n--- a/providers/amazon/tests/unit/amazon/aws/links/test_glue.py\n+++ b/providers/amazon/tests/unit/amazon/aws/links/test_glue.py\n@@ -30,7 +30,7 @@ class TestGlueJobRunDetailsLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"ap-southeast-2\",\ndiff --git a/providers/amazon/tests/unit/amazon/aws/links/test_logs.py b/providers/amazon/tests/unit/amazon/aws/links/test_logs.py\nindex 8c642e55c1a6a..2c90eecd232ad 100644\n--- a/providers/amazon/tests/unit/amazon/aws/links/test_logs.py\n+++ b/providers/amazon/tests/unit/amazon/aws/links/test_logs.py\n@@ -30,7 +30,7 @@ class TestCloudWatchEventsLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"us-west-1\",\ndiff --git a/providers/amazon/tests/unit/amazon/aws/links/test_sagemaker.py b/providers/amazon/tests/unit/amazon/aws/links/test_sagemaker.py\nindex 25bae4efdfa62..f08d7df93d509 100644\n--- a/providers/amazon/tests/unit/amazon/aws/links/test_sagemaker.py\n+++ b/providers/amazon/tests/unit/amazon/aws/links/test_sagemaker.py\n@@ -31,7 +31,7 @@ class TestSageMakerTransformDetailsLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"sagemaker_transform_job_details\",\n                 value={\n                     \"region_name\": \"us-east-1\",\ndiff --git a/providers/amazon/tests/unit/amazon/aws/links/test_sagemaker_unified_studio.py b/providers/amazon/tests/unit/amazon/aws/links/test_sagemaker_unified_studio.py\nindex 1b3cbed1f142d..bb749727323e2 100644\n--- a/providers/amazon/tests/unit/amazon/aws/links/test_sagemaker_unified_studio.py\n+++ b/providers/amazon/tests/unit/amazon/aws/links/test_sagemaker_unified_studio.py\n@@ -30,7 +30,7 @@ class TestSageMakerUnifiedStudioLink(BaseAwsLinksTestCase):\n \n     def test_extra_link(self, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"us-east-1\",\ndiff --git a/providers/amazon/tests/unit/amazon/aws/links/test_step_function.py b/providers/amazon/tests/unit/amazon/aws/links/test_step_function.py\nindex ca4505855e9f8..acfad7e98e96c 100644\n--- a/providers/amazon/tests/unit/amazon/aws/links/test_step_function.py\n+++ b/providers/amazon/tests/unit/amazon/aws/links/test_step_function.py\n@@ -47,7 +47,7 @@ class TestStateMachineDetailsLink(BaseAwsLinksTestCase):\n     )\n     def test_extra_link(self, state_machine_arn, expected_url: str, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"eu-west-1\",\n@@ -82,7 +82,7 @@ class TestStateMachineExecutionsDetailsLink(BaseAwsLinksTestCase):\n     )\n     def test_extra_link(self, execution_arn, expected_url: str, mock_supervisor_comms):\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=self.link_class.key,\n                 value={\n                     \"region_name\": \"eu-west-1\",\ndiff --git a/providers/common/io/tests/unit/common/io/xcom/test_backend.py b/providers/common/io/tests/unit/common/io/xcom/test_backend.py\nindex 99fb46a66c7e5..50c71c99be938 100644\n--- a/providers/common/io/tests/unit/common/io/xcom/test_backend.py\n+++ b/providers/common/io/tests/unit/common/io/xcom/test_backend.py\n@@ -106,9 +106,7 @@ def test_value_db(self, task_instance, mock_supervisor_comms, session):\n         if AIRFLOW_V_3_0_PLUS:\n             # When using XComObjectStorageBackend, the value is stored in the db is serialized with json dumps\n             # so we need to mimic that same behavior below.\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n-                key=\"return_value\", value={\"key\": \"value\"}\n-            )\n+            mock_supervisor_comms.send.return_value = XComResult(key=\"return_value\", value={\"key\": \"value\"})\n \n         value = XCom.get_value(\n             key=XCOM_RETURN_KEY,\n@@ -169,7 +167,7 @@ def test_value_storage(self, task_instance, mock_supervisor_comms, session):\n         assert p.exists() is True\n \n         if AIRFLOW_V_3_0_PLUS:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=XCOM_RETURN_KEY, value={\"key\": \"bigvaluebigvaluebigvalue\" * 100}\n             )\n \n@@ -213,7 +211,12 @@ def test_clear(self, task_instance, session, mock_supervisor_comms):\n         )\n \n         if AIRFLOW_V_3_0_PLUS:\n-            path = mock_supervisor_comms.send_request.call_args_list[-1].kwargs[\"msg\"].value\n+            if hasattr(mock_supervisor_comms, \"send_request\"):\n+                # Back-compat of task-sdk. Only affects us when we manually create these objects in tests.\n+                last_call = mock_supervisor_comms.send_request.call_args_list[-1]\n+            else:\n+                last_call = mock_supervisor_comms.send.call_args_list[-1]\n+            path = (last_call.kwargs.get(\"msg\") or last_call.args[0]).value\n             XComModel.set(\n                 key=XCOM_RETURN_KEY,\n                 value=path,\n@@ -251,7 +254,7 @@ def test_clear(self, task_instance, session, mock_supervisor_comms):\n         assert p.exists() is True\n \n         if AIRFLOW_V_3_0_PLUS:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=XCOM_RETURN_KEY, value={\"key\": \"superlargevalue\" * 100}\n             )\n         value = XCom.get_value(\n@@ -261,7 +264,7 @@ def test_clear(self, task_instance, session, mock_supervisor_comms):\n         assert value\n \n         if AIRFLOW_V_3_0_PLUS:\n-            mock_supervisor_comms.get_message.return_value = XComResult(key=XCOM_RETURN_KEY, value=path)\n+            mock_supervisor_comms.send.return_value = XComResult(key=XCOM_RETURN_KEY, value=path)\n             XCom.delete(\n                 dag_id=task_instance.dag_id,\n                 task_id=task_instance.task_id,\n@@ -356,7 +359,7 @@ def test_compression(self, task_instance, session, mock_supervisor_comms):\n         assert data.endswith(\".gz\")\n \n         if AIRFLOW_V_3_0_PLUS:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=XCOM_RETURN_KEY, value={\"key\": \"superlargevalue\" * 100}\n             )\n \ndiff --git a/providers/dbt/cloud/tests/unit/dbt/cloud/operators/test_dbt.py b/providers/dbt/cloud/tests/unit/dbt/cloud/operators/test_dbt.py\nindex 9e8408c56955c..616423d5ba984 100644\n--- a/providers/dbt/cloud/tests/unit/dbt/cloud/operators/test_dbt.py\n+++ b/providers/dbt/cloud/tests/unit/dbt/cloud/operators/test_dbt.py\n@@ -665,7 +665,7 @@ def test_run_job_operator_link(\n         ti.xcom_push(key=\"job_run_url\", value=_run_response[\"data\"][\"href\"])\n \n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"job_run_url\",\n                 value=EXPECTED_JOB_RUN_OP_EXTRA_LINK.format(\n                     account_id=account_id or DEFAULT_ACCOUNT_ID,\ndiff --git a/providers/google/tests/unit/google/cloud/links/test_cloud_run.py b/providers/google/tests/unit/google/cloud/links/test_cloud_run.py\nindex 7b115655102e2..5f3c348698c19 100644\n--- a/providers/google/tests/unit/google/cloud/links/test_cloud_run.py\n+++ b/providers/google/tests/unit/google/cloud/links/test_cloud_run.py\n@@ -68,7 +68,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task, log_uri=TEST_LOG_URI)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value=TEST_LOG_URI,\n             )\ndiff --git a/providers/google/tests/unit/google/cloud/links/test_dataplex.py b/providers/google/tests/unit/google/cloud/links/test_dataplex.py\nindex 3ec2905751390..babdf9127c8e1 100644\n--- a/providers/google/tests/unit/google/cloud/links/test_dataplex.py\n+++ b/providers/google/tests/unit/google/cloud/links/test_dataplex.py\n@@ -123,7 +123,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         link.persist(context={\"ti\": ti}, task_instance=ti.task)\n \n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"lake_id\": ti.task.lake_id,\n@@ -153,7 +153,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"project_id\": ti.task.project_id,\n@@ -183,7 +183,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"lake_id\": ti.task.lake_id,\n@@ -212,7 +212,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"entry_group_id\": ti.task.entry_group_id,\n@@ -242,7 +242,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"location\": ti.task.location,\n@@ -270,7 +270,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"entry_type_id\": ti.task.entry_type_id,\n@@ -300,7 +300,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"location\": ti.task.location,\n@@ -328,7 +328,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"aspect_type_id\": ti.task.aspect_type_id,\n@@ -358,7 +358,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"location\": ti.task.location,\n@@ -387,7 +387,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"entry_id\": ti.task.entry_id,\ndiff --git a/providers/google/tests/unit/google/cloud/links/test_translate.py b/providers/google/tests/unit/google/cloud/links/test_translate.py\nindex ddfc3e205e8e2..640b958a3841e 100644\n--- a/providers/google/tests/unit/google/cloud/links/test_translate.py\n+++ b/providers/google/tests/unit/google/cloud/links/test_translate.py\n@@ -62,7 +62,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task, dataset_id=DATASET, project_id=GCP_PROJECT_ID)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\"location\": ti.task.location, \"dataset_id\": DATASET, \"project_id\": GCP_PROJECT_ID},\n             )\n@@ -85,7 +85,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n         session.commit()\n         link.persist(context={\"ti\": ti}, task_instance=ti.task, project_id=GCP_PROJECT_ID)\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"project_id\": GCP_PROJECT_ID,\n@@ -121,7 +121,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n             project_id=GCP_PROJECT_ID,\n         )\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"location\": ti.task.location,\n@@ -158,7 +158,7 @@ def test_get_link(self, create_task_instance_of_operator, session, mock_supervis\n             project_id=GCP_PROJECT_ID,\n         )\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"key\",\n                 value={\n                     \"location\": ti.task.location,\ndiff --git a/providers/google/tests/unit/google/cloud/operators/test_dataproc.py b/providers/google/tests/unit/google/cloud/operators/test_dataproc.py\nindex e10eba067b6a8..986cfba4113f5 100644\n--- a/providers/google/tests/unit/google/cloud/operators/test_dataproc.py\n+++ b/providers/google/tests/unit/google/cloud/operators/test_dataproc.py\n@@ -1136,7 +1136,7 @@ def test_create_cluster_operator_extra_links(\n     assert operator_extra_link.name == \"Dataproc Cluster\"\n \n     if AIRFLOW_V_3_0_PLUS:\n-        mock_supervisor_comms.get_message.return_value = XComResult(\n+        mock_supervisor_comms.send.return_value = XComResult(\n             key=\"key\",\n             value=\"\",\n         )\n@@ -1146,7 +1146,7 @@ def test_create_cluster_operator_extra_links(\n     ti.xcom_push(key=\"dataproc_cluster\", value=DATAPROC_CLUSTER_EXPECTED)\n \n     if AIRFLOW_V_3_0_PLUS:\n-        mock_supervisor_comms.get_message.return_value = XComResult(\n+        mock_supervisor_comms.send.return_value = XComResult(\n             key=\"key\",\n             value={\"cluster_id\": \"cluster_name\", \"project_id\": \"test-project\", \"region\": \"test-location\"},\n         )\n@@ -2021,7 +2021,7 @@ def test_submit_job_operator_extra_links(\n     assert operator_extra_link.name == \"Dataproc Job\"\n \n     if AIRFLOW_V_3_0_PLUS:\n-        mock_supervisor_comms.get_message.return_value = XComResult(\n+        mock_supervisor_comms.send.return_value = XComResult(\n             key=\"dataproc_job\",\n             value=\"\",\n         )\n@@ -2032,7 +2032,7 @@ def test_submit_job_operator_extra_links(\n     ti.xcom_push(key=\"dataproc_job\", value=DATAPROC_JOB_EXPECTED)\n \n     if AIRFLOW_V_3_0_PLUS:\n-        mock_supervisor_comms.get_message.return_value = XComResult(\n+        mock_supervisor_comms.send.return_value = XComResult(\n             key=\"dataproc_job\",\n             value=DATAPROC_JOB_EXPECTED,\n         )\n@@ -2237,7 +2237,7 @@ def test_update_cluster_operator_extra_links(\n     assert operator_extra_link.name == \"Dataproc Cluster\"\n \n     if AIRFLOW_V_3_0_PLUS:\n-        mock_supervisor_comms.get_message.return_value = XComResult(\n+        mock_supervisor_comms.send.return_value = XComResult(\n             key=\"dataproc_cluster\",\n             value=\"\",\n         )\n@@ -2247,7 +2247,7 @@ def test_update_cluster_operator_extra_links(\n     ti.xcom_push(key=\"dataproc_cluster\", value=DATAPROC_CLUSTER_EXPECTED)\n \n     if AIRFLOW_V_3_0_PLUS:\n-        mock_supervisor_comms.get_message.return_value = XComResult(\n+        mock_supervisor_comms.send.return_value = XComResult(\n             key=\"dataproc_cluster\",\n             value=DATAPROC_CLUSTER_EXPECTED,\n         )\n@@ -2463,7 +2463,7 @@ def test_instantiate_workflow_operator_extra_links(\n     assert operator_extra_link.name == \"Dataproc Workflow\"\n \n     if AIRFLOW_V_3_0_PLUS:\n-        mock_supervisor_comms.get_message.return_value = XComResult(\n+        mock_supervisor_comms.send.return_value = XComResult(\n             key=\"dataproc_workflow\",\n             value=\"\",\n         )\n@@ -2472,7 +2472,7 @@ def test_instantiate_workflow_operator_extra_links(\n \n     ti.xcom_push(key=\"dataproc_workflow\", value=DATAPROC_WORKFLOW_EXPECTED)\n     if AIRFLOW_V_3_0_PLUS:\n-        mock_supervisor_comms.get_message.return_value = XComResult(\n+        mock_supervisor_comms.send.return_value = XComResult(\n             key=\"dataproc_workflow\",\n             value=DATAPROC_WORKFLOW_EXPECTED,\n         )\n@@ -3148,7 +3148,7 @@ def test_instantiate_inline_workflow_operator_extra_links(\n     operator_extra_link = deserialized_dag.tasks[0].operator_extra_links[0]\n     assert operator_extra_link.name == \"Dataproc Workflow\"\n     if AIRFLOW_V_3_0_PLUS:\n-        mock_supervisor_comms.get_message.return_value = XComResult(\n+        mock_supervisor_comms.send.return_value = XComResult(\n             key=\"dataproc_workflow\",\n             value=\"\",\n         )\n@@ -3157,7 +3157,7 @@ def test_instantiate_inline_workflow_operator_extra_links(\n \n     ti.xcom_push(key=\"dataproc_workflow\", value=DATAPROC_WORKFLOW_EXPECTED)\n     if AIRFLOW_V_3_0_PLUS:\n-        mock_supervisor_comms.get_message.return_value = XComResult(\n+        mock_supervisor_comms.send.return_value = XComResult(\n             key=\"dataproc_workflow\", value=DATAPROC_WORKFLOW_EXPECTED\n         )\n \ndiff --git a/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_data_factory.py b/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_data_factory.py\nindex 788f52efc3772..23d7d18d4a407 100644\n--- a/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_data_factory.py\n+++ b/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_data_factory.py\n@@ -253,7 +253,7 @@ def test_run_pipeline_operator_link(\n         ti.xcom_push(key=\"run_id\", value=PIPELINE_RUN_RESPONSE[\"run_id\"])\n \n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"run_id\",\n                 value=PIPELINE_RUN_RESPONSE[\"run_id\"],\n             )\ndiff --git a/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_synapse.py b/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_synapse.py\nindex 45c538c2c400c..62a1021ead8a5 100644\n--- a/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_synapse.py\n+++ b/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_synapse.py\n@@ -292,7 +292,7 @@ def test_run_pipeline_operator_link(self, create_task_instance_of_operator, mock\n \n         ti.xcom_push(key=\"run_id\", value=PIPELINE_RUN_RESPONSE[\"run_id\"])\n         if AIRFLOW_V_3_0_PLUS and mock_supervisor_comms:\n-            mock_supervisor_comms.get_message.return_value = XComResult(\n+            mock_supervisor_comms.send.return_value = XComResult(\n                 key=\"run_id\",\n                 value=PIPELINE_RUN_RESPONSE[\"run_id\"],\n             )\ndiff --git a/task-sdk/tests/task_sdk/bases/test_sensor.py b/task-sdk/tests/task_sdk/bases/test_sensor.py\nindex 2c0a82783d4d6..d0e2b430d2c74 100644\n--- a/task-sdk/tests/task_sdk/bases/test_sensor.py\n+++ b/task-sdk/tests/task_sdk/bases/test_sensor.py\n@@ -205,7 +205,7 @@ def test_fail_with_reschedule(self, run_task, make_sensor, time_machine, mock_su\n         time_machine.coordinates.shift(sensor.poke_interval)\n \n         # Mocking values from DB/API-server\n-        mock_supervisor_comms.get_message.return_value = TaskRescheduleStartDate(start_date=date1)\n+        mock_supervisor_comms.send.return_value = TaskRescheduleStartDate(start_date=date1)\n         state, msg, error = run_task(task=sensor, context_update={\"task_reschedule_count\": 1})\n \n         assert state == State.FAILED\n@@ -227,7 +227,7 @@ def test_soft_fail_with_reschedule(self, run_task, make_sensor, time_machine, mo\n         time_machine.coordinates.shift(sensor.poke_interval)\n \n         # Mocking values from DB/API-server\n-        mock_supervisor_comms.get_message.return_value = TaskRescheduleStartDate(start_date=date1)\n+        mock_supervisor_comms.send.return_value = TaskRescheduleStartDate(start_date=date1)\n         state, msg, _ = run_task(task=sensor, context_update={\"task_reschedule_count\": 1})\n         assert state == State.SKIPPED\n \n@@ -254,7 +254,7 @@ def run_duration():\n             return (timezone.utcnow() - task_start_date).total_seconds()\n \n         new_interval = 0\n-        mock_supervisor_comms.get_message.return_value = TaskRescheduleStartDate(start_date=task_start_date)\n+        mock_supervisor_comms.send.return_value = TaskRescheduleStartDate(start_date=task_start_date)\n \n         # loop poke returns false\n         for _poke_count in range(1, false_count + 1):\n@@ -516,9 +516,9 @@ def _run_task():\n             # For timeout calculation, we need to use the first reschedule date\n             # This ensures the timeout is calculated from the start of the task\n             if test_state[\"first_reschedule_date\"] is None:\n-                mock_supervisor_comms.get_message.return_value = TaskRescheduleStartDate(start_date=None)\n+                mock_supervisor_comms.send.return_value = TaskRescheduleStartDate(start_date=None)\n             else:\n-                mock_supervisor_comms.get_message.return_value = TaskRescheduleStartDate(\n+                mock_supervisor_comms.send.return_value = TaskRescheduleStartDate(\n                     start_date=test_state[\"first_reschedule_date\"]\n                 )\n \ndiff --git a/task-sdk/tests/task_sdk/definitions/conftest.py b/task-sdk/tests/task_sdk/definitions/conftest.py\nindex c0ac385628388..3f89f34b4d2da 100644\n--- a/task-sdk/tests/task_sdk/definitions/conftest.py\n+++ b/task-sdk/tests/task_sdk/definitions/conftest.py\n@@ -36,12 +36,12 @@ def run(dag: DAG, task_id: str, map_index: int):\n \n         log = structlog.get_logger(__name__)\n \n-        mock_supervisor_comms.send_request.reset_mock()\n+        mock_supervisor_comms.send.reset_mock()\n         ti = create_runtime_ti(dag.task_dict[task_id], map_index=map_index)\n         run(ti, ti.get_template_context(), log)\n \n-        for call in mock_supervisor_comms.send_request.mock_calls:\n-            msg = call.kwargs[\"msg\"]\n+        for call in mock_supervisor_comms.send.mock_calls:\n+            msg = call.kwargs.get(\"msg\") or call.args[0]\n             if isinstance(msg, (TaskState, SucceedTask)):\n                 return msg.state\n         raise RuntimeError(\"Unable to find call to TaskState\")\ndiff --git a/task-sdk/tests/task_sdk/definitions/test_asset_decorators.py b/task-sdk/tests/task_sdk/definitions/test_asset_decorators.py\nindex 264cfee06290a..c066b70d99c9f 100644\n--- a/task-sdk/tests/task_sdk/definitions/test_asset_decorators.py\n+++ b/task-sdk/tests/task_sdk/definitions/test_asset_decorators.py\n@@ -295,7 +295,7 @@ def test_determine_kwargs(\n             example_asset_func_with_valid_arg_as_inlet_asset\n         )\n \n-        mock_supervisor_comms.get_message.side_effect = [\n+        mock_supervisor_comms.send.side_effect = [\n             AssetResult(\n                 name=\"example_asset_func\",\n                 uri=\"s3://bucket/object\",\n@@ -326,12 +326,9 @@ def test_determine_kwargs(\n         }\n \n         assert mock_supervisor_comms.mock_calls == [\n-            mock.call.send_request(mock.ANY, GetAssetByName(name=\"example_asset_func\")),\n-            mock.call.get_message(),\n-            mock.call.send_request(mock.ANY, GetAssetByName(name=\"inlet_asset_1\")),\n-            mock.call.get_message(),\n-            mock.call.send_request(mock.ANY, GetAssetByName(name=\"inlet_asset_2\")),\n-            mock.call.get_message(),\n+            mock.call.send(GetAssetByName(name=\"example_asset_func\")),\n+            mock.call.send(GetAssetByName(name=\"inlet_asset_1\")),\n+            mock.call.send(GetAssetByName(name=\"inlet_asset_2\")),\n         ]\n \n     @mock.patch(\"airflow.sdk.execution_time.task_runner.SUPERVISOR_COMMS\", create=True)\n@@ -342,7 +339,7 @@ def test_determine_kwargs_defaults(\n     ):\n         asset_definition = asset(schedule=None)(example_asset_func_with_valid_arg_as_inlet_asset_and_default)\n \n-        mock_supervisor_comms.get_message.side_effect = [\n+        mock_supervisor_comms.send.side_effect = [\n             AssetResult(name=\"inlet_asset_1\", uri=\"s3://bucket/object1\", group=\"asset\", extra=None),\n         ]\n \n@@ -360,6 +357,5 @@ def test_determine_kwargs_defaults(\n         }\n \n         assert mock_supervisor_comms.mock_calls == [\n-            mock.call.send_request(mock.ANY, GetAssetByName(name=\"inlet_asset_1\")),\n-            mock.call.get_message(),\n+            mock.call.send(GetAssetByName(name=\"inlet_asset_1\")),\n         ]\ndiff --git a/task-sdk/tests/task_sdk/definitions/test_connections.py b/task-sdk/tests/task_sdk/definitions/test_connections.py\nindex 102e85d36b30d..3bbb63a769788 100644\n--- a/task-sdk/tests/task_sdk/definitions/test_connections.py\n+++ b/task-sdk/tests/task_sdk/definitions/test_connections.py\n@@ -104,7 +104,7 @@ def test_get_uri(self):\n \n     def test_conn_get(self, mock_supervisor_comms):\n         conn_result = ConnectionResult(conn_id=\"mysql_conn\", conn_type=\"mysql\", host=\"mysql\", port=3306)\n-        mock_supervisor_comms.get_message.return_value = conn_result\n+        mock_supervisor_comms.send.return_value = conn_result\n \n         conn = Connection.get(conn_id=\"mysql_conn\")\n         assert conn is not None\ndiff --git a/task-sdk/tests/task_sdk/definitions/test_mappedoperator.py b/task-sdk/tests/task_sdk/definitions/test_mappedoperator.py\nindex cdb9c954fd916..5c81b64b605b3 100644\n--- a/task-sdk/tests/task_sdk/definitions/test_mappedoperator.py\n+++ b/task-sdk/tests/task_sdk/definitions/test_mappedoperator.py\n@@ -251,7 +251,7 @@ def execute(self, context):\n         )\n         mapped = callable(mapped, task1.output)\n \n-    mock_supervisor_comms.get_message.return_value = XComResult(key=\"return_value\", value=[\"{{ ds }}\"])\n+    mock_supervisor_comms.send.return_value = XComResult(key=\"return_value\", value=[\"{{ ds }}\"])\n \n     mapped_ti = create_runtime_ti(task=mapped, map_index=0, upstream_map_indexes={task1.task_id: 1})\n \n@@ -299,7 +299,7 @@ def test_expand_kwargs_render_template_fields_validating_operator(\n         task1 = BaseOperator(task_id=\"op1\")\n         mapped = MockOperator.partial(task_id=\"a\", arg2=\"{{ ti.task_id }}\").expand_kwargs(task1.output)\n \n-    mock_supervisor_comms.get_message.return_value = XComResult(\n+    mock_supervisor_comms.send.return_value = XComResult(\n         key=\"return_value\", value=[{\"arg1\": \"{{ ds }}\"}, {\"arg1\": 2}]\n     )\n \n@@ -427,16 +427,14 @@ def show(number, letter):\n \n         show.expand(number=emit_numbers(), letter=emit_letters())\n \n-    def xcom_get():\n-        # TODO: Tidy this after #45927 is reopened and fixed properly\n-        last_request = mock_supervisor_comms.send_request.mock_calls[-1].kwargs[\"msg\"]\n-        if not isinstance(last_request, GetXCom):\n+    def xcom_get(msg):\n+        if not isinstance(msg, GetXCom):\n             return mock.DEFAULT\n-        task = dag.get_task(last_request.task_id)\n+        task = dag.get_task(msg.task_id)\n         value = task.python_callable()\n         return XComResult(key=\"return_value\", value=value)\n \n-    mock_supervisor_comms.get_message.side_effect = xcom_get\n+    mock_supervisor_comms.send.side_effect = xcom_get\n \n     states = [run_ti(dag, \"show\", map_index) for map_index in range(6)]\n     assert states == [TaskInstanceState.SUCCESS] * 6\n@@ -467,16 +465,14 @@ def show(a, b):\n         emit_task = emit_numbers()\n         show.expand(a=emit_task, b=emit_task)\n \n-    def xcom_get():\n-        # TODO: Tidy this after #45927 is reopened and fixed properly\n-        last_request = mock_supervisor_comms.send_request.mock_calls[-1].kwargs[\"msg\"]\n-        if not isinstance(last_request, GetXCom):\n+    def xcom_get(msg):\n+        if not isinstance(msg, GetXCom):\n             return mock.DEFAULT\n-        task = dag.get_task(last_request.task_id)\n+        task = dag.get_task(msg.task_id)\n         value = task.python_callable()\n         return XComResult(key=\"return_value\", value=value)\n \n-    mock_supervisor_comms.get_message.side_effect = xcom_get\n+    mock_supervisor_comms.send.side_effect = xcom_get\n \n     states = [run_ti(dag, \"show\", map_index) for map_index in range(4)]\n     assert states == [TaskInstanceState.SUCCESS] * 4\n@@ -594,22 +590,20 @@ def tg(va):\n         # Aggregates results from task group.\n         t.override(task_id=\"t3\")(tg1)\n \n-    def xcom_get():\n-        # TODO: Tidy this after #45927 is reopened and fixed properly\n-        last_request = mock_supervisor_comms.send_request.mock_calls[-1].kwargs[\"msg\"]\n-        if not isinstance(last_request, GetXCom):\n+    def xcom_get(msg):\n+        if not isinstance(msg, GetXCom):\n             return mock.DEFAULT\n-        key = (last_request.task_id, last_request.map_index)\n+        key = (msg.task_id, msg.map_index)\n         if key in expected_values:\n             value = expected_values[key]\n             return XComResult(key=\"return_value\", value=value)\n-        if last_request.map_index is None:\n+        if msg.map_index is None:\n             # Get all mapped XComValues for this ti\n-            value = [v for k, v in expected_values.items() if k[0] == last_request.task_id]\n+            value = [v for k, v in expected_values.items() if k[0] == msg.task_id]\n             return XComResult(key=\"return_value\", value=value)\n         return mock.DEFAULT\n \n-    mock_supervisor_comms.get_message.side_effect = xcom_get\n+    mock_supervisor_comms.send.side_effect = xcom_get\n \n     expected_values = {\n         (\"tg.t1\", 0): [\"a\", \"b\"],\n@@ -683,10 +677,9 @@ def group(x):\n             ti.task.execute(context)\n \n     assert ti\n-    mock_supervisor_comms.send_request.assert_has_calls(\n+    mock_supervisor_comms.send.assert_has_calls(\n         [\n             mock.call(\n-                log=mock.ANY,\n                 msg=SetXCom(\n                     key=\"skipmixin_key\",\n                     value={\"skipped\": [\"group.empty_task\"]},\ndiff --git a/task-sdk/tests/task_sdk/definitions/test_variables.py b/task-sdk/tests/task_sdk/definitions/test_variables.py\nindex 8bcf9ef28199a..c85924df6a6d9 100644\n--- a/task-sdk/tests/task_sdk/definitions/test_variables.py\n+++ b/task-sdk/tests/task_sdk/definitions/test_variables.py\n@@ -51,7 +51,7 @@ class TestVariables:\n     )\n     def test_var_get(self, deserialize_json, value, expected_value, mock_supervisor_comms):\n         var_result = VariableResult(key=\"my_key\", value=value)\n-        mock_supervisor_comms.get_message.return_value = var_result\n+        mock_supervisor_comms.send.return_value = var_result\n \n         var = Variable.get(key=\"my_key\", deserialize_json=deserialize_json)\n         assert var is not None\n@@ -83,8 +83,7 @@ def test_var_set(self, key, value, description, serialize_json, mock_supervisor_\n         if serialize_json:\n             expected_value = json.dumps(value, indent=2)\n \n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_called_once_with(\n             msg=PutVariable(\n                 key=key, value=expected_value, description=description, serialize_json=serialize_json\n             ),\ndiff --git a/task-sdk/tests/task_sdk/definitions/test_xcom_arg.py b/task-sdk/tests/task_sdk/definitions/test_xcom_arg.py\nindex d313b45dae265..73468dcb9e12b 100644\n--- a/task-sdk/tests/task_sdk/definitions/test_xcom_arg.py\n+++ b/task-sdk/tests/task_sdk/definitions/test_xcom_arg.py\n@@ -52,7 +52,7 @@ def pull(value):\n     assert set(dag.task_dict) == {\"push\", \"pull\"}\n \n     # Mock xcom result from push task\n-    mock_supervisor_comms.get_message.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n+    mock_supervisor_comms.send.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n \n     for map_index in range(3):\n         assert run_ti(dag, \"pull\", map_index) == TaskInstanceState.SUCCESS\n@@ -81,7 +81,7 @@ def c_to_none(v):\n         pull.expand(value=push().map(c_to_none))\n \n     # Mock xcom result from push task\n-    mock_supervisor_comms.get_message.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n+    mock_supervisor_comms.send.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n \n     # Run \"pull\". This should automatically convert \"c\" to None.\n     for map_index in range(3):\n@@ -111,7 +111,7 @@ def c_to_none(v):\n         pull.expand_kwargs(push().map(c_to_none))\n \n     # Mock xcom result from push task\n-    mock_supervisor_comms.get_message.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n+    mock_supervisor_comms.send.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n \n     # The first two \"pull\" tis should succeed.\n     for map_index in range(2):\n@@ -165,7 +165,7 @@ def does_not_work_with_c(v):\n         pull.expand_kwargs(push().map(does_not_work_with_c))\n \n     # Mock xcom result from push task\n-    mock_supervisor_comms.get_message.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n+    mock_supervisor_comms.send.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n     # The third one (for \"c\") will fail.\n     assert run_ti(dag, \"pull\", 2) == TaskInstanceState.FAILED\n \n@@ -209,7 +209,7 @@ def pull(value):\n         pull.expand_kwargs(converted)\n \n     # Mock xcom result from push task\n-    mock_supervisor_comms.get_message.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n+    mock_supervisor_comms.send.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n \n     # Now \"pull\" should apply the mapping functions in order.\n     for map_index in range(3):\n@@ -243,20 +243,18 @@ def convert_zipped(zipped):\n \n         pull.expand(value=combined.map(convert_zipped))\n \n-    def xcom_get():\n-        # TODO: Tidy this after #45927 is reopened and fixed properly\n-        last_request = mock_supervisor_comms.send_request.mock_calls[-1].kwargs[\"msg\"]\n-        if not isinstance(last_request, GetXCom):\n+    def xcom_get(msg):\n+        if not isinstance(msg, GetXCom):\n             return mock.DEFAULT\n-        if last_request.task_id == \"push_letters\":\n+        if msg.task_id == \"push_letters\":\n             value = push_letters.function()\n             return XComResult(key=\"return_value\", value=value)\n-        if last_request.task_id == \"push_numbers\":\n+        if msg.task_id == \"push_numbers\":\n             value = push_numbers.function()\n             return XComResult(key=\"return_value\", value=value)\n         return mock.DEFAULT\n \n-    mock_supervisor_comms.get_message.side_effect = xcom_get\n+    mock_supervisor_comms.send.side_effect = xcom_get\n \n     # Run \"pull\".\n     for map_index in range(4):\n@@ -286,7 +284,7 @@ def skip_c(v):\n         forward.expand_kwargs(push().map(skip_c))\n \n     # Mock xcom result from push task\n-    mock_supervisor_comms.get_message.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n+    mock_supervisor_comms.send.return_value = XComResult(key=\"return_value\", value=[\"a\", \"b\", \"c\"])\n \n     # Run \"forward\". This should automatically skip \"c\".\n     states = [run_ti(dag, \"forward\", map_index) for map_index in range(3)]\n@@ -341,20 +339,18 @@ def pull_all(value):\n         pull_one.expand(value=pushed_values)\n         pull_all(pushed_values)\n \n-    def xcom_get():\n-        # TODO: Tidy this after #45927 is reopened and fixed properly\n-        last_request = mock_supervisor_comms.send_request.mock_calls[-1].kwargs[\"msg\"]\n-        if not isinstance(last_request, GetXCom):\n+    def xcom_get(msg):\n+        if not isinstance(msg, GetXCom):\n             return mock.DEFAULT\n-        if last_request.task_id == \"push_letters\":\n+        if msg.task_id == \"push_letters\":\n             value = push_letters.function()\n             return XComResult(key=\"return_value\", value=value)\n-        if last_request.task_id == \"push_numbers\":\n+        if msg.task_id == \"push_numbers\":\n             value = push_numbers.function()\n             return XComResult(key=\"return_value\", value=value)\n         return mock.DEFAULT\n \n-    mock_supervisor_comms.get_message.side_effect = xcom_get\n+    mock_supervisor_comms.send.side_effect = xcom_get\n \n     # Run \"pull_one\" and \"pull_all\".\n     assert run_ti(dag, \"pull_all\", None) == TaskInstanceState.SUCCESS\ndiff --git a/task-sdk/tests/task_sdk/execution_time/test_comms.py b/task-sdk/tests/task_sdk/execution_time/test_comms.py\nnew file mode 100644\nindex 0000000000000..5adaa2562abc7\n--- /dev/null\n+++ b/task-sdk/tests/task_sdk/execution_time/test_comms.py\n@@ -0,0 +1,83 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+from __future__ import annotations\n+\n+import uuid\n+from socket import socketpair\n+\n+import msgspec\n+import pytest\n+\n+from airflow.sdk.execution_time.comms import BundleInfo, StartupDetails, _ResponseFrame\n+from airflow.sdk.execution_time.task_runner import CommsDecoder\n+from airflow.utils import timezone\n+\n+\n+class TestCommsDecoder:\n+    \"\"\"Test the communication between the subprocess and the \"supervisor\".\"\"\"\n+\n+    @pytest.mark.usefixtures(\"disable_capturing\")\n+    def test_recv_StartupDetails(self):\n+        r, w = socketpair()\n+\n+        msg = {\n+            \"type\": \"StartupDetails\",\n+            \"ti\": {\n+                \"id\": uuid.UUID(\"4d828a62-a417-4936-a7a6-2b3fabacecab\"),\n+                \"task_id\": \"a\",\n+                \"try_number\": 1,\n+                \"run_id\": \"b\",\n+                \"dag_id\": \"c\",\n+            },\n+            \"ti_context\": {\n+                \"dag_run\": {\n+                    \"dag_id\": \"c\",\n+                    \"run_id\": \"b\",\n+                    \"logical_date\": \"2024-12-01T01:00:00Z\",\n+                    \"data_interval_start\": \"2024-12-01T00:00:00Z\",\n+                    \"data_interval_end\": \"2024-12-01T01:00:00Z\",\n+                    \"start_date\": \"2024-12-01T01:00:00Z\",\n+                    \"run_after\": \"2024-12-01T01:00:00Z\",\n+                    \"end_date\": None,\n+                    \"run_type\": \"manual\",\n+                    \"conf\": None,\n+                    \"consumed_asset_events\": [],\n+                },\n+                \"max_tries\": 0,\n+                \"should_retry\": False,\n+                \"variables\": None,\n+                \"connections\": None,\n+            },\n+            \"file\": \"/dev/null\",\n+            \"start_date\": \"2024-12-01T01:00:00Z\",\n+            \"dag_rel_path\": \"/dev/null\",\n+            \"bundle_info\": {\"name\": \"any-name\", \"version\": \"any-version\"},\n+        }\n+        bytes = msgspec.msgpack.encode(_ResponseFrame(0, msg, None))\n+        w.sendall(len(bytes).to_bytes(4, byteorder=\"big\") + bytes)\n+\n+        decoder = CommsDecoder(socket=r, log=None)\n+\n+        msg = decoder._get_response()\n+        assert isinstance(msg, StartupDetails)\n+        assert msg.ti.id == uuid.UUID(\"4d828a62-a417-4936-a7a6-2b3fabacecab\")\n+        assert msg.ti.task_id == \"a\"\n+        assert msg.ti.dag_id == \"c\"\n+        assert msg.dag_rel_path == \"/dev/null\"\n+        assert msg.bundle_info == BundleInfo(name=\"any-name\", version=\"any-version\")\n+        assert msg.start_date == timezone.datetime(2024, 12, 1, 1)\ndiff --git a/task-sdk/tests/task_sdk/execution_time/test_context.py b/task-sdk/tests/task_sdk/execution_time/test_context.py\nindex 59266da758892..6880c656b01cd 100644\n--- a/task-sdk/tests/task_sdk/execution_time/test_context.py\n+++ b/task-sdk/tests/task_sdk/execution_time/test_context.py\n@@ -190,7 +190,7 @@ def test_getattr_connection(self, mock_supervisor_comms):\n         # Conn from the supervisor / API Server\n         conn_result = ConnectionResult(conn_id=\"mysql_conn\", conn_type=\"mysql\", host=\"mysql\", port=3306)\n \n-        mock_supervisor_comms.get_message.return_value = conn_result\n+        mock_supervisor_comms.send.return_value = conn_result\n \n         # Fetch the connection; triggers __getattr__\n         conn = accessor.mysql_conn\n@@ -203,7 +203,7 @@ def test_get_method_valid_connection(self, mock_supervisor_comms):\n         accessor = ConnectionAccessor()\n         conn_result = ConnectionResult(conn_id=\"mysql_conn\", conn_type=\"mysql\", host=\"mysql\", port=3306)\n \n-        mock_supervisor_comms.get_message.return_value = conn_result\n+        mock_supervisor_comms.send.return_value = conn_result\n \n         conn = accessor.get(\"mysql_conn\")\n         assert conn == Connection(conn_id=\"mysql_conn\", conn_type=\"mysql\", host=\"mysql\", port=3306)\n@@ -216,7 +216,7 @@ def test_get_method_with_default(self, mock_supervisor_comms):\n             error=ErrorType.CONNECTION_NOT_FOUND, detail={\"conn_id\": \"nonexistent_conn\"}\n         )\n \n-        mock_supervisor_comms.get_message.return_value = error_response\n+        mock_supervisor_comms.send.return_value = error_response\n \n         conn = accessor.get(\"nonexistent_conn\", default_conn=default_conn)\n         assert conn == default_conn\n@@ -233,7 +233,7 @@ def test_getattr_connection_for_extra_dejson(self, mock_supervisor_comms):\n             extra='{\"extra_key\": \"extra_value\"}',\n         )\n \n-        mock_supervisor_comms.get_message.return_value = conn_result\n+        mock_supervisor_comms.send.return_value = conn_result\n \n         # Fetch the connection's dejson; triggers __getattr__\n         dejson = accessor.mysql_conn.extra_dejson\n@@ -251,7 +251,7 @@ def test_getattr_connection_for_extra_dejson_decode_error(self, mock_log, mock_s\n             conn_id=\"mysql_conn\", conn_type=\"mysql\", host=\"mysql\", port=3306, extra=\"This is not JSON!\"\n         )\n \n-        mock_supervisor_comms.get_message.return_value = conn_result\n+        mock_supervisor_comms.send.return_value = conn_result\n \n         # Fetch the connection's dejson; triggers __getattr__\n         dejson = accessor.mysql_conn.extra_dejson\n@@ -274,7 +274,7 @@ def test_getattr_variable(self, mock_supervisor_comms):\n         # Variable from the supervisor / API Server\n         var_result = VariableResult(key=\"test_key\", value=\"test_value\")\n \n-        mock_supervisor_comms.get_message.return_value = var_result\n+        mock_supervisor_comms.send.return_value = var_result\n \n         # Fetch the variable; triggers __getattr__\n         value = accessor.test_key\n@@ -286,7 +286,7 @@ def test_get_method_valid_variable(self, mock_supervisor_comms):\n         accessor = VariableAccessor(deserialize_json=False)\n         var_result = VariableResult(key=\"test_key\", value=\"test_value\")\n \n-        mock_supervisor_comms.get_message.return_value = var_result\n+        mock_supervisor_comms.send.return_value = var_result\n \n         val = accessor.get(\"test_key\")\n         assert val == var_result.value\n@@ -297,7 +297,7 @@ def test_get_method_with_default(self, mock_supervisor_comms):\n         accessor = VariableAccessor(deserialize_json=False)\n         error_response = ErrorResponse(error=ErrorType.VARIABLE_NOT_FOUND, detail={\"test_key\": \"test_value\"})\n \n-        mock_supervisor_comms.get_message.return_value = error_response\n+        mock_supervisor_comms.send.return_value = error_response\n \n         val = accessor.get(\"nonexistent_var_key\", default=\"default_value\")\n         assert val == \"default_value\"\n@@ -367,7 +367,7 @@ class TestOutletEventAccessor:\n         ),\n     )\n     def test_add(self, add_arg, key, asset_alias_events, mock_supervisor_comms):\n-        mock_supervisor_comms.get_message.return_value = AssetResponse(name=\"name\", uri=\"uri\", group=\"\")\n+        mock_supervisor_comms.send.return_value = AssetResponse(name=\"name\", uri=\"uri\", group=\"\")\n \n         outlet_event_accessor = OutletEventAccessor(key=key, extra={})\n         outlet_event_accessor.add(add_arg)\n@@ -398,7 +398,7 @@ def test_add(self, add_arg, key, asset_alias_events, mock_supervisor_comms):\n         ),\n     )\n     def test_add_with_db(self, add_arg, key, asset_alias_events, mock_supervisor_comms):\n-        mock_supervisor_comms.get_message.return_value = AssetResponse(name=\"name\", uri=\"uri\", group=\"\")\n+        mock_supervisor_comms.send.return_value = AssetResponse(name=\"name\", uri=\"uri\", group=\"\")\n \n         outlet_event_accessor = OutletEventAccessor(key=key, extra={\"not\": \"\"})\n         outlet_event_accessor.add(add_arg, extra={})\n@@ -497,11 +497,11 @@ def test_getitem_name_ref(\n         resolved_asset,\n         result_indexes,\n     ):\n-        mock_supervisor_comms.get_message.return_value = resolved_asset\n+        mock_supervisor_comms.send.return_value = resolved_asset\n         expected = [AssetEventDagRunReferenceResult.model_validate(event_data[i]) for i in result_indexes]\n         assert accessor[Asset.ref(name=name)] == expected\n-        assert len(mock_supervisor_comms.send_request.mock_calls) == 1\n-        assert mock_supervisor_comms.send_request.mock_calls[0].kwargs[\"msg\"] == GetAssetByName(name=name)\n+\n+        mock_supervisor_comms.send.assert_called_once_with(GetAssetByName(name=name, type=\"GetAssetByName\"))\n         assert _AssetRefResolutionMixin._asset_ref_cache\n \n     @pytest.mark.parametrize(\n@@ -520,11 +520,10 @@ def test_getitem_uri_ref(\n         resolved_asset,\n         result_indexes,\n     ):\n-        mock_supervisor_comms.get_message.return_value = resolved_asset\n+        mock_supervisor_comms.send.return_value = resolved_asset\n         expected = [AssetEventDagRunReferenceResult.model_validate(event_data[i]) for i in result_indexes]\n         assert accessor[Asset.ref(uri=uri)] == expected\n-        assert len(mock_supervisor_comms.send_request.mock_calls) == 1\n-        assert mock_supervisor_comms.send_request.mock_calls[0].kwargs[\"msg\"] == GetAssetByUri(uri=uri)\n+        mock_supervisor_comms.send.assert_called_once_with(GetAssetByUri(uri=uri))\n         assert _AssetRefResolutionMixin._asset_ref_cache\n \n     def test_source_task_instance_xcom_pull(self, mock_supervisor_comms, accessor):\n@@ -534,22 +533,19 @@ def test_source_task_instance_xcom_pull(self, mock_supervisor_comms, accessor):\n         assert source == AssetEventSourceTaskInstance(dag_id=\"d1\", task_id=\"t2\", run_id=\"r1\", map_index=-1)\n \n         mock_supervisor_comms.reset_mock()\n-        mock_supervisor_comms.get_message.side_effect = [\n+        mock_supervisor_comms.send.side_effect = [\n             XComResult(key=\"return_value\", value=\"__example_xcom_value__\"),\n         ]\n         assert source.xcom_pull() == \"__example_xcom_value__\"\n-        assert mock_supervisor_comms.send_request.mock_calls == [\n-            mock.call(\n-                log=mock.ANY,\n-                msg=GetXCom(\n-                    key=\"return_value\",\n-                    dag_id=\"d1\",\n-                    run_id=\"r1\",\n-                    task_id=\"t2\",\n-                    map_index=-1,\n-                ),\n+        mock_supervisor_comms.send.assert_called_once_with(\n+            msg=GetXCom(\n+                key=\"return_value\",\n+                dag_id=\"d1\",\n+                run_id=\"r1\",\n+                task_id=\"t2\",\n+                map_index=-1,\n             ),\n-        ]\n+        )\n \n \n TEST_ASSET = Asset(name=\"test_uri\", uri=\"test://test\")\n@@ -593,7 +589,7 @@ def test__get_item__asset_ref(self, access_key, asset, mock_supervisor_comms):\n         assert len(outlet_event_accessors) == 0\n \n         # Asset from the API Server via the supervisor\n-        mock_supervisor_comms.get_message.return_value = AssetResult(\n+        mock_supervisor_comms.send.return_value = AssetResult(\n             name=asset.name,\n             uri=asset.uri,\n             group=asset.group,\n@@ -628,7 +624,7 @@ def test_for_asset_alias(self, mocked__getitem__):\n class TestInletEventAccessor:\n     @pytest.fixture\n     def sample_inlet_evnets_accessor(self, mock_supervisor_comms):\n-        mock_supervisor_comms.get_message.side_effect = [\n+        mock_supervisor_comms.send.side_effect = [\n             AssetResult(name=\"test_uri\", uri=\"test://test\", group=\"asset\"),\n             AssetResult(name=\"test_uri\", uri=\"test://test\", group=\"asset\"),\n         ]\n@@ -656,7 +652,7 @@ def test__get_item__(self, key, sample_inlet_evnets_accessor, mock_supervisor_co\n             asset=AssetResponse(name=\"test\", uri=\"test\", group=\"asset\"),\n         )\n         events_result = AssetEventsResult(asset_events=[asset_event_resp])\n-        mock_supervisor_comms.get_message.side_effect = [events_result] * 4\n+        mock_supervisor_comms.send.side_effect = [events_result] * 4\n \n         assert sample_inlet_evnets_accessor[key] == [asset_event_resp]\n \n@@ -684,7 +680,7 @@ def test_for_asset_alias(self, mocked__getitem__, sample_inlet_evnets_accessor):\n         assert mocked__getitem__.call_args[0][0] == TEST_ASSET_ALIAS\n \n     def test_source_task_instance_xcom_pull(self, sample_inlet_evnets_accessor, mock_supervisor_comms):\n-        mock_supervisor_comms.get_message.side_effect = [\n+        mock_supervisor_comms.send.side_effect = [\n             AssetEventsResult(\n                 asset_events=[\n                     AssetEventResponse(\n@@ -707,9 +703,7 @@ def test_source_task_instance_xcom_pull(self, sample_inlet_evnets_accessor, mock\n             )\n         ]\n         events = sample_inlet_evnets_accessor[Asset.ref(name=\"test_uri\")]\n-        assert mock_supervisor_comms.send_request.mock_calls == [\n-            mock.call(log=mock.ANY, msg=GetAssetEventByAsset(name=\"test_uri\", uri=None)),\n-        ]\n+        mock_supervisor_comms.send.assert_called_once_with(GetAssetEventByAsset(name=\"test_uri\", uri=None))\n \n         assert len(events) == 2\n         assert events[1].source_task_instance is None\n@@ -723,19 +717,16 @@ def test_source_task_instance_xcom_pull(self, sample_inlet_evnets_accessor, mock\n         )\n \n         mock_supervisor_comms.reset_mock()\n-        mock_supervisor_comms.get_message.side_effect = [\n+        mock_supervisor_comms.send.side_effect = [\n             XComResult(key=\"return_value\", value=\"__example_xcom_value__\"),\n         ]\n         assert source.xcom_pull() == \"__example_xcom_value__\"\n-        assert mock_supervisor_comms.send_request.mock_calls == [\n-            mock.call(\n-                log=mock.ANY,\n-                msg=GetXCom(\n-                    key=\"return_value\",\n-                    dag_id=\"__dag__\",\n-                    run_id=\"__run__\",\n-                    task_id=\"__task__\",\n-                    map_index=0,\n-                ),\n+        mock_supervisor_comms.send.assert_called_once_with(\n+            msg=GetXCom(\n+                key=\"return_value\",\n+                dag_id=\"__dag__\",\n+                run_id=\"__run__\",\n+                task_id=\"__task__\",\n+                map_index=0,\n             ),\n-        ]\n+        )\ndiff --git a/task-sdk/tests/task_sdk/execution_time/test_lazy_sequence.py b/task-sdk/tests/task_sdk/execution_time/test_lazy_sequence.py\nindex e4943196a09da..65dd30b2c7c7b 100644\n--- a/task-sdk/tests/task_sdk/execution_time/test_lazy_sequence.py\n+++ b/task-sdk/tests/task_sdk/execution_time/test_lazy_sequence.py\n@@ -17,7 +17,7 @@\n \n from __future__ import annotations\n \n-from unittest.mock import ANY, Mock, call\n+from unittest.mock import Mock, call\n \n import pytest\n \n@@ -66,121 +66,109 @@ def deserialize_value(cls, xcom):\n \n \n def test_len(mock_supervisor_comms, lazy_sequence):\n-    mock_supervisor_comms.get_message.return_value = XComCountResponse(len=3)\n+    mock_supervisor_comms.send.return_value = XComCountResponse(len=3)\n     assert len(lazy_sequence) == 3\n-    assert mock_supervisor_comms.send_request.mock_calls == [\n-        call(log=ANY, msg=GetXComCount(key=\"return_value\", dag_id=\"dag\", task_id=\"task\", run_id=\"run\")),\n-    ]\n+    mock_supervisor_comms.send.assert_called_once_with(\n+        msg=GetXComCount(key=\"return_value\", dag_id=\"dag\", task_id=\"task\", run_id=\"run\"),\n+    )\n \n \n def test_iter(mock_supervisor_comms, lazy_sequence):\n     it = iter(lazy_sequence)\n \n-    mock_supervisor_comms.get_message.side_effect = [\n+    mock_supervisor_comms.send.side_effect = [\n         XComSequenceIndexResult(root=\"f\"),\n         ErrorResponse(error=ErrorType.XCOM_NOT_FOUND, detail={\"oops\": \"sorry!\"}),\n     ]\n     assert list(it) == [\"f\"]\n-    assert mock_supervisor_comms.send_request.mock_calls == [\n-        call(\n-            log=ANY,\n-            msg=GetXComSequenceItem(\n-                key=\"return_value\",\n-                dag_id=\"dag\",\n-                task_id=\"task\",\n-                run_id=\"run\",\n-                offset=0,\n+    mock_supervisor_comms.send.assert_has_calls(\n+        [\n+            call(\n+                msg=GetXComSequenceItem(\n+                    key=\"return_value\",\n+                    dag_id=\"dag\",\n+                    task_id=\"task\",\n+                    run_id=\"run\",\n+                    offset=0,\n+                ),\n             ),\n-        ),\n-        call(\n-            log=ANY,\n-            msg=GetXComSequenceItem(\n-                key=\"return_value\",\n-                dag_id=\"dag\",\n-                task_id=\"task\",\n-                run_id=\"run\",\n-                offset=1,\n+            call(\n+                msg=GetXComSequenceItem(\n+                    key=\"return_value\",\n+                    dag_id=\"dag\",\n+                    task_id=\"task\",\n+                    run_id=\"run\",\n+                    offset=1,\n+                ),\n             ),\n-        ),\n-    ]\n+        ]\n+    )\n \n \n def test_getitem_index(mock_supervisor_comms, lazy_sequence):\n-    mock_supervisor_comms.get_message.return_value = XComSequenceIndexResult(root=\"f\")\n+    mock_supervisor_comms.send.return_value = XComSequenceIndexResult(root=\"f\")\n     assert lazy_sequence[4] == \"f\"\n-    assert mock_supervisor_comms.send_request.mock_calls == [\n-        call(\n-            log=ANY,\n-            msg=GetXComSequenceItem(\n-                key=\"return_value\",\n-                dag_id=\"dag\",\n-                task_id=\"task\",\n-                run_id=\"run\",\n-                offset=4,\n-            ),\n+    mock_supervisor_comms.send.assert_called_once_with(\n+        GetXComSequenceItem(\n+            key=\"return_value\",\n+            dag_id=\"dag\",\n+            task_id=\"task\",\n+            run_id=\"run\",\n+            offset=4,\n         ),\n-    ]\n+    )\n \n \n @conf_vars({(\"core\", \"xcom_backend\"): \"task_sdk.execution_time.test_lazy_sequence.CustomXCom\"})\n def test_getitem_calls_correct_deserialise(monkeypatch, mock_supervisor_comms, lazy_sequence):\n-    mock_supervisor_comms.get_message.return_value = XComSequenceIndexResult(root=\"some-value\")\n+    mock_supervisor_comms.send.return_value = XComSequenceIndexResult(root=\"some-value\")\n \n     xcom = resolve_xcom_backend()\n     assert xcom.__name__ == \"CustomXCom\"\n     monkeypatch.setattr(airflow.sdk.execution_time.xcom, \"XCom\", xcom)\n \n     assert lazy_sequence[4] == \"Made with CustomXCom: some-value\"\n-    assert mock_supervisor_comms.send_request.mock_calls == [\n-        call(\n-            log=ANY,\n-            msg=GetXComSequenceItem(\n-                key=\"return_value\",\n-                dag_id=\"dag\",\n-                task_id=\"task\",\n-                run_id=\"run\",\n-                offset=4,\n-            ),\n+    mock_supervisor_comms.send.assert_called_once_with(\n+        GetXComSequenceItem(\n+            key=\"return_value\",\n+            dag_id=\"dag\",\n+            task_id=\"task\",\n+            run_id=\"run\",\n+            offset=4,\n         ),\n-    ]\n+    )\n \n \n def test_getitem_indexerror(mock_supervisor_comms, lazy_sequence):\n-    mock_supervisor_comms.get_message.return_value = ErrorResponse(\n+    mock_supervisor_comms.send.return_value = ErrorResponse(\n         error=ErrorType.XCOM_NOT_FOUND,\n         detail={\"oops\": \"sorry!\"},\n     )\n     with pytest.raises(IndexError) as ctx:\n         lazy_sequence[4]\n     assert ctx.value.args == (4,)\n-    assert mock_supervisor_comms.send_request.mock_calls == [\n-        call(\n-            log=ANY,\n-            msg=GetXComSequenceItem(\n-                key=\"return_value\",\n-                dag_id=\"dag\",\n-                task_id=\"task\",\n-                run_id=\"run\",\n-                offset=4,\n-            ),\n+    mock_supervisor_comms.send.assert_called_once_with(\n+        GetXComSequenceItem(\n+            key=\"return_value\",\n+            dag_id=\"dag\",\n+            task_id=\"task\",\n+            run_id=\"run\",\n+            offset=4,\n         ),\n-    ]\n+    )\n \n \n def test_getitem_slice(mock_supervisor_comms, lazy_sequence):\n-    mock_supervisor_comms.get_message.return_value = XComSequenceSliceResult(root=[6, 4, 1])\n+    mock_supervisor_comms.send.return_value = XComSequenceSliceResult(root=[6, 4, 1])\n     assert lazy_sequence[:5] == [6, 4, 1]\n-    assert mock_supervisor_comms.send_request.mock_calls == [\n-        call(\n-            log=ANY,\n-            msg=GetXComSequenceSlice(\n-                key=\"return_value\",\n-                dag_id=\"dag\",\n-                task_id=\"task\",\n-                run_id=\"run\",\n-                start=None,\n-                stop=5,\n-                step=None,\n-            ),\n+    mock_supervisor_comms.send.assert_called_once_with(\n+        GetXComSequenceSlice(\n+            key=\"return_value\",\n+            dag_id=\"dag\",\n+            task_id=\"task\",\n+            run_id=\"run\",\n+            start=None,\n+            stop=5,\n+            step=None,\n         ),\n-    ]\n+    )\ndiff --git a/task-sdk/tests/task_sdk/execution_time/test_supervisor.py b/task-sdk/tests/task_sdk/execution_time/test_supervisor.py\nindex 4f5e4cfc7ac9d..d2f8ef40c0881 100644\n--- a/task-sdk/tests/task_sdk/execution_time/test_supervisor.py\n+++ b/task-sdk/tests/task_sdk/execution_time/test_supervisor.py\n@@ -27,13 +27,14 @@\n import socket\n import sys\n import time\n-from io import BytesIO\n from operator import attrgetter\n+from random import randint\n from time import sleep\n from typing import TYPE_CHECKING\n from unittest.mock import MagicMock, patch\n \n import httpx\n+import msgspec\n import psutil\n import pytest\n from pytest_unordered import unordered\n@@ -56,6 +57,7 @@\n from airflow.sdk.execution_time.comms import (\n     AssetEventsResult,\n     AssetResult,\n+    CommsDecoder,\n     ConnectionResult,\n     DagRunStateResult,\n     DeferTask,\n@@ -97,17 +99,16 @@\n     XComResult,\n     XComSequenceIndexResult,\n     XComSequenceSliceResult,\n+    _RequestFrame,\n+    _ResponseFrame,\n )\n from airflow.sdk.execution_time.supervisor import (\n-    BUFFER_SIZE,\n     ActivitySubprocess,\n     InProcessSupervisorComms,\n     InProcessTestSupervisor,\n-    mkpipe,\n     set_supervisor_comms,\n     supervise,\n )\n-from airflow.sdk.execution_time.task_runner import CommsDecoder\n from airflow.utils import timezone, timezone as tz\n \n if TYPE_CHECKING:\n@@ -136,18 +137,25 @@ def local_dag_bundle_cfg(path, name=\"my-bundle\"):\n     }\n \n \n+@pytest.fixture\n+def client_with_ti_start(make_ti_context):\n+    client = MagicMock(spec=sdk_client.Client)\n+    client.task_instances.start.return_value = make_ti_context()\n+    return client\n+\n+\n @pytest.mark.usefixtures(\"disable_capturing\")\n class TestWatchedSubprocess:\n     @pytest.fixture(autouse=True)\n     def disable_log_upload(self, spy_agency):\n         spy_agency.spy_on(ActivitySubprocess._upload_logs, call_original=False)\n \n-    def test_reading_from_pipes(self, captured_logs, time_machine):\n+    def test_reading_from_pipes(self, captured_logs, time_machine, client_with_ti_start):\n         def subprocess_main():\n             # This is run in the subprocess!\n \n-            # Ensure we follow the \"protocol\" and get the startup message before we do anything\n-            sys.stdin.readline()\n+            # Ensure we follow the \"protocol\" and get the startup message before we do anything else\n+            CommsDecoder()._get_response()\n \n             import logging\n             import warnings\n@@ -180,7 +188,7 @@ def subprocess_main():\n                 run_id=\"d\",\n                 try_number=1,\n             ),\n-            client=MagicMock(spec=sdk_client.Client),\n+            client=client_with_ti_start,\n             target=subprocess_main,\n         )\n \n@@ -228,12 +236,12 @@ def subprocess_main():\n             ]\n         )\n \n-    def test_subprocess_sigkilled(self):\n+    def test_subprocess_sigkilled(self, client_with_ti_start):\n         main_pid = os.getpid()\n \n         def subprocess_main():\n             # Ensure we follow the \"protocol\" and get the startup message before we do anything\n-            sys.stdin.readline()\n+            CommsDecoder()._get_response()\n \n             assert os.getpid() != main_pid\n             os.kill(os.getpid(), signal.SIGKILL)\n@@ -248,7 +256,7 @@ def subprocess_main():\n                 run_id=\"d\",\n                 try_number=1,\n             ),\n-            client=MagicMock(spec=sdk_client.Client),\n+            client=client_with_ti_start,\n             target=subprocess_main,\n         )\n \n@@ -285,7 +293,7 @@ def test_regular_heartbeat(self, spy_agency: kgb.SpyAgency, monkeypatch, mocker,\n         monkeypatch.setattr(airflow.sdk.execution_time.supervisor, \"MIN_HEARTBEAT_INTERVAL\", 0.1)\n \n         def subprocess_main():\n-            sys.stdin.readline()\n+            CommsDecoder()._get_response()\n \n             for _ in range(5):\n                 print(\"output\", flush=True)\n@@ -314,7 +322,7 @@ def test_no_heartbeat_in_overtime(self, spy_agency: kgb.SpyAgency, monkeypatch,\n         monkeypatch.setattr(airflow.sdk.execution_time.supervisor, \"MIN_HEARTBEAT_INTERVAL\", 0.1)\n \n         def subprocess_main():\n-            sys.stdin.readline()\n+            CommsDecoder()._get_response()\n \n             for _ in range(5):\n                 print(\"output\", flush=True)\n@@ -340,7 +348,7 @@ def _on_child_started(self, *args, **kwargs):\n         assert proc.wait() == 0\n         spy_agency.assert_spy_not_called(heartbeat_spy)\n \n-    def test_run_simple_dag(self, test_dags_dir, captured_logs, time_machine, mocker, make_ti_context):\n+    def test_run_simple_dag(self, test_dags_dir, captured_logs, time_machine, mocker, client_with_ti_start):\n         \"\"\"Test running a simple DAG in a subprocess and capturing the output.\"\"\"\n \n         instant = tz.datetime(2024, 11, 7, 12, 34, 56, 78901)\n@@ -355,11 +363,6 @@ def test_run_simple_dag(self, test_dags_dir, captured_logs, time_machine, mocker\n             try_number=1,\n         )\n \n-        # Create a mock client to assert calls to the client\n-        # We assume the implementation of the client is correct and only need to check the calls\n-        mock_client = mocker.Mock(spec=sdk_client.Client)\n-        mock_client.task_instances.start.return_value = make_ti_context()\n-\n         bundle_info = BundleInfo(name=\"my-bundle\", version=None)\n         with patch.dict(os.environ, local_dag_bundle_cfg(test_dags_dir, bundle_info.name)):\n             exit_code = supervise(\n@@ -368,7 +371,7 @@ def test_run_simple_dag(self, test_dags_dir, captured_logs, time_machine, mocker\n                 token=\"\",\n                 server=\"\",\n                 dry_run=True,\n-                client=mock_client,\n+                client=client_with_ti_start,\n                 bundle_info=bundle_info,\n             )\n             assert exit_code == 0, captured_logs\n@@ -498,7 +501,7 @@ def test_state_conflict_on_heartbeat(self, captured_logs, monkeypatch, mocker, m\n         monkeypatch.setattr(airflow.sdk.execution_time.supervisor, \"MIN_HEARTBEAT_INTERVAL\", 0.0)\n \n         def subprocess_main():\n-            sys.stdin.readline()\n+            CommsDecoder()._get_response()\n             sleep(5)\n             # Shouldn't get here\n             exit(5)\n@@ -611,7 +614,6 @@ def test_heartbeat_failures_handling(self, monkeypatch, mocker, captured_logs, t\n             stdin=mocker.MagicMock(),\n             client=client,\n             process=mock_process,\n-            requests_fd=-1,\n         )\n \n         time_now = tz.datetime(2024, 11, 28, 12, 0, 0)\n@@ -701,7 +703,6 @@ def test_overtime_handling(\n             stdin=mocker.Mock(),\n             process=mocker.Mock(),\n             client=mocker.Mock(),\n-            requests_fd=-1,\n         )\n \n         # Set the terminal state and task end datetime\n@@ -738,7 +739,7 @@ def test_overtime_handling(\n             ),\n         ),\n     )\n-    def test_exit_by_signal(self, monkeypatch, signal_to_raise, log_pattern, cap_structlog):\n+    def test_exit_by_signal(self, signal_to_raise, log_pattern, cap_structlog, client_with_ti_start):\n         def subprocess_main():\n             import faulthandler\n             import os\n@@ -748,7 +749,7 @@ def subprocess_main():\n                 faulthandler.disable()\n \n             # Ensure we follow the \"protocol\" and get the startup message before we do anything\n-            sys.stdin.readline()\n+            CommsDecoder()._get_response()\n \n             os.kill(os.getpid(), signal_to_raise)\n \n@@ -762,7 +763,7 @@ def subprocess_main():\n                 run_id=\"d\",\n                 try_number=1,\n             ),\n-            client=MagicMock(spec=sdk_client.Client),\n+            client=client_with_ti_start,\n             target=subprocess_main,\n         )\n \n@@ -791,26 +792,26 @@ def test_cleanup_sockets_after_delay(self, monkeypatch, mocker, time_machine):\n             stdin=mocker.MagicMock(),\n             client=mocker.MagicMock(),\n             process=mock_process,\n-            requests_fd=-1,\n         )\n \n         proc.selector = mocker.MagicMock()\n         proc.selector.select.return_value = []\n \n         proc._exit_code = 0\n-        proc._num_open_sockets = 1\n+        # Create a fake placeholder in the open socket weakref\n+        proc._open_sockets[mocker.MagicMock()] = \"test placeholder\"\n         proc._process_exit_monotonic = time.monotonic()\n \n         mocker.patch.object(\n             ActivitySubprocess,\n             \"_cleanup_open_sockets\",\n-            side_effect=lambda: setattr(proc, \"_num_open_sockets\", 0),\n+            side_effect=lambda: setattr(proc, \"_open_sockets\", {}),\n         )\n \n         time_machine.shift(2)\n \n         proc._monitor_subprocess()\n-        assert proc._num_open_sockets == 0\n+        assert len(proc._open_sockets) == 0\n \n \n class TestWatchedSubprocessKill:\n@@ -829,7 +830,6 @@ def watched_subprocess(self, mocker, mock_process):\n             stdin=mocker.Mock(),\n             client=mocker.Mock(),\n             process=mock_process,\n-            requests_fd=-1,\n         )\n         # Mock the selector\n         mock_selector = mocker.Mock(spec=selectors.DefaultSelector)\n@@ -888,7 +888,7 @@ def test_kill_process_custom_signal(self, watched_subprocess, mock_process):\n             ),\n         ],\n     )\n-    def test_kill_escalation_path(self, signal_to_send, exit_after, mocker, captured_logs, monkeypatch):\n+    def test_kill_escalation_path(self, signal_to_send, exit_after, captured_logs, client_with_ti_start):\n         def subprocess_main():\n             import signal\n \n@@ -905,7 +905,7 @@ def _handler(sig, frame):\n             signal.signal(signal.SIGINT, _handler)\n             signal.signal(signal.SIGTERM, _handler)\n             try:\n-                sys.stdin.readline()\n+                CommsDecoder()._get_response()\n                 print(\"Ready\")\n                 sleep(10)\n             except Exception as e:\n@@ -919,7 +919,7 @@ def _handler(sig, frame):\n             dag_rel_path=os.devnull,\n             bundle_info=FAKE_BUNDLE,\n             what=TaskInstance(id=ti_id, task_id=\"b\", dag_id=\"c\", run_id=\"d\", try_number=1),\n-            client=MagicMock(spec=sdk_client.Client),\n+            client=client_with_ti_start,\n             target=subprocess_main,\n         )\n \n@@ -976,9 +976,11 @@ def test_service_subprocess(self, watched_subprocess, mock_process, mocker):\n         mock_stdout_handler = mocker.Mock(return_value=False)  # Simulate EOF for stdout\n         mock_stderr_handler = mocker.Mock(return_value=True)  # Continue processing for stderr\n \n+        mock_on_close = mocker.Mock()\n+\n         # Mock selector to return events\n-        mock_key_stdout = mocker.Mock(fileobj=mock_stdout, data=mock_stdout_handler)\n-        mock_key_stderr = mocker.Mock(fileobj=mock_stderr, data=mock_stderr_handler)\n+        mock_key_stdout = mocker.Mock(fileobj=mock_stdout, data=(mock_stdout_handler, mock_on_close))\n+        mock_key_stderr = mocker.Mock(fileobj=mock_stderr, data=(mock_stderr_handler, mock_on_close))\n         watched_subprocess.selector.select.return_value = [(mock_key_stdout, None), (mock_key_stderr, None)]\n \n         # Mock to simulate process exited successfully\n@@ -996,8 +998,7 @@ def test_service_subprocess(self, watched_subprocess, mock_process, mocker):\n         mock_stderr_handler.assert_called_once_with(mock_stderr)\n \n         # Validate unregistering and closing of EOF file object\n-        watched_subprocess.selector.unregister.assert_called_once_with(mock_stdout)\n-        mock_stdout.close.assert_called_once()\n+        mock_on_close.assert_called_once_with(mock_stdout)\n \n         # Validate that `_check_subprocess_exit` is called\n         mock_process.wait.assert_called_once_with(timeout=0)\n@@ -1073,16 +1074,15 @@ def test_max_wait_time_calculation_edge_cases(\n class TestHandleRequest:\n     @pytest.fixture\n     def watched_subprocess(self, mocker):\n-        read_end, write_end = mkpipe(remote_read=True)\n+        read_end, write_end = socket.socketpair()\n \n         subprocess = ActivitySubprocess(\n             process_log=mocker.MagicMock(),\n             id=TI_ID,\n             pid=12345,\n-            stdin=write_end,  # this is the writer side\n+            stdin=write_end,\n             client=mocker.Mock(),\n             process=mocker.Mock(),\n-            requests_fd=-1,\n         )\n \n         return subprocess, read_end\n@@ -1091,7 +1091,7 @@ def watched_subprocess(self, mocker):\n     @pytest.mark.parametrize(\n         [\n             \"message\",\n-            \"expected_buffer\",\n+            \"expected_body\",\n             \"client_attr_path\",\n             \"method_arg\",\n             \"method_kwarg\",\n@@ -1101,7 +1101,7 @@ def watched_subprocess(self, mocker):\n         [\n             pytest.param(\n                 GetConnection(conn_id=\"test_conn\"),\n-                b'{\"conn_id\":\"test_conn\",\"conn_type\":\"mysql\",\"type\":\"ConnectionResult\"}\\n',\n+                {\"conn_id\": \"test_conn\", \"conn_type\": \"mysql\", \"type\": \"ConnectionResult\"},\n                 \"connections.get\",\n                 (\"test_conn\",),\n                 {},\n@@ -1111,7 +1111,12 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetConnection(conn_id=\"test_conn\"),\n-                b'{\"conn_id\":\"test_conn\",\"conn_type\":\"mysql\",\"password\":\"password\",\"type\":\"ConnectionResult\"}\\n',\n+                {\n+                    \"conn_id\": \"test_conn\",\n+                    \"conn_type\": \"mysql\",\n+                    \"password\": \"password\",\n+                    \"type\": \"ConnectionResult\",\n+                },\n                 \"connections.get\",\n                 (\"test_conn\",),\n                 {},\n@@ -1121,7 +1126,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetConnection(conn_id=\"test_conn\"),\n-                b'{\"conn_id\":\"test_conn\",\"conn_type\":\"mysql\",\"schema\":\"mysql\",\"type\":\"ConnectionResult\"}\\n',\n+                {\"conn_id\": \"test_conn\", \"conn_type\": \"mysql\", \"schema\": \"mysql\", \"type\": \"ConnectionResult\"},\n                 \"connections.get\",\n                 (\"test_conn\",),\n                 {},\n@@ -1131,7 +1136,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetVariable(key=\"test_key\"),\n-                b'{\"key\":\"test_key\",\"value\":\"test_value\",\"type\":\"VariableResult\"}\\n',\n+                {\"key\": \"test_key\", \"value\": \"test_value\", \"type\": \"VariableResult\"},\n                 \"variables.get\",\n                 (\"test_key\",),\n                 {},\n@@ -1141,7 +1146,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 PutVariable(key=\"test_key\", value=\"test_value\", description=\"test_description\"),\n-                b\"\",\n+                None,\n                 \"variables.set\",\n                 (\"test_key\", \"test_value\", \"test_description\"),\n                 {},\n@@ -1151,7 +1156,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 DeleteVariable(key=\"test_key\"),\n-                b'{\"ok\":true,\"type\":\"OKResponse\"}\\n',\n+                {\"ok\": True, \"type\": \"OKResponse\"},\n                 \"variables.delete\",\n                 (\"test_key\",),\n                 {},\n@@ -1161,7 +1166,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 DeferTask(next_method=\"execute_callback\", classpath=\"my-classpath\"),\n-                b\"\",\n+                None,\n                 \"task_instances.defer\",\n                 (TI_ID, DeferTask(next_method=\"execute_callback\", classpath=\"my-classpath\")),\n                 {},\n@@ -1174,7 +1179,7 @@ def watched_subprocess(self, mocker):\n                     reschedule_date=timezone.parse(\"2024-10-31T12:00:00Z\"),\n                     end_date=timezone.parse(\"2024-10-31T12:00:00Z\"),\n                 ),\n-                b\"\",\n+                None,\n                 \"task_instances.reschedule\",\n                 (\n                     TI_ID,\n@@ -1190,7 +1195,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetXCom(dag_id=\"test_dag\", run_id=\"test_run\", task_id=\"test_task\", key=\"test_key\"),\n-                b'{\"key\":\"test_key\",\"value\":\"test_value\",\"type\":\"XComResult\"}\\n',\n+                {\"key\": \"test_key\", \"value\": \"test_value\", \"type\": \"XComResult\"},\n                 \"xcoms.get\",\n                 (\"test_dag\", \"test_run\", \"test_task\", \"test_key\", None, False),\n                 {},\n@@ -1202,7 +1207,7 @@ def watched_subprocess(self, mocker):\n                 GetXCom(\n                     dag_id=\"test_dag\", run_id=\"test_run\", task_id=\"test_task\", key=\"test_key\", map_index=2\n                 ),\n-                b'{\"key\":\"test_key\",\"value\":\"test_value\",\"type\":\"XComResult\"}\\n',\n+                {\"key\": \"test_key\", \"value\": \"test_value\", \"type\": \"XComResult\"},\n                 \"xcoms.get\",\n                 (\"test_dag\", \"test_run\", \"test_task\", \"test_key\", 2, False),\n                 {},\n@@ -1212,7 +1217,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetXCom(dag_id=\"test_dag\", run_id=\"test_run\", task_id=\"test_task\", key=\"test_key\"),\n-                b'{\"key\":\"test_key\",\"value\":null,\"type\":\"XComResult\"}\\n',\n+                {\"key\": \"test_key\", \"value\": None, \"type\": \"XComResult\"},\n                 \"xcoms.get\",\n                 (\"test_dag\", \"test_run\", \"test_task\", \"test_key\", None, False),\n                 {},\n@@ -1228,7 +1233,7 @@ def watched_subprocess(self, mocker):\n                     key=\"test_key\",\n                     include_prior_dates=True,\n                 ),\n-                b'{\"key\":\"test_key\",\"value\":null,\"type\":\"XComResult\"}\\n',\n+                {\"key\": \"test_key\", \"value\": None, \"type\": \"XComResult\"},\n                 \"xcoms.get\",\n                 (\"test_dag\", \"test_run\", \"test_task\", \"test_key\", None, True),\n                 {},\n@@ -1244,7 +1249,7 @@ def watched_subprocess(self, mocker):\n                     key=\"test_key\",\n                     value='{\"key\": \"test_key\", \"value\": {\"key2\": \"value2\"}}',\n                 ),\n-                b\"\",\n+                None,\n                 \"xcoms.set\",\n                 (\n                     \"test_dag\",\n@@ -1269,7 +1274,7 @@ def watched_subprocess(self, mocker):\n                     value='{\"key\": \"test_key\", \"value\": {\"key2\": \"value2\"}}',\n                     map_index=2,\n                 ),\n-                b\"\",\n+                None,\n                 \"xcoms.set\",\n                 (\n                     \"test_dag\",\n@@ -1295,7 +1300,7 @@ def watched_subprocess(self, mocker):\n                     map_index=2,\n                     mapped_length=3,\n                 ),\n-                b\"\",\n+                None,\n                 \"xcoms.set\",\n                 (\n                     \"test_dag\",\n@@ -1319,15 +1324,9 @@ def watched_subprocess(self, mocker):\n                     key=\"test_key\",\n                     map_index=2,\n                 ),\n-                b\"\",\n+                None,\n                 \"xcoms.delete\",\n-                (\n-                    \"test_dag\",\n-                    \"test_run\",\n-                    \"test_task\",\n-                    \"test_key\",\n-                    2,\n-                ),\n+                (\"test_dag\", \"test_run\", \"test_task\", \"test_key\", 2),\n                 {},\n                 OKResponse(ok=True),\n                 None,\n@@ -1337,7 +1336,7 @@ def watched_subprocess(self, mocker):\n             # if it can handle TaskState message\n             pytest.param(\n                 TaskState(state=TaskInstanceState.SKIPPED, end_date=timezone.parse(\"2024-10-31T12:00:00Z\")),\n-                b\"\",\n+                None,\n                 \"\",\n                 (),\n                 {},\n@@ -1349,7 +1348,7 @@ def watched_subprocess(self, mocker):\n                 RetryTask(\n                     end_date=timezone.parse(\"2024-10-31T12:00:00Z\"), rendered_map_index=\"test retry task\"\n                 ),\n-                b\"\",\n+                None,\n                 \"task_instances.retry\",\n                 (),\n                 {\n@@ -1363,7 +1362,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 SetRenderedFields(rendered_fields={\"field1\": \"rendered_value1\", \"field2\": \"rendered_value2\"}),\n-                b\"\",\n+                None,\n                 \"task_instances.set_rtif\",\n                 (TI_ID, {\"field1\": \"rendered_value1\", \"field2\": \"rendered_value2\"}),\n                 {},\n@@ -1373,7 +1372,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetAssetByName(name=\"asset\"),\n-                b'{\"name\":\"asset\",\"uri\":\"s3://bucket/obj\",\"group\":\"asset\",\"type\":\"AssetResult\"}\\n',\n+                {\"name\": \"asset\", \"uri\": \"s3://bucket/obj\", \"group\": \"asset\", \"type\": \"AssetResult\"},\n                 \"assets.get\",\n                 [],\n                 {\"name\": \"asset\"},\n@@ -1383,7 +1382,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetAssetByUri(uri=\"s3://bucket/obj\"),\n-                b'{\"name\":\"asset\",\"uri\":\"s3://bucket/obj\",\"group\":\"asset\",\"type\":\"AssetResult\"}\\n',\n+                {\"name\": \"asset\", \"uri\": \"s3://bucket/obj\", \"group\": \"asset\", \"type\": \"AssetResult\"},\n                 \"assets.get\",\n                 [],\n                 {\"uri\": \"s3://bucket/obj\"},\n@@ -1393,11 +1392,17 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetAssetEventByAsset(uri=\"s3://bucket/obj\", name=\"test\"),\n-                (\n-                    b'{\"asset_events\":'\n-                    b'[{\"id\":1,\"timestamp\":\"2024-10-31T12:00:00Z\",\"asset\":{\"name\":\"asset\",\"uri\":\"s3://bucket/obj\",\"group\":\"asset\"},'\n-                    b'\"created_dagruns\":[]}],\"type\":\"AssetEventsResult\"}\\n'\n-                ),\n+                {\n+                    \"asset_events\": [\n+                        {\n+                            \"id\": 1,\n+                            \"timestamp\": timezone.parse(\"2024-10-31T12:00:00Z\"),\n+                            \"asset\": {\"name\": \"asset\", \"uri\": \"s3://bucket/obj\", \"group\": \"asset\"},\n+                            \"created_dagruns\": [],\n+                        }\n+                    ],\n+                    \"type\": \"AssetEventsResult\",\n+                },\n                 \"asset_events.get\",\n                 [],\n                 {\"uri\": \"s3://bucket/obj\", \"name\": \"test\"},\n@@ -1416,11 +1421,17 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetAssetEventByAsset(uri=\"s3://bucket/obj\", name=None),\n-                (\n-                    b'{\"asset_events\":'\n-                    b'[{\"id\":1,\"timestamp\":\"2024-10-31T12:00:00Z\",\"asset\":{\"name\":\"asset\",\"uri\":\"s3://bucket/obj\",\"group\":\"asset\"},'\n-                    b'\"created_dagruns\":[]}],\"type\":\"AssetEventsResult\"}\\n'\n-                ),\n+                {\n+                    \"asset_events\": [\n+                        {\n+                            \"id\": 1,\n+                            \"timestamp\": timezone.parse(\"2024-10-31T12:00:00Z\"),\n+                            \"asset\": {\"name\": \"asset\", \"uri\": \"s3://bucket/obj\", \"group\": \"asset\"},\n+                            \"created_dagruns\": [],\n+                        }\n+                    ],\n+                    \"type\": \"AssetEventsResult\",\n+                },\n                 \"asset_events.get\",\n                 [],\n                 {\"uri\": \"s3://bucket/obj\", \"name\": None},\n@@ -1439,11 +1450,17 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetAssetEventByAsset(uri=None, name=\"test\"),\n-                (\n-                    b'{\"asset_events\":'\n-                    b'[{\"id\":1,\"timestamp\":\"2024-10-31T12:00:00Z\",\"asset\":{\"name\":\"asset\",\"uri\":\"s3://bucket/obj\",\"group\":\"asset\"},'\n-                    b'\"created_dagruns\":[]}],\"type\":\"AssetEventsResult\"}\\n'\n-                ),\n+                {\n+                    \"asset_events\": [\n+                        {\n+                            \"id\": 1,\n+                            \"timestamp\": timezone.parse(\"2024-10-31T12:00:00Z\"),\n+                            \"asset\": {\"name\": \"asset\", \"uri\": \"s3://bucket/obj\", \"group\": \"asset\"},\n+                            \"created_dagruns\": [],\n+                        }\n+                    ],\n+                    \"type\": \"AssetEventsResult\",\n+                },\n                 \"asset_events.get\",\n                 [],\n                 {\"uri\": None, \"name\": \"test\"},\n@@ -1462,11 +1479,17 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetAssetEventByAssetAlias(alias_name=\"test_alias\"),\n-                (\n-                    b'{\"asset_events\":'\n-                    b'[{\"id\":1,\"timestamp\":\"2024-10-31T12:00:00Z\",\"asset\":{\"name\":\"asset\",\"uri\":\"s3://bucket/obj\",\"group\":\"asset\"},'\n-                    b'\"created_dagruns\":[]}],\"type\":\"AssetEventsResult\"}\\n'\n-                ),\n+                {\n+                    \"asset_events\": [\n+                        {\n+                            \"id\": 1,\n+                            \"timestamp\": timezone.parse(\"2024-10-31T12:00:00Z\"),\n+                            \"asset\": {\"name\": \"asset\", \"uri\": \"s3://bucket/obj\", \"group\": \"asset\"},\n+                            \"created_dagruns\": [],\n+                        }\n+                    ],\n+                    \"type\": \"AssetEventsResult\",\n+                },\n                 \"asset_events.get\",\n                 [],\n                 {\"alias_name\": \"test_alias\"},\n@@ -1485,7 +1508,10 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 ValidateInletsAndOutlets(ti_id=TI_ID),\n-                b'{\"inactive_assets\":[{\"name\":\"asset_name\",\"uri\":\"asset_uri\",\"type\":\"asset\"}],\"type\":\"InactiveAssetsResult\"}\\n',\n+                {\n+                    \"inactive_assets\": [{\"name\": \"asset_name\", \"uri\": \"asset_uri\", \"type\": \"asset\"}],\n+                    \"type\": \"InactiveAssetsResult\",\n+                },\n                 \"task_instances.validate_inlets_and_outlets\",\n                 (TI_ID,),\n                 {},\n@@ -1499,7 +1525,7 @@ def watched_subprocess(self, mocker):\n                 SucceedTask(\n                     end_date=timezone.parse(\"2024-10-31T12:00:00Z\"), rendered_map_index=\"test success task\"\n                 ),\n-                b\"\",\n+                None,\n                 \"task_instances.succeed\",\n                 (),\n                 {\n@@ -1515,11 +1541,13 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetPrevSuccessfulDagRun(ti_id=TI_ID),\n-                (\n-                    b'{\"data_interval_start\":\"2025-01-10T12:00:00Z\",\"data_interval_end\":\"2025-01-10T14:00:00Z\",'\n-                    b'\"start_date\":\"2025-01-10T12:00:00Z\",\"end_date\":\"2025-01-10T14:00:00Z\",'\n-                    b'\"type\":\"PrevSuccessfulDagRunResult\"}\\n'\n-                ),\n+                {\n+                    \"data_interval_start\": timezone.parse(\"2025-01-10T12:00:00Z\"),\n+                    \"data_interval_end\": timezone.parse(\"2025-01-10T14:00:00Z\"),\n+                    \"start_date\": timezone.parse(\"2025-01-10T12:00:00Z\"),\n+                    \"end_date\": timezone.parse(\"2025-01-10T14:00:00Z\"),\n+                    \"type\": \"PrevSuccessfulDagRunResult\",\n+                },\n                 \"task_instances.get_previous_successful_dagrun\",\n                 (TI_ID,),\n                 {},\n@@ -1540,7 +1568,7 @@ def watched_subprocess(self, mocker):\n                     logical_date=timezone.datetime(2025, 1, 1),\n                     reset_dag_run=True,\n                 ),\n-                b'{\"ok\":true,\"type\":\"OKResponse\"}\\n',\n+                {\"ok\": True, \"type\": \"OKResponse\"},\n                 \"dag_runs.trigger\",\n                 (\"test_dag\", \"test_run\", {\"key\": \"value\"}, timezone.datetime(2025, 1, 1), True),\n                 {},\n@@ -1549,8 +1577,9 @@ def watched_subprocess(self, mocker):\n                 id=\"dag_run_trigger\",\n             ),\n             pytest.param(\n+                # TODO: This should be raise an exception, not returning an ErrorResponse. Fix this before PR\n                 TriggerDagRun(dag_id=\"test_dag\", run_id=\"test_run\"),\n-                b'{\"error\":\"DAGRUN_ALREADY_EXISTS\",\"detail\":null,\"type\":\"ErrorResponse\"}\\n',\n+                {\"error\": \"DAGRUN_ALREADY_EXISTS\", \"detail\": None, \"type\": \"ErrorResponse\"},\n                 \"dag_runs.trigger\",\n                 (\"test_dag\", \"test_run\", None, None, False),\n                 {},\n@@ -1560,7 +1589,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetDagRunState(dag_id=\"test_dag\", run_id=\"test_run\"),\n-                b'{\"state\":\"running\",\"type\":\"DagRunStateResult\"}\\n',\n+                {\"state\": \"running\", \"type\": \"DagRunStateResult\"},\n                 \"dag_runs.get_state\",\n                 (\"test_dag\", \"test_run\"),\n                 {},\n@@ -1570,7 +1599,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetTaskRescheduleStartDate(ti_id=TI_ID),\n-                b'{\"start_date\":\"2024-10-31T12:00:00Z\",\"type\":\"TaskRescheduleStartDate\"}\\n',\n+                {\"start_date\": timezone.parse(\"2024-10-31T12:00:00Z\"), \"type\": \"TaskRescheduleStartDate\"},\n                 \"task_instances.get_reschedule_start_date\",\n                 (TI_ID, 1),\n                 {},\n@@ -1580,7 +1609,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetTICount(dag_id=\"test_dag\", task_ids=[\"task1\", \"task2\"]),\n-                b'{\"count\":2,\"type\":\"TICount\"}\\n',\n+                {\"count\": 2, \"type\": \"TICount\"},\n                 \"task_instances.get_count\",\n                 (),\n                 {\n@@ -1598,7 +1627,7 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetDRCount(dag_id=\"test_dag\", states=[\"success\", \"failed\"]),\n-                b'{\"count\":2,\"type\":\"DRCount\"}\\n',\n+                {\"count\": 2, \"type\": \"DRCount\"},\n                 \"dag_runs.get_count\",\n                 (),\n                 {\n@@ -1613,7 +1642,10 @@ def watched_subprocess(self, mocker):\n             ),\n             pytest.param(\n                 GetTaskStates(dag_id=\"test_dag\", task_group_id=\"test_group\"),\n-                b'{\"task_states\":{\"run_id\":{\"task1\":\"success\",\"task2\":\"failed\"}},\"type\":\"TaskStatesResult\"}\\n',\n+                {\n+                    \"task_states\": {\"run_id\": {\"task1\": \"success\", \"task2\": \"failed\"}},\n+                    \"type\": \"TaskStatesResult\",\n+                },\n                 \"task_instances.get_task_states\",\n                 (),\n                 {\n@@ -1636,7 +1668,7 @@ def watched_subprocess(self, mocker):\n                     task_id=\"test_task\",\n                     offset=0,\n                 ),\n-                b'{\"root\":\"test_value\",\"type\":\"XComSequenceIndexResult\"}\\n',\n+                {\"root\": \"test_value\", \"type\": \"XComSequenceIndexResult\"},\n                 \"xcoms.get_sequence_item\",\n                 (\"test_dag\", \"test_run\", \"test_task\", \"test_key\", 0),\n                 {},\n@@ -1645,6 +1677,7 @@ def watched_subprocess(self, mocker):\n                 id=\"get_xcom_seq_item\",\n             ),\n             pytest.param(\n+                # TODO: This should be raise an exception, not returning an ErrorResponse. Fix this before PR\n                 GetXComSequenceItem(\n                     key=\"test_key\",\n                     dag_id=\"test_dag\",\n@@ -1652,7 +1685,7 @@ def watched_subprocess(self, mocker):\n                     task_id=\"test_task\",\n                     offset=2,\n                 ),\n-                b'{\"error\":\"XCOM_NOT_FOUND\",\"detail\":null,\"type\":\"ErrorResponse\"}\\n',\n+                {\"error\": \"XCOM_NOT_FOUND\", \"detail\": None, \"type\": \"ErrorResponse\"},\n                 \"xcoms.get_sequence_item\",\n                 (\"test_dag\", \"test_run\", \"test_task\", \"test_key\", 2),\n                 {},\n@@ -1670,7 +1703,7 @@ def watched_subprocess(self, mocker):\n                     stop=None,\n                     step=None,\n                 ),\n-                b'{\"root\":[\"foo\",\"bar\"],\"type\":\"XComSequenceSliceResult\"}\\n',\n+                {\"root\": [\"foo\", \"bar\"], \"type\": \"XComSequenceSliceResult\"},\n                 \"xcoms.get_sequence_slice\",\n                 (\"test_dag\", \"test_run\", \"test_task\", \"test_key\", None, None, None),\n                 {},\n@@ -1687,7 +1720,7 @@ def test_handle_requests(\n         mocker,\n         time_machine,\n         message,\n-        expected_buffer,\n+        expected_body,\n         client_attr_path,\n         method_arg,\n         method_kwarg,\n@@ -1715,8 +1748,9 @@ def test_handle_requests(\n         generator = watched_subprocess.handle_requests(log=mocker.Mock())\n         # Initialize the generator\n         next(generator)\n-        msg = message.model_dump_json().encode() + b\"\\n\"\n-        generator.send(msg)\n+\n+        req_frame = _RequestFrame(id=randint(1, 2**32 - 1), body=message.model_dump())\n+        generator.send(req_frame)\n \n         if mask_secret_args:\n             mock_mask_secret.assert_called_with(*mask_secret_args)\n@@ -1729,33 +1763,22 @@ def test_handle_requests(\n \n         # Read response from the read end of the socket\n         read_socket.settimeout(0.1)\n-        val = b\"\"\n-        try:\n-            while not val.endswith(b\"\\n\"):\n-                chunk = read_socket.recv(BUFFER_SIZE)\n-                if not chunk:\n-                    break\n-                val += chunk\n-        except (BlockingIOError, TimeoutError, socket.timeout):\n-            # no response written, valid for some message types like setters and TI operations.\n-            pass\n+        frame_len = int.from_bytes(read_socket.recv(4), \"big\")\n+        bytes = read_socket.recv(frame_len)\n+        frame = msgspec.msgpack.Decoder(_ResponseFrame).decode(bytes)\n+\n+        assert frame.id == req_frame.id\n \n         # Verify the response was added to the buffer\n-        assert val == expected_buffer\n+        assert frame.body == expected_body\n \n         # Verify the response is correctly decoded\n         # This is important because the subprocess/task runner will read the response\n         # and deserialize it to the correct message type\n \n-        # Only decode the buffer if it contains data. An empty buffer implies no response was written.\n-        if not val and (mock_response and not isinstance(mock_response, OKResponse)):\n-            pytest.fail(\"Expected a response, but got an empty buffer.\")\n-\n-        if val:\n-            # Using BytesIO to simulate a readable stream for CommsDecoder.\n-            input_stream = BytesIO(val)\n-            decoder = CommsDecoder(input=input_stream)\n-            assert decoder.get_message() == mock_response\n+        if frame.body is not None:\n+            decoder = CommsDecoder(socket=None).body_decoder\n+            assert decoder.validate_python(frame.body) == mock_response\n \n     def test_handle_requests_api_server_error(self, watched_subprocess, mocker):\n         \"\"\"Test that API server errors are properly handled and sent back to the task.\"\"\"\n@@ -1777,28 +1800,32 @@ def test_handle_requests_api_server_error(self, watched_subprocess, mocker):\n \n         next(generator)\n \n-        msg = SucceedTask(end_date=timezone.parse(\"2024-10-31T12:00:00Z\")).model_dump_json().encode() + b\"\\n\"\n-        generator.send(msg)\n+        msg = SucceedTask(end_date=timezone.parse(\"2024-10-31T12:00:00Z\"))\n+        req_frame = _RequestFrame(id=randint(1, 2**32 - 1), body=msg.model_dump())\n+        generator.send(req_frame)\n \n-        # Read response directly from the reader socket\n+        # Read response from the read end of the socket\n         read_socket.settimeout(0.1)\n-        val = b\"\"\n-        try:\n-            while not val.endswith(b\"\\n\"):\n-                val += read_socket.recv(4096)\n-        except (BlockingIOError, TimeoutError):\n-            pass\n-\n-        assert val == (\n-            b'{\"error\":\"API_SERVER_ERROR\",\"detail\":{\"status_code\":500,\"message\":\"API Server Error\",'\n-            b'\"detail\":{\"detail\":\"Internal Server Error\"}},\"type\":\"ErrorResponse\"}\\n'\n-        )\n+        frame_len = int.from_bytes(read_socket.recv(4), \"big\")\n+        bytes = read_socket.recv(frame_len)\n+        frame = msgspec.msgpack.Decoder(_ResponseFrame).decode(bytes)\n+\n+        assert frame.id == req_frame.id\n+\n+        assert frame.error == {\n+            \"error\": \"API_SERVER_ERROR\",\n+            \"detail\": {\n+                \"status_code\": 500,\n+                \"message\": \"API Server Error\",\n+                \"detail\": {\"detail\": \"Internal Server Error\"},\n+            },\n+            \"type\": \"ErrorResponse\",\n+        }\n \n         # Verify the error can be decoded correctly\n-        input_stream = BytesIO(val)\n-        decoder = CommsDecoder(input=input_stream)\n+        comms = CommsDecoder(socket=None)\n         with pytest.raises(AirflowRuntimeError) as exc_info:\n-            decoder.get_message()\n+            comms._from_frame(frame)\n \n         assert exc_info.value.error.error == ErrorType.API_SERVER_ERROR\n         assert exc_info.value.error.detail == {\n@@ -1864,14 +1891,13 @@ def test_inprocess_supervisor_comms_roundtrip(self):\n         \"\"\"\n \n         class MinimalSupervisor(InProcessTestSupervisor):\n-            def _handle_request(self, msg, log):\n+            def _handle_request(self, msg, log, req_id):\n                 resp = VariableResult(key=msg.key, value=\"value\")\n-                self.send_msg(resp)\n+                self.send_msg(resp, req_id)\n \n         supervisor = MinimalSupervisor(\n             id=\"test\",\n             pid=123,\n-            requests_fd=-1,\n             process=MagicMock(),\n             process_log=MagicMock(),\n             client=MagicMock(),\n@@ -1881,9 +1907,8 @@ def _handle_request(self, msg, log):\n \n         test_msg = GetVariable(key=\"test_key\")\n \n-        comms.send_request(log=MagicMock(), msg=test_msg)\n+        response = comms.send(test_msg)\n \n         # Ensure we got back what we expect\n-        response = comms.get_message()\n         assert isinstance(response, VariableResult)\n         assert response.value == \"value\"\ndiff --git a/task-sdk/tests/task_sdk/execution_time/test_task_runner.py b/task-sdk/tests/task_sdk/execution_time/test_task_runner.py\nindex d786fdaa5b2bc..5ce460c455bf4 100644\n--- a/task-sdk/tests/task_sdk/execution_time/test_task_runner.py\n+++ b/task-sdk/tests/task_sdk/execution_time/test_task_runner.py\n@@ -22,11 +22,9 @@\n import json\n import os\n import textwrap\n-import uuid\n from collections.abc import Iterable\n from datetime import datetime, timedelta\n from pathlib import Path\n-from socket import socketpair\n from typing import TYPE_CHECKING\n from unittest import mock\n from unittest.mock import patch\n@@ -103,7 +101,6 @@\n     VariableAccessor,\n )\n from airflow.sdk.execution_time.task_runner import (\n-    CommsDecoder,\n     RuntimeTaskInstance,\n     TaskRunnerMarker,\n     _push_xcom_if_needed,\n@@ -139,47 +136,6 @@ def execute(self, context):\n         print(f\"Hello World {task_id}!\")\n \n \n-class TestCommsDecoder:\n-    \"\"\"Test the communication between the subprocess and the \"supervisor\".\"\"\"\n-\n-    @pytest.mark.usefixtures(\"disable_capturing\")\n-    def test_recv_StartupDetails(self):\n-        r, w = socketpair()\n-        # Create a valid FD for the decoder to open\n-        _, w2 = socketpair()\n-\n-        w.makefile(\"wb\").write(\n-            b'{\"type\":\"StartupDetails\", \"ti\": {'\n-            b'\"id\": \"4d828a62-a417-4936-a7a6-2b3fabacecab\", \"task_id\": \"a\", \"try_number\": 1, \"run_id\": \"b\", '\n-            b'\"dag_id\": \"c\"}, \"ti_context\":{\"dag_run\":{\"dag_id\":\"c\",\"run_id\":\"b\",'\n-            b'\"logical_date\":\"2024-12-01T01:00:00Z\",'\n-            b'\"data_interval_start\":\"2024-12-01T00:00:00Z\",\"data_interval_end\":\"2024-12-01T01:00:00Z\",'\n-            b'\"start_date\":\"2024-12-01T01:00:00Z\",\"run_after\":\"2024-12-01T01:00:00Z\",\"end_date\":null,'\n-            b'\"run_type\":\"manual\",\"conf\":null,\"consumed_asset_events\":[]},'\n-            b'\"max_tries\":0,\"should_retry\":false,\"variables\":null,\"connections\":null},\"file\": \"/dev/null\",'\n-            b'\"start_date\":\"2024-12-01T01:00:00Z\", \"dag_rel_path\": \"/dev/null\", \"bundle_info\": {\"name\": '\n-            b'\"any-name\", \"version\": \"any-version\"}, \"requests_fd\": '\n-            + str(w2.fileno()).encode(\"ascii\")\n-            + b\"}\\n\"\n-        )\n-\n-        decoder = CommsDecoder(input=r.makefile(\"r\"))\n-\n-        msg = decoder.get_message()\n-        assert isinstance(msg, StartupDetails)\n-        assert msg.ti.id == uuid.UUID(\"4d828a62-a417-4936-a7a6-2b3fabacecab\")\n-        assert msg.ti.task_id == \"a\"\n-        assert msg.ti.dag_id == \"c\"\n-        assert msg.dag_rel_path == \"/dev/null\"\n-        assert msg.bundle_info == BundleInfo(name=\"any-name\", version=\"any-version\")\n-        assert msg.start_date == timezone.datetime(2024, 12, 1, 1)\n-\n-        # Since this was a StartupDetails message, the decoder should open the other socket\n-        assert decoder.request_socket is not None\n-        assert decoder.request_socket.writable()\n-        assert decoder.request_socket.fileno() == w2.fileno()\n-\n-\n def test_parse(test_dags_dir: Path, make_ti_context):\n     \"\"\"Test that checks parsing of a basic dag with an un-mocked parse.\"\"\"\n     what = StartupDetails(\n@@ -192,7 +148,6 @@ def test_parse(test_dags_dir: Path, make_ti_context):\n         ),\n         dag_rel_path=\"super_basic.py\",\n         bundle_info=BundleInfo(name=\"my-bundle\", version=None),\n-        requests_fd=0,\n         ti_context=make_ti_context(),\n         start_date=timezone.utcnow(),\n     )\n@@ -248,7 +203,6 @@ def test_parse_not_found(test_dags_dir: Path, make_ti_context, dag_id, task_id,\n         ),\n         dag_rel_path=\"super_basic.py\",\n         bundle_info=BundleInfo(name=\"my-bundle\", version=None),\n-        requests_fd=0,\n         ti_context=make_ti_context(),\n         start_date=timezone.utcnow(),\n     )\n@@ -302,7 +256,6 @@ def test_parse_module_in_bundle_root(tmp_path: Path, make_ti_context):\n         ),\n         dag_rel_path=\"path_test.py\",\n         bundle_info=BundleInfo(name=\"my-bundle\", version=None),\n-        requests_fd=0,\n         ti_context=make_ti_context(),\n         start_date=timezone.utcnow(),\n     )\n@@ -360,8 +313,8 @@ def test_run_deferred_basic(time_machine, create_runtime_ti, mock_supervisor_com\n \n     assert ti.state == TaskInstanceState.DEFERRED\n \n-    # send_request will only be called when the TaskDeferred exception is raised\n-    mock_supervisor_comms.send_request.assert_any_call(msg=expected_defer_task, log=mock.ANY)\n+    # send will only be called when the TaskDeferred exception is raised\n+    mock_supervisor_comms.send.assert_any_call(expected_defer_task)\n \n \n def test_run_downstream_skipped(mocked_parse, create_runtime_ti, mock_supervisor_comms):\n@@ -384,8 +337,8 @@ def execute(self, context):\n \n     assert listener.state == [TaskInstanceState.RUNNING, TaskInstanceState.SUCCESS]\n     log.info.assert_called_with(\"Skipping downstream tasks.\")\n-    mock_supervisor_comms.send_request.assert_any_call(\n-        log=mock.ANY, msg=SkipDownstreamTasks(tasks=[\"task1\", \"task2\"], type=\"SkipDownstreamTasks\")\n+    mock_supervisor_comms.send.assert_any_call(\n+        SkipDownstreamTasks(tasks=[\"task1\", \"task2\"], type=\"SkipDownstreamTasks\")\n     )\n \n \n@@ -436,8 +389,8 @@ def test_run_basic_skipped(time_machine, create_runtime_ti, mock_supervisor_comm\n \n     assert ti.state == TaskInstanceState.SKIPPED\n \n-    mock_supervisor_comms.send_request.assert_called_with(\n-        msg=TaskState(state=TaskInstanceState.SKIPPED, end_date=instant), log=mock.ANY\n+    mock_supervisor_comms.send.assert_called_with(\n+        TaskState(state=TaskInstanceState.SKIPPED, end_date=instant)\n     )\n \n \n@@ -458,12 +411,11 @@ def test_run_raises_base_exception(time_machine, create_runtime_ti, mock_supervi\n \n     assert ti.state == TaskInstanceState.FAILED\n \n-    mock_supervisor_comms.send_request.assert_called_with(\n+    mock_supervisor_comms.send.assert_called_with(\n         msg=TaskState(\n             state=TaskInstanceState.FAILED,\n             end_date=instant,\n         ),\n-        log=mock.ANY,\n     )\n \n \n@@ -485,13 +437,7 @@ def test_run_raises_system_exit(time_machine, create_runtime_ti, mock_supervisor\n \n     assert ti.state == TaskInstanceState.FAILED\n \n-    mock_supervisor_comms.send_request.assert_called_with(\n-        msg=TaskState(\n-            state=TaskInstanceState.FAILED,\n-            end_date=instant,\n-        ),\n-        log=mock.ANY,\n-    )\n+    mock_supervisor_comms.send.assert_called_with(TaskState(state=TaskInstanceState.FAILED, end_date=instant))\n \n     log.exception.assert_not_called()\n     log.error.assert_called_with(mock.ANY, exit_code=10)\n@@ -516,13 +462,7 @@ def test_run_raises_airflow_exception(time_machine, create_runtime_ti, mock_supe\n \n     assert ti.state == TaskInstanceState.FAILED\n \n-    mock_supervisor_comms.send_request.assert_called_with(\n-        msg=TaskState(\n-            state=TaskInstanceState.FAILED,\n-            end_date=instant,\n-        ),\n-        log=mock.ANY,\n-    )\n+    mock_supervisor_comms.send.assert_called_with(TaskState(state=TaskInstanceState.FAILED, end_date=instant))\n \n \n def test_run_task_timeout(time_machine, create_runtime_ti, mock_supervisor_comms):\n@@ -545,13 +485,7 @@ def test_run_task_timeout(time_machine, create_runtime_ti, mock_supervisor_comms\n     assert ti.state == TaskInstanceState.FAILED\n \n     # this state can only be reached if the try block passed down the exception to handler of AirflowTaskTimeout\n-    mock_supervisor_comms.send_request.assert_called_with(\n-        msg=TaskState(\n-            state=TaskInstanceState.FAILED,\n-            end_date=instant,\n-        ),\n-        log=mock.ANY,\n-    )\n+    mock_supervisor_comms.send.assert_called_with(TaskState(state=TaskInstanceState.FAILED, end_date=instant))\n \n \n def test_basic_templated_dag(mocked_parse, make_ti_context, mock_supervisor_comms, spy_agency):\n@@ -573,7 +507,6 @@ def test_basic_templated_dag(mocked_parse, make_ti_context, mock_supervisor_comm\n         ),\n         bundle_info=FAKE_BUNDLE,\n         dag_rel_path=\"\",\n-        requests_fd=0,\n         ti_context=make_ti_context(),\n         start_date=timezone.utcnow(),\n     )\n@@ -583,7 +516,6 @@ def test_basic_templated_dag(mocked_parse, make_ti_context, mock_supervisor_comm\n     spy_agency.spy_on(task.prepare_for_execution)\n     assert not task._lock_for_execution\n \n-    # mock_supervisor_comms.get_message.return_value = what\n     run(ti, context=ti.get_template_context(), log=mock.Mock())\n \n     spy_agency.assert_spy_called(task.prepare_for_execution)\n@@ -591,7 +523,7 @@ def test_basic_templated_dag(mocked_parse, make_ti_context, mock_supervisor_comm\n     assert ti.task is not task, \"ti.task should be a copy of the original task\"\n     assert ti.state == TaskInstanceState.SUCCESS\n \n-    mock_supervisor_comms.send_request.assert_any_call(\n+    mock_supervisor_comms.send.assert_any_call(\n         msg=SetRenderedFields(\n             rendered_fields={\n                 \"bash_command\": \"echo 'Logical date is 2024-12-01 01:00:00+00:00'\",\n@@ -599,7 +531,6 @@ def test_basic_templated_dag(mocked_parse, make_ti_context, mock_supervisor_comm\n                 \"env\": None,\n             }\n         ),\n-        log=mock.ANY,\n     )\n \n \n@@ -689,7 +620,6 @@ def execute(self, context):\n         ),\n         dag_rel_path=\"\",\n         bundle_info=FAKE_BUNDLE,\n-        requests_fd=0,\n         ti_context=make_ti_context(),\n         start_date=timezone.utcnow(),\n     )\n@@ -697,22 +627,18 @@ def execute(self, context):\n \n     time_machine.move_to(instant, tick=False)\n \n-    mock_supervisor_comms.get_message.return_value = what\n+    mock_supervisor_comms._get_response.return_value = what\n \n     run(*startup())\n     expected_calls = [\n-        mock.call.send_request(\n-            msg=SetRenderedFields(rendered_fields=expected_rendered_fields),\n-            log=mock.ANY,\n-        ),\n-        mock.call.send_request(\n+        mock.call.send(SetRenderedFields(rendered_fields=expected_rendered_fields)),\n+        mock.call.send(\n             msg=SucceedTask(\n                 end_date=instant,\n                 state=TaskInstanceState.SUCCESS,\n                 task_outlets=[],\n                 outlet_events=[],\n             ),\n-            log=mock.ANY,\n         ),\n     ]\n     mock_supervisor_comms.assert_has_calls(expected_calls)\n@@ -766,9 +692,8 @@ def execute(self, context):\n     assert ti.state == TaskInstanceState.SUCCESS\n \n     # Ensure the task is Successful\n-    mock_supervisor_comms.send_request.assert_called_once_with(\n+    mock_supervisor_comms.send.assert_called_once_with(\n         msg=SucceedTask(state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]),\n-        log=mock.ANY,\n     )\n \n \n@@ -815,8 +740,8 @@ def execute(self, context):\n \n     assert ti.state == TaskInstanceState.FAILED\n \n-    mock_supervisor_comms.send_request.assert_called_once_with(\n-        msg=TaskState(state=TaskInstanceState.FAILED, end_date=instant), log=mock.ANY\n+    mock_supervisor_comms.send.assert_called_once_with(\n+        msg=TaskState(state=TaskInstanceState.FAILED, end_date=instant)\n     )\n \n \n@@ -834,12 +759,11 @@ def test_dag_parsing_context(make_ti_context, mock_supervisor_comms, monkeypatch\n         ti=TaskInstance(id=uuid7(), task_id=task_id, dag_id=dag_id, run_id=\"c\", try_number=1),\n         dag_rel_path=\"dag_parsing_context.py\",\n         bundle_info=BundleInfo(name=\"my-bundle\", version=None),\n-        requests_fd=0,\n         ti_context=make_ti_context(dag_id=dag_id, run_id=\"c\"),\n         start_date=timezone.utcnow(),\n     )\n \n-    mock_supervisor_comms.get_message.return_value = what\n+    mock_supervisor_comms._get_response.return_value = what\n \n     # Set the environment variable for DAG bundles\n     # We use the DAG defined in `task_sdk/tests/dags/dag_parsing_context.py` for this test!\n@@ -955,7 +879,7 @@ def test_run_with_asset_outlets(\n \n     validate_mock.assert_called_once()\n \n-    mock_supervisor_comms.send_request.assert_any_call(msg=expected_msg, log=mock.ANY)\n+    mock_supervisor_comms.send.assert_any_call(expected_msg)\n \n \n def test_run_with_asset_inlets(create_runtime_ti, mock_supervisor_comms):\n@@ -967,7 +891,7 @@ def test_run_with_asset_inlets(create_runtime_ti, mock_supervisor_comms):\n         asset=AssetResponse(name=\"test\", uri=\"test\", group=\"asset\"),\n     )\n     events_result = AssetEventsResult(asset_events=[asset_event_resp])\n-    mock_supervisor_comms.get_message.return_value = events_result\n+    mock_supervisor_comms.send.return_value = events_result\n \n     from airflow.providers.standard.operators.bash import BashOperator\n \n@@ -1121,7 +1045,7 @@ def test_get_context_with_ti_context_from_server(self, create_runtime_ti, mock_s\n \n         dr = runtime_ti._ti_context_from_server.dag_run\n \n-        mock_supervisor_comms.get_message.return_value = PrevSuccessfulDagRunResult(\n+        mock_supervisor_comms.send.return_value = PrevSuccessfulDagRunResult(\n             data_interval_end=dr.logical_date - timedelta(hours=1),\n             data_interval_start=dr.logical_date - timedelta(hours=2),\n             start_date=dr.start_date - timedelta(hours=1),\n@@ -1171,7 +1095,7 @@ def test_lazy_loading_not_triggered_until_accessed(self, create_runtime_ti, mock\n         task = BaseOperator(task_id=\"hello\")\n         runtime_ti = create_runtime_ti(task=task, dag_id=\"basic_task\")\n \n-        mock_supervisor_comms.get_message.return_value = PrevSuccessfulDagRunResult(\n+        mock_supervisor_comms.send.return_value = PrevSuccessfulDagRunResult(\n             data_interval_end=timezone.datetime(2025, 1, 1, 2, 0, 0),\n             data_interval_start=timezone.datetime(2025, 1, 1, 1, 0, 0),\n             start_date=timezone.datetime(2025, 1, 1, 1, 0, 0),\n@@ -1181,13 +1105,13 @@ def test_lazy_loading_not_triggered_until_accessed(self, create_runtime_ti, mock\n         context = runtime_ti.get_template_context()\n \n         # Assert lazy attributes are not resolved initially\n-        mock_supervisor_comms.get_message.assert_not_called()\n+        mock_supervisor_comms.send.assert_not_called()\n \n         # Access a lazy-loaded attribute to trigger computation\n         assert context[\"prev_data_interval_start_success\"] == timezone.datetime(2025, 1, 1, 1, 0, 0)\n \n         # Now the lazy attribute should trigger the call\n-        mock_supervisor_comms.get_message.assert_called_once()\n+        mock_supervisor_comms.send.assert_called_once()\n \n     def test_get_connection_from_context(self, create_runtime_ti, mock_supervisor_comms):\n         \"\"\"Test that the connection is fetched from the API server via the Supervisor lazily when accessed\"\"\"\n@@ -1206,22 +1130,18 @@ def test_get_connection_from_context(self, create_runtime_ti, mock_supervisor_co\n         )\n \n         runtime_ti = create_runtime_ti(task=task, dag_id=\"test_get_connection_from_context\")\n-        mock_supervisor_comms.get_message.return_value = conn\n+        mock_supervisor_comms.send.return_value = conn\n \n         context = runtime_ti.get_template_context()\n \n         # Assert that the connection is not fetched from the API server yet!\n         # The connection should be only fetched connection is accessed\n-        mock_supervisor_comms.send_request.assert_not_called()\n-        mock_supervisor_comms.get_message.assert_not_called()\n+        mock_supervisor_comms.send.assert_not_called()\n \n         # Access the connection from the context\n         conn_from_context = context[\"conn\"].test_conn\n \n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            log=mock.ANY, msg=GetConnection(conn_id=\"test_conn\")\n-        )\n-        mock_supervisor_comms.get_message.assert_called_once_with()\n+        mock_supervisor_comms.send.assert_called_once_with(GetConnection(conn_id=\"test_conn\"))\n \n         assert conn_from_context == Connection(\n             conn_id=\"test_conn\",\n@@ -1280,7 +1200,7 @@ def test_template_with_connection(\n             extra='{\"extra__asana__workspace\": \"extra1\"}',\n         )\n \n-        mock_supervisor_comms.get_message.return_value = conn\n+        mock_supervisor_comms.send.return_value = conn\n \n         context = runtime_ti.get_template_context()\n         result = runtime_ti.task.render_template(content, context)\n@@ -1307,22 +1227,18 @@ def test_get_variable_from_context(\n \n         var = VariableResult(key=\"test_key\", value=var_value)\n \n-        mock_supervisor_comms.get_message.return_value = var\n+        mock_supervisor_comms.send.return_value = var\n \n         context = runtime_ti.get_template_context()\n \n         # Assert that the variable is not fetched from the API server yet!\n         # The variable should be only fetched connection is accessed\n-        mock_supervisor_comms.send_request.assert_not_called()\n-        mock_supervisor_comms.get_message.assert_not_called()\n+        mock_supervisor_comms.send.assert_not_called()\n \n         # Access the variable from the context\n         var_from_context = context[\"var\"][accessor_type].test_key\n \n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            log=mock.ANY, msg=GetVariable(key=\"test_key\")\n-        )\n-        mock_supervisor_comms.get_message.assert_called_once_with()\n+        mock_supervisor_comms.send.assert_called_once_with(GetVariable(key=\"test_key\"))\n \n         assert var_from_context == expected_value\n \n@@ -1390,16 +1306,14 @@ def execute(self, context):\n \n         ser_value = BaseXCom.serialize_value(xcom_values)\n \n-        def mock_get_message_side_effect(*args, **kwargs):\n-            calls = mock_supervisor_comms.send_request.call_args_list\n-            if calls:\n-                last_call = calls[-1]\n-                msg = last_call[1][\"msg\"]\n-                if isinstance(msg, GetXComSequenceSlice):\n-                    return XComSequenceSliceResult(root=[ser_value])\n+        def mock_send_side_effect(*args, **kwargs):\n+            msg = kwargs.get(\"msg\") or args[0]\n+            print(f\"{args=}, {kwargs=}, {msg=}\")\n+            if isinstance(msg, GetXComSequenceSlice):\n+                return XComSequenceSliceResult(root=[ser_value])\n             return XComResult(key=\"key\", value=ser_value)\n \n-        mock_supervisor_comms.get_message.side_effect = mock_get_message_side_effect\n+        mock_supervisor_comms.send.side_effect = mock_send_side_effect\n \n         run(runtime_ti, context=runtime_ti.get_template_context(), log=mock.MagicMock())\n \n@@ -1415,8 +1329,7 @@ def mock_get_message_side_effect(*args, **kwargs):\n                 task_id = test_task_id\n             for map_index in map_indexes:\n                 if map_index == NOTSET:\n-                    mock_supervisor_comms.send_request.assert_any_call(\n-                        log=mock.ANY,\n+                    mock_supervisor_comms.send.assert_any_call(\n                         msg=GetXComSequenceSlice(\n                             key=\"key\",\n                             dag_id=\"test_dag\",\n@@ -1429,8 +1342,7 @@ def mock_get_message_side_effect(*args, **kwargs):\n                     )\n                 else:\n                     expected_map_index = map_index if map_index is not None else None\n-                    mock_supervisor_comms.send_request.assert_any_call(\n-                        log=mock.ANY,\n+                    mock_supervisor_comms.send.assert_any_call(\n                         msg=GetXCom(\n                             key=\"key\",\n                             dag_id=\"test_dag\",\n@@ -1591,11 +1503,8 @@ def execute(self, context):\n         context = runtime_ti.get_template_context()\n         run(runtime_ti, context=context, log=mock.MagicMock())\n \n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            msg=SucceedTask(\n-                state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]\n-            ),\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_called_once_with(\n+            SucceedTask(state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]),\n         )\n \n         with mock.patch.object(XCom, \"_set_xcom_in_db\") as mock_xcom_set:\n@@ -1644,9 +1553,8 @@ def __init__(self, bash_command, *args, **kwargs):\n             log=mock.MagicMock(),\n         )\n \n-        mock_supervisor_comms.send_request.assert_called_with(\n-            msg=SetRenderedFields(rendered_fields={\"bash_command\": rendered_cmd}),\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_called_with(\n+            msg=SetRenderedFields(rendered_fields={\"bash_command\": rendered_cmd})\n         )\n \n     @pytest.mark.parametrize(\n@@ -1669,7 +1577,7 @@ def test_get_first_reschedule_date(\n         task = BaseOperator(task_id=\"hello\")\n         runtime_ti = create_runtime_ti(task=task, task_reschedule_count=task_reschedule_count)\n \n-        mock_supervisor_comms.get_message.return_value = TaskRescheduleStartDate(\n+        mock_supervisor_comms.send.return_value = TaskRescheduleStartDate(\n             start_date=timezone.datetime(2025, 1, 1)\n         )\n \n@@ -1678,7 +1586,7 @@ def test_get_first_reschedule_date(\n \n     def test_get_ti_count(self, mock_supervisor_comms):\n         \"\"\"Test that get_ti_count sends the correct request and returns the count.\"\"\"\n-        mock_supervisor_comms.get_message.return_value = TICount(count=2)\n+        mock_supervisor_comms.send.return_value = TICount(count=2)\n \n         count = RuntimeTaskInstance.get_ti_count(\n             dag_id=\"test_dag\",\n@@ -1689,8 +1597,7 @@ def test_get_ti_count(self, mock_supervisor_comms):\n             states=[\"success\", \"failed\"],\n         )\n \n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_called_once_with(\n             msg=GetTICount(\n                 dag_id=\"test_dag\",\n                 task_ids=[\"task1\", \"task2\"],\n@@ -1704,7 +1611,7 @@ def test_get_ti_count(self, mock_supervisor_comms):\n \n     def test_get_dr_count(self, mock_supervisor_comms):\n         \"\"\"Test that get_dr_count sends the correct request and returns the count.\"\"\"\n-        mock_supervisor_comms.get_message.return_value = DRCount(count=2)\n+        mock_supervisor_comms.send.return_value = DRCount(count=2)\n \n         count = RuntimeTaskInstance.get_dr_count(\n             dag_id=\"test_dag\",\n@@ -1713,8 +1620,7 @@ def test_get_dr_count(self, mock_supervisor_comms):\n             states=[\"success\", \"failed\"],\n         )\n \n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_called_once_with(\n             msg=GetDRCount(\n                 dag_id=\"test_dag\",\n                 logical_dates=[timezone.datetime(2024, 1, 1)],\n@@ -1726,27 +1632,21 @@ def test_get_dr_count(self, mock_supervisor_comms):\n \n     def test_get_dagrun_state(self, mock_supervisor_comms):\n         \"\"\"Test that get_dagrun_state sends the correct request and returns the state.\"\"\"\n-        mock_supervisor_comms.get_message.return_value = DagRunStateResult(state=\"running\")\n+        mock_supervisor_comms.send.return_value = DagRunStateResult(state=\"running\")\n \n         state = RuntimeTaskInstance.get_dagrun_state(\n             dag_id=\"test_dag\",\n             run_id=\"run1\",\n         )\n \n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            log=mock.ANY,\n-            msg=GetDagRunState(\n-                dag_id=\"test_dag\",\n-                run_id=\"run1\",\n-            ),\n+        mock_supervisor_comms.send.assert_called_once_with(\n+            msg=GetDagRunState(dag_id=\"test_dag\", run_id=\"run1\"),\n         )\n         assert state == \"running\"\n \n     def test_get_task_states(self, mock_supervisor_comms):\n         \"\"\"Test that get_task_states sends the correct request and returns the states.\"\"\"\n-        mock_supervisor_comms.get_message.return_value = TaskStatesResult(\n-            task_states={\"run1\": {\"task1\": \"running\"}}\n-        )\n+        mock_supervisor_comms.send.return_value = TaskStatesResult(task_states={\"run1\": {\"task1\": \"running\"}})\n \n         states = RuntimeTaskInstance.get_task_states(\n             dag_id=\"test_dag\",\n@@ -1754,8 +1654,7 @@ def test_get_task_states(self, mock_supervisor_comms):\n             run_ids=[\"run1\"],\n         )\n \n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_called_once_with(\n             msg=GetTaskStates(\n                 dag_id=\"test_dag\",\n                 task_ids=[\"task1\"],\n@@ -1929,7 +1828,6 @@ def execute(self, context):\n         assert not any(\n             x\n             == mock.call(\n-                log=mock.ANY,\n                 msg=SetXCom(\n                     key=\"key\",\n                     value=\"pushing to xcom backend!\",\n@@ -1939,7 +1837,7 @@ def execute(self, context):\n                     map_index=-1,\n                 ),\n             )\n-            for x in mock_supervisor_comms.send_request.call_args_list\n+            for x in mock_supervisor_comms.send.call_args_list\n         )\n \n     def test_xcom_pull_from_custom_xcom_backend(\n@@ -1966,7 +1864,6 @@ def execute(self, context):\n         assert not any(\n             x\n             == mock.call(\n-                log=mock.ANY,\n                 msg=GetXCom(\n                     key=\"key\",\n                     dag_id=\"test_dag\",\n@@ -1975,7 +1872,7 @@ def execute(self, context):\n                     map_index=-1,\n                 ),\n             )\n-            for x in mock_supervisor_comms.send_request.call_args_list\n+            for x in mock_supervisor_comms.send.call_args_list\n         )\n \n \n@@ -2006,11 +1903,8 @@ def execute(self, context):\n \n         run(runtime_ti, context=runtime_ti.get_template_context(), log=mock.MagicMock())\n \n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            msg=SucceedTask(\n-                state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]\n-            ),\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_called_once_with(\n+            SucceedTask(state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]),\n         )\n \n     def test_dag_param_dag_overwrite(self, create_runtime_ti, mock_supervisor_comms, time_machine):\n@@ -2035,11 +1929,8 @@ def execute(self, context):\n             task=task, dag_id=\"dag_with_dag_params_overwrite\", conf={\"value\": \"new_value\"}\n         )\n         run(runtime_ti, context=runtime_ti.get_template_context(), log=mock.MagicMock())\n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            msg=SucceedTask(\n-                state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]\n-            ),\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_called_once_with(\n+            SucceedTask(state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]),\n         )\n \n     def test_dag_param_dag_default(self, create_runtime_ti, mock_supervisor_comms, time_machine):\n@@ -2062,11 +1953,8 @@ def execute(self, context):\n         runtime_ti = create_runtime_ti(task=task, dag_id=\"dag_with_dag_params_default\")\n \n         run(runtime_ti, context=runtime_ti.get_template_context(), log=mock.MagicMock())\n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            msg=SucceedTask(\n-                state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]\n-            ),\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_called_once_with(\n+            SucceedTask(state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]),\n         )\n \n     def test_dag_param_resolves(\n@@ -2097,11 +1985,8 @@ def execute(self, context):\n \n         run(runtime_ti, context=runtime_ti.get_template_context(), log=mock.MagicMock())\n \n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            msg=SucceedTask(\n-                state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]\n-            ),\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_called_once_with(\n+            SucceedTask(state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]),\n         )\n \n     def test_dag_param_dagrun_parameterized(\n@@ -2136,11 +2021,8 @@ def execute(self, context):\n \n         run(runtime_ti, context=runtime_ti.get_template_context(), log=mock.MagicMock())\n \n-        mock_supervisor_comms.send_request.assert_called_once_with(\n-            msg=SucceedTask(\n-                state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]\n-            ),\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_called_once_with(\n+            SucceedTask(state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]),\n         )\n \n     @pytest.mark.parametrize(\"value\", [VALUE, 0])\n@@ -2167,11 +2049,8 @@ def return_num(num):\n \n         run(runtime_ti, context=runtime_ti.get_template_context(), log=mock.MagicMock())\n \n-        mock_supervisor_comms.send_request.assert_any_call(\n-            msg=SucceedTask(\n-                state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]\n-            ),\n-            log=mock.ANY,\n+        mock_supervisor_comms.send.assert_any_call(\n+            SucceedTask(state=TaskInstanceState.SUCCESS, end_date=instant, task_outlets=[], outlet_events=[]),\n         )\n \n \n@@ -2234,12 +2113,11 @@ def execute(self, context):\n             ),\n             dag_rel_path=\"\",\n             bundle_info=FAKE_BUNDLE,\n-            requests_fd=0,\n             ti_context=make_ti_context(),\n             start_date=timezone.utcnow(),\n         )\n \n-        mock_supervisor_comms.get_message.return_value = what\n+        mock_supervisor_comms._get_response.return_value = what\n         mocked_parse(what, \"basic_dag\", task)\n \n         runtime_ti, context, log = startup()\n@@ -2539,17 +2417,15 @@ def test_handle_trigger_dag_run(self, create_runtime_ti, mock_supervisor_comms):\n         assert msg.state == TaskInstanceState.SUCCESS\n \n         expected_calls = [\n-            mock.call.send_request(\n+            mock.call.send(\n                 msg=TriggerDagRun(\n                     dag_id=\"test_dag\",\n                     run_id=\"test_run_id\",\n                     reset_dag_run=False,\n                     logical_date=datetime(2025, 1, 1, 0, 0, 0, tzinfo=timezone.utc),\n                 ),\n-                log=mock.ANY,\n             ),\n-            mock.call.get_message(),\n-            mock.call.send_request(\n+            mock.call.send(\n                 msg=SetXCom(\n                     key=\"trigger_run_id\",\n                     value=\"test_run_id\",\n@@ -2558,7 +2434,6 @@ def test_handle_trigger_dag_run(self, create_runtime_ti, mock_supervisor_comms):\n                     run_id=\"test_run\",\n                     map_index=-1,\n                 ),\n-                log=mock.ANY,\n             ),\n         ]\n         mock_supervisor_comms.assert_has_calls(expected_calls)\n@@ -2586,23 +2461,21 @@ def test_handle_trigger_dag_run_conflict(\n         ti = create_runtime_ti(dag_id=\"test_handle_trigger_dag_run_conflict\", run_id=\"test_run\", task=task)\n \n         log = mock.MagicMock()\n-        mock_supervisor_comms.get_message.return_value = ErrorResponse(error=ErrorType.DAGRUN_ALREADY_EXISTS)\n+        mock_supervisor_comms.send.return_value = ErrorResponse(error=ErrorType.DAGRUN_ALREADY_EXISTS)\n         state, msg, _ = run(ti, ti.get_template_context(), log)\n \n         assert state == expected_state\n         assert msg.state == expected_state\n \n         expected_calls = [\n-            mock.call.send_request(\n+            mock.call.send(\n                 msg=TriggerDagRun(\n                     dag_id=\"test_dag\",\n                     logical_date=datetime(2025, 1, 1, 0, 0, 0, tzinfo=timezone.utc),\n                     run_id=\"test_run_id\",\n                     reset_dag_run=False,\n                 ),\n-                log=mock.ANY,\n             ),\n-            mock.call.get_message(),\n         ]\n         mock_supervisor_comms.assert_has_calls(expected_calls)\n \n@@ -2647,13 +2520,19 @@ def test_handle_trigger_dag_run_wait_for_completion(\n         )\n \n         log = mock.MagicMock()\n-        mock_supervisor_comms.get_message.side_effect = [\n+        mock_supervisor_comms.send.side_effect = [\n+            # Set RTIF\n+            None,\n             # Successful Dag Run trigger\n             OKResponse(ok=True),\n+            # Set XCOM,\n+            None,\n             # Dag Run is still running\n             DagRunStateResult(state=DagRunState.RUNNING),\n             # Dag Run completes execution on the next poll\n             DagRunStateResult(state=target_dr_state),\n+            # Succeed/Fail task\n+            None,\n         ]\n         with mock.patch(\"time.sleep\", return_value=None):\n             state, msg, _ = run(ti, ti.get_template_context(), log)\n@@ -2662,16 +2541,14 @@ def test_handle_trigger_dag_run_wait_for_completion(\n         assert msg.state == expected_task_state\n \n         expected_calls = [\n-            mock.call.send_request(\n+            mock.call.send(\n                 msg=TriggerDagRun(\n                     dag_id=\"test_dag\",\n                     run_id=\"test_run_id\",\n                     logical_date=datetime(2025, 1, 1, 0, 0, 0, tzinfo=timezone.utc),\n                 ),\n-                log=mock.ANY,\n             ),\n-            mock.call.get_message(),\n-            mock.call.send_request(\n+            mock.call.send(\n                 msg=SetXCom(\n                     key=\"trigger_run_id\",\n                     value=\"test_run_id\",\n@@ -2680,22 +2557,18 @@ def test_handle_trigger_dag_run_wait_for_completion(\n                     run_id=\"test_run\",\n                     map_index=-1,\n                 ),\n-                log=mock.ANY,\n             ),\n-            mock.call.send_request(\n+            mock.call.send(\n                 msg=GetDagRunState(\n                     dag_id=\"test_dag\",\n                     run_id=\"test_run_id\",\n                 ),\n-                log=mock.ANY,\n             ),\n-            mock.call.get_message(),\n-            mock.call.send_request(\n+            mock.call.send(\n                 msg=GetDagRunState(\n                     dag_id=\"test_dag\",\n                     run_id=\"test_run_id\",\n                 ),\n-                log=mock.ANY,\n             ),\n         ]\n         mock_supervisor_comms.assert_has_calls(expected_calls)\n","problem_statement":"Trigger runner process locked with multiple Workflow triggers\n### Apache Airflow version\n\n3.0.0\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nI am running a few dags that can start multiple workflow triggers. When the amount of triggers is low, below 10 or so; all works fine but whenever it goes higher the trigger runner seems to get stuck. I traced it back to one of the dag_count calls on RuntimeTaskInstance getting stuck/taking too long which locks all the other triggers and the syncing of the trigger runner as the SUPERVISOR_COMMS object stays in its locked state. The triggers seems to be started at the same time.\n\nI already tried to solve the problem by introducing a timeout on the count calls but that made the trigger runner crash as the incoming messages are getting mixed up (response of a cancelled call finally getting through). Only solution I see right now is to perform API calls on the public endpoint for the dag count method calls.\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nHave more than twenty or so workflow triggers running started at the same, I suppose they poll states at about the same time.\n\n### Operating System\n\ndebian 12\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\nAirflow DAG trigger with Amazon SQS Asset watcher trigger not working properly\n### Apache Airflow Provider(s)\n\namazon\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-amazon 9.8.0\n\n### Apache Airflow version\n\n3.0.1\n\n### Operating System\n\nUbuntu 22.04.3 LTS\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\nI'm using the official airflow docker compose file to spin up the airflow 3.0.1\n\n### What happened\n\nI have a DAG configured with an Amazon SQS trigger using Airflow 3.0.1 and the airflow/assets approach to auto-trigger DAGs upon message arrival in the queue. The DAG works as expected for a short period (approximately 30 minutes), triggering whenever a message is sent to the SQS queue.\n\nHowever, after this initial period:\n\n- The DAG stops triggering despite new messages being available in the SQS queue.\n- Restarting Airflow using docker compose restart temporarily restores the triggering functionality.\n- After another ~30 minutes, the problem recurs.\n- In some cases, even after restarting, the DAG never triggers again.\n- The aws_default connection used for the SQS integration remains valid and operational (confirmed via manual testing).\n\n### What you think should happen instead\n\nThe SQS-triggered DAG should:\n\n- Continuously monitor the queue and trigger the DAG when a new message arrives.\n- Remain functional indefinitely without needing to restart Airflow.\n- Provide meaningful logs or error messages if/when it stops functioning.\n\n### How to reproduce\n\n1. Use Airflow 3.0.1 official airflow docker compose\n2. Deploy with Docker Compose and start the Airflow environment.\n3. Define a DAG with an SQS trigger using Airflow assets.\n4. Set up a valid aws_default connection with appropriate IAM permissions.\n5. Send messages to the SQS queue  observe that the DAG is triggered.\n6. Wait  after a while, send another message. (Or use a separate python script that sends messages to the SQS at regular intervals)\n7. Observe that the DAG is no longer triggered.\n8. Restart Airflow with docker compose restart, and observe that triggering works again briefly, then fails. (Sometimes even after the restart also it never gets triggered).\n9. Below is the airflow DAG that I'm using.\n\n```from airflow.providers.common.messaging.triggers.msg_queue import MessageQueueTrigger\nfrom airflow.sdk import Asset, AssetWatcher, dag, task\nimport os\n\n# Define the SQS queue URL\n# Replace my_account_id and my_queue_name\nSQS_QUEUE = \"https://sqs.us-east-1.amazonaws.com/<my_account_id>/<my_queue_name>\"\n\n# Define a trigger that listens to an external message queue (AWS SQS in this case)\ntrigger = MessageQueueTrigger(\n    aws_conn_id=\"aws_default\",\n    queue=SQS_QUEUE,\n    waiter_delay=10,  # delay in seconds between polls\n)\n\n# Define an asset that watches for messages on the queue\nsqs_queue_asset = Asset(\n    \"sqs_queue_asset\", watchers=[AssetWatcher(name=\"sqs_watcher\", trigger=trigger)]\n)\n\n\n# Schedule the DAG to run when the asset is triggered\n@dag(schedule=[sqs_queue_asset])\ndef event_driven_dag():\n    @task\n    def process_message(**context):\n        # Extract the triggering asset events from the context\n        triggering_asset_events = context[\"triggering_asset_events\"]\n        for event in triggering_asset_events[sqs_queue_asset]:\n            # Get the message from the TriggerEvent payload\n            print(\n                f\"Processing message: {event.extra[\"payload\"][\"message_batch\"][0][\"Body\"]}\"\n            )\n\n    process_message()\n\n\nevent_driven_dag()\n","hints_text":"Is it possible to write a dag that can reliably reproduce this?\ncc @gopidesupavan \nYeah will look at this today.\n@gopidesupavan are you still looking into this?\nFor now I solved througt the creation of an async Task SDK client which the triggers use directly. In my setup the triggers always have the api server running on the same instances, hence I could use localhost traffic. In this way it is rather similar like the in-process traffic but I do not overload the data pipe between trigger runner and supervisor. From my investigation it seems that using too much external workload triggers (count calls) together with the supervisor-runner syncs congests that pipe in such a way that all calls were blocked indefinitely resulting in a trigger process that seems alive (heartbeats still worked) but in reality it is stuck.\n\nProbably this is not the designed pattern, but for now this solution makes it possible for me to role out airflow 3.0 at our office.  \nI am able to re produce it with one of example ExternalTaskSensor dags. It looks blocking somewhere and triggers are not getting to added to further process.\n\nNeed to workout why it's blocking though we use async lock. Will check and update.\n> For now I solved througt the creation of an async Task SDK client which the triggers use directly. In my setup the triggers always have the api server running on the same instances, hence I could use localhost traffic. In this way it is rather similar like the in-process traffic but I do not overload the data pipe between trigger runner and supervisor. From my investigation it seems that using too much external workload triggers (count calls) together with the supervisor-runner syncs congests that pipe in such a way that all calls were blocked indefinitely resulting in a trigger process that seems alive (heartbeats still worked) but in reality it is stuck.\n> \n> Probably this is not the designed pattern, but for now this solution makes it possible for me to role out airflow 3.0 at our office.\n\n@ArvidMartensRenson we dont have async client in Task SDK, did you overwrite the Triggerer client method, does it fixed? \nI created an async client from the sync client, using httpx.AsyncClient as the base (like httpx.Client is for the sync client). Inherited the init client from the sync task client. Only problem I had was that the operations had to be rewritten as an async method, so I copied the code from the sync client and changed the method definition to async. I found this implementation nicer than using plain api calls with the async httpx client. \n\nTraffic is over localhost so even then a token is required, I reused the methods that are used in the creation of the task workload to get the token. I considered dropping the token requirement but it seemed safer and whenever the trigger service and api server are separated it will be needed.\n\nI implemented this solution last Thursday and for now the trigger service has not been stuck and the external workload triggers all completed. So for now I would conclude that the solution works.\n\nIf it is desirable to have an async client I am happy to help on creating the client, however I am a little worried with duplicate code I have now, same code for the sync and async client. If someone knows a solution for this or a way to handle that problem, that would be nice. \n\nFor the record, I am using a custom implementation of the workload trigger. I was an early adopter of the trigger paradigm and the external workload trigger did not exist at that time, so we had to implement our own trigger. We never came around converting to the standard provider implementation.\nThanks @ArvidMartensRenson that explains me what's happening. During debugging, I noticed that requests are being sent, but no responses are being returned. Since we use a lock when sending requests, the absence of a response causes the lock to remain active, which in turn blocks any further triggers from being processed.\n\nWill check on async client part over this week.\n> I created an async client from the sync client, using httpx.AsyncClient as the base (like httpx.Client is for the sync client). Inherited the init client from the sync task client. Only problem I had was that the operations had to be rewritten as an async method, so I copied the code from the sync client and changed the method definition to async. I found this implementation nicer than using plain api calls with the async httpx client.\n> \n> Traffic is over localhost so even then a token is required, I reused the methods that are used in the creation of the task workload to get the token. I considered dropping the token requirement but it seemed safer and whenever the trigger service and api server are separated it will be needed.\n> \n> I implemented this solution last Thursday and for now the trigger service has not been stuck and the external workload triggers all completed. So for now I would conclude that the solution works.\n> \n> If it is desirable to have an async client I am happy to help on creating the client, however I am a little worried with duplicate code I have now, same code for the sync and async client. If someone knows a solution for this or a way to handle that problem, that would be nice.\n\nI don't see any issues with using an async client, it's part of the triggerer, and most triggers are already asynchronous. I do agree that there's some code duplication. Please feel free to submit your approach; itll be great to have more eyes on it and gather suggestions to refine the solution. :) \n\n> \n> For the record, I am using a custom implementation of the workload trigger. I was an early adopter of the trigger paradigm and the external workload trigger did not exist at that time, so we had to implement our own trigger. We never came around converting to the standard provider implementation.\n\n\nLooks like the lock inside sync_to_async function causing issues, its likely messing up with threads acquired in side the sync_to_async. We use get_ti_count, get_task_states, get_dr_count from the `RuntimeTaskInstance`. likely this is not a good choice.\n\nI have Moved those functions to async version without using sync_to_async and working fine, did some stress test around 400 triggers nothing locked everything running fine.\n\n@ArvidMartensRenson we may not need async client. i will raise changes for moving the functions to async, please test it once i raised? async client type also would be good , but in this case i dont see any need. its working without async client. \n@ArvidMartensRenson https://github.com/apache/airflow/pull/51085 would be helpful if you test this change please?\n\ni ran with aroud 400 triggerers and dont see any locks.\n@ArvidMartensRenson, I would like to clarify: do threads get stuck when trying to acquire the lock when some other thread is already performing some actions holding the lock, or do they get stuck for no reason? I ask because in the second case it would mean that the problem is most likely on my side - in which case I would consider it a critical bug and release a fix version. However, I tested the lock with `sync_to_async()` yesterday right after the PR appeared and was unable to reproduce the problem in isolated tests (with both stable and the latest version of aiologic).\n@x42005e1f can i have your example you tested? In our trigger process we use lock inside sync_to_async.\n> [@x42005e1f](https://github.com/x42005e1f) can i have your example you tested? In our trigger process we use lock inside sync_to_async.\n\nYes, here is a simple mixed test:\n\n<details>\n<summary>test.py</summary>\n\n```python\nimport asyncio\nimport sys\nimport time\n\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nimport aiologic\n\nfrom asgiref.sync import sync_to_async\n\nif sys.version_info >= (3, 11):\n    WaitTimeout = TimeoutError\nelse:\n    from concurrent.futures import TimeoutError as WaitTimeout\n\nDELAY = 1e-6\nTIMEOUT = 6.0  # must be reached (success)\nTHREADS = 100\nTASKS = 100\n\nlock = aiologic.Lock()\nbarrier = aiologic.Latch(THREADS)\nstopped = False\n\n\ndef sync_acquire_release():\n    with lock:\n        time.sleep(DELAY)\n\n\nasync def async_acquire_release(i):\n    if i % 2 == 0:\n        while True:\n            await sync_to_async(sync_acquire_release)()\n\n            if stopped:\n                break\n    else:\n        while True:\n            async with lock:\n                await asyncio.sleep(DELAY)\n\n            if stopped:\n                break\n\n\nasync def hub():\n    await barrier\n    await asyncio.gather(*(\n        asyncio.create_task(async_acquire_release(i))\n        for i in range(TASKS)\n    ))\n\n\nif __name__ == \"__main__\":\n    with ThreadPoolExecutor(THREADS) as executor:\n        interval = sys.getswitchinterval()\n        sys.setswitchinterval(min(1e-6, interval))\n\n        try:\n            futures = [\n                executor.submit(asyncio.run, hub())\n                for _ in range(THREADS)\n            ]\n\n            for future in as_completed(futures, timeout=TIMEOUT):\n                future.result()  # reraise\n        except WaitTimeout:\n            pass\n        finally:\n            sys.setswitchinterval(interval)\n            stopped = True\n```\n\n</details>\n\nBut note that `sync_to_async()` uses only one worker thread by default, which blocks all further calls until the first one is completed. This can be seen, for example, in this example:\n\n<details>\n<summary>example.py</summary>\n\n```python\nimport asyncio\nimport sys\nimport threading\n\nfrom asgiref.sync import sync_to_async\n\n\nasync def main():\n    loop = asyncio.get_running_loop()\n    first = asyncio.Event()\n    second = threading.Event()\n\n    def a():\n        print(\"'a' started!\")\n        loop.call_soon_threadsafe(first.set)\n        second.wait()\n\n    def b():\n        print(\"'b' started!\")  # will never be printed!\n        second.set()\n\n    f1 = asyncio.create_task(sync_to_async(a)())\n    await first.wait()\n    print(\"notified!\")\n    f2 = asyncio.create_task(sync_to_async(b)())\n\n    await asyncio.gather(f1, f2)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n</details>\n\nWhat is indicative is that my test works well in free-threaded mode. In this mode threads are more aggressive, especially on multi-core systems, which better reveals thread-safety issues.\nAlso note that the different APIs that aiologic provides should not be mixed in the same thread if threading is used for the green (sync) API. This is a fundamental problem due to the fact that primitives work at the task level: `lock.green_acquire()` can block the event loop when `aiologic.lowlevel.current_green_library() == \"threading\"`, and as a result lock will never be released. This is well demonstrated in the following example:\n\n<details>\n<summary>example.py</summary>\n\n```python\nimport asyncio\nimport time\n\nimport aiologic\n\n\nasync def main():\n    lock = aiologic.Lock()\n\n    async def a():\n        async with lock:\n            print(\"before\")\n            await asyncio.sleep(0)\n            print(\"after\")  # will never be printed!\n\n    async def b():\n        with lock:\n            time.sleep(0)\n\n    await asyncio.gather(a(), b())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n</details>\n\nThis is the same as using `threading.Lock` along with asyncio tasks. If you have such calls somewhere, it can be the cause of deadlocks.\n\nThis problem of mixing two APIs in the same thread can be solved by using [GLock](https://gist.github.com/x42005e1f/a50d0744013b7bbbd7ded608d6a3845b), which can be a thread-level lock - if you use the read-preferring readers-writer locks condition (`self.group is None or self.group == group`), its APIs can safely mix (just replace `lock = aiologic.Lock()` with `lock = GLock(default_group_factory=current_thread_ident)` and that example will work without hanging). However, since it would no longer be a task level, it would not be coroutine-safe.\n> Also note that the different APIs that aiologic provides should not be mixed in the same thread if threading is used for the green (sync) API. This is a fundamental problem due to the fact that primitives work at the task level: `lock.green_acquire()` can block the event loop when `aiologic.lowlevel.current_green_library() == \"threading\"`, and as a result lock will never be released. This is well demonstrated in the following example:\n> \n> example.py\n> This is the same as using `threading.Lock` along with asyncio tasks. If you have such calls somewhere, it can be the cause of deadlocks.\n> \n> This problem of mixing two APIs in the same thread can be solved by using [GLock](https://gist.github.com/x42005e1f/a50d0744013b7bbbd7ded608d6a3845b), which can be a thread-level lock - if you use the read-preferring readers-writer locks condition (`self.group is None or self.group == group`), its APIs can safely mix (just replace `lock = aiologic.Lock()` with `lock = GLock(default_group_factory=current_thread_ident)` and that example will work without hanging). However, since it would no longer be a task level, it would not be coroutine-safe.\n\nlooks like we are using that similar way some places. this is async one https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/triggerer_job_runner.py#L965 and in other places we are using like `with SUPERVISOR_COMMS.lock:`  \n\nBTW GLock where can i find this package i dont see it on PyPI? i will try with this\nIf the problem is the same as what I described (which is likely), the (relatively) easiest way to solve it is to use libraries like [greenback](https://github.com/oremanj/greenback) to execute an asynchronous version when the code runs in a thread with an event loop. Requires explicit portal creation, greenlet support, and interrupt support on the synchronous code side.\n\nIt is also possible to create asynchronous versions of functions to use directly, or delegate them to execute sequentially in the worker thread via `sync_to_async()`, but this can be difficult, especially since [`SUPERVISOR_COMMS.lock` is used even in `__getitem__()`](https://github.com/apache/airflow/blob/7ebba78236b945e0ce569607480f397e5f2e58ba/task-sdk/src/airflow/sdk/execution_time/lazy_sequence.py#L147).\n\n> BTW GLock where can i find this package i dont see it on PyPI? i will try with this\n\nYou can directly copy the code from gist and make the mentioned change. GLock [was created to provide readers-writer locks](https://github.com/x42005e1f/aiologic/discussions/6) before Grouper appears in a future version of aiologic. Right now my efforts are focused on moving aiologic to the beta development stage, so releases are currently delayed.\n\nBut be aware that if you choose GLock, you will have to deal with the loss of coroutine-safety: in particular, the asynchronous usage you mentioned will be executed in one thread at a time, but will also be able to be executed in different tasks at the same time.\n> If the problem is the same as what I described (which is likely), the (relatively) easiest way to solve it is to use libraries like [greenback](https://github.com/oremanj/greenback) to execute an asynchronous version when the code runs in a thread with an event loop. Requires explicit portal creation, greenlet support, and interrupt support on the synchronous code side.\n> \n> It is also possible to create asynchronous versions of functions to use directly, or delegate them to execute sequentially in the worker thread via `sync_to_async()`, but this can be difficult, especially since [`SUPERVISOR_COMMS.lock` is used even in `__getitem__()`](https://github.com/apache/airflow/blob/7ebba78236b945e0ce569607480f397e5f2e58ba/task-sdk/src/airflow/sdk/execution_time/lazy_sequence.py#L147).\n> \n> > BTW GLock where can i find this package i dont see it on PyPI? i will try with this\n> \n> You can directly copy the code from gist and make the mentioned change. GLock [was created to provide readers-writer locks](https://github.com/x42005e1f/aiologic/discussions/6) before Grouper appears in a future version of aiologic. Right now my efforts are focused on moving aiologic to the beta development stage, so releases are currently delayed.\n> \n> But be aware that if you choose GLock, you will have to deal with the loss of coroutine-safety: in particular, the asynchronous usage you mentioned will be executed in one thread at a time, but will also be able to be executed in different tasks at the same time.\n\n@x42005e1f Thats a good suggestion, greenback seems working, did some tests and everything seems fine. no triggers blocked. \n\nLooks like this package is not maintained or not sure, no activity since last Feb 2024.\n> Looks like this package is not maintained or not sure, no activity since last Feb 2024.\n\nIt is not always possible to tell from the commit activity of such packages whether they are maintained or not. Sometimes they are not updated just because there are no serious issues, in which case the author can focus on other projects. Even more ambiguous is the situation when tests are regularly run for new versions of dependencies and a lockfile is updated - in this case the repository may look alive with a huge number of commits, but in fact it has not been updated for a very long time, having very few commits in the source code for the whole history.\n\nThere are also alternatives such as [greenletio](https://github.com/miguelgrinberg/greenletio) or [awaitlet](https://github.com/sqlalchemy/awaitlet). But if you want, you can implement something similar on your side - for example, SQLAlchemy has long used [its own module](https://github.com/sqlalchemy/sqlalchemy/blob/05b2442132d5ae31cfcc7a1fe95e0f6b739aa995/lib/sqlalchemy/util/concurrency.py) as part of asynchronous API implementation.\n\nThese solutions have some disadvantages associated with stack growth, but they are usually insignificant. A more comprehensive solution is to implement (generators and) coroutines via greenlets - what I called (genlets and) corolets 3 years ago (for a non-public tutorial on asynchronous library design). But it is redundant for this task.\n> > Looks like this package is not maintained or not sure, no activity since last Feb 2024.\n> \n> It is not always possible to tell from the commit activity of such packages whether they are maintained or not. Sometimes they are not updated just because there are no serious issues, in which case the author can focus on other projects. Even more ambiguous is the situation when tests are regularly run for new versions of dependencies and a lockfile is updated - in this case the repository may look alive with a huge number of commits, but in fact it has not been updated for a very long time, having very few commits in the source code for the whole history.\n> \n> There are also alternatives such as [greenletio](https://github.com/miguelgrinberg/greenletio) or [awaitlet](https://github.com/sqlalchemy/awaitlet). But if you want, you can implement something similar on your side - for example, SQLAlchemy has long used [its own module](https://github.com/sqlalchemy/sqlalchemy/blob/05b2442132d5ae31cfcc7a1fe95e0f6b739aa995/lib/sqlalchemy/util/concurrency.py) as part of asynchronous API implementation.\n> \n> These solutions have some disadvantages associated with stack growth, but they are usually insignificant. A more comprehensive solution is to implement (generators and) coroutines via greenlets - what I called (genlets and) corolets 3 years ago (for a non-public tutorial on asynchronous library design). But it is redundant for this task.\n\nagree :)\n@kaxil @uranusjr considering the above all discussion this leaves me i guess two options IMHO. Whats your suggestion or thoughts ?\n1. Create async version of all the comms methods that access via triggerer\n2.  Use something like [greenback](https://github.com/oremanj/greenback) , execute the sync version inside greenback portal. this we no need to have copy of async version.\n\nFor option two i am thinking something like this below we can have decorators on the methods. and if the comms methods called from  async function then these executes in greenback portal. for normal sync invocation it works without any issues. \n\n```\nlock = aiologic.Lock()\n\nglobal is_async_process\nis_async_process = False\n\ndef sync_or_async(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if is_async_process:\n            async def async_wrapper():\n                return await greenback.with_portal_run_sync(func, *args, **kwargs)\n            return async_wrapper()\n        else:\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\n@sync_or_async\ndef sample_func(a, b):\n    with lock:\n        return a + b\n\nasync def async_main():\n    global is_async_process\n    is_async_process = True\n    await sample_func(4, 4)\n\n\nif __name__ == \"__main__\":\n    sample_func(1, 2)\n    asyncio.run(async_main())\n```\n\nNot sure of any side effects to use with decorators, when i tested its working fine above approach.\n\n@x42005e1f please add your thoughts.\n> [@x42005e1f](https://github.com/x42005e1f) please add your thoughts.\n\nI do not think that making the function return type dependent on runtime is a good pattern. This approach is poorly compatible with static analyzers and it will also not work correctly in all cases. Instead, I suggest shifting the focus not on how to create greenlets, but on how to pass awaitable objects from them. Here is how it can be implemented with a ready-made example for `aiologic.Lock`:\n\n```python\nfrom __future__ import annotations\n\nimport inspect\nimport sys\n\nfrom collections.abc import Awaitable, Callable\nfrom functools import wraps\nfrom typing import TypeVar, cast\n\nimport aiologic\nimport anyio\nimport greenback\n\nif sys.version_info >= (3, 10):\n    from typing import ParamSpec\nelse:\n    from typing_extensions import ParamSpec\n\n_T = TypeVar(\"_T\")\n_P = ParamSpec(\"_P\")\n\n\ndef sync_or_async(\n    sync_func: Callable[_P, _T],\n    async_func: Callable[_P, _T] | Callable[_P, Awaitable[_T]],\n) -> Callable[_P, _T]:\n    if inspect.iscoroutinefunction(async_func):\n        async_impl = greenback.autoawait(async_func)\n    else:\n        async_impl = async_func\n\n    async_impl = cast(Callable[_P, _T], async_impl)\n\n    @wraps(sync_func)\n    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n        if greenback.has_portal():\n            return async_impl(*args, **kwargs)\n        else:\n            return sync_func(*args, **kwargs)\n\n    return wrapper\n\n\nclass CommsLock(aiologic.Lock):\n    __slots__ = ()\n\n    async def _async_acquire_with_timeout(\n        self,\n        /,\n        *,\n        blocking: bool = True,\n        timeout: float | None = None,\n    ) -> bool:\n        with anyio.move_on_after(timeout):\n            return await self.async_acquire(blocking=blocking)\n\n        return False\n\n    green_acquire = sync_or_async(\n        aiologic.Lock.green_acquire,\n        _async_acquire_with_timeout,  # type: ignore[arg-type]\n    )\n    green_release = sync_or_async(\n        aiologic.Lock.green_release,\n        aiologic.Lock.async_release,\n    )\n```\n\n```python\nlock = CommsLock()\n\nasync def noop() -> None:\n    pass\n\nasync def holding() -> None:\n    async with lock:\n        await asyncio.sleep(0)  # switch back to the event loop\n\ndef acquire_release() -> None:\n    with lock:\n        pass  # do something\n\n# make CommsLock implicitly awaitable for the current task\nawait greenback.ensure_portal()\n\n# hold the lock with another task\nholder = asyncio.create_task(holding())\nawait asyncio.sleep(0)\nassert lock.locked()\n\n# a task to verify that CommsLock does indeed yield to the event loop\ntask = asyncio.create_task(noop())\nassert not task.done()\n\nacquire_release()  # sync-or-async call\nassert task.done()  # there was a context switch!\n```\nAlso note that this greenlet approach requires interrupt support - synchronous functions that contain such sync-or-async calls at least indirectly (via subcalls) must be aware that they can be interrupted and called again (i.e. they must be coroutine-safe). Otherwise, the effect will be like a multithreaded function call, but with cooperative multitasking, the possibility of deadlocks when using primitives from the threading module, and broken `threading.local`.\n\nIf it is possible to write asynchronous versions for functions that use `SUPERVISOR_COMMS.lock` with little effort, it is better to do so. If not, the above approach can be considered as a temporary or permanent solution (depending on whether you are willing to keep an eye on coroutine-safety).\n\nNote that selecting the first option (separate asynchronous functions) requires that there be such functions for each use case of `SUPERVISOR_COMMS.lock` via the asynchronous API. If an asynchronous function calls at least one synchronous function that directly or indirectly uses `SUPERVISOR_COMMS.lock`, this immediately risks deadlocks, possibly even hard-to-detect ones.\n\nThis caveat is only true for calls within the same thread. If `SUPERVISOR_COMMS.lock` is used synchronously in a separate thread (e.g. via `sync_to_async()`), everything is fine.\nWell, I think the best solution is not to synchronize. If it is possible to rewrite communication so that messages go really atomically (or without collisions), it will get rid of all this abstruse headache.\n@x42005e1f Really appreciate your thoughts here and all the suggestions :)\n\n Yes, I agree, having an async version would be a good way to avoid all these patches. If I remember correctly, the lock was introduced primarily to prevent mixing API communication messages with triggerer workloads.\n\n@kaxil  @ashb  It seems to me that having a separate channel for triggerer workloads and API communication would be ideal in this scenario to avoid deadlocks IMHO. Alternatively, implementing async versions of all relevant methods could also work. Or, as @x42005e1f suggested, we could adopt the method he mentionedbut as he noted, well need to closely monitor its coroutine-safety.\n> > [@x42005e1f](https://github.com/x42005e1f) please add your thoughts.\n> \n> I do not think that making the function return type dependent on runtime is a good pattern. This approach is poorly compatible with static analyzers and it will also not work correctly in all cases. Instead, I suggest shifting the focus not on how to create greenlets, but on how to pass awaitable objects from them. Here is how it can be implemented with a ready-made example for `aiologic.Lock`:\n> \n> from __future__ import annotations\n> \n> import inspect\n> import sys\n> \n> from collections.abc import Awaitable, Callable\n> from functools import wraps\n> from typing import TypeVar, cast\n> \n> import aiologic\n> import anyio\n> import greenback\n> \n> if sys.version_info >= (3, 10):\n>     from typing import ParamSpec\n> else:\n>     from typing_extensions import ParamSpec\n> \n> _T = TypeVar(\"_T\")\n> _P = ParamSpec(\"_P\")\n> \n> \n> def sync_or_async(\n>     sync_func: Callable[_P, _T],\n>     async_func: Callable[_P, _T] | Callable[_P, Awaitable[_T]],\n> ) -> Callable[_P, _T]:\n>     if inspect.iscoroutinefunction(async_func):\n>         async_impl = greenback.autoawait(async_func)\n>     else:\n>         async_impl = async_func\n> \n>     async_impl = cast(Callable[_P, _T], async_impl)\n> \n>     @wraps(sync_func)\n>     def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n>         if greenback.has_portal():\n>             return async_impl(*args, **kwargs)\n>         else:\n>             return sync_func(*args, **kwargs)\n> \n>     return wrapper\n> \n> \n> class CommsLock(aiologic.Lock):\n>     __slots__ = ()\n> \n>     async def _async_acquire_with_timeout(\n>         self,\n>         /,\n>         *,\n>         blocking: bool = True,\n>         timeout: float | None = None,\n>     ) -> bool:\n>         with anyio.move_on_after(timeout):\n>             return await self.async_acquire(blocking=blocking)\n> \n>         return False\n> \n>     green_acquire = sync_or_async(\n>         aiologic.Lock.green_acquire,\n>         _async_acquire_with_timeout,  # type: ignore[arg-type]\n>     )\n>     green_release = sync_or_async(\n>         aiologic.Lock.green_release,\n>         aiologic.Lock.async_release,\n>     )\n> lock = CommsLock()\n> \n> async def noop() -> None:\n>     pass\n> \n> async def holding() -> None:\n>     async with lock:\n>         await asyncio.sleep(0)  # switch back to the event loop\n> \n> def acquire_release() -> None:\n>     with lock:\n>         pass  # do something\n> \n> # make CommsLock implicitly awaitable for the current task\n> await greenback.ensure_portal()\n> \n> # hold the lock with another task\n> holder = asyncio.create_task(holding())\n> await asyncio.sleep(0)\n> assert lock.locked()\n> \n> # a task to verify that CommsLock does indeed yield to the event loop\n> task = asyncio.create_task(noop())\n> assert not task.done()\n> \n> acquire_release()  # sync-or-async call\n> assert task.done()  # there was a context switch!\n\ncan this `greenback.ensure_portal()` be opened on global level in trigger process, why i am asking is its bit problematic to add this before every function that calls from triggerer. Also users have their own custom triggers they have to update this before every function call. \n> [@kaxil](https://github.com/kaxil) [@ashb](https://github.com/ashb) It seems to me that having a separate channel for triggerer workloads and API communication would be ideal in this scenario to avoid deadlocks IMHO.\n\nThere is also a very audacious solution. Let's go back to the thread-level lock. If we add an asynchronous lock to it for the current event loop, it will:\n\n1. Guarantee exclusive access for the thread.\n2. Guarantee exclusive access for the task.\n3. But allow access through a synchronous function called by another task.\n\nAnd let's just suspend the message transmission to the asynchronous task and let the response come to the synchronous function first, and resume sending to the asynchronous function afterwards. This way we will eliminate both collisions and deadlocks - due to cooperative multitasking it can be done relatively easily. But this mechanism will have to be implemented properly.\n\n> can this `greenback.ensure_portal()` be opened on global level in trigger process, why i am asking is its bit problematic to add this before every function that calls from triggerer. Also users have their own custom triggers they have to update this before every function call.\n\nJust call somewhere within the current task that will make sync-or-async calls. `greenback.ensure_portal()` keeps track of registered tasks, so it can be called multiple times.\nPerhaps I should clarify the thought. Instead of waiting in the synchronous function to finish reading the message in the asynchronous task, which obviously will never complete without switching back to the event loop, we can finish reading the message in the synchronous function. Once we finish reading that message, we will send that remainder to the asynchronous task on the next switch. And before that, the synchronous function will make the request it wants to make.\n\nAlso, instead of adding a separate asynchronous lock, we can reuse the task-owner group ident for synchronous calls in the same thread.\n@x42005e1f sorry was having hard time last two days at my day job, back to this now, Could you please elaborate with some example i couldn't think of what you referring sorry?\n\nIf not i will go ahead and try implement the above Commslock approach with sync_or_async. i feel its worth doing.  \n> [@x42005e1f](https://github.com/x42005e1f) sorry was having hard time last two days at my day job, back to this now, Could you please elaborate with some example i couldn't think of what you referring sorry?\n\nCollisions occur due to simultaneous reading of a single descriptor - usually the one referenced by `sys.stdin`. In `CommsDecoder.get_message()`, it is read synchronously via `sys.stdin` directly. In `TriggerRunner.sync_state_to_supervisor()`, it is read asynchronously via `asyncio.StreamReader`. The idea is that to resolve collisions and avoid deadlocks, it is sufficient to allow the file descriptor to be read in mixed mode in the same thread, but associate the read with the one who sent the request.\n\nWhen using the lock described above (which can be implemented via GLock or independently), there is only one such mixed read situation - async -> sync. In this case we can, for example, do three things:\n\n1. Replace `asyncio.StreamReader` with `concurrent.futures.ThreadPoolExecutor(1)`.\n2. Call `executor.submit()` in `TriggerRunner.sync_state_to_supervisor()`, store a reference to the future in `SUPERVISOR_COMMS`, wait for the future asynchronously with `asyncio.wrap_future()`.\n3. In `CommsDecoder.get_message()`, wait for the future synchronously, if any, and only then read the `sys.stdin` content itself.\n\nThat way we can eliminate this type of collision with minimal pain. At least hypothetically.\nAlso note that if we make all communication as `executor.submit()` calls, we can do without synchronization. Since the calls will be executed by a single worker thread, and the futures can be waited for either synchronously or asynchronously. The disadvantage is only a slight performance loss.\n\nObviously, this will also mean that you will no longer need aiologic, at least not for the kind of `SUPERVISOR_COMMS` synchronization you have now. Well, sophisticated solutions are rarely ever really needed in a proper architecture.\n> Also note that if we make all communication as `executor.submit()` calls, we can do without synchronization. Since the calls will be executed by a single worker thread, and the futures can be waited for either synchronously or asynchronously. The disadvantage is only a slight performance loss.\n> \n> Obviously, this will also mean that you will no longer need aiologic, at least not for the kind of `SUPERVISOR_COMMS` synchronization you have now. Well, sophisticated solutions are rarely ever really needed in a proper architecture.\n\nAgree, generally nowadays people are using triggers largely, not sure about the how much performance it hit. \n> > [@x42005e1f](https://github.com/x42005e1f) sorry was having hard time last two days at my day job, back to this now, Could you please elaborate with some example i couldn't think of what you referring sorry?\n> \n> Collisions occur due to simultaneous reading of a single descriptor - usually the one referenced by `sys.stdin`. In `CommsDecoder.get_message()`, it is read synchronously via `sys.stdin` directly. In `TriggerRunner.sync_state_to_supervisor()`, it is read asynchronously via `asyncio.StreamReader`. The idea is that to resolve collisions and avoid deadlocks, it is sufficient to allow the file descriptor to be read in mixed mode in the same thread, but associate the read with the one who sent the request.\n> \n> When using the lock described above (which can be implemented via GLock or independently), there is only one such mixed read situation - async -> sync. In this case we can, for example, do three things:\n> \n> 1. Replace `asyncio.StreamReader` with `concurrent.futures.ThreadPoolExecutor(1)`.\n> 2. Call `executor.submit()` in `TriggerRunner.sync_state_to_supervisor()`, store a reference to the future in `SUPERVISOR_COMMS`, wait for the future asynchronously with `asyncio.wrap_future()`.\n> 3. In `CommsDecoder.get_message()`, wait for the future synchronously, if any, and only then read the `sys.stdin` content itself.\n> \n> That way we can eliminate this type of collision with minimal pain. At least hypothetically.\n\nLooks promising, looking at now. :) thanks.\n@x42005e1f looks like the solution working, i dont see any too much performance issue. may be in my local environment un noticeable. and good to know the locks are not required as it uses future object to wait.  \nThe difference can be seen only in some very subtle cases where performing context switching might be too expensive (usually a large number of running threads that give O(n) time, possibly with additional cycles), but this is hardly the Apache Airflow case.\n\nAnd the futures solution to the problem is explained quite simply. When we use any kind of synchronization in the same thread, we have a deadlock because the previous context cannot continue its execution. However, when we use ThreadPoolExecutor, both operations are executed separately, because of which they can be executed without deadlocks - the second synchronous wait can be safely done without completing the asynchronous first one.\nCool thanks , can you please have a look ? https://github.com/apache/airflow/pull/51279 \nBy the way, below is an excerpt from my unwritten section for the aiologic documentation, which illustrates just how expensive context switching can actually be. It does not show the whole picture, but it may be useful for understanding what seldom-mentioned problems asynchronous programming has, especially in [lock-free and wait-free](https://concurrencyfreaks.blogspot.com/2013/05/lock-free-and-wait-free-definition-and.html) contexts.\n\n<details>\n<summary>challenges.md</summary>\n\nChallenges\n==========\n\nBridging between concurrency libraries is not the only thing that aiologic was\ndesigned to do. The purpose of this section is to show some of the problems\nconsidered in its design, in the hope that the interested reader will be able\nto make the best use of this library by clearly understanding its ideas.\n\nA world full of squares\n-----------------------\n\nHow much time are you willing to spend to get all the threads up and running?\nThis may seem like a strange question, but it is not as simple as it seems at\nfirst glance.\n\nLiving in the world of data, we tend to consider the time complexity of the\nalgorithms we know. But what about the asynchronous world? We are used to\nseeing this world as a black box, forgetting that it is built on the same\nalgorithms, albeit at a level that is not always available to us. And that\ncomes at a price.\n\nSuppose we want to launch N threads to perform some long work. Whether it is\nfor parallel processing of some NumPy arrays, for network operations, for\nsimulating some game processes - it does not matter. Here is an example that\nmodels our task:\n\n```python\nimport threading\nimport time\n\nN = ...\n\nstopped = False\n\n\ndef work(i):\n    global stopped\n\n    if i == N - 1:  # the last thread\n        stopped = True  # stop the work\n\n    while not stopped:\n        time.sleep(0)  # do some work\n\n\nfor i in range(N):\n    threading.Thread(target=work, args=[i]).start()\n```\n\nIn this example, we run the work in separate threads until the last thread\nstarts. Let's see what happens if we set different N's.\n\n* N=100: 0.17 seconds\n* N=200: 0.55 seconds\n* N=300: 1.19 seconds\n* N=400: 2.14 seconds\n* N=500: 3.14 seconds\n* N=600: 4.69 seconds\n* N=700: 6.38 seconds\n* N=800: 8.11 seconds\n* N=900: 10.48 seconds\n* N=1000: 12.95 seconds\n\nWhoa! Increasing the number of threads by only 10 times increased the time by\nover 50 times! We can clearly see that the dependence of execution time on the\nnumber of threads is not linear - in fact, it is quadratic. Why does this\nhappen?\n\nStarting a thread in Python is based on two main operations:\n\n1. Asking the operating system to start a thread.\n2. Waiting for that thread to start (e.g. to detect memory leaks).\n\nStarting each new thread forces the main thread to do a context switch.\nHowever, the operating system needs to emulate the concurrent execution of all\nthreads, so the picture will usually not look like ping-pong between the main\nthread and the newly created thread - it will need to give CPU resources to the\nalready running threads to execute as well. With a fair scheduling strategy\nthat might look something like this:\n\n1. main thread\n2. thread 1 -> main thread\n3. thread 1 -> thread 2 -> main thread\n4. thread 1 -> thread 2 -> thread 3 -> main thread\n5. ...\n\nWith each new thread, the required number of context switches to start the next\none increases. We see a triangle, which becomes a *square* when the constant is\ndiscarded - that is where the quadratic complexity comes from!\n\n</details>\n\nAgain, this is not specific to Apache Airflow, however as a learning material it may have some value. Just as a footnote to how non-trivial the impact of approaches that rely on context switching is.\n\n\nDo you have any logs from your Triggerer?\n@jroachgolf84 , Please find the logs of triggerer [here](https://pastebin.com/npyyDJBA). The latest logs are at 2:17 UTC thats when I started the airflow. After that I sent a couple of messages to SQS from the AWS console and there are no new logs from the triggerer after that and the messages in SQS are not processed yet. I don't see the triggerer polling for the messages in the logs at SQS for every 10 seconds as configured in the dag after 2:17 UTC\nWhat do the logs look like for that period after 2:17 UTC? Do you see messages like the one below?\n\n```\n2025-05-30 02:17:21 [info     ] trigger ID 3 starting          [airflow.jobs.triggerer_job_runner]\n2025-05-30 02:17:21 [info     ] Secrets backends loaded for worker [supervisor] [airflow.jobs.triggerer_job_runner] backend_classes=['EnvironmentVariablesBackend'] count=1\n2025-05-30 02:17:21 [info     ] Connection Retrieved 'aws_default' [airflow.hooks.base] [airflow.jobs.triggerer_job_runner]\n2025-05-30 02:17:21 [info     ] AWS Connection (conn_id='aws_default', conn_type='aws') credentials retrieved from login and password. [airflow.providers.amazon.aws.utils.connection_wrapper.AwsConnectionWrapper] [airflow.jobs.triggerer_job_runner]\n2025-05-30 02:17:21 [info     ] AWS Connection (conn_id='aws_default', conn_type='aws') session token retrieved from extra, please note you are responsible for renewing these. [airflow.providers.amazon.aws.utils.connection_wrapper.AwsConnectionWrapper] [airflow.jobs.triggerer_job_runner]\n```\n@jroachgolf84 , I have the airflow up and running continuously and please find the latest log file [here](https://pastebin.com/L6hBinhG) those were the logs that were generated till now. The messages in SQS were not processed by the airflow yet.\nI see that you're using a session token to create your AWS connection. What is the time-to-live on that session token?\n@jroachgolf84 , Since I'm working on POC I'm using temporary credentials with TTL. The TTL for this is 6 hours. I replace the credentials in airflow connection every 6 hrs. Although the airflow is not processing the messages that are created with in the 6hrs also.\n@jroachgolf84  Any thoughts on why do you think this is happening?\n@ajayganti3, apologies for the delay. I need to go ahead and recreated this exception next week. I'll let you know when/if I'm able to.\n@jroachgolf84 ,  Thanks for the update. I will be waiting for your response\nI'm facing this exact similar issue. The dag gets triggered when triggerer pod(runnning on k8s using IRSA) starts and then the triggers stops working. It doesn't process the new messages. Here are the triggerer [logs](https://pastebin.com/wXpF2y8b).\n\nWhen I restart the pod, things work for a while may be 3-4 mins and then it stops processing new messages. And then the triggerer spams those warnings into the stdout.\nAfter multiple restarts, i can observe a trend, the sqs triggerer coroutine is making no progress after printing this line.\n\n```\n2025-05-30 02:17:24 [info     ] Secrets backends loaded for worker [supervisor] [airflow.jobs.triggerer_job_runner] backend_classes=['EnvironmentVariablesBackend'] count=1\n```\nLooking at the previous triggerer execution cycles, it tries to retrieve the Connection from the DB. \n\n```\n[2025-05-30T02:17:17.276+0000] {_client.py:1026} INFO - HTTP Request: GET http://in-process.invalid./connections/aws_default \"HTTP/1.1 200 OK\"\n2025-05-30 02:17:17 [info     ] Connection Retrieved 'aws_default' [airflow.hooks.base] [airflow.jobs.triggerer_job_runner]\n```\nSomehow, the triggerer is not able to reach here. I don't think this has any problems with the aws auth methods at this point.\n@tardunge , you are right. The trigger is not polling the SQS once the poll interval that was defined in the DAG is completed.\n@jroachgolf84 Any update on this please?\nI was able to reproduce the same behavior. @tardunge, are you interested in submitting a PR for this issue (if you've identified a possible fix)?\n@jroachgolf84 Didn't find a fix but I think i'm close to the root cause. The co-routine might not be progressing due to a potential deadlock.\nI see the issue happening more often and consistently after some message from sqs has been consumed.\nWhen a message get's consumed, the triggerer yields a TriggerEvent and then breaks out of the run method at [L184](https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/sqs.py#L184)\n\nAfter this, the triggerer gets into a completed state and the main job responsible for running the triggerer coroutines marks it for removal and adds a new instance.\nThe new instance gets spawned and this is where the stalling happens at [L187](https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/sqs.py#L187)\nThe get connection method, eventually talks to the supervisor at [triggerer_job_runner.py L394](https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/triggerer_job_runner.py#L394).\n\nMeanwhile, we have this busy loop responsible for spawning and maintaining the lifecycle of triggerers at [L749](https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/triggerer_job_runner.py#L749) and this loops uses a method called `sync_state_to_supervisor` at [L936](https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/triggerer_job_runner.py#L936).\n\nI highly suspect the `GetConnection` and the main event loop are contending for this `LOCK` at [L965](https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/triggerer_job_runner.py#L965) and resulting in a deadlock at some time.\nIt would be great if there is a determinisitc simulated test case for things like these.\n@ashb \nCan you confirm this?\nI see the locking behaviour has been changed [recently](https://github.com/apache/airflow/pull/48880) couple of months back.\nThanks for looking into that issue folks! Very much appreciated. This is indeed a scenario I have not tested while working on the implementation. I appreciate the testing and effort the fix it :)\nAny update on the fix please?\n\n","all_hints_text":"Is it possible to write a dag that can reliably reproduce this?\ncc @gopidesupavan \nYeah will look at this today.\n@gopidesupavan are you still looking into this?\nFor now I solved througt the creation of an async Task SDK client which the triggers use directly. In my setup the triggers always have the api server running on the same instances, hence I could use localhost traffic. In this way it is rather similar like the in-process traffic but I do not overload the data pipe between trigger runner and supervisor. From my investigation it seems that using too much external workload triggers (count calls) together with the supervisor-runner syncs congests that pipe in such a way that all calls were blocked indefinitely resulting in a trigger process that seems alive (heartbeats still worked) but in reality it is stuck.\n\nProbably this is not the designed pattern, but for now this solution makes it possible for me to role out airflow 3.0 at our office.  \nI am able to re produce it with one of example ExternalTaskSensor dags. It looks blocking somewhere and triggers are not getting to added to further process.\n\nNeed to workout why it's blocking though we use async lock. Will check and update.\n> For now I solved througt the creation of an async Task SDK client which the triggers use directly. In my setup the triggers always have the api server running on the same instances, hence I could use localhost traffic. In this way it is rather similar like the in-process traffic but I do not overload the data pipe between trigger runner and supervisor. From my investigation it seems that using too much external workload triggers (count calls) together with the supervisor-runner syncs congests that pipe in such a way that all calls were blocked indefinitely resulting in a trigger process that seems alive (heartbeats still worked) but in reality it is stuck.\n> \n> Probably this is not the designed pattern, but for now this solution makes it possible for me to role out airflow 3.0 at our office.\n\n@ArvidMartensRenson we dont have async client in Task SDK, did you overwrite the Triggerer client method, does it fixed? \nI created an async client from the sync client, using httpx.AsyncClient as the base (like httpx.Client is for the sync client). Inherited the init client from the sync task client. Only problem I had was that the operations had to be rewritten as an async method, so I copied the code from the sync client and changed the method definition to async. I found this implementation nicer than using plain api calls with the async httpx client. \n\nTraffic is over localhost so even then a token is required, I reused the methods that are used in the creation of the task workload to get the token. I considered dropping the token requirement but it seemed safer and whenever the trigger service and api server are separated it will be needed.\n\nI implemented this solution last Thursday and for now the trigger service has not been stuck and the external workload triggers all completed. So for now I would conclude that the solution works.\n\nIf it is desirable to have an async client I am happy to help on creating the client, however I am a little worried with duplicate code I have now, same code for the sync and async client. If someone knows a solution for this or a way to handle that problem, that would be nice. \n\nFor the record, I am using a custom implementation of the workload trigger. I was an early adopter of the trigger paradigm and the external workload trigger did not exist at that time, so we had to implement our own trigger. We never came around converting to the standard provider implementation.\nThanks @ArvidMartensRenson that explains me what's happening. During debugging, I noticed that requests are being sent, but no responses are being returned. Since we use a lock when sending requests, the absence of a response causes the lock to remain active, which in turn blocks any further triggers from being processed.\n\nWill check on async client part over this week.\n> I created an async client from the sync client, using httpx.AsyncClient as the base (like httpx.Client is for the sync client). Inherited the init client from the sync task client. Only problem I had was that the operations had to be rewritten as an async method, so I copied the code from the sync client and changed the method definition to async. I found this implementation nicer than using plain api calls with the async httpx client.\n> \n> Traffic is over localhost so even then a token is required, I reused the methods that are used in the creation of the task workload to get the token. I considered dropping the token requirement but it seemed safer and whenever the trigger service and api server are separated it will be needed.\n> \n> I implemented this solution last Thursday and for now the trigger service has not been stuck and the external workload triggers all completed. So for now I would conclude that the solution works.\n> \n> If it is desirable to have an async client I am happy to help on creating the client, however I am a little worried with duplicate code I have now, same code for the sync and async client. If someone knows a solution for this or a way to handle that problem, that would be nice.\n\nI don't see any issues with using an async client, it's part of the triggerer, and most triggers are already asynchronous. I do agree that there's some code duplication. Please feel free to submit your approach; itll be great to have more eyes on it and gather suggestions to refine the solution. :) \n\n> \n> For the record, I am using a custom implementation of the workload trigger. I was an early adopter of the trigger paradigm and the external workload trigger did not exist at that time, so we had to implement our own trigger. We never came around converting to the standard provider implementation.\n\n\nLooks like the lock inside sync_to_async function causing issues, its likely messing up with threads acquired in side the sync_to_async. We use get_ti_count, get_task_states, get_dr_count from the `RuntimeTaskInstance`. likely this is not a good choice.\n\nI have Moved those functions to async version without using sync_to_async and working fine, did some stress test around 400 triggers nothing locked everything running fine.\n\n@ArvidMartensRenson we may not need async client. i will raise changes for moving the functions to async, please test it once i raised? async client type also would be good , but in this case i dont see any need. its working without async client. \n@ArvidMartensRenson https://github.com/apache/airflow/pull/51085 would be helpful if you test this change please?\n\ni ran with aroud 400 triggerers and dont see any locks.\n@ArvidMartensRenson, I would like to clarify: do threads get stuck when trying to acquire the lock when some other thread is already performing some actions holding the lock, or do they get stuck for no reason? I ask because in the second case it would mean that the problem is most likely on my side - in which case I would consider it a critical bug and release a fix version. However, I tested the lock with `sync_to_async()` yesterday right after the PR appeared and was unable to reproduce the problem in isolated tests (with both stable and the latest version of aiologic).\n@x42005e1f can i have your example you tested? In our trigger process we use lock inside sync_to_async.\n> [@x42005e1f](https://github.com/x42005e1f) can i have your example you tested? In our trigger process we use lock inside sync_to_async.\n\nYes, here is a simple mixed test:\n\n<details>\n<summary>test.py</summary>\n\n```python\nimport asyncio\nimport sys\nimport time\n\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nimport aiologic\n\nfrom asgiref.sync import sync_to_async\n\nif sys.version_info >= (3, 11):\n    WaitTimeout = TimeoutError\nelse:\n    from concurrent.futures import TimeoutError as WaitTimeout\n\nDELAY = 1e-6\nTIMEOUT = 6.0  # must be reached (success)\nTHREADS = 100\nTASKS = 100\n\nlock = aiologic.Lock()\nbarrier = aiologic.Latch(THREADS)\nstopped = False\n\n\ndef sync_acquire_release():\n    with lock:\n        time.sleep(DELAY)\n\n\nasync def async_acquire_release(i):\n    if i % 2 == 0:\n        while True:\n            await sync_to_async(sync_acquire_release)()\n\n            if stopped:\n                break\n    else:\n        while True:\n            async with lock:\n                await asyncio.sleep(DELAY)\n\n            if stopped:\n                break\n\n\nasync def hub():\n    await barrier\n    await asyncio.gather(*(\n        asyncio.create_task(async_acquire_release(i))\n        for i in range(TASKS)\n    ))\n\n\nif __name__ == \"__main__\":\n    with ThreadPoolExecutor(THREADS) as executor:\n        interval = sys.getswitchinterval()\n        sys.setswitchinterval(min(1e-6, interval))\n\n        try:\n            futures = [\n                executor.submit(asyncio.run, hub())\n                for _ in range(THREADS)\n            ]\n\n            for future in as_completed(futures, timeout=TIMEOUT):\n                future.result()  # reraise\n        except WaitTimeout:\n            pass\n        finally:\n            sys.setswitchinterval(interval)\n            stopped = True\n```\n\n</details>\n\nBut note that `sync_to_async()` uses only one worker thread by default, which blocks all further calls until the first one is completed. This can be seen, for example, in this example:\n\n<details>\n<summary>example.py</summary>\n\n```python\nimport asyncio\nimport sys\nimport threading\n\nfrom asgiref.sync import sync_to_async\n\n\nasync def main():\n    loop = asyncio.get_running_loop()\n    first = asyncio.Event()\n    second = threading.Event()\n\n    def a():\n        print(\"'a' started!\")\n        loop.call_soon_threadsafe(first.set)\n        second.wait()\n\n    def b():\n        print(\"'b' started!\")  # will never be printed!\n        second.set()\n\n    f1 = asyncio.create_task(sync_to_async(a)())\n    await first.wait()\n    print(\"notified!\")\n    f2 = asyncio.create_task(sync_to_async(b)())\n\n    await asyncio.gather(f1, f2)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n</details>\n\nWhat is indicative is that my test works well in free-threaded mode. In this mode threads are more aggressive, especially on multi-core systems, which better reveals thread-safety issues.\nAlso note that the different APIs that aiologic provides should not be mixed in the same thread if threading is used for the green (sync) API. This is a fundamental problem due to the fact that primitives work at the task level: `lock.green_acquire()` can block the event loop when `aiologic.lowlevel.current_green_library() == \"threading\"`, and as a result lock will never be released. This is well demonstrated in the following example:\n\n<details>\n<summary>example.py</summary>\n\n```python\nimport asyncio\nimport time\n\nimport aiologic\n\n\nasync def main():\n    lock = aiologic.Lock()\n\n    async def a():\n        async with lock:\n            print(\"before\")\n            await asyncio.sleep(0)\n            print(\"after\")  # will never be printed!\n\n    async def b():\n        with lock:\n            time.sleep(0)\n\n    await asyncio.gather(a(), b())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n</details>\n\nThis is the same as using `threading.Lock` along with asyncio tasks. If you have such calls somewhere, it can be the cause of deadlocks.\n\nThis problem of mixing two APIs in the same thread can be solved by using [GLock](https://gist.github.com/x42005e1f/a50d0744013b7bbbd7ded608d6a3845b), which can be a thread-level lock - if you use the read-preferring readers-writer locks condition (`self.group is None or self.group == group`), its APIs can safely mix (just replace `lock = aiologic.Lock()` with `lock = GLock(default_group_factory=current_thread_ident)` and that example will work without hanging). However, since it would no longer be a task level, it would not be coroutine-safe.\n> Also note that the different APIs that aiologic provides should not be mixed in the same thread if threading is used for the green (sync) API. This is a fundamental problem due to the fact that primitives work at the task level: `lock.green_acquire()` can block the event loop when `aiologic.lowlevel.current_green_library() == \"threading\"`, and as a result lock will never be released. This is well demonstrated in the following example:\n> \n> example.py\n> This is the same as using `threading.Lock` along with asyncio tasks. If you have such calls somewhere, it can be the cause of deadlocks.\n> \n> This problem of mixing two APIs in the same thread can be solved by using [GLock](https://gist.github.com/x42005e1f/a50d0744013b7bbbd7ded608d6a3845b), which can be a thread-level lock - if you use the read-preferring readers-writer locks condition (`self.group is None or self.group == group`), its APIs can safely mix (just replace `lock = aiologic.Lock()` with `lock = GLock(default_group_factory=current_thread_ident)` and that example will work without hanging). However, since it would no longer be a task level, it would not be coroutine-safe.\n\nlooks like we are using that similar way some places. this is async one https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/triggerer_job_runner.py#L965 and in other places we are using like `with SUPERVISOR_COMMS.lock:`  \n\nBTW GLock where can i find this package i dont see it on PyPI? i will try with this\nIf the problem is the same as what I described (which is likely), the (relatively) easiest way to solve it is to use libraries like [greenback](https://github.com/oremanj/greenback) to execute an asynchronous version when the code runs in a thread with an event loop. Requires explicit portal creation, greenlet support, and interrupt support on the synchronous code side.\n\nIt is also possible to create asynchronous versions of functions to use directly, or delegate them to execute sequentially in the worker thread via `sync_to_async()`, but this can be difficult, especially since [`SUPERVISOR_COMMS.lock` is used even in `__getitem__()`](https://github.com/apache/airflow/blob/7ebba78236b945e0ce569607480f397e5f2e58ba/task-sdk/src/airflow/sdk/execution_time/lazy_sequence.py#L147).\n\n> BTW GLock where can i find this package i dont see it on PyPI? i will try with this\n\nYou can directly copy the code from gist and make the mentioned change. GLock [was created to provide readers-writer locks](https://github.com/x42005e1f/aiologic/discussions/6) before Grouper appears in a future version of aiologic. Right now my efforts are focused on moving aiologic to the beta development stage, so releases are currently delayed.\n\nBut be aware that if you choose GLock, you will have to deal with the loss of coroutine-safety: in particular, the asynchronous usage you mentioned will be executed in one thread at a time, but will also be able to be executed in different tasks at the same time.\n> If the problem is the same as what I described (which is likely), the (relatively) easiest way to solve it is to use libraries like [greenback](https://github.com/oremanj/greenback) to execute an asynchronous version when the code runs in a thread with an event loop. Requires explicit portal creation, greenlet support, and interrupt support on the synchronous code side.\n> \n> It is also possible to create asynchronous versions of functions to use directly, or delegate them to execute sequentially in the worker thread via `sync_to_async()`, but this can be difficult, especially since [`SUPERVISOR_COMMS.lock` is used even in `__getitem__()`](https://github.com/apache/airflow/blob/7ebba78236b945e0ce569607480f397e5f2e58ba/task-sdk/src/airflow/sdk/execution_time/lazy_sequence.py#L147).\n> \n> > BTW GLock where can i find this package i dont see it on PyPI? i will try with this\n> \n> You can directly copy the code from gist and make the mentioned change. GLock [was created to provide readers-writer locks](https://github.com/x42005e1f/aiologic/discussions/6) before Grouper appears in a future version of aiologic. Right now my efforts are focused on moving aiologic to the beta development stage, so releases are currently delayed.\n> \n> But be aware that if you choose GLock, you will have to deal with the loss of coroutine-safety: in particular, the asynchronous usage you mentioned will be executed in one thread at a time, but will also be able to be executed in different tasks at the same time.\n\n@x42005e1f Thats a good suggestion, greenback seems working, did some tests and everything seems fine. no triggers blocked. \n\nLooks like this package is not maintained or not sure, no activity since last Feb 2024.\n> Looks like this package is not maintained or not sure, no activity since last Feb 2024.\n\nIt is not always possible to tell from the commit activity of such packages whether they are maintained or not. Sometimes they are not updated just because there are no serious issues, in which case the author can focus on other projects. Even more ambiguous is the situation when tests are regularly run for new versions of dependencies and a lockfile is updated - in this case the repository may look alive with a huge number of commits, but in fact it has not been updated for a very long time, having very few commits in the source code for the whole history.\n\nThere are also alternatives such as [greenletio](https://github.com/miguelgrinberg/greenletio) or [awaitlet](https://github.com/sqlalchemy/awaitlet). But if you want, you can implement something similar on your side - for example, SQLAlchemy has long used [its own module](https://github.com/sqlalchemy/sqlalchemy/blob/05b2442132d5ae31cfcc7a1fe95e0f6b739aa995/lib/sqlalchemy/util/concurrency.py) as part of asynchronous API implementation.\n\nThese solutions have some disadvantages associated with stack growth, but they are usually insignificant. A more comprehensive solution is to implement (generators and) coroutines via greenlets - what I called (genlets and) corolets 3 years ago (for a non-public tutorial on asynchronous library design). But it is redundant for this task.\n> > Looks like this package is not maintained or not sure, no activity since last Feb 2024.\n> \n> It is not always possible to tell from the commit activity of such packages whether they are maintained or not. Sometimes they are not updated just because there are no serious issues, in which case the author can focus on other projects. Even more ambiguous is the situation when tests are regularly run for new versions of dependencies and a lockfile is updated - in this case the repository may look alive with a huge number of commits, but in fact it has not been updated for a very long time, having very few commits in the source code for the whole history.\n> \n> There are also alternatives such as [greenletio](https://github.com/miguelgrinberg/greenletio) or [awaitlet](https://github.com/sqlalchemy/awaitlet). But if you want, you can implement something similar on your side - for example, SQLAlchemy has long used [its own module](https://github.com/sqlalchemy/sqlalchemy/blob/05b2442132d5ae31cfcc7a1fe95e0f6b739aa995/lib/sqlalchemy/util/concurrency.py) as part of asynchronous API implementation.\n> \n> These solutions have some disadvantages associated with stack growth, but they are usually insignificant. A more comprehensive solution is to implement (generators and) coroutines via greenlets - what I called (genlets and) corolets 3 years ago (for a non-public tutorial on asynchronous library design). But it is redundant for this task.\n\nagree :)\n@kaxil @uranusjr considering the above all discussion this leaves me i guess two options IMHO. Whats your suggestion or thoughts ?\n1. Create async version of all the comms methods that access via triggerer\n2.  Use something like [greenback](https://github.com/oremanj/greenback) , execute the sync version inside greenback portal. this we no need to have copy of async version.\n\nFor option two i am thinking something like this below we can have decorators on the methods. and if the comms methods called from  async function then these executes in greenback portal. for normal sync invocation it works without any issues. \n\n```\nlock = aiologic.Lock()\n\nglobal is_async_process\nis_async_process = False\n\ndef sync_or_async(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        if is_async_process:\n            async def async_wrapper():\n                return await greenback.with_portal_run_sync(func, *args, **kwargs)\n            return async_wrapper()\n        else:\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\n@sync_or_async\ndef sample_func(a, b):\n    with lock:\n        return a + b\n\nasync def async_main():\n    global is_async_process\n    is_async_process = True\n    await sample_func(4, 4)\n\n\nif __name__ == \"__main__\":\n    sample_func(1, 2)\n    asyncio.run(async_main())\n```\n\nNot sure of any side effects to use with decorators, when i tested its working fine above approach.\n\n@x42005e1f please add your thoughts.\n> [@x42005e1f](https://github.com/x42005e1f) please add your thoughts.\n\nI do not think that making the function return type dependent on runtime is a good pattern. This approach is poorly compatible with static analyzers and it will also not work correctly in all cases. Instead, I suggest shifting the focus not on how to create greenlets, but on how to pass awaitable objects from them. Here is how it can be implemented with a ready-made example for `aiologic.Lock`:\n\n```python\nfrom __future__ import annotations\n\nimport inspect\nimport sys\n\nfrom collections.abc import Awaitable, Callable\nfrom functools import wraps\nfrom typing import TypeVar, cast\n\nimport aiologic\nimport anyio\nimport greenback\n\nif sys.version_info >= (3, 10):\n    from typing import ParamSpec\nelse:\n    from typing_extensions import ParamSpec\n\n_T = TypeVar(\"_T\")\n_P = ParamSpec(\"_P\")\n\n\ndef sync_or_async(\n    sync_func: Callable[_P, _T],\n    async_func: Callable[_P, _T] | Callable[_P, Awaitable[_T]],\n) -> Callable[_P, _T]:\n    if inspect.iscoroutinefunction(async_func):\n        async_impl = greenback.autoawait(async_func)\n    else:\n        async_impl = async_func\n\n    async_impl = cast(Callable[_P, _T], async_impl)\n\n    @wraps(sync_func)\n    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n        if greenback.has_portal():\n            return async_impl(*args, **kwargs)\n        else:\n            return sync_func(*args, **kwargs)\n\n    return wrapper\n\n\nclass CommsLock(aiologic.Lock):\n    __slots__ = ()\n\n    async def _async_acquire_with_timeout(\n        self,\n        /,\n        *,\n        blocking: bool = True,\n        timeout: float | None = None,\n    ) -> bool:\n        with anyio.move_on_after(timeout):\n            return await self.async_acquire(blocking=blocking)\n\n        return False\n\n    green_acquire = sync_or_async(\n        aiologic.Lock.green_acquire,\n        _async_acquire_with_timeout,  # type: ignore[arg-type]\n    )\n    green_release = sync_or_async(\n        aiologic.Lock.green_release,\n        aiologic.Lock.async_release,\n    )\n```\n\n```python\nlock = CommsLock()\n\nasync def noop() -> None:\n    pass\n\nasync def holding() -> None:\n    async with lock:\n        await asyncio.sleep(0)  # switch back to the event loop\n\ndef acquire_release() -> None:\n    with lock:\n        pass  # do something\n\n# make CommsLock implicitly awaitable for the current task\nawait greenback.ensure_portal()\n\n# hold the lock with another task\nholder = asyncio.create_task(holding())\nawait asyncio.sleep(0)\nassert lock.locked()\n\n# a task to verify that CommsLock does indeed yield to the event loop\ntask = asyncio.create_task(noop())\nassert not task.done()\n\nacquire_release()  # sync-or-async call\nassert task.done()  # there was a context switch!\n```\nAlso note that this greenlet approach requires interrupt support - synchronous functions that contain such sync-or-async calls at least indirectly (via subcalls) must be aware that they can be interrupted and called again (i.e. they must be coroutine-safe). Otherwise, the effect will be like a multithreaded function call, but with cooperative multitasking, the possibility of deadlocks when using primitives from the threading module, and broken `threading.local`.\n\nIf it is possible to write asynchronous versions for functions that use `SUPERVISOR_COMMS.lock` with little effort, it is better to do so. If not, the above approach can be considered as a temporary or permanent solution (depending on whether you are willing to keep an eye on coroutine-safety).\n\nNote that selecting the first option (separate asynchronous functions) requires that there be such functions for each use case of `SUPERVISOR_COMMS.lock` via the asynchronous API. If an asynchronous function calls at least one synchronous function that directly or indirectly uses `SUPERVISOR_COMMS.lock`, this immediately risks deadlocks, possibly even hard-to-detect ones.\n\nThis caveat is only true for calls within the same thread. If `SUPERVISOR_COMMS.lock` is used synchronously in a separate thread (e.g. via `sync_to_async()`), everything is fine.\nWell, I think the best solution is not to synchronize. If it is possible to rewrite communication so that messages go really atomically (or without collisions), it will get rid of all this abstruse headache.\n@x42005e1f Really appreciate your thoughts here and all the suggestions :)\n\n Yes, I agree, having an async version would be a good way to avoid all these patches. If I remember correctly, the lock was introduced primarily to prevent mixing API communication messages with triggerer workloads.\n\n@kaxil  @ashb  It seems to me that having a separate channel for triggerer workloads and API communication would be ideal in this scenario to avoid deadlocks IMHO. Alternatively, implementing async versions of all relevant methods could also work. Or, as @x42005e1f suggested, we could adopt the method he mentionedbut as he noted, well need to closely monitor its coroutine-safety.\n> > [@x42005e1f](https://github.com/x42005e1f) please add your thoughts.\n> \n> I do not think that making the function return type dependent on runtime is a good pattern. This approach is poorly compatible with static analyzers and it will also not work correctly in all cases. Instead, I suggest shifting the focus not on how to create greenlets, but on how to pass awaitable objects from them. Here is how it can be implemented with a ready-made example for `aiologic.Lock`:\n> \n> from __future__ import annotations\n> \n> import inspect\n> import sys\n> \n> from collections.abc import Awaitable, Callable\n> from functools import wraps\n> from typing import TypeVar, cast\n> \n> import aiologic\n> import anyio\n> import greenback\n> \n> if sys.version_info >= (3, 10):\n>     from typing import ParamSpec\n> else:\n>     from typing_extensions import ParamSpec\n> \n> _T = TypeVar(\"_T\")\n> _P = ParamSpec(\"_P\")\n> \n> \n> def sync_or_async(\n>     sync_func: Callable[_P, _T],\n>     async_func: Callable[_P, _T] | Callable[_P, Awaitable[_T]],\n> ) -> Callable[_P, _T]:\n>     if inspect.iscoroutinefunction(async_func):\n>         async_impl = greenback.autoawait(async_func)\n>     else:\n>         async_impl = async_func\n> \n>     async_impl = cast(Callable[_P, _T], async_impl)\n> \n>     @wraps(sync_func)\n>     def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _T:\n>         if greenback.has_portal():\n>             return async_impl(*args, **kwargs)\n>         else:\n>             return sync_func(*args, **kwargs)\n> \n>     return wrapper\n> \n> \n> class CommsLock(aiologic.Lock):\n>     __slots__ = ()\n> \n>     async def _async_acquire_with_timeout(\n>         self,\n>         /,\n>         *,\n>         blocking: bool = True,\n>         timeout: float | None = None,\n>     ) -> bool:\n>         with anyio.move_on_after(timeout):\n>             return await self.async_acquire(blocking=blocking)\n> \n>         return False\n> \n>     green_acquire = sync_or_async(\n>         aiologic.Lock.green_acquire,\n>         _async_acquire_with_timeout,  # type: ignore[arg-type]\n>     )\n>     green_release = sync_or_async(\n>         aiologic.Lock.green_release,\n>         aiologic.Lock.async_release,\n>     )\n> lock = CommsLock()\n> \n> async def noop() -> None:\n>     pass\n> \n> async def holding() -> None:\n>     async with lock:\n>         await asyncio.sleep(0)  # switch back to the event loop\n> \n> def acquire_release() -> None:\n>     with lock:\n>         pass  # do something\n> \n> # make CommsLock implicitly awaitable for the current task\n> await greenback.ensure_portal()\n> \n> # hold the lock with another task\n> holder = asyncio.create_task(holding())\n> await asyncio.sleep(0)\n> assert lock.locked()\n> \n> # a task to verify that CommsLock does indeed yield to the event loop\n> task = asyncio.create_task(noop())\n> assert not task.done()\n> \n> acquire_release()  # sync-or-async call\n> assert task.done()  # there was a context switch!\n\ncan this `greenback.ensure_portal()` be opened on global level in trigger process, why i am asking is its bit problematic to add this before every function that calls from triggerer. Also users have their own custom triggers they have to update this before every function call. \n> [@kaxil](https://github.com/kaxil) [@ashb](https://github.com/ashb) It seems to me that having a separate channel for triggerer workloads and API communication would be ideal in this scenario to avoid deadlocks IMHO.\n\nThere is also a very audacious solution. Let's go back to the thread-level lock. If we add an asynchronous lock to it for the current event loop, it will:\n\n1. Guarantee exclusive access for the thread.\n2. Guarantee exclusive access for the task.\n3. But allow access through a synchronous function called by another task.\n\nAnd let's just suspend the message transmission to the asynchronous task and let the response come to the synchronous function first, and resume sending to the asynchronous function afterwards. This way we will eliminate both collisions and deadlocks - due to cooperative multitasking it can be done relatively easily. But this mechanism will have to be implemented properly.\n\n> can this `greenback.ensure_portal()` be opened on global level in trigger process, why i am asking is its bit problematic to add this before every function that calls from triggerer. Also users have their own custom triggers they have to update this before every function call.\n\nJust call somewhere within the current task that will make sync-or-async calls. `greenback.ensure_portal()` keeps track of registered tasks, so it can be called multiple times.\nPerhaps I should clarify the thought. Instead of waiting in the synchronous function to finish reading the message in the asynchronous task, which obviously will never complete without switching back to the event loop, we can finish reading the message in the synchronous function. Once we finish reading that message, we will send that remainder to the asynchronous task on the next switch. And before that, the synchronous function will make the request it wants to make.\n\nAlso, instead of adding a separate asynchronous lock, we can reuse the task-owner group ident for synchronous calls in the same thread.\n@x42005e1f sorry was having hard time last two days at my day job, back to this now, Could you please elaborate with some example i couldn't think of what you referring sorry?\n\nIf not i will go ahead and try implement the above Commslock approach with sync_or_async. i feel its worth doing.  \n> [@x42005e1f](https://github.com/x42005e1f) sorry was having hard time last two days at my day job, back to this now, Could you please elaborate with some example i couldn't think of what you referring sorry?\n\nCollisions occur due to simultaneous reading of a single descriptor - usually the one referenced by `sys.stdin`. In `CommsDecoder.get_message()`, it is read synchronously via `sys.stdin` directly. In `TriggerRunner.sync_state_to_supervisor()`, it is read asynchronously via `asyncio.StreamReader`. The idea is that to resolve collisions and avoid deadlocks, it is sufficient to allow the file descriptor to be read in mixed mode in the same thread, but associate the read with the one who sent the request.\n\nWhen using the lock described above (which can be implemented via GLock or independently), there is only one such mixed read situation - async -> sync. In this case we can, for example, do three things:\n\n1. Replace `asyncio.StreamReader` with `concurrent.futures.ThreadPoolExecutor(1)`.\n2. Call `executor.submit()` in `TriggerRunner.sync_state_to_supervisor()`, store a reference to the future in `SUPERVISOR_COMMS`, wait for the future asynchronously with `asyncio.wrap_future()`.\n3. In `CommsDecoder.get_message()`, wait for the future synchronously, if any, and only then read the `sys.stdin` content itself.\n\nThat way we can eliminate this type of collision with minimal pain. At least hypothetically.\nAlso note that if we make all communication as `executor.submit()` calls, we can do without synchronization. Since the calls will be executed by a single worker thread, and the futures can be waited for either synchronously or asynchronously. The disadvantage is only a slight performance loss.\n\nObviously, this will also mean that you will no longer need aiologic, at least not for the kind of `SUPERVISOR_COMMS` synchronization you have now. Well, sophisticated solutions are rarely ever really needed in a proper architecture.\n> Also note that if we make all communication as `executor.submit()` calls, we can do without synchronization. Since the calls will be executed by a single worker thread, and the futures can be waited for either synchronously or asynchronously. The disadvantage is only a slight performance loss.\n> \n> Obviously, this will also mean that you will no longer need aiologic, at least not for the kind of `SUPERVISOR_COMMS` synchronization you have now. Well, sophisticated solutions are rarely ever really needed in a proper architecture.\n\nAgree, generally nowadays people are using triggers largely, not sure about the how much performance it hit. \n> > [@x42005e1f](https://github.com/x42005e1f) sorry was having hard time last two days at my day job, back to this now, Could you please elaborate with some example i couldn't think of what you referring sorry?\n> \n> Collisions occur due to simultaneous reading of a single descriptor - usually the one referenced by `sys.stdin`. In `CommsDecoder.get_message()`, it is read synchronously via `sys.stdin` directly. In `TriggerRunner.sync_state_to_supervisor()`, it is read asynchronously via `asyncio.StreamReader`. The idea is that to resolve collisions and avoid deadlocks, it is sufficient to allow the file descriptor to be read in mixed mode in the same thread, but associate the read with the one who sent the request.\n> \n> When using the lock described above (which can be implemented via GLock or independently), there is only one such mixed read situation - async -> sync. In this case we can, for example, do three things:\n> \n> 1. Replace `asyncio.StreamReader` with `concurrent.futures.ThreadPoolExecutor(1)`.\n> 2. Call `executor.submit()` in `TriggerRunner.sync_state_to_supervisor()`, store a reference to the future in `SUPERVISOR_COMMS`, wait for the future asynchronously with `asyncio.wrap_future()`.\n> 3. In `CommsDecoder.get_message()`, wait for the future synchronously, if any, and only then read the `sys.stdin` content itself.\n> \n> That way we can eliminate this type of collision with minimal pain. At least hypothetically.\n\nLooks promising, looking at now. :) thanks.\n@x42005e1f looks like the solution working, i dont see any too much performance issue. may be in my local environment un noticeable. and good to know the locks are not required as it uses future object to wait.  \nThe difference can be seen only in some very subtle cases where performing context switching might be too expensive (usually a large number of running threads that give O(n) time, possibly with additional cycles), but this is hardly the Apache Airflow case.\n\nAnd the futures solution to the problem is explained quite simply. When we use any kind of synchronization in the same thread, we have a deadlock because the previous context cannot continue its execution. However, when we use ThreadPoolExecutor, both operations are executed separately, because of which they can be executed without deadlocks - the second synchronous wait can be safely done without completing the asynchronous first one.\nCool thanks , can you please have a look ? https://github.com/apache/airflow/pull/51279 \nBy the way, below is an excerpt from my unwritten section for the aiologic documentation, which illustrates just how expensive context switching can actually be. It does not show the whole picture, but it may be useful for understanding what seldom-mentioned problems asynchronous programming has, especially in [lock-free and wait-free](https://concurrencyfreaks.blogspot.com/2013/05/lock-free-and-wait-free-definition-and.html) contexts.\n\n<details>\n<summary>challenges.md</summary>\n\nChallenges\n==========\n\nBridging between concurrency libraries is not the only thing that aiologic was\ndesigned to do. The purpose of this section is to show some of the problems\nconsidered in its design, in the hope that the interested reader will be able\nto make the best use of this library by clearly understanding its ideas.\n\nA world full of squares\n-----------------------\n\nHow much time are you willing to spend to get all the threads up and running?\nThis may seem like a strange question, but it is not as simple as it seems at\nfirst glance.\n\nLiving in the world of data, we tend to consider the time complexity of the\nalgorithms we know. But what about the asynchronous world? We are used to\nseeing this world as a black box, forgetting that it is built on the same\nalgorithms, albeit at a level that is not always available to us. And that\ncomes at a price.\n\nSuppose we want to launch N threads to perform some long work. Whether it is\nfor parallel processing of some NumPy arrays, for network operations, for\nsimulating some game processes - it does not matter. Here is an example that\nmodels our task:\n\n```python\nimport threading\nimport time\n\nN = ...\n\nstopped = False\n\n\ndef work(i):\n    global stopped\n\n    if i == N - 1:  # the last thread\n        stopped = True  # stop the work\n\n    while not stopped:\n        time.sleep(0)  # do some work\n\n\nfor i in range(N):\n    threading.Thread(target=work, args=[i]).start()\n```\n\nIn this example, we run the work in separate threads until the last thread\nstarts. Let's see what happens if we set different N's.\n\n* N=100: 0.17 seconds\n* N=200: 0.55 seconds\n* N=300: 1.19 seconds\n* N=400: 2.14 seconds\n* N=500: 3.14 seconds\n* N=600: 4.69 seconds\n* N=700: 6.38 seconds\n* N=800: 8.11 seconds\n* N=900: 10.48 seconds\n* N=1000: 12.95 seconds\n\nWhoa! Increasing the number of threads by only 10 times increased the time by\nover 50 times! We can clearly see that the dependence of execution time on the\nnumber of threads is not linear - in fact, it is quadratic. Why does this\nhappen?\n\nStarting a thread in Python is based on two main operations:\n\n1. Asking the operating system to start a thread.\n2. Waiting for that thread to start (e.g. to detect memory leaks).\n\nStarting each new thread forces the main thread to do a context switch.\nHowever, the operating system needs to emulate the concurrent execution of all\nthreads, so the picture will usually not look like ping-pong between the main\nthread and the newly created thread - it will need to give CPU resources to the\nalready running threads to execute as well. With a fair scheduling strategy\nthat might look something like this:\n\n1. main thread\n2. thread 1 -> main thread\n3. thread 1 -> thread 2 -> main thread\n4. thread 1 -> thread 2 -> thread 3 -> main thread\n5. ...\n\nWith each new thread, the required number of context switches to start the next\none increases. We see a triangle, which becomes a *square* when the constant is\ndiscarded - that is where the quadratic complexity comes from!\n\n</details>\n\nAgain, this is not specific to Apache Airflow, however as a learning material it may have some value. Just as a footnote to how non-trivial the impact of approaches that rely on context switching is.\n\nAs part of addressing [#46426](https://github.com/apache/airflow/issues/46426) (which I'm working on right now) I'm totally overhauling the protocol between supervisor and child process.\n\nI'm adding a request ID (integer, atomic auto inc using `itertools.counter`) and I was wondering about if we could remove the lock and replace it with .... some other primitive from aiologic to send the response instead back to async code (possibly a queue? Maybe a flag? Not sure tbh, just a thought)\n\nMy WIP branch is https://github.com/astronomer/airflow/tree/rework-tasksdk-supervisor-comms-protocol\n> I'm adding a request ID (integer, atomic auto inc using `itertools.counter`) and I was wondering about if we could remove the lock and replace it with .... some other primitive from aiologic to send the response instead back to async code (possibly a queue? Maybe a flag? Not sure tbh, just a thought)\n\nIf you only need to wait for a result, currently the most appropriate option is to use either a high-level event (`aiologic.Event` if there are many waiters of the same result) or a low-level event (`aiologic.lowlevel.create_<type>_event()` (>=0.15.0) / `aiologic.lowlevel.<Type>Event` (<0.15.0) if there is only one waiter). Just set the event after the result is saved somewhere.\n\nIf you want to sequentially wait for multiple results, then yes, `aiologic.Queue` might be a good option. You may also be interested in [`culsans.Queue`](https://github.com/x42005e1f/culsans), which has more features.\n\nFlags (`aiologic.Flag` (>=0.15.0) / `aiologic.lowlevel.Flag` (<0.15.0)) have a slightly different meaning. They are used in situations where you want to know which thread (or state) was first. The analogy from which they get their name is related to conquering the mountain / moon / and so on.\n\nI have a plan to add some kind of Future / AsyncResult in the future, but I have not decided on the interface yet.\nThinking about this a bit more and re-reading the your super helpful comments, I'm not sure this approach will get around the fundamental problem of sync_to_async.\n\nWe can't easily (today/now) add async versions of get_connection etc, so we have the issue that the calling code ends up doing:\n\n```python\n    connection = await sync_to_async(self.get_connection)(self.aws_conn_id)\n```\n\nGiven that constraint (that we sort of really want to support this code, sadly) I'm not sure my queue etc approach will help at all.\n> Given that constraint (that we sort of really want to support this code, sadly) I'm not sure my queue etc approach will help at all.\n\nAs I said in the [comment on the linked PR](https://github.com/apache/airflow/pull/51279#issuecomment-2934712813), the problem is tied to the async -> sync case in the same thread, which is fundamentally unsolvable. However, it is enough to make synchronous and asynchronous codes independent of each other to bypass the problem. For example, if every read is actually performed in a worker thread.\n\nIf your approach has something similar to this, or otherwise removes the dependency (created due to synchronization), then it solves this issue as well.\nThanks, yeah. I'm only just now getting back around to looking at this issue and the linked PR in detail.\n\nI don't have any reader/writer thread in my WIP right now, and wasn't thinking of adding any, so I don't think my changes will in help at all this way. (In my head I was thinking \"we can just send all the responses from sync code, make note of the request_id, then have asnyc code wait for a response to that matching request_id. But that won't help squat here)\n\nThanks very much for the detailed and helpful comments @x42005e1f.\nActually, how about this @x42005e1f:\n\n- We remove the lock on CommsDecoder.\n- We create a new subclass called TriggererCommsDecoder (so we know when we are operating in this hybrid sync/async env)\n- On that we have two queues - one for requests, the other for responses.\n- The code that pulls off of the requests Q and writes to requests/reads from stdin is 100% async code\n- The sync send code (in my Pr I combined `send_msg() + get_response()` into a single function as it's a nicer API anyway) does something like this:\n\n    ```python\n    req_id = next(self.id_counter)\n    self._q_requests.put(Frame(..., id=req_id)\n    while True:\n        resp = self._q_responses.peek()\n        if resp.id != req_id:\n            # Response to someone else's message. Try again\n            continue\n        self._q_responses.get() # Though this feels racy? Maybe we need a sanity check and to requeue it if the req_id doesn't match?\n    ```\n\nIs that actually any better (clearer/easier to understand/more performant) than just using a n=1 Thread pool?\n\nWDYT as well @gopidesupavan?\n\n\n> Actually, how about this [@x42005e1f](https://github.com/x42005e1f):\n\nIf queues are also used for asynchronous code, the problem remains. Let's imagine that asynchronous code sent a request and then the event loop switched execution to another task that called the sync send code. Then the first response will be for the asynchronous code, and the synchronous code will wait forever for that response to be removed from the queue, which will never happen because execution will never switch back to the asynchronous code. This is now not a deadlock, but an eternal load on the processor.\n\nIf the queues are only for synchronous code, then yes, that might solve the problem. However, in any case, I suggest removing `self._q_responses`, since such polls are almost always a bad thing. Just send a future (`concurrent.futures.Future`) along with the frame. Call `future.result()` in the sync send code, and let the producer put the response in the future via `future.set_result(response)`. Then the consumer will wake up and get their response from the future.\n> * The code that pulls off of the requests Q and writes to requests/reads from stdin is 100% async code\n\nHowever, I read it again and realized that this is an attempt to delegate work to asynchronous code from synchronous code. No, it will not work that way because, again, the synchronous code will not be able to interrupt its execution to resume execution of asynchronous code. It will only work if they are different threads.\n> the synchronous code will not be able to interrupt its execution to resume execution of asynchronous code. It will only work if they are different threads.\n\nAnd we don't get that via `sync_to_async` as all calls to that use a single Thread?\n> And we don't get that via `sync_to_async` as all calls to that use a single Thread?\n\n`sync_to_async()` delegates execution to a worker thread. This eliminates the requirement to switch from synchronous to asynchronous code (since asynchronous code just waits for a future object), which in turn bypasses the problem - you can even synchronize in synchronous code, as long as synchronous code does not depend on asynchronous code in the same thread. Of course, this is true only when `sync_to_async()` is used for full operations - if it is used for single reads that are part of the whole `get_message()`, the problem remains.\nTo put it another way, a feature of the approach with delegating execution to another thread is that the code is executed independently of the synchronous code in the current thread. This means that any operation that has been delegated by the asynchronous code and that the synchronous code needs to wait for will be able to complete in parallel. This eliminates the dependency, and thus bypasses the problem.\nI see that the airflow 3.0.3 is released, but I don't see this bug in the release notes. Can someone confirm if the fix is already a part of 3.0.3 release?\n@ajayganti3  Yes it is\nThe [first item](https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html#airflow-3-0-3-2025-07-14:~:text=Fix%20task%20execution%20failures%20with%20large%20data%20by%20improving%20internal%20communication%20protocol%20(%2351924%2C%20%2353194)) in bug-fixes:\n\n>Fix task execution failures with large data by improving internal communication protocol ([#51924](https://github.com/apache/airflow/pull/51924), [#53194](https://github.com/apache/airflow/pull/53194))\n\nDo you have any logs from your Triggerer?\n@jroachgolf84 , Please find the logs of triggerer [here](https://pastebin.com/npyyDJBA). The latest logs are at 2:17 UTC thats when I started the airflow. After that I sent a couple of messages to SQS from the AWS console and there are no new logs from the triggerer after that and the messages in SQS are not processed yet. I don't see the triggerer polling for the messages in the logs at SQS for every 10 seconds as configured in the dag after 2:17 UTC\nWhat do the logs look like for that period after 2:17 UTC? Do you see messages like the one below?\n\n```\n2025-05-30 02:17:21 [info     ] trigger ID 3 starting          [airflow.jobs.triggerer_job_runner]\n2025-05-30 02:17:21 [info     ] Secrets backends loaded for worker [supervisor] [airflow.jobs.triggerer_job_runner] backend_classes=['EnvironmentVariablesBackend'] count=1\n2025-05-30 02:17:21 [info     ] Connection Retrieved 'aws_default' [airflow.hooks.base] [airflow.jobs.triggerer_job_runner]\n2025-05-30 02:17:21 [info     ] AWS Connection (conn_id='aws_default', conn_type='aws') credentials retrieved from login and password. [airflow.providers.amazon.aws.utils.connection_wrapper.AwsConnectionWrapper] [airflow.jobs.triggerer_job_runner]\n2025-05-30 02:17:21 [info     ] AWS Connection (conn_id='aws_default', conn_type='aws') session token retrieved from extra, please note you are responsible for renewing these. [airflow.providers.amazon.aws.utils.connection_wrapper.AwsConnectionWrapper] [airflow.jobs.triggerer_job_runner]\n```\n@jroachgolf84 , I have the airflow up and running continuously and please find the latest log file [here](https://pastebin.com/L6hBinhG) those were the logs that were generated till now. The messages in SQS were not processed by the airflow yet.\nI see that you're using a session token to create your AWS connection. What is the time-to-live on that session token?\n@jroachgolf84 , Since I'm working on POC I'm using temporary credentials with TTL. The TTL for this is 6 hours. I replace the credentials in airflow connection every 6 hrs. Although the airflow is not processing the messages that are created with in the 6hrs also.\n@jroachgolf84  Any thoughts on why do you think this is happening?\n@ajayganti3, apologies for the delay. I need to go ahead and recreated this exception next week. I'll let you know when/if I'm able to.\n@jroachgolf84 ,  Thanks for the update. I will be waiting for your response\nI'm facing this exact similar issue. The dag gets triggered when triggerer pod(runnning on k8s using IRSA) starts and then the triggers stops working. It doesn't process the new messages. Here are the triggerer [logs](https://pastebin.com/wXpF2y8b).\n\nWhen I restart the pod, things work for a while may be 3-4 mins and then it stops processing new messages. And then the triggerer spams those warnings into the stdout.\nAfter multiple restarts, i can observe a trend, the sqs triggerer coroutine is making no progress after printing this line.\n\n```\n2025-05-30 02:17:24 [info     ] Secrets backends loaded for worker [supervisor] [airflow.jobs.triggerer_job_runner] backend_classes=['EnvironmentVariablesBackend'] count=1\n```\nLooking at the previous triggerer execution cycles, it tries to retrieve the Connection from the DB. \n\n```\n[2025-05-30T02:17:17.276+0000] {_client.py:1026} INFO - HTTP Request: GET http://in-process.invalid./connections/aws_default \"HTTP/1.1 200 OK\"\n2025-05-30 02:17:17 [info     ] Connection Retrieved 'aws_default' [airflow.hooks.base] [airflow.jobs.triggerer_job_runner]\n```\nSomehow, the triggerer is not able to reach here. I don't think this has any problems with the aws auth methods at this point.\n@tardunge , you are right. The trigger is not polling the SQS once the poll interval that was defined in the DAG is completed.\n@jroachgolf84 Any update on this please?\nI was able to reproduce the same behavior. @tardunge, are you interested in submitting a PR for this issue (if you've identified a possible fix)?\n@jroachgolf84 Didn't find a fix but I think i'm close to the root cause. The co-routine might not be progressing due to a potential deadlock.\nI see the issue happening more often and consistently after some message from sqs has been consumed.\nWhen a message get's consumed, the triggerer yields a TriggerEvent and then breaks out of the run method at [L184](https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/sqs.py#L184)\n\nAfter this, the triggerer gets into a completed state and the main job responsible for running the triggerer coroutines marks it for removal and adds a new instance.\nThe new instance gets spawned and this is where the stalling happens at [L187](https://github.com/apache/airflow/blob/main/providers/amazon/src/airflow/providers/amazon/aws/triggers/sqs.py#L187)\nThe get connection method, eventually talks to the supervisor at [triggerer_job_runner.py L394](https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/triggerer_job_runner.py#L394).\n\nMeanwhile, we have this busy loop responsible for spawning and maintaining the lifecycle of triggerers at [L749](https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/triggerer_job_runner.py#L749) and this loops uses a method called `sync_state_to_supervisor` at [L936](https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/triggerer_job_runner.py#L936).\n\nI highly suspect the `GetConnection` and the main event loop are contending for this `LOCK` at [L965](https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/jobs/triggerer_job_runner.py#L965) and resulting in a deadlock at some time.\nIt would be great if there is a determinisitc simulated test case for things like these.\n@ashb \nCan you confirm this?\nI see the locking behaviour has been changed [recently](https://github.com/apache/airflow/pull/48880) couple of months back.\nThanks for looking into that issue folks! Very much appreciated. This is indeed a scenario I have not tested while working on the implementation. I appreciate the testing and effort the fix it :)\nAny update on the fix please?\nSounds like this might be a duplicate of https://github.com/apache/airflow/issues/50185\nThanks. I will test this and let you know.\n\n","commit_urls":["https://github.com/apache/airflow/commit/314799b3ddf1385f9475bc5f9724bf2185f74e8e","https://github.com/apache/airflow/commit/a34253c6fa1239c4ac2dbbabfc46b66b38bd94ea","https://github.com/apache/airflow/commit/e4b2623ae369d2de4a4dcaa323d1afcd3ee83d00","https://github.com/apache/airflow/commit/ed48d7283e59176950d28b701fd711b53bd96e22"],"created_at":"2025-06-13T16:20:45Z","classification":"Efficiency"}
{"repo":"apache/airflow","pull_number":51463,"instance_id":"apache__airflow-51463","issue_numbers":[50514],"base_commit":"35bc037e96692cad20784864c53392ae4a615a9f","patch":"diff --git a/providers/snowflake/src/airflow/providers/snowflake/hooks/snowflake_sql_api.py b/providers/snowflake/src/airflow/providers/snowflake/hooks/snowflake_sql_api.py\nindex 7e4ef8dfe0288..185a49ffaba06 100644\n--- a/providers/snowflake/src/airflow/providers/snowflake/hooks/snowflake_sql_api.py\n+++ b/providers/snowflake/src/airflow/providers/snowflake/hooks/snowflake_sql_api.py\n@@ -25,8 +25,18 @@\n \n import aiohttp\n import requests\n+from aiohttp import ClientConnectionError, ClientResponseError\n from cryptography.hazmat.backends import default_backend\n from cryptography.hazmat.primitives import serialization\n+from requests.exceptions import ConnectionError, HTTPError, Timeout\n+from tenacity import (\n+    AsyncRetrying,\n+    Retrying,\n+    before_sleep_log,\n+    retry_if_exception,\n+    stop_after_attempt,\n+    wait_exponential,\n+)\n \n from airflow.exceptions import AirflowException, AirflowProviderDeprecationWarning\n from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n@@ -65,6 +75,7 @@ class SnowflakeSqlApiHook(SnowflakeHook):\n     :param token_life_time: lifetime of the JWT Token in timedelta\n     :param token_renewal_delta: Renewal time of the JWT Token in timedelta\n     :param deferrable: Run operator in the deferrable mode.\n+    :param api_retry_args: An optional dictionary with arguments passed to ``tenacity.Retrying`` & ``tenacity.AsyncRetrying`` classes.\n     \"\"\"\n \n     LIFETIME = timedelta(minutes=59)  # The tokens will have a 59 minute lifetime\n@@ -75,15 +86,27 @@ def __init__(\n         snowflake_conn_id: str,\n         token_life_time: timedelta = LIFETIME,\n         token_renewal_delta: timedelta = RENEWAL_DELTA,\n+        api_retry_args: dict[Any, Any] | None = None,  # Optional retry arguments passed to tenacity.retry\n         *args: Any,\n         **kwargs: Any,\n     ):\n         self.snowflake_conn_id = snowflake_conn_id\n         self.token_life_time = token_life_time\n         self.token_renewal_delta = token_renewal_delta\n+\n         super().__init__(snowflake_conn_id, *args, **kwargs)\n         self.private_key: Any = None\n \n+        self.retry_config = {\n+            \"retry\": retry_if_exception(self._should_retry_on_error),\n+            \"wait\": wait_exponential(multiplier=1, min=1, max=60),\n+            \"stop\": stop_after_attempt(5),\n+            \"before_sleep\": before_sleep_log(self.log, log_level=20),  # INFO level\n+            \"reraise\": True,\n+        }\n+        if api_retry_args:\n+            self.retry_config.update(api_retry_args)\n+\n     def get_private_key(self) -> None:\n         \"\"\"Get the private key from snowflake connection.\"\"\"\n         conn = self.get_connection(self.snowflake_conn_id)\n@@ -168,13 +191,8 @@ def execute_query(\n                 \"query_tag\": query_tag,\n             },\n         }\n-        response = requests.post(url, json=data, headers=headers, params=params)\n-        try:\n-            response.raise_for_status()\n-        except requests.exceptions.HTTPError as e:  # pragma: no cover\n-            msg = f\"Response: {e.response.content.decode()} Status Code: {e.response.status_code}\"\n-            raise AirflowException(msg)\n-        json_response = response.json()\n+\n+        _, json_response = self._make_api_call_with_retries(\"POST\", url, headers, params, data)\n         self.log.info(\"Snowflake SQL POST API response: %s\", json_response)\n         if \"statementHandles\" in json_response:\n             self.query_ids = json_response[\"statementHandles\"]\n@@ -259,13 +277,10 @@ def check_query_output(self, query_ids: list[str]) -> None:\n         \"\"\"\n         for query_id in query_ids:\n             header, params, url = self.get_request_url_header_params(query_id)\n-            try:\n-                response = requests.get(url, headers=header, params=params)\n-                response.raise_for_status()\n-                self.log.info(response.json())\n-            except requests.exceptions.HTTPError as e:\n-                msg = f\"Response: {e.response.content.decode()}, Status Code: {e.response.status_code}\"\n-                raise AirflowException(msg)\n+            _, response_json = self._make_api_call_with_retries(\n+                method=\"GET\", url=url, headers=header, params=params\n+            )\n+            self.log.info(response_json)\n \n     def _process_response(self, status_code, resp):\n         self.log.info(\"Snowflake SQL GET statements status API response: %s\", resp)\n@@ -295,9 +310,7 @@ def get_sql_api_query_status(self, query_id: str) -> dict[str, str | list[str]]:\n         \"\"\"\n         self.log.info(\"Retrieving status for query id %s\", query_id)\n         header, params, url = self.get_request_url_header_params(query_id)\n-        response = requests.get(url, params=params, headers=header)\n-        status_code = response.status_code\n-        resp = response.json()\n+        status_code, resp = self._make_api_call_with_retries(\"GET\", url, header, params)\n         return self._process_response(status_code, resp)\n \n     async def get_sql_api_query_status_async(self, query_id: str) -> dict[str, str | list[str]]:\n@@ -308,10 +321,85 @@ async def get_sql_api_query_status_async(self, query_id: str) -> dict[str, str |\n         \"\"\"\n         self.log.info(\"Retrieving status for query id %s\", query_id)\n         header, params, url = self.get_request_url_header_params(query_id)\n-        async with (\n-            aiohttp.ClientSession(headers=header) as session,\n-            session.get(url, params=params) as response,\n+        status_code, resp = await self._make_api_call_with_retries_async(\"GET\", url, header, params)\n+        return self._process_response(status_code, resp)\n+\n+    @staticmethod\n+    def _should_retry_on_error(exception) -> bool:\n+        \"\"\"\n+        Determine if the exception should trigger a retry based on error type and status code.\n+\n+        Retries on HTTP errors 429 (Too Many Requests), 503 (Service Unavailable),\n+        and 504 (Gateway Timeout) as recommended by Snowflake error handling docs.\n+        Retries on connection errors and timeouts.\n+\n+        :param exception: The exception to check\n+        :return: True if the request should be retried, False otherwise\n+        \"\"\"\n+        if isinstance(exception, HTTPError):\n+            return exception.response.status_code in [429, 503, 504]\n+        if isinstance(exception, ClientResponseError):\n+            return exception.status in [429, 503, 504]\n+        if isinstance(\n+            exception,\n+            (\n+                ConnectionError,\n+                Timeout,\n+                ClientConnectionError,\n+            ),\n         ):\n-            status_code = response.status\n-            resp = await response.json()\n-            return self._process_response(status_code, resp)\n+            return True\n+        return False\n+\n+    def _make_api_call_with_retries(\n+        self, method: str, url: str, headers: dict, params: dict | None = None, json: dict | None = None\n+    ):\n+        \"\"\"\n+        Make an API call to the Snowflake SQL API with retry logic for specific HTTP errors.\n+\n+        Error handling implemented based on Snowflake error handling docs:\n+        https://docs.snowflake.com/en/developer-guide/sql-api/handling-errors\n+\n+        :param method: The HTTP method to use for the API call.\n+        :param url: The URL for the API endpoint.\n+        :param headers: The headers to include in the API call.\n+        :param params: (Optional) The query parameters to include in the API call.\n+        :param data: (Optional) The data to include in the API call.\n+        :return: The response object from the API call.\n+        \"\"\"\n+        with requests.Session() as session:\n+            for attempt in Retrying(**self.retry_config):  # type: ignore\n+                with attempt:\n+                    if method.upper() in (\"GET\", \"POST\"):\n+                        response = session.request(\n+                            method=method.lower(), url=url, headers=headers, params=params, json=json\n+                        )\n+                    else:\n+                        raise ValueError(f\"Unsupported HTTP method: {method}\")\n+                    response.raise_for_status()\n+                    return response.status_code, response.json()\n+\n+    async def _make_api_call_with_retries_async(self, method, url, headers, params=None):\n+        \"\"\"\n+        Make an API call to the Snowflake SQL API asynchronously with retry logic for specific HTTP errors.\n+\n+        Error handling implemented based on Snowflake error handling docs:\n+        https://docs.snowflake.com/en/developer-guide/sql-api/handling-errors\n+\n+        :param method: The HTTP method to use for the API call. Only GET is supported as  is synchronous.\n+        :param url: The URL for the API endpoint.\n+        :param headers: The headers to include in the API call.\n+        :param params: (Optional) The query parameters to include in the API call.\n+        :return: The response object from the API call.\n+        \"\"\"\n+        async with aiohttp.ClientSession(headers=headers) as session:\n+            async for attempt in AsyncRetrying(**self.retry_config):  # type: ignore\n+                with attempt:\n+                    if method.upper() == \"GET\":\n+                        async with session.request(method=method.lower(), url=url, params=params) as response:\n+                            response.raise_for_status()\n+                            # Return status and json content for async processing\n+                            content = await response.json()\n+                            return response.status, content\n+                    else:\n+                        raise ValueError(f\"Unsupported HTTP method: {method}\")\ndiff --git a/providers/snowflake/src/airflow/providers/snowflake/operators/snowflake.py b/providers/snowflake/src/airflow/providers/snowflake/operators/snowflake.py\nindex 2c8db1391e82b..48086e53cd8c7 100644\n--- a/providers/snowflake/src/airflow/providers/snowflake/operators/snowflake.py\n+++ b/providers/snowflake/src/airflow/providers/snowflake/operators/snowflake.py\n@@ -355,6 +355,7 @@ class SnowflakeSqlApiOperator(SQLExecuteQueryOperator):\n             When executing the statement, Snowflake replaces placeholders (? and :name) in\n             the statement with these specified values.\n     :param deferrable: Run operator in the deferrable mode.\n+    :param snowflake_api_retry_args: An optional dictionary with arguments passed to ``tenacity.Retrying`` & ``tenacity.AsyncRetrying`` classes.\n     \"\"\"\n \n     LIFETIME = timedelta(minutes=59)  # The tokens will have a 59 minutes lifetime\n@@ -381,6 +382,7 @@ def __init__(\n         token_renewal_delta: timedelta = RENEWAL_DELTA,\n         bindings: dict[str, Any] | None = None,\n         deferrable: bool = conf.getboolean(\"operators\", \"default_deferrable\", fallback=False),\n+        snowflake_api_retry_args: dict[str, Any] | None = None,\n         **kwargs: Any,\n     ) -> None:\n         self.snowflake_conn_id = snowflake_conn_id\n@@ -390,6 +392,7 @@ def __init__(\n         self.token_renewal_delta = token_renewal_delta\n         self.bindings = bindings\n         self.execute_async = False\n+        self.snowflake_api_retry_args = snowflake_api_retry_args or {}\n         self.deferrable = deferrable\n         self.query_ids: list[str] = []\n         if any([warehouse, database, role, schema, authenticator, session_parameters]):  # pragma: no cover\n@@ -412,6 +415,7 @@ def _hook(self):\n             token_life_time=self.token_life_time,\n             token_renewal_delta=self.token_renewal_delta,\n             deferrable=self.deferrable,\n+            api_retry_args=self.snowflake_api_retry_args,\n             **self.hook_params,\n         )\n \n","test_patch":"diff --git a/providers/snowflake/tests/unit/snowflake/hooks/test_snowflake_sql_api.py b/providers/snowflake/tests/unit/snowflake/hooks/test_snowflake_sql_api.py\nindex 21a3fa7a999a8..410588a7a71bb 100644\n--- a/providers/snowflake/tests/unit/snowflake/hooks/test_snowflake_sql_api.py\n+++ b/providers/snowflake/tests/unit/snowflake/hooks/test_snowflake_sql_api.py\n@@ -21,14 +21,15 @@\n import uuid\n from typing import TYPE_CHECKING, Any\n from unittest import mock\n-from unittest.mock import AsyncMock, PropertyMock\n+from unittest.mock import AsyncMock, PropertyMock, call\n \n+import aiohttp\n import pytest\n import requests\n+import tenacity\n from cryptography.hazmat.backends import default_backend\n from cryptography.hazmat.primitives import serialization\n from cryptography.hazmat.primitives.asymmetric import rsa\n-from responses import RequestsMock\n \n from airflow.exceptions import AirflowException, AirflowProviderDeprecationWarning\n from airflow.models import Connection\n@@ -149,6 +150,28 @@\n     \"role\": \"airflow_role\",\n }\n \n+API_URL = \"https://test.snowflakecomputing.com/api/v2/statements/test\"\n+\n+\n+@pytest.fixture\n+def mock_requests():\n+    with mock.patch(\n+        \"airflow.providers.snowflake.hooks.snowflake_sql_api.requests.Session\"\n+    ) as mock_session_cls:\n+        mock_session = mock.MagicMock()\n+        mock_session_cls.return_value.__enter__.return_value = mock_session\n+        yield mock_session\n+\n+\n+@pytest.fixture\n+def mock_async_request():\n+    with mock.patch(\n+        \"airflow.providers.snowflake.hooks.snowflake_sql_api.aiohttp.ClientSession.request\"\n+    ) as mock_session_cls:\n+        mock_request = mock.MagicMock()\n+        mock_session_cls.return_value = mock_request\n+        yield mock_request\n+\n \n def create_successful_response_mock(content):\n     \"\"\"Create mock response for success state\"\"\"\n@@ -167,6 +190,35 @@ def create_post_side_effect(status_code=429):\n     return response\n \n \n+def create_async_request_client_response_error(request_info=None, history=None, status_code=429):\n+    \"\"\"Create mock response for async request side effect\"\"\"\n+    response = mock.MagicMock()\n+    request_info = mock.MagicMock() if request_info is None else request_info\n+    history = mock.MagicMock() if history is None else history\n+    response.status = status_code\n+    response.reason = f\"{status_code} Error\"\n+    response.raise_for_status.side_effect = aiohttp.ClientResponseError(\n+        request_info=request_info, history=history, status=status_code, message=response.reason\n+    )\n+    return response\n+\n+\n+def create_async_connection_error():\n+    response = mock.MagicMock()\n+    response.raise_for_status.side_effect = aiohttp.ClientConnectionError()\n+    return response\n+\n+\n+def create_async_request_client_response_success(json=GET_RESPONSE, status_code=200):\n+    \"\"\"Create mock response for async request side effect\"\"\"\n+    response = mock.MagicMock()\n+    response.status = status_code\n+    response.reason = \"test\"\n+    response.json = AsyncMock(return_value=json)\n+    response.raise_for_status.side_effect = None\n+    return response\n+\n+\n class TestSnowflakeSqlApiHook:\n     @pytest.mark.parametrize(\n         \"sql,statement_count,expected_response, expected_query_ids\",\n@@ -175,7 +227,6 @@ class TestSnowflakeSqlApiHook:\n             (SQL_MULTIPLE_STMTS, 4, {\"statementHandles\": [\"uuid\", \"uuid1\"]}, [\"uuid\", \"uuid1\"]),\n         ],\n     )\n-    @mock.patch(\"airflow.providers.snowflake.hooks.snowflake_sql_api.requests\")\n     @mock.patch(\n         \"airflow.providers.snowflake.hooks.snowflake_sql_api.SnowflakeSqlApiHook._get_conn_params\",\n         new_callable=PropertyMock,\n@@ -185,25 +236,24 @@ def test_execute_query(\n         self,\n         mock_get_header,\n         mock_conn_param,\n-        mock_requests,\n         sql,\n         statement_count,\n         expected_response,\n         expected_query_ids,\n+        mock_requests,\n     ):\n         \"\"\"Test execute_query method, run query by mocking post request method and return the query ids\"\"\"\n         mock_requests.codes.ok = 200\n-        mock_requests.post.side_effect = [\n+        mock_requests.request.side_effect = [\n             create_successful_response_mock(expected_response),\n         ]\n         status_code_mock = mock.PropertyMock(return_value=200)\n-        type(mock_requests.post.return_value).status_code = status_code_mock\n+        type(mock_requests.request.return_value).status_code = status_code_mock\n \n         hook = SnowflakeSqlApiHook(\"mock_conn_id\")\n         query_ids = hook.execute_query(sql, statement_count)\n         assert query_ids == expected_query_ids\n \n-    @mock.patch(\"airflow.providers.snowflake.hooks.snowflake_sql_api.requests\")\n     @mock.patch(\n         \"airflow.providers.snowflake.hooks.snowflake_sql_api.SnowflakeSqlApiHook._get_conn_params\",\n         new_callable=PropertyMock,\n@@ -221,11 +271,11 @@ def test_execute_query_multiple_times_give_fresh_query_ids_each_time(\n         )\n \n         mock_requests.codes.ok = 200\n-        mock_requests.post.side_effect = [\n+        mock_requests.request.side_effect = [\n             create_successful_response_mock(expected_response),\n         ]\n         status_code_mock = mock.PropertyMock(return_value=200)\n-        type(mock_requests.post.return_value).status_code = status_code_mock\n+        type(mock_requests.request.return_value).status_code = status_code_mock\n \n         hook = SnowflakeSqlApiHook(\"mock_conn_id\")\n         query_ids = hook.execute_query(sql, statement_count)\n@@ -237,7 +287,7 @@ def test_execute_query_multiple_times_give_fresh_query_ids_each_time(\n             {\"statementHandle\": \"uuid\"},\n             [\"uuid\"],\n         )\n-        mock_requests.post.side_effect = [\n+        mock_requests.request.side_effect = [\n             create_successful_response_mock(expected_response),\n         ]\n         query_ids = hook.execute_query(sql, statement_count)\n@@ -247,7 +297,6 @@ def test_execute_query_multiple_times_give_fresh_query_ids_each_time(\n         \"sql,statement_count,expected_response, expected_query_ids\",\n         [(SINGLE_STMT, 1, {\"statementHandle\": \"uuid\"}, [\"uuid\"])],\n     )\n-    @mock.patch(\"airflow.providers.snowflake.hooks.snowflake_sql_api.requests\")\n     @mock.patch(\n         \"airflow.providers.snowflake.hooks.snowflake_sql_api.SnowflakeSqlApiHook._get_conn_params\",\n         new_callable=PropertyMock,\n@@ -257,18 +306,18 @@ def test_execute_query_exception_without_statement_handle(\n         self,\n         mock_get_header,\n         mock_conn_param,\n-        mock_requests,\n         sql,\n         statement_count,\n         expected_response,\n         expected_query_ids,\n+        mock_requests,\n     ):\n         \"\"\"\n         Test execute_query method by mocking the exception response and raise airflow exception\n         without statementHandle in the response\n         \"\"\"\n         side_effect = create_post_side_effect()\n-        mock_requests.post.side_effect = side_effect\n+        mock_requests.request.side_effect = side_effect\n         hook = SnowflakeSqlApiHook(\"mock_conn_id\")\n \n         with pytest.raises(AirflowException) as exception_info:\n@@ -281,7 +330,6 @@ def test_execute_query_exception_without_statement_handle(\n             (SQL_MULTIPLE_STMTS, 4, {\"1\": {\"type\": \"FIXED\", \"value\": \"123\"}}),\n         ],\n     )\n-    @mock.patch(\"airflow.providers.snowflake.hooks.snowflake_sql_api.requests\")\n     @mock.patch(\n         \"airflow.providers.snowflake.hooks.snowflake_sql_api.SnowflakeSqlApiHook._get_conn_params\",\n         new_callable=PropertyMock,\n@@ -291,15 +339,15 @@ def test_execute_query_bindings_warning(\n         self,\n         mock_get_headers,\n         mock_conn_params,\n-        mock_requests,\n         sql,\n         statement_count,\n         bindings,\n+        mock_requests,\n     ):\n         \"\"\"Test execute_query method logs warning when bindings are provided for multi-statement queries\"\"\"\n         mock_conn_params.return_value = CONN_PARAMS\n         mock_get_headers.return_value = HEADERS\n-        mock_requests.post.return_value = create_successful_response_mock(\n+        mock_requests.request.return_value = create_successful_response_mock(\n             {\"statementHandles\": [\"uuid\", \"uuid1\"]}\n         )\n \n@@ -316,28 +364,32 @@ def test_execute_query_bindings_warning(\n             ([\"uuid\", \"uuid1\"]),\n         ],\n     )\n-    @mock.patch(\"airflow.providers.snowflake.hooks.snowflake_sql_api.requests\")\n     @mock.patch(\n         \"airflow.providers.snowflake.hooks.snowflake_sql_api.SnowflakeSqlApiHook.\"\n         \"get_request_url_header_params\"\n     )\n-    def test_check_query_output(self, mock_geturl_header_params, mock_requests, query_ids):\n+    def test_check_query_output(self, mock_geturl_header_params, query_ids, mock_requests):\n         \"\"\"Test check_query_output by passing query ids as params and mock get_request_url_header_params\"\"\"\n         req_id = uuid.uuid4()\n         params = {\"requestId\": str(req_id), \"page\": 2, \"pageSize\": 10}\n         mock_geturl_header_params.return_value = HEADERS, params, \"/test/airflow/\"\n-        mock_requests.get.return_value.json.return_value = GET_RESPONSE\n+        mock_requests.request.return_value.json.return_value = GET_RESPONSE\n         hook = SnowflakeSqlApiHook(\"mock_conn_id\")\n         with mock.patch.object(hook.log, \"info\") as mock_log_info:\n             hook.check_query_output(query_ids)\n         mock_log_info.assert_called_with(GET_RESPONSE)\n \n-    @pytest.mark.parametrize(\"query_ids\", [([\"uuid\", \"uuid1\"])])\n+    @pytest.mark.parametrize(\"query_ids\", [[\"uuid\", \"uuid1\"]])\n     @mock.patch(\n         \"airflow.providers.snowflake.hooks.snowflake_sql_api.SnowflakeSqlApiHook.\"\n         \"get_request_url_header_params\"\n     )\n-    def test_check_query_output_exception(self, mock_geturl_header_params, query_ids):\n+    def test_check_query_output_exception(\n+        self,\n+        mock_geturl_header_params,\n+        query_ids,\n+        mock_requests,\n+    ):\n         \"\"\"\n         Test check_query_output by passing query ids as params and mock get_request_url_header_params\n         to raise airflow exception and mock with http error\n@@ -345,11 +397,13 @@ def test_check_query_output_exception(self, mock_geturl_header_params, query_ids\n         req_id = uuid.uuid4()\n         params = {\"requestId\": str(req_id), \"page\": 2, \"pageSize\": 10}\n         mock_geturl_header_params.return_value = HEADERS, params, \"https://test/airflow/\"\n-        hook = SnowflakeSqlApiHook(\"mock_conn_id\")\n-        with mock.patch.object(hook.log, \"error\"), RequestsMock() as requests_mock:\n-            requests_mock.get(url=\"https://test/airflow/\", json={\"foo\": \"bar\"}, status=500)\n-            with pytest.raises(AirflowException, match='Response: {\"foo\": \"bar\"}, Status Code: 500'):\n-                hook.check_query_output(query_ids)\n+        custom_retry_args = {\n+            \"stop\": tenacity.stop_after_attempt(2),  # Only 2 attempts instead of default 5\n+        }\n+        hook = SnowflakeSqlApiHook(\"mock_conn_id\", api_retry_args=custom_retry_args)\n+        mock_requests.request.side_effect = [create_post_side_effect(status_code=500)] * 3\n+        with pytest.raises(requests.exceptions.HTTPError):\n+            hook.check_query_output(query_ids)\n \n     @mock.patch(\n         \"airflow.providers.snowflake.hooks.snowflake_sql_api.SnowflakeSqlApiHook._get_conn_params\",\n@@ -605,9 +659,8 @@ def test_get_private_key_should_support_private_auth_with_unencrypted_key(\n         \"airflow.providers.snowflake.hooks.snowflake_sql_api.SnowflakeSqlApiHook.\"\n         \"get_request_url_header_params\"\n     )\n-    @mock.patch(\"airflow.providers.snowflake.hooks.snowflake_sql_api.requests\")\n     def test_get_sql_api_query_status(\n-        self, mock_requests, mock_geturl_header_params, status_code, response, expected_response\n+        self, mock_geturl_header_params, status_code, response, expected_response, mock_requests\n     ):\n         \"\"\"Test get_sql_api_query_status function by mocking the status, response and expected\n         response\"\"\"\n@@ -623,7 +676,10 @@ def __init__(self, status_code, data):\n             def json(self):\n                 return self.data\n \n-        mock_requests.get.return_value = MockResponse(status_code, response)\n+            def raise_for_status(self):\n+                return\n+\n+        mock_requests.request.return_value = MockResponse(status_code, response)\n         hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n         assert hook.get_sql_api_query_status(\"uuid\") == expected_response\n \n@@ -666,17 +722,16 @@ def json(self):\n         \"airflow.providers.snowflake.hooks.snowflake_sql_api.SnowflakeSqlApiHook.\"\n         \"get_request_url_header_params\"\n     )\n-    @mock.patch(\"airflow.providers.snowflake.hooks.snowflake_sql_api.aiohttp.ClientSession.get\")\n     async def test_get_sql_api_query_status_async(\n-        self, mock_get, mock_geturl_header_params, status_code, response, expected_response\n+        self, mock_geturl_header_params, status_code, response, expected_response, mock_async_request\n     ):\n         \"\"\"Test Async get_sql_api_query_status_async function by mocking the status,\n         response and expected response\"\"\"\n         req_id = uuid.uuid4()\n         params = {\"requestId\": str(req_id), \"page\": 2, \"pageSize\": 10}\n         mock_geturl_header_params.return_value = HEADERS, params, \"/test/airflow/\"\n-        mock_get.return_value.__aenter__.return_value.status = status_code\n-        mock_get.return_value.__aenter__.return_value.json = AsyncMock(return_value=response)\n+        mock_async_request.__aenter__.return_value.status = status_code\n+        mock_async_request.__aenter__.return_value.json = AsyncMock(return_value=response)\n         hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n         response = await hook.get_sql_api_query_status_async(\"uuid\")\n         assert response == expected_response\n@@ -774,7 +829,6 @@ def test_hook_parameter_propagation(self, hook_params):\n         ],\n     )\n     @mock.patch(\"uuid.uuid4\")\n-    @mock.patch(\"airflow.providers.snowflake.hooks.snowflake_sql_api.requests\")\n     @mock.patch(\n         \"airflow.providers.snowflake.hooks.snowflake_sql_api.SnowflakeSqlApiHook._get_conn_params\",\n         new_callable=PropertyMock,\n@@ -784,13 +838,13 @@ def test_proper_parametrization_of_execute_query_api_request(\n         self,\n         mock_get_headers,\n         mock_conn_param,\n-        mock_requests,\n         mock_uuid,\n         test_hook_params,\n         sql,\n         statement_count,\n         expected_payload,\n         expected_response,\n+        mock_requests,\n     ):\n         \"\"\"\n         This tests if the query execution ordered by POST request to Snowflake API\n@@ -801,7 +855,7 @@ def test_proper_parametrization_of_execute_query_api_request(\n         mock_conn_param.return_value = CONN_PARAMS\n         mock_get_headers.return_value = HEADERS\n         mock_requests.codes.ok = 200\n-        mock_requests.post.side_effect = [\n+        mock_requests.request.side_effect = [\n             create_successful_response_mock(expected_response),\n         ]\n         status_code_mock = mock.PropertyMock(return_value=200)\n@@ -812,4 +866,316 @@ def test_proper_parametrization_of_execute_query_api_request(\n \n         hook.execute_query(sql, statement_count)\n \n-        mock_requests.post.assert_called_once_with(url, headers=HEADERS, json=expected_payload, params=params)\n+        mock_requests.request.assert_called_once_with(\n+            method=\"post\", url=url, headers=HEADERS, json=expected_payload, params=params\n+        )\n+\n+    @pytest.mark.parametrize(\n+        \"status_code,should_retry\",\n+        [\n+            (429, True),  # Too Many Requests - should retry\n+            (503, True),  # Service Unavailable - should retry\n+            (504, True),  # Gateway Timeout - should retry\n+            (500, False),  # Internal Server Error - should not retry\n+            (400, False),  # Bad Request - should not retry\n+            (401, False),  # Unauthorized - should not retry\n+            (404, False),  # Not Found - should not retry\n+        ],\n+    )\n+    def test_make_api_call_with_retries_http_errors(self, status_code, should_retry, mock_requests):\n+        \"\"\"\n+        Test that _make_api_call_with_retries method only retries on specific HTTP status codes.\n+        Should retry on 429, 503, 504 but not on other error codes.\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+\n+        # Mock failed response\n+        failed_response = mock.MagicMock()\n+        failed_response.status_code = status_code\n+        failed_response.json.return_value = {\"error\": \"test error\"}\n+        failed_response.raise_for_status.side_effect = requests.exceptions.HTTPError(response=failed_response)\n+\n+        # Mock successful response for retries\n+        success_response = mock.MagicMock()\n+        success_response.status_code = 200\n+        success_response.json.return_value = {\"statementHandle\": \"uuid\"}\n+        success_response.raise_for_status.return_value = None\n+\n+        if should_retry:\n+            # For retryable errors, first call fails, second succeeds\n+            mock_requests.request.side_effect = [failed_response, success_response]\n+            status_code, resp_json = hook._make_api_call_with_retries(\n+                method=\"GET\",\n+                url=API_URL,\n+                headers=HEADERS,\n+            )\n+            assert status_code == 200\n+            assert resp_json == {\"statementHandle\": \"uuid\"}\n+            assert mock_requests.request.call_count == 2\n+            mock_requests.request.assert_has_calls(\n+                [\n+                    call(\n+                        method=\"get\",\n+                        json=None,\n+                        url=API_URL,\n+                        params=None,\n+                        headers=HEADERS,\n+                    ),\n+                    call(\n+                        method=\"get\",\n+                        json=None,\n+                        url=API_URL,\n+                        params=None,\n+                        headers=HEADERS,\n+                    ),\n+                ]\n+            )\n+        else:\n+            # For non-retryable errors, should fail immediately\n+            mock_requests.request.side_effect = [failed_response]\n+            with pytest.raises(requests.exceptions.HTTPError):\n+                hook._make_api_call_with_retries(\n+                    method=\"GET\",\n+                    url=API_URL,\n+                    headers=HEADERS,\n+                )\n+            assert mock_requests.request.call_count == 1\n+            mock_requests.request.assert_called_with(\n+                method=\"get\",\n+                json=None,\n+                url=API_URL,\n+                params=None,\n+                headers=HEADERS,\n+            )\n+\n+    def test_make_api_call_with_retries_connection_errors(self, mock_requests):\n+        \"\"\"\n+        Test that _make_api_call_with_retries method retries on connection errors.\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+\n+        # Mock connection error then success\n+        success_response = mock.MagicMock()\n+        success_response.status_code = 200\n+        success_response.json.return_value = {\"statementHandle\": \"uuid\"}\n+        success_response.raise_for_status.return_value = None\n+\n+        mock_requests.request.side_effect = [\n+            requests.exceptions.ConnectionError(\"Connection failed\"),\n+            success_response,\n+        ]\n+\n+        status_code, resp_json = hook._make_api_call_with_retries(\n+            \"POST\", API_URL, HEADERS, json={\"test\": \"data\"}\n+        )\n+\n+        assert status_code == 200\n+        mock_requests.request.assert_called_with(\n+            method=\"post\",\n+            url=API_URL,\n+            params=None,\n+            headers=HEADERS,\n+            json={\"test\": \"data\"},\n+        )\n+        assert resp_json == {\"statementHandle\": \"uuid\"}\n+        assert mock_requests.request.call_count == 2\n+\n+    def test_make_api_call_with_retries_timeout_errors(self, mock_requests):\n+        \"\"\"\n+        Test that _make_api_call_with_retries method retries on timeout errors.\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+\n+        # Mock timeout error then success\n+        success_response = mock.MagicMock()\n+        success_response.status_code = 200\n+        success_response.json.return_value = {\"statementHandle\": \"uuid\"}\n+        success_response.raise_for_status.return_value = None\n+\n+        mock_requests.request.side_effect = [\n+            requests.exceptions.Timeout(\"Request timed out\"),\n+            success_response,\n+        ]\n+\n+        status_code, resp_json = hook._make_api_call_with_retries(\"GET\", API_URL, HEADERS)\n+\n+        assert status_code == 200\n+        assert resp_json == {\"statementHandle\": \"uuid\"}\n+        assert mock_requests.request.call_count == 2\n+\n+    def test_make_api_call_with_retries_max_attempts(self, mock_requests):\n+        \"\"\"\n+        Test that _make_api_call_with_retries method respects max retry attempts.\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+\n+        # Mock response that always fails with retryable error\n+        failed_response = mock.MagicMock()\n+        failed_response.status_code = 429\n+        failed_response.json.return_value = {\"error\": \"rate limited\"}\n+        failed_response.raise_for_status.side_effect = requests.exceptions.HTTPError(response=failed_response)\n+\n+        mock_requests.request.side_effect = [failed_response] * 10  # More failures than max retries\n+\n+        with pytest.raises(requests.exceptions.HTTPError):\n+            hook._make_api_call_with_retries(\"GET\", API_URL, HEADERS)\n+\n+        # Should attempt 5 times (initial + 4 retries) based on default retry config\n+        assert mock_requests.request.call_count == 5\n+\n+    def test_make_api_call_with_retries_success_no_retry(self, mock_requests):\n+        \"\"\"\n+        Test that _make_api_call_with_retries method doesn't retry on successful requests.\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+\n+        # Mock successful response\n+        success_response = mock.MagicMock()\n+        success_response.status_code = 200\n+        success_response.json.return_value = {\"statementHandle\": \"uuid\"}\n+        success_response.raise_for_status.return_value = None\n+\n+        mock_requests.request.return_value = success_response\n+\n+        status_code, resp_json = hook._make_api_call_with_retries(\n+            \"POST\", API_URL, HEADERS, json={\"test\": \"data\"}\n+        )\n+\n+        assert status_code == 200\n+        assert resp_json == {\"statementHandle\": \"uuid\"}\n+        assert mock_requests.request.call_count == 1\n+\n+    def test_make_api_call_with_retries_unsupported_method(self):\n+        \"\"\"\n+        Test that _make_api_call_with_retries method raises ValueError for unsupported HTTP methods.\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+\n+        with pytest.raises(ValueError, match=\"Unsupported HTTP method: PUT\"):\n+            hook._make_api_call_with_retries(\"PUT\", API_URL, HEADERS)\n+\n+    def test_make_api_call_with_retries_custom_retry_config(self, mock_requests):\n+        \"\"\"\n+        Test that _make_api_call_with_retries method respects custom retry configuration.\n+        \"\"\"\n+\n+        # Create hook with custom retry config\n+        custom_retry_args = {\n+            \"stop\": tenacity.stop_after_attempt(2),  # Only 2 attempts instead of default 5\n+        }\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\", api_retry_args=custom_retry_args)\n+\n+        # Mock response that always fails with retryable error\n+        failed_response = mock.MagicMock()\n+        failed_response.status_code = 503\n+        failed_response.json.return_value = {\"error\": \"service unavailable\"}\n+        failed_response.raise_for_status.side_effect = requests.exceptions.HTTPError(response=failed_response)\n+\n+        mock_requests.request.side_effect = [failed_response] * 3\n+\n+        with pytest.raises(requests.exceptions.HTTPError):\n+            hook._make_api_call_with_retries(\"GET\", API_URL, HEADERS)\n+\n+        # Should attempt only 2 times due to custom config\n+        assert mock_requests.request.call_count == 2\n+\n+    @pytest.mark.asyncio\n+    async def test_make_api_call_with_retries_async_success(self, mock_async_request):\n+        \"\"\"\n+        Test that _make_api_call_with_retries_async returns response on success.\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+\n+        mock_response = create_async_request_client_response_success()\n+        mock_async_request.__aenter__.return_value = mock_response\n+        status_code, resp_json = await hook._make_api_call_with_retries_async(\"GET\", API_URL, HEADERS)\n+        assert status_code == 200\n+        assert resp_json == GET_RESPONSE\n+        assert mock_async_request.__aenter__.call_count == 1\n+\n+    @pytest.mark.asyncio\n+    async def test_make_api_call_with_retries_async_retryable_http_error(self, mock_async_request):\n+        \"\"\"\n+        Test that _make_api_call_with_retries_async retries on retryable HTTP errors (429, 503, 504).\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+\n+        # First response: 429, then 200\n+        mock_response_429 = create_async_request_client_response_error()\n+        mock_response_200 = create_async_request_client_response_success()\n+        # Side effect for request context manager\n+        mock_async_request.__aenter__.side_effect = [\n+            mock_response_429,\n+            mock_response_200,\n+        ]\n+\n+        status_code, resp_json = await hook._make_api_call_with_retries_async(\"GET\", API_URL, HEADERS)\n+        assert status_code == 200\n+        assert resp_json == GET_RESPONSE\n+        assert mock_async_request.__aenter__.call_count == 2\n+\n+    @pytest.mark.asyncio\n+    async def test_make_api_call_with_retries_async_non_retryable_http_error(self, mock_async_request):\n+        \"\"\"\n+        Test that _make_api_call_with_retries_async does not retry on non-retryable HTTP errors.\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+\n+        mock_response_400 = create_async_request_client_response_error(status_code=400)\n+\n+        mock_async_request.__aenter__.return_value = mock_response_400\n+\n+        with pytest.raises(aiohttp.ClientResponseError):\n+            await hook._make_api_call_with_retries_async(\"GET\", API_URL, HEADERS)\n+        assert mock_async_request.__aenter__.call_count == 1\n+\n+    @pytest.mark.asyncio\n+    async def test_make_api_call_with_retries_async_connection_error(self, mock_async_request):\n+        \"\"\"\n+        Test that _make_api_call_with_retries_async retries on connection errors.\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+\n+        # First: connection error, then: success\n+        failed_conn = create_async_connection_error()\n+\n+        mock_request_200 = create_async_request_client_response_success()\n+\n+        # Side effect for request context manager\n+        mock_async_request.__aenter__.side_effect = [\n+            failed_conn,\n+            mock_request_200,\n+        ]\n+\n+        status_code, resp_json = await hook._make_api_call_with_retries_async(\"GET\", API_URL, HEADERS)\n+        assert status_code == 200\n+        assert resp_json == GET_RESPONSE\n+        assert mock_async_request.__aenter__.call_count == 2\n+\n+    @pytest.mark.asyncio\n+    async def test_make_api_call_with_retries_async_max_attempts(self, mock_async_request):\n+        \"\"\"\n+        Test that _make_api_call_with_retries_async respects max retry attempts.\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+        mock_request_429 = create_async_request_client_response_error(status_code=429)\n+\n+        # Always returns 429\n+        mock_async_request.__aenter__.side_effect = [mock_request_429] * 5\n+\n+        with pytest.raises(aiohttp.ClientResponseError):\n+            await hook._make_api_call_with_retries_async(\"GET\", API_URL, HEADERS)\n+        # Should attempt 5 times (default max retries)\n+        assert mock_async_request.__aenter__.call_count == 5\n+\n+    @pytest.mark.asyncio\n+    async def test_make_api_call_with_retries_async_unsupported_method(self, mock_async_request):\n+        \"\"\"\n+        Test that _make_api_call_with_retries_async raises ValueError for unsupported HTTP methods.\n+        \"\"\"\n+        hook = SnowflakeSqlApiHook(snowflake_conn_id=\"test_conn\")\n+\n+        with pytest.raises(ValueError, match=\"Unsupported HTTP method: PATCH\"):\n+            await hook._make_api_call_with_retries_async(\"PATCH\", API_URL, HEADERS)\n+        # No HTTP call should be made\n+        assert mock_async_request.__aenter__.call_count == 0\n","problem_statement":"Enhance Snowflake SQL API Hook with Retry Logic for Query Status Polling\n### Apache Airflow Provider(s)\n\nsnowflake\n\n### Versions of Apache Airflow Providers\n\nWeve identified an issue with the SnowflakeSqlApiHook used by the SnowflakeSqlApiOperator in the Apache Airflow Snowflake provider. In certain cases, after submitting a query to Snowflakes SQL API, the initial request succeeds, but polling the status endpoint fails due to a RemoteDisconnected error, caused by the remote Snowflake endpoint forcefully closing a connection. This is likely due to the reuse of a stale connection or other transient TCP-level issues.\n\nWhy This Matters\n\t\tEnsures robustness of asynchronous query execution using the SQL API.\n\t\tAvoids failed tasks due to transient connection pool or network issues.\n\t\tAligns with best practices in client design for polling APIs over HTTP.\n\n### Apache Airflow version\n\n2.9.3\n\n### Operating System\n\nDebian\n\n### Deployment\n\nAstronomer\n\n### Deployment details\n\n_No response_\n\n### What happened\n\n\nSummary of Issue\n\t\tThe initial POST to submit the query succeeds.\n\t\tThe subsequent GET to the statementStatusUrl fails with:\n`RemoteDisconnected('Remote end closed connection without response')`\nThe failure is raised at the application level and results in the task being marked as failed.\n\t\tRetrying at the task/operator level is not viable when:\n\t\tThe task is considered successful from Airflows point of view (query submitted).\n\t\tThe query is not idempotent (e.g., it modifies data), so re-running it would cause issues.\n\nSnowflake confirmed this is a client-side (Airflow hook) implementation gap:\n\t\tThe polling request should retry on connection-level failures (like RemoteDisconnected), possibly with exponential backoff.\n\t\tThis would avoid needing to retry the entire query from the start.\n\t\tThe current implementation makes a single attempt to poll the status endpoint, which is fragile in cloud network environments.\n\n### What you think should happen instead\n\n_No response_\n\n### How to reproduce\n\n1. Create a long running query.\n2. Kill the TCP Connection during polling.\n3. Logs will show something like \n`RemoteDisconnected('Remote end closed connection without response')\n...\nraise ValueError({\"status\": \"error\", \"message\": str(e)})`\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n","hints_text":"@rawwar please assign to me, thanks!\n> @rawwar please assign to me, thanks!\n\nDone.\n\n","all_hints_text":"@rawwar please assign to me, thanks!\n> @rawwar please assign to me, thanks!\n\nDone.\n[PR to resolve this issue](https://github.com/apache/airflow/pull/51463)\n\n","commit_urls":["https://github.com/apache/airflow/commit/a43e59521b5ab67ee1ff9806988864567f5e7e7e","https://github.com/apache/airflow/commit/3a4c95fd168c73f1b8ff491772337ed88898c7b7","https://github.com/apache/airflow/commit/18b5c90ec78df6399c22c64bf2f47bfff211e32f","https://github.com/apache/airflow/commit/488290c77da2932a266f0c33f181d1e60890297b","https://github.com/apache/airflow/commit/d913a96bed9e085188e183efac98c2ba03dd86b2","https://github.com/apache/airflow/commit/6df437b7a6ac9dfb86f5fc9c5f7be28d235bf1e6","https://github.com/apache/airflow/commit/f710a1c5701babbd3e4e00330a2c524e5c9f65ce","https://github.com/apache/airflow/commit/ebe29c649f2f59436f4f1c179f2ddcb63e118e77","https://github.com/apache/airflow/commit/bb87d361a293d132a3c661242230e359c8317f97","https://github.com/apache/airflow/commit/088fc172977fe78419f3c598a6b48b28992b80b3"],"created_at":"2025-06-05T19:11:49Z","classification":"Efficiency"}
{"repo":"apache/airflow","pull_number":51397,"instance_id":"apache__airflow-51397","issue_numbers":[50529],"base_commit":"8cbe785ffe928eba20237b53c027f9d51ab23df4","patch":"diff --git a/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/powerbi.py b/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/powerbi.py\nindex c8e9fa76424fd..0500e869de6df 100644\n--- a/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/powerbi.py\n+++ b/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/powerbi.py\n@@ -189,12 +189,15 @@ async def get_refresh_details_by_refresh_id(\n \n         return refresh_details\n \n-    async def trigger_dataset_refresh(self, *, dataset_id: str, group_id: str) -> str:\n+    async def trigger_dataset_refresh(\n+        self, *, dataset_id: str, group_id: str, request_body: dict[str, Any] | None = None\n+    ) -> str:\n         \"\"\"\n         Triggers a refresh for the specified dataset from the given group id.\n \n         :param dataset_id: The dataset id.\n         :param group_id: The workspace id.\n+        :param request_body: Additional arguments to pass to the request body, as described in https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/refresh-dataset-in-group#request-body.\n \n         :return: Request id of the dataset refresh request.\n         \"\"\"\n@@ -207,6 +210,7 @@ async def trigger_dataset_refresh(self, *, dataset_id: str, group_id: str) -> st\n                     \"group_id\": group_id,\n                     \"dataset_id\": dataset_id,\n                 },\n+                data=request_body,\n             )\n \n             request_id = response.get(\"requestid\")\ndiff --git a/providers/microsoft/azure/src/airflow/providers/microsoft/azure/operators/powerbi.py b/providers/microsoft/azure/src/airflow/providers/microsoft/azure/operators/powerbi.py\nindex 9abab3da977a3..444100dc667f7 100644\n--- a/providers/microsoft/azure/src/airflow/providers/microsoft/azure/operators/powerbi.py\n+++ b/providers/microsoft/azure/src/airflow/providers/microsoft/azure/operators/powerbi.py\n@@ -72,6 +72,7 @@ class PowerBIDatasetRefreshOperator(BaseOperator):\n     :param timeout: Time in seconds to wait for a dataset to reach a terminal status for asynchronous waits. Used only if ``wait_for_termination`` is True.\n     :param check_interval: Number of seconds to wait before rechecking the\n         refresh status.\n+    :param request_body: Additional arguments to pass to the request body, as described in https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/refresh-dataset-in-group#request-body.\n     \"\"\"\n \n     template_fields: Sequence[str] = (\n@@ -92,6 +93,7 @@ def __init__(\n         proxies: dict | None = None,\n         api_version: APIVersion | str | None = None,\n         check_interval: int = 60,\n+        request_body: dict[str, Any] | None = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -102,6 +104,7 @@ def __init__(\n         self.conn_id = conn_id\n         self.timeout = timeout\n         self.check_interval = check_interval\n+        self.request_body = request_body\n \n     @property\n     def proxies(self) -> dict | None:\n@@ -124,6 +127,7 @@ def execute(self, context: Context):\n                     api_version=self.api_version,\n                     check_interval=self.check_interval,\n                     wait_for_termination=self.wait_for_termination,\n+                    request_body=self.request_body,\n                 ),\n                 method_name=self.get_refresh_status.__name__,\n             )\ndiff --git a/providers/microsoft/azure/src/airflow/providers/microsoft/azure/triggers/powerbi.py b/providers/microsoft/azure/src/airflow/providers/microsoft/azure/triggers/powerbi.py\nindex bcba15a2af1ff..042916f796637 100644\n--- a/providers/microsoft/azure/src/airflow/providers/microsoft/azure/triggers/powerbi.py\n+++ b/providers/microsoft/azure/src/airflow/providers/microsoft/azure/triggers/powerbi.py\n@@ -20,7 +20,7 @@\n import asyncio\n import time\n from collections.abc import AsyncIterator\n-from typing import TYPE_CHECKING\n+from typing import TYPE_CHECKING, Any\n \n import tenacity\n \n@@ -51,9 +51,9 @@ class PowerBITrigger(BaseTrigger):\n     :param dataset_id: The dataset Id to refresh.\n     :param dataset_refresh_id: The dataset refresh Id to poll for the status, if not provided a new refresh will be triggered.\n     :param group_id: The workspace Id where dataset is located.\n-    :param end_time: Time in seconds when trigger should stop polling.\n     :param check_interval: Time in seconds to wait between each poll.\n     :param wait_for_termination: Wait for the dataset refresh to complete or fail.\n+    :param request_body: Additional arguments to pass to the request body, as described in https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/refresh-dataset-in-group#request-body.\n     \"\"\"\n \n     def __init__(\n@@ -67,6 +67,7 @@ def __init__(\n         api_version: APIVersion | str | None = None,\n         check_interval: int = 60,\n         wait_for_termination: bool = True,\n+        request_body: dict[str, Any] | None = None,\n     ):\n         super().__init__()\n         self.hook = PowerBIHook(conn_id=conn_id, proxies=proxies, api_version=api_version, timeout=timeout)\n@@ -76,6 +77,7 @@ def __init__(\n         self.group_id = group_id\n         self.check_interval = check_interval\n         self.wait_for_termination = wait_for_termination\n+        self.request_body = request_body\n \n     def serialize(self):\n         \"\"\"Serialize the trigger instance.\"\"\"\n@@ -91,6 +93,7 @@ def serialize(self):\n                 \"timeout\": self.timeout,\n                 \"check_interval\": self.check_interval,\n                 \"wait_for_termination\": self.wait_for_termination,\n+                \"request_body\": self.request_body,\n             },\n         )\n \n@@ -113,6 +116,7 @@ async def run(self) -> AsyncIterator[TriggerEvent]:\n             dataset_refresh_id = await self.hook.trigger_dataset_refresh(\n                 dataset_id=self.dataset_id,\n                 group_id=self.group_id,\n+                request_body=self.request_body,\n             )\n \n             if dataset_refresh_id:\n","test_patch":"diff --git a/providers/microsoft/azure/tests/system/microsoft/azure/example_powerbi_dataset_refresh.py b/providers/microsoft/azure/tests/system/microsoft/azure/example_powerbi_dataset_refresh.py\nindex 5453caff6463b..b772405da8601 100644\n--- a/providers/microsoft/azure/tests/system/microsoft/azure/example_powerbi_dataset_refresh.py\n+++ b/providers/microsoft/azure/tests/system/microsoft/azure/example_powerbi_dataset_refresh.py\n@@ -66,6 +66,12 @@ def create_connection(conn_id_name: str):\n         group_id=GROUP_ID,\n         check_interval=30,\n         timeout=120,\n+        request_body={\n+            \"type\": \"full\",\n+            \"retryCount\": 3,\n+            \"commitMode\": \"transactional\",\n+            \"notifyOption\": \"MailOnFailure\",\n+        },\n     )\n     # [END howto_operator_powerbi_refresh_async]\n \ndiff --git a/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_powerbi.py b/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_powerbi.py\nindex 43794c43880f1..6d1bdddeef90c 100644\n--- a/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_powerbi.py\n+++ b/providers/microsoft/azure/tests/unit/microsoft/azure/operators/test_powerbi.py\n@@ -39,6 +39,13 @@\n TASK_ID = \"run_powerbi_operator\"\n GROUP_ID = \"group_id\"\n DATASET_ID = \"dataset_id\"\n+REQUEST_BODY = {\n+    \"type\": \"full\",\n+    \"commitMode\": \"transactional\",\n+    \"objects\": [{\"table\": \"Customer\", \"partition\": \"Robert\"}],\n+    \"applyRefreshPolicy\": \"false\",\n+    \"timeout\": \"05:00:00\",\n+}\n CONFIG = {\n     \"task_id\": TASK_ID,\n     \"conn_id\": DEFAULT_CONNECTION_CLIENT_SECRET,\n@@ -46,6 +53,7 @@\n     \"dataset_id\": DATASET_ID,\n     \"check_interval\": 1,\n     \"timeout\": 3,\n+    \"request_body\": REQUEST_BODY,\n }\n NEW_REFRESH_REQUEST_ID = \"5e2d9921-e91b-491f-b7e1-e7d8db49194c\"\n \ndiff --git a/providers/microsoft/azure/tests/unit/microsoft/azure/triggers/test_powerbi.py b/providers/microsoft/azure/tests/unit/microsoft/azure/triggers/test_powerbi.py\nindex 950d707498e61..658b6e3046f3b 100644\n--- a/providers/microsoft/azure/tests/unit/microsoft/azure/triggers/test_powerbi.py\n+++ b/providers/microsoft/azure/tests/unit/microsoft/azure/triggers/test_powerbi.py\n@@ -44,6 +44,13 @@\n TIMEOUT = 5\n MODULE = \"airflow.providers.microsoft.azure\"\n CHECK_INTERVAL = 1\n+REQUEST_BODY = {\n+    \"type\": \"full\",\n+    \"commitMode\": \"transactional\",\n+    \"objects\": [{\"table\": \"Customer\", \"partition\": \"Robert\"}],\n+    \"applyRefreshPolicy\": \"false\",\n+    \"timeout\": \"05:00:00\",\n+}\n API_VERSION = \"v1.0\"\n \n \n@@ -102,6 +109,7 @@ def test_powerbi_trigger_serialization(self, connection):\n             check_interval=CHECK_INTERVAL,\n             wait_for_termination=True,\n             timeout=TIMEOUT,\n+            request_body=REQUEST_BODY,\n         )\n \n         classpath, kwargs = powerbi_trigger.serialize()\n@@ -116,6 +124,7 @@ def test_powerbi_trigger_serialization(self, connection):\n             \"api_version\": API_VERSION,\n             \"check_interval\": CHECK_INTERVAL,\n             \"wait_for_termination\": True,\n+            \"request_body\": REQUEST_BODY,\n         }\n \n     @pytest.mark.asyncio\n","problem_statement":"Add \"objects\" parameters to PowerBiDatasetRefreshOperator\n### Description\n\nWould be great to be able to refresh only specific tables/partitions with the powerbi dataset refresh operator\n\n    Args:\n        objects (list(dict), optional): the list of refresh objects based on provided tables and partitions.If not specified, refresh the entire dataset. Ex: [{\"table\": \"Customer\", \"partition\": \"Robert\"}]\n        refresh_type (str, optional):The type of refresh operation to perform.Common values are \"full\" for a complete refresh or \"automatic\" for an incremental refresh based on data source settings. Defaults to \"full\".\n        commit_mode (str, optional): Defines how the refresh operation commits changes.Acceptable values include \"transactional\" and \"partialBatch\".Defaults to \"transactional\".\n\n\nThen, pass the following args to the request:\n\npbi_args = {\n                \"type\": refresh_type,\n                \"commitMode\": commit_mode,\n                \"objects\": objects,\n                \"applyRefreshPolicy\": \"false\"\n            }\n\n### Use case/motivation\n\n_No response_\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n","hints_text":"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\nI am also interested in this functionality, and might do an implementation for this in the future.\n\nTo help myself in the future:\n\nSome places that most likely require slight changes to enable this functionality when I take a glance at the code:\n\nhttps://github.com/apache/airflow/blob/5248f0e81dc21267fbefe42da4fb511a6fdfa032/providers/microsoft/azure/src/airflow/providers/microsoft/azure/operators/powerbi.py#L61 And\nhttps://github.com/apache/airflow/blob/5248f0e81dc21267fbefe42da4fb511a6fdfa032/providers/microsoft/azure/src/airflow/providers/microsoft/azure/triggers/powerbi.py#L38 And\nhttps://github.com/apache/airflow/blob/5248f0e81dc21267fbefe42da4fb511a6fdfa032/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/powerbi.py#L192 Add `request_args` as optional argument, allowing to set any of the [DatasetRefreshRequest](https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/refresh-dataset-in-group#request-body) arguments that are passed in the request body. (Possibly Pydantic class, pure json e.g. Python dict is also an option, but less resilient and user-friendly).\n\nThen pass data as argument, since `self.run` in the context of `PowerBIHook` is inherited from `KiotaRequestAdapterHook`:\n\nhttps://github.com/apache/airflow/blob/5248f0e81dc21267fbefe42da4fb511a6fdfa032/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/msgraph.py#L398\nAlso, currently the timeout is not passed correctly to Powerbi. The airflow task timeout and terminates, but not the Powerbi refresh itself.\n> Also, currently the timeout is not passed correctly to Powerbi. The airflow task timeout and terminates, but not the Powerbi refresh itself.\n\nCheck, good to know! Maybe we can tackle that as well within the same contribution.\njust need to pass the provided timeout to request_args={\"timeout\": self.timeout, \"objects\": {...}, ...}\n> just need to pass the provided timeout to request_args={\"timeout\": self.timeout, \"objects\": {...}, ...}\n\nYes, would like to actually support the [whole spec](https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/refresh-dataset-in-group#request-body) at once.\n\n","all_hints_text":"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\nI am also interested in this functionality, and might do an implementation for this in the future.\n\nTo help myself in the future:\n\nSome places that most likely require slight changes to enable this functionality when I take a glance at the code:\n\nhttps://github.com/apache/airflow/blob/5248f0e81dc21267fbefe42da4fb511a6fdfa032/providers/microsoft/azure/src/airflow/providers/microsoft/azure/operators/powerbi.py#L61 And\nhttps://github.com/apache/airflow/blob/5248f0e81dc21267fbefe42da4fb511a6fdfa032/providers/microsoft/azure/src/airflow/providers/microsoft/azure/triggers/powerbi.py#L38 And\nhttps://github.com/apache/airflow/blob/5248f0e81dc21267fbefe42da4fb511a6fdfa032/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/powerbi.py#L192 Add `request_args` as optional argument, allowing to set any of the [DatasetRefreshRequest](https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/refresh-dataset-in-group#request-body) arguments that are passed in the request body. (Possibly Pydantic class, pure json e.g. Python dict is also an option, but less resilient and user-friendly).\n\nThen pass data as argument, since `self.run` in the context of `PowerBIHook` is inherited from `KiotaRequestAdapterHook`:\n\nhttps://github.com/apache/airflow/blob/5248f0e81dc21267fbefe42da4fb511a6fdfa032/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/msgraph.py#L398\nAlso, currently the timeout is not passed correctly to Powerbi. The airflow task timeout and terminates, but not the Powerbi refresh itself.\n> Also, currently the timeout is not passed correctly to Powerbi. The airflow task timeout and terminates, but not the Powerbi refresh itself.\n\nCheck, good to know! Maybe we can tackle that as well within the same contribution.\njust need to pass the provided timeout to request_args={\"timeout\": self.timeout, \"objects\": {...}, ...}\n> just need to pass the provided timeout to request_args={\"timeout\": self.timeout, \"objects\": {...}, ...}\n\nYes, would like to actually support the [whole spec](https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/refresh-dataset-in-group#request-body) at once.\nWell done thank you\n\n","commit_urls":["https://github.com/apache/airflow/commit/665eb32e194b8546bcecebc1571c28a4849bac41","https://github.com/apache/airflow/commit/5bc345948abeed5debd52899af2dbc03409d75f0","https://github.com/apache/airflow/commit/6e97777ea6906a2dc45649b06e85681303835fee","https://github.com/apache/airflow/commit/00314fd5679f1754505479af49395135095d3ddb","https://github.com/apache/airflow/commit/e2f14ba5b71fb26b3ef1175e43ee158932b7c152","https://github.com/apache/airflow/commit/967ccc5046a80b0c2fc337d0646f7236557d3419","https://github.com/apache/airflow/commit/ee9664dce966de803e72a2d0c53cbba7738c1789","https://github.com/apache/airflow/commit/82bfc6d7d3e075baa2d6dfd361e4717329fb4347"],"created_at":"2025-06-04T11:21:29Z","classification":"Efficiency"}
{"repo":"apache/airflow","pull_number":50773,"instance_id":"apache__airflow-50773","issue_numbers":[50654],"base_commit":"b1bbc82c80d28bd0d8066cec81e13640a08a04f4","patch":"diff --git a/airflow-core/src/airflow/api_fastapi/execution_api/datamodels/taskinstance.py b/airflow-core/src/airflow/api_fastapi/execution_api/datamodels/taskinstance.py\nindex b83d731a54e35..c43c931f3e28a 100644\n--- a/airflow-core/src/airflow/api_fastapi/execution_api/datamodels/taskinstance.py\n+++ b/airflow-core/src/airflow/api_fastapi/execution_api/datamodels/taskinstance.py\n@@ -345,3 +345,9 @@ class TaskStatesResponse(BaseModel):\n     \"\"\"Response for task states with run_id, task and state.\"\"\"\n \n     task_states: dict[str, Any]\n+\n+\n+class InactiveAssetsResponse(BaseModel):\n+    \"\"\"Response for inactive assets.\"\"\"\n+\n+    inactive_assets: Annotated[list[AssetProfile], Field(default_factory=list)]\ndiff --git a/airflow-core/src/airflow/api_fastapi/execution_api/routes/task_instances.py b/airflow-core/src/airflow/api_fastapi/execution_api/routes/task_instances.py\nindex ac1d1602460de..15cdb0a40cad6 100644\n--- a/airflow-core/src/airflow/api_fastapi/execution_api/routes/task_instances.py\n+++ b/airflow-core/src/airflow/api_fastapi/execution_api/routes/task_instances.py\n@@ -17,12 +17,15 @@\n \n from __future__ import annotations\n \n+import contextlib\n+import itertools\n import json\n from collections import defaultdict\n from collections.abc import Iterator\n from typing import TYPE_CHECKING, Annotated, Any\n from uuid import UUID\n \n+import attrs\n import structlog\n from cadwyn import VersionedAPIRouter\n from fastapi import Body, HTTPException, Query, status\n@@ -37,6 +40,7 @@\n from airflow.api_fastapi.common.db.common import SessionDep\n from airflow.api_fastapi.common.types import UtcDateTime\n from airflow.api_fastapi.execution_api.datamodels.taskinstance import (\n+    InactiveAssetsResponse,\n     PrevSuccessfulDagRunResponse,\n     TaskStatesResponse,\n     TIDeferredStatePayload,\n@@ -51,6 +55,8 @@\n     TITerminalStatePayload,\n )\n from airflow.api_fastapi.execution_api.deps import JWTBearerTIPathDep\n+from airflow.exceptions import AirflowInactiveAssetInInletOrOutletException, TaskNotFound\n+from airflow.models.asset import AssetActive\n from airflow.models.dagbag import DagBag\n from airflow.models.dagrun import DagRun as DR\n from airflow.models.taskinstance import TaskInstance as TI, _stop_remaining_tasks\n@@ -58,6 +64,7 @@\n from airflow.models.trigger import Trigger\n from airflow.models.xcom import XComModel\n from airflow.sdk.definitions._internal.expandinput import NotFullyPopulated\n+from airflow.sdk.definitions.asset import Asset, AssetUniqueKey\n from airflow.sdk.definitions.taskgroup import MappedTaskGroup\n from airflow.utils import timezone\n from airflow.utils.state import DagRunState, TaskInstanceState, TerminalTIState\n@@ -400,12 +407,16 @@ def ti_update_state(\n         query = TI.duration_expression_update(ti_patch_payload.end_date, query, session.bind)\n         updated_state = ti_patch_payload.state\n         task_instance = session.get(TI, ti_id_str)\n-        TI.register_asset_changes_in_db(\n-            task_instance,\n-            ti_patch_payload.task_outlets,  # type: ignore\n-            ti_patch_payload.outlet_events,\n-            session,\n-        )\n+        try:\n+            TI.register_asset_changes_in_db(\n+                task_instance,\n+                ti_patch_payload.task_outlets,  # type: ignore\n+                ti_patch_payload.outlet_events,\n+                session,\n+            )\n+        except AirflowInactiveAssetInInletOrOutletException as err:\n+            log.error(\"Asset registration failed due to conflicting asset: %s\", err)\n+\n         query = query.values(state=updated_state)\n     elif isinstance(ti_patch_payload, TIDeferredStatePayload):\n         # Calculate timeout if it was passed\n@@ -840,5 +851,67 @@ def _get_group_tasks(dag_id: str, task_group_id: str, session: SessionDep, logic\n     return group_tasks\n \n \n+@ti_id_router.get(\n+    \"/{task_instance_id}/validate-inlets-and-outlets\",\n+    status_code=status.HTTP_200_OK,\n+    responses={\n+        status.HTTP_404_NOT_FOUND: {\"description\": \"Task Instance not found\"},\n+    },\n+)\n+def validate_inlets_and_outlets(\n+    task_instance_id: UUID,\n+    session: SessionDep,\n+    dag_bag: DagBagDep,\n+) -> InactiveAssetsResponse:\n+    \"\"\"Validate whether there're inactive assets in inlets and outlets of a given task instance.\"\"\"\n+    ti_id_str = str(task_instance_id)\n+    bind_contextvars(ti_id=ti_id_str)\n+\n+    ti = session.scalar(select(TI).where(TI.id == ti_id_str))\n+    if not ti or not ti.logical_date:\n+        log.error(\"Task Instance not found\")\n+        raise HTTPException(\n+            status_code=status.HTTP_404_NOT_FOUND,\n+            detail={\n+                \"reason\": \"not_found\",\n+                \"message\": \"Task Instance not found\",\n+            },\n+        )\n+\n+    if not ti.task:\n+        dag = dag_bag.get_dag(ti.dag_id)\n+        if dag:\n+            with contextlib.suppress(TaskNotFound):\n+                ti.task = dag.get_task(ti.task_id)\n+\n+    inlets = [asset.asprofile() for asset in ti.task.inlets if isinstance(asset, Asset)]\n+    outlets = [asset.asprofile() for asset in ti.task.outlets if isinstance(asset, Asset)]\n+    if not (inlets or outlets):\n+        return InactiveAssetsResponse(inactive_assets=[])\n+\n+    all_asset_unique_keys: set[AssetUniqueKey] = {\n+        AssetUniqueKey.from_asset(inlet_or_outlet)  # type: ignore\n+        for inlet_or_outlet in itertools.chain(inlets, outlets)\n+    }\n+    active_asset_unique_keys = {\n+        AssetUniqueKey(name, uri)\n+        for name, uri in session.execute(\n+            select(AssetActive.name, AssetActive.uri).where(\n+                tuple_(AssetActive.name, AssetActive.uri).in_(\n+                    attrs.astuple(key) for key in all_asset_unique_keys\n+                )\n+            )\n+        )\n+    }\n+    different = all_asset_unique_keys - active_asset_unique_keys\n+\n+    return InactiveAssetsResponse(\n+        inactive_assets=[\n+            asset_unique_key.to_asset().asprofile()  # type: ignore\n+            for asset_unique_key in different\n+        ]\n+    )\n+\n+\n # This line should be at the end of the file to ensure all routes are registered\n router.include_router(ti_id_router)\ndiff --git a/airflow-core/src/airflow/models/dagrun.py b/airflow-core/src/airflow/models/dagrun.py\nindex 5f17ff54709c7..cd5ec56a83993 100644\n--- a/airflow-core/src/airflow/models/dagrun.py\n+++ b/airflow-core/src/airflow/models/dagrun.py\n@@ -1894,6 +1894,7 @@ def schedule_tis(\n                 and not ti.task.on_execute_callback\n                 and not ti.task.on_success_callback\n                 and not ti.task.outlets\n+                and not ti.task.inlets\n             ):\n                 empty_ti_ids.append(ti.id)\n             # check \"start_trigger_args\" to see whether the operator supports start execution from triggerer\ndiff --git a/task-sdk/src/airflow/sdk/api/client.py b/task-sdk/src/airflow/sdk/api/client.py\nindex b9d0a4511ea10..8764c59976172 100644\n--- a/task-sdk/src/airflow/sdk/api/client.py\n+++ b/task-sdk/src/airflow/sdk/api/client.py\n@@ -40,6 +40,7 @@\n     ConnectionResponse,\n     DagRunStateResponse,\n     DagRunType,\n+    InactiveAssetsResponse,\n     PrevSuccessfulDagRunResponse,\n     TaskInstanceState,\n     TaskStatesResponse,\n@@ -273,6 +274,11 @@ def get_task_states(\n         resp = self.client.get(\"task-instances/states\", params=params)\n         return TaskStatesResponse.model_validate_json(resp.read())\n \n+    def validate_inlets_and_outlets(self, id: uuid.UUID) -> InactiveAssetsResponse:\n+        \"\"\"Validate whether there're inactive assets in inlets and outlets of a given task instance.\"\"\"\n+        resp = self.client.get(f\"task-instances/{id}/validate-inlets-and-outlets\")\n+        return InactiveAssetsResponse.model_validate_json(resp.read())\n+\n \n class ConnectionOperations:\n     __slots__ = (\"client\",)\ndiff --git a/task-sdk/src/airflow/sdk/api/datamodels/_generated.py b/task-sdk/src/airflow/sdk/api/datamodels/_generated.py\nindex f6b1c907ef529..ac1e51d5e55c9 100644\n--- a/task-sdk/src/airflow/sdk/api/datamodels/_generated.py\n+++ b/task-sdk/src/airflow/sdk/api/datamodels/_generated.py\n@@ -154,6 +154,14 @@ class DagRunType(str, Enum):\n     ASSET_TRIGGERED = \"asset_triggered\"\n \n \n+class InactiveAssetsResponse(BaseModel):\n+    \"\"\"\n+    Response for inactive assets.\n+    \"\"\"\n+\n+    inactive_assets: Annotated[list[AssetProfile] | None, Field(title=\"Inactive Assets\")] = None\n+\n+\n class IntermediateTIState(str, Enum):\n     \"\"\"\n     States that a Task Instance can be in that indicate it is not yet in a terminal or running state.\ndiff --git a/task-sdk/src/airflow/sdk/definitions/asset/__init__.py b/task-sdk/src/airflow/sdk/definitions/asset/__init__.py\nindex c81732cf40414..9cb913807ee91 100644\n--- a/task-sdk/src/airflow/sdk/definitions/asset/__init__.py\n+++ b/task-sdk/src/airflow/sdk/definitions/asset/__init__.py\n@@ -86,6 +86,18 @@ def from_str(key: str) -> AssetUniqueKey:\n     def to_str(self) -> str:\n         return json.dumps(attrs.asdict(self))\n \n+    @staticmethod\n+    def from_profile(profile: AssetProfile) -> AssetUniqueKey:\n+        if profile.name and profile.uri:\n+            return AssetUniqueKey(name=profile.name, uri=profile.uri)\n+\n+        if name := profile.name:\n+            return AssetUniqueKey(name=name, uri=name)\n+        if uri := profile.uri:\n+            return AssetUniqueKey(name=uri, uri=uri)\n+\n+        raise ValueError(\"name and uri cannot both be empty\")\n+\n \n @attrs.define(frozen=True)\n class AssetAliasUniqueKey:\ndiff --git a/task-sdk/src/airflow/sdk/execution_time/comms.py b/task-sdk/src/airflow/sdk/execution_time/comms.py\nindex ecc34852252e0..d0622cf6ffc86 100644\n--- a/task-sdk/src/airflow/sdk/execution_time/comms.py\n+++ b/task-sdk/src/airflow/sdk/execution_time/comms.py\n@@ -61,6 +61,7 @@\n     BundleInfo,\n     ConnectionResponse,\n     DagRunStateResponse,\n+    InactiveAssetsResponse,\n     PrevSuccessfulDagRunResponse,\n     TaskInstance,\n     TaskInstanceState,\n@@ -208,6 +209,24 @@ def source_task_instance(self) -> AssetEventSourceTaskInstance | None:\n         )\n \n \n+class InactiveAssetsResult(InactiveAssetsResponse):\n+    \"\"\"Response of InactiveAssets requests.\"\"\"\n+\n+    type: Literal[\"InactiveAssetsResult\"] = \"InactiveAssetsResult\"\n+\n+    @classmethod\n+    def from_inactive_assets_response(\n+        cls, inactive_assets_response: InactiveAssetsResponse\n+    ) -> InactiveAssetsResult:\n+        \"\"\"\n+        Get InactiveAssetsResponse from InactiveAssetsResult.\n+\n+        InactiveAssetsResponse is autogenerated from the API schema, so we need to convert it to InactiveAssetsResult\n+        for communication between the Supervisor and the task process.\n+        \"\"\"\n+        return cls(**inactive_assets_response.model_dump(exclude_defaults=True), type=\"InactiveAssetsResult\")\n+\n+\n class XComResult(XComResponse):\n     \"\"\"Response to ReadXCom request.\"\"\"\n \n@@ -376,6 +395,7 @@ class OKResponse(BaseModel):\n         XComResult,\n         XComSequenceIndexResult,\n         XComSequenceSliceResult,\n+        InactiveAssetsResult,\n         OKResponse,\n     ],\n     Field(discriminator=\"type\"),\n@@ -590,6 +610,11 @@ class GetAssetEventByAssetAlias(BaseModel):\n     type: Literal[\"GetAssetEventByAssetAlias\"] = \"GetAssetEventByAssetAlias\"\n \n \n+class ValidateInletsAndOutlets(BaseModel):\n+    ti_id: UUID\n+    type: Literal[\"ValidateInletsAndOutlets\"] = \"ValidateInletsAndOutlets\"\n+\n+\n class GetPrevSuccessfulDagRun(BaseModel):\n     ti_id: UUID\n     type: Literal[\"GetPrevSuccessfulDagRun\"] = \"GetPrevSuccessfulDagRun\"\n@@ -657,6 +682,7 @@ class GetDRCount(BaseModel):\n         SetXCom,\n         SkipDownstreamTasks,\n         SucceedTask,\n+        ValidateInletsAndOutlets,\n         TaskState,\n         TriggerDagRun,\n         DeleteVariable,\ndiff --git a/task-sdk/src/airflow/sdk/execution_time/supervisor.py b/task-sdk/src/airflow/sdk/execution_time/supervisor.py\nindex 65d05cc023d51..7e6b89043d221 100644\n--- a/task-sdk/src/airflow/sdk/execution_time/supervisor.py\n+++ b/task-sdk/src/airflow/sdk/execution_time/supervisor.py\n@@ -88,6 +88,7 @@\n     GetXComCount,\n     GetXComSequenceItem,\n     GetXComSequenceSlice,\n+    InactiveAssetsResult,\n     PrevSuccessfulDagRunResult,\n     PutVariable,\n     RescheduleTask,\n@@ -101,6 +102,7 @@\n     TaskStatesResult,\n     ToSupervisor,\n     TriggerDagRun,\n+    ValidateInletsAndOutlets,\n     VariableResult,\n     XComCountResponse,\n     XComResult,\n@@ -1215,6 +1217,10 @@ def _handle_request(self, msg: ToSupervisor, log: FilteringBoundLogger):\n             )\n         elif isinstance(msg, DeleteVariable):\n             resp = self.client.variables.delete(msg.key)\n+        elif isinstance(msg, ValidateInletsAndOutlets):\n+            inactive_assets_resp = self.client.task_instances.validate_inlets_and_outlets(msg.ti_id)\n+            resp = InactiveAssetsResult.from_inactive_assets_response(inactive_assets_resp)\n+            dump_opts = {\"exclude_unset\": True}\n         else:\n             log.error(\"Unhandled request\", msg=msg)\n             return\ndiff --git a/task-sdk/src/airflow/sdk/execution_time/task_runner.py b/task-sdk/src/airflow/sdk/execution_time/task_runner.py\nindex cfe354f784e07..bbc7394a87a0d 100644\n--- a/task-sdk/src/airflow/sdk/execution_time/task_runner.py\n+++ b/task-sdk/src/airflow/sdk/execution_time/task_runner.py\n@@ -41,6 +41,7 @@\n \n from airflow.dag_processing.bundles.base import BaseDagBundle, BundleVersionLock\n from airflow.dag_processing.bundles.manager import DagBundlesManager\n+from airflow.exceptions import AirflowInactiveAssetInInletOrOutletException\n from airflow.listeners.listener import get_listener_manager\n from airflow.sdk.api.datamodels._generated import (\n     AssetProfile,\n@@ -67,6 +68,7 @@\n     GetTaskRescheduleStartDate,\n     GetTaskStates,\n     GetTICount,\n+    InactiveAssetsResult,\n     RescheduleTask,\n     RetryTask,\n     SetRenderedFields,\n@@ -80,6 +82,7 @@\n     ToSupervisor,\n     ToTask,\n     TriggerDagRun,\n+    ValidateInletsAndOutlets,\n )\n from airflow.sdk.execution_time.context import (\n     ConnectionAccessor,\n@@ -784,6 +787,8 @@ def _prepare(ti: RuntimeTaskInstance, log: Logger, context: Context) -> ToSuperv\n         # so that we do not call the API unnecessarily\n         SUPERVISOR_COMMS.send_request(log=log, msg=SetRenderedFields(rendered_fields=rendered_fields))\n \n+    _validate_task_inlets_and_outlets(ti=ti, log=log)\n+\n     try:\n         # TODO: Call pre execute etc.\n         get_listener_manager().hook.on_task_instance_running(\n@@ -796,6 +801,22 @@ def _prepare(ti: RuntimeTaskInstance, log: Logger, context: Context) -> ToSuperv\n     return None\n \n \n+def _validate_task_inlets_and_outlets(*, ti: RuntimeTaskInstance, log: Logger) -> None:\n+    if not ti.task.inlets and not ti.task.outlets:\n+        return\n+\n+    SUPERVISOR_COMMS.send_request(msg=ValidateInletsAndOutlets(ti_id=ti.id), log=log)\n+    inactive_assets_resp = SUPERVISOR_COMMS.get_message()\n+    if TYPE_CHECKING:\n+        assert isinstance(inactive_assets_resp, InactiveAssetsResult)\n+    if inactive_assets := inactive_assets_resp.inactive_assets:\n+        raise AirflowInactiveAssetInInletOrOutletException(\n+            inactive_asset_keys=[\n+                AssetUniqueKey.from_profile(asset_profile) for asset_profile in inactive_assets\n+            ]\n+        )\n+\n+\n def _defer_task(\n     defer: TaskDeferred, ti: RuntimeTaskInstance, log: Logger\n ) -> tuple[ToSupervisor, TaskInstanceState]:\n","test_patch":"diff --git a/airflow-core/tests/unit/api_fastapi/execution_api/versions/head/test_task_instances.py b/airflow-core/tests/unit/api_fastapi/execution_api/versions/head/test_task_instances.py\nindex 21c733a5cced2..49a8717c36aa2 100644\n--- a/airflow-core/tests/unit/api_fastapi/execution_api/versions/head/test_task_instances.py\n+++ b/airflow-core/tests/unit/api_fastapi/execution_api/versions/head/test_task_instances.py\n@@ -33,7 +33,7 @@\n from airflow.models.taskinstance import TaskInstance\n from airflow.models.taskinstancehistory import TaskInstanceHistory\n from airflow.providers.standard.operators.empty import EmptyOperator\n-from airflow.sdk import TaskGroup, task, task_group\n+from airflow.sdk import Asset, TaskGroup, task, task_group\n from airflow.utils import timezone\n from airflow.utils.state import State, TaskInstanceState, TerminalTIState\n \n@@ -2139,3 +2139,54 @@ def add_one(x):\n         response = client.get(\"/execution/task-instances/states\", params={\"dag_id\": dr.dag_id, **params})\n         assert response.status_code == 200\n         assert response.json() == {\"task_states\": {dr.run_id: expected}}\n+\n+\n+class TestInvactiveInletsAndOutlets:\n+    def test_ti_inactive_inlets_and_outlets(self, client, dag_maker):\n+        \"\"\"Test the inactive assets in inlets and outlets can be found.\"\"\"\n+        with dag_maker(\"test_inlets_and_outlets\"):\n+            EmptyOperator(\n+                task_id=\"task1\",\n+                inlets=[Asset(name=\"inlet-name\"), Asset(name=\"inlet-name\", uri=\"but-different-uri\")],\n+                outlets=[\n+                    Asset(name=\"outlet-name\", uri=\"uri\"),\n+                    Asset(name=\"outlet-name\", uri=\"second-different-uri\"),\n+                ],\n+            )\n+\n+        dr = dag_maker.create_dagrun()\n+\n+        task1_ti = dr.get_task_instance(\"task1\")\n+        response = client.get(f\"/execution/task-instances/{task1_ti.id}/validate-inlets-and-outlets\")\n+        assert response.status_code == 200\n+        inactive_assets = response.json()[\"inactive_assets\"]\n+        expected_inactive_assets = (\n+            {\n+                \"name\": \"inlet-name\",\n+                \"type\": \"Asset\",\n+                \"uri\": \"but-different-uri\",\n+            },\n+            {\n+                \"name\": \"outlet-name\",\n+                \"type\": \"Asset\",\n+                \"uri\": \"second-different-uri\",\n+            },\n+        )\n+        for asset in expected_inactive_assets:\n+            assert asset in inactive_assets\n+\n+    def test_ti_inactive_inlets_and_outlets_without_inactive_assets(self, client, dag_maker):\n+        \"\"\"Test the task without inactive assets in its inlets or outlets returns empty list.\"\"\"\n+        with dag_maker(\"test_inlets_and_outlets_inactive\"):\n+            EmptyOperator(\n+                task_id=\"inactive_task1\",\n+                inlets=[Asset(name=\"inlet-name\")],\n+                outlets=[Asset(name=\"outlet-name\", uri=\"uri\")],\n+            )\n+\n+        dr = dag_maker.create_dagrun()\n+\n+        task1_ti = dr.get_task_instance(\"inactive_task1\")\n+        response = client.get(f\"/execution/task-instances/{task1_ti.id}/validate-inlets-and-outlets\")\n+        assert response.status_code == 200\n+        assert response.json() == {\"inactive_assets\": []}\ndiff --git a/task-sdk/tests/task_sdk/definitions/test_asset.py b/task-sdk/tests/task_sdk/definitions/test_asset.py\nindex 8328b061811ce..2a25c0907c7dc 100644\n--- a/task-sdk/tests/task_sdk/definitions/test_asset.py\n+++ b/task-sdk/tests/task_sdk/definitions/test_asset.py\n@@ -17,6 +17,7 @@\n \n from __future__ import annotations\n \n+import json\n import os\n from typing import Callable\n from unittest import mock\n@@ -24,6 +25,7 @@\n import pytest\n \n from airflow.providers.standard.operators.empty import EmptyOperator\n+from airflow.sdk.api.datamodels._generated import AssetProfile\n from airflow.sdk.definitions.asset import (\n     Asset,\n     AssetAlias,\n@@ -384,6 +386,39 @@ def test_normalize_uri_valid_uri(mock_get_normalized_scheme):\n     assert asset.normalized_uri == \"valid_aip60_uri\"\n \n \n+class TestAssetUniqueKey:\n+    def test_from_asset(self):\n+        asset = Asset(name=\"test\", uri=\"test://test/\")\n+\n+        assert AssetUniqueKey.from_asset(asset) == AssetUniqueKey(name=\"test\", uri=\"test://test/\")\n+\n+    def test_to_asset(self):\n+        assert AssetUniqueKey(name=\"test\", uri=\"test://test/\").to_asset() == Asset(\n+            name=\"test\", uri=\"test://test/\"\n+        )\n+\n+    def test_from_str(self):\n+        json_str = json.dumps({\"name\": \"test\", \"uri\": \"test://test/\"})\n+        assert AssetUniqueKey.from_str(json_str) == AssetUniqueKey(name=\"test\", uri=\"test://test/\")\n+\n+    def test_to_str(self):\n+        assert AssetUniqueKey(name=\"test\", uri=\"test://test/\").to_str() == json.dumps(\n+            {\"name\": \"test\", \"uri\": \"test://test/\"}\n+        )\n+\n+    @pytest.mark.parametrize(\n+        \"name, uri, expected_asset_unique_key\",\n+        [\n+            (\"test\", None, AssetUniqueKey(name=\"test\", uri=\"test\")),\n+            (None, \"test://test/\", AssetUniqueKey(name=\"test://test/\", uri=\"test://test/\")),\n+            (\"test\", \"test://test/\", AssetUniqueKey(name=\"test\", uri=\"test://test/\")),\n+        ],\n+    )\n+    def test_from_profile(self, name, uri, expected_asset_unique_key):\n+        profile = AssetProfile(name=name, uri=uri, type=\"Asset\")\n+        assert AssetUniqueKey.from_profile(profile) == expected_asset_unique_key\n+\n+\n class TestAssetAlias:\n     def test_as_expression(self):\n         alias = AssetAlias(name=\"test_name\", group=\"test\")\ndiff --git a/task-sdk/tests/task_sdk/execution_time/test_supervisor.py b/task-sdk/tests/task_sdk/execution_time/test_supervisor.py\nindex 86a2e747e0f0f..4f5e4cfc7ac9d 100644\n--- a/task-sdk/tests/task_sdk/execution_time/test_supervisor.py\n+++ b/task-sdk/tests/task_sdk/execution_time/test_supervisor.py\n@@ -45,6 +45,7 @@\n from airflow.sdk.api.client import ServerResponseError\n from airflow.sdk.api.datamodels._generated import (\n     AssetEventResponse,\n+    AssetProfile,\n     AssetResponse,\n     DagRunState,\n     TaskInstance,\n@@ -77,6 +78,7 @@\n     GetXCom,\n     GetXComSequenceItem,\n     GetXComSequenceSlice,\n+    InactiveAssetsResult,\n     OKResponse,\n     PrevSuccessfulDagRunResult,\n     PutVariable,\n@@ -90,6 +92,7 @@\n     TaskStatesResult,\n     TICount,\n     TriggerDagRun,\n+    ValidateInletsAndOutlets,\n     VariableResult,\n     XComResult,\n     XComSequenceIndexResult,\n@@ -1480,6 +1483,18 @@ def watched_subprocess(self, mocker):\n                 None,\n                 id=\"get_asset_events_by_asset_alias\",\n             ),\n+            pytest.param(\n+                ValidateInletsAndOutlets(ti_id=TI_ID),\n+                b'{\"inactive_assets\":[{\"name\":\"asset_name\",\"uri\":\"asset_uri\",\"type\":\"asset\"}],\"type\":\"InactiveAssetsResult\"}\\n',\n+                \"task_instances.validate_inlets_and_outlets\",\n+                (TI_ID,),\n+                {},\n+                InactiveAssetsResult(\n+                    inactive_assets=[AssetProfile(name=\"asset_name\", uri=\"asset_uri\", type=\"asset\")]\n+                ),\n+                None,\n+                id=\"validate_inlets_and_outlets\",\n+            ),\n             pytest.param(\n                 SucceedTask(\n                     end_date=timezone.parse(\"2024-10-31T12:00:00Z\"), rendered_map_index=\"test success task\"\ndiff --git a/task-sdk/tests/task_sdk/execution_time/test_task_runner.py b/task-sdk/tests/task_sdk/execution_time/test_task_runner.py\nindex df4be21960fbf..a994e995bace1 100644\n--- a/task-sdk/tests/task_sdk/execution_time/test_task_runner.py\n+++ b/task-sdk/tests/task_sdk/execution_time/test_task_runner.py\n@@ -946,7 +946,12 @@ def test_run_with_asset_outlets(\n     instant = timezone.datetime(2024, 12, 3, 10, 0)\n     time_machine.move_to(instant, tick=False)\n \n-    run(ti, context=ti.get_template_context(), log=mock.MagicMock())\n+    with mock.patch(\n+        \"airflow.sdk.execution_time.task_runner._validate_task_inlets_and_outlets\"\n+    ) as validate_mock:\n+        run(ti, context=ti.get_template_context(), log=mock.MagicMock())\n+\n+    validate_mock.assert_called_once()\n \n     mock_supervisor_comms.send_request.assert_any_call(msg=expected_msg, log=mock.ANY)\n \n","problem_statement":"Task is taking more than 5 minutes to fail\n### Apache Airflow version\n\nmain (development)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWhen there are 2 assets with same name and there is task which has these 2 assets as outlet then task is taking more than 5 minutes to fail.\n\n<img width=\"1637\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b9b22c99-e45a-41d4-b73c-14b2f63b65d6\" />\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nRun below dag on main latest and notice 'task1' is taking more than 5 minutes to fail\n\n```python\nfrom airflow.providers.standard.operators.empty import EmptyOperator\nfrom airflow.providers.standard.operators.python import PythonOperator\nfrom airflow.sdk import Asset\nfrom airflow.sdk import DAG\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom pendulum import today\n\nfrom dags.plugins.airflow_dag_introspection import assert_the_task_states\n\nleft = Asset(name='asset_name')\nright = Asset(name='asset_name', uri=\"asset_uri\")\n\nwith DAG(\n    dag_id=\"asset_with_duplicate_name\",\n    start_date=today('UTC').add(days=-2),\n    schedule=None,\n    max_active_runs=1,\n    tags=[\"asset\", \"AIP-74\"],\n) as dag:\n    task1 = EmptyOperator(task_id='task1', outlets=[left, right])\n    status = PythonOperator(\n        task_id=\"assert_task_status\",\n        python_callable=assert_the_task_states,\n        op_args=[{\"task1\": \"failed\"}],\n        trigger_rule=TriggerRule.ALL_DONE,\n    )\n    task1 >> status\n\n``` \n\n### Operating System\n\nLinux\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOther\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n","hints_text":"@Lee-W this is happening with a Dag which is trying to use 2 assets with the same name.\nI am expecting task to fail fast.\nIt fails immediately on my end. Is there another setup I can try?\nanother potential bug found, let's see whether that fixes this issue\nDue to the latest task sdk change, this issue occurs in all tasks instead of just the empty operator. The checking logic is totally removed somehow. New root cause found. A total rework needed\n\n","all_hints_text":"@Lee-W this is happening with a Dag which is trying to use 2 assets with the same name.\nI am expecting task to fail fast.\nIt fails immediately on my end. Is there another setup I can try?\nanother potential bug found, let's see whether that fixes this issue\nDue to the latest task sdk change, this issue occurs in all tasks instead of just the empty operator. The checking logic is totally removed somehow. New root cause found. A total rework needed\nWe should fix this in TaskRunner by handling the exceptions  https://github.com/apache/airflow/blob/main/task-sdk/src/airflow/sdk/execution_time/task_runner.py#L939\nAfter a few more tests, I discovered that my statement was not accurate. The API server as a whole doesn't die. Only the API endpoint itself dies without being correctly handled. It's still the root cause of why the task is hanging. But what we need to do is not handle \n\nhttps://github.com/apache/airflow/blob/807ba69ad3c8e2e5552b2fa840a1f9b47bfd8318/task-sdk/src/airflow/sdk/execution_time/task_runner.py#L939\n\nbut https://github.com/apache/airflow/blob/22d7ba8f6764dab5d669d7de618ff6fcbc55c761/airflow-core/src/airflow/api_fastapi/execution_api/routes/task_instances.py#L318.\n\n\nThe task fails to update it state correctly and hangs there till it fails\n\n","commit_urls":["https://github.com/apache/airflow/commit/f3b9eb883d5b93a0d239fbcba5de1713e7f7a7d1","https://github.com/apache/airflow/commit/3d199ccd0d8d54f2c8cb2a2a6e6ff0173607a7ba","https://github.com/apache/airflow/commit/b02c9f5c1c3f4e35d3c73df2302b2bef37e0487d","https://github.com/apache/airflow/commit/faf0bb1bbc79f3fb69634fd7fdad8f72ec353eb6","https://github.com/apache/airflow/commit/896f4cdf911093bf222de1a168189d09401ec0de","https://github.com/apache/airflow/commit/27ec77e9200c853953310b985cb87c3cc39963f3","https://github.com/apache/airflow/commit/96eeded60e5ce159c10bb216267cf69c5ee110e6","https://github.com/apache/airflow/commit/6efcf615a8558b15121320aee640af6a11349528","https://github.com/apache/airflow/commit/aa4b1db1ddb014c8121fc458561a92a2b431c25c","https://github.com/apache/airflow/commit/b242ede242ee0bed6a587fd286d2af2fc1339e6d","https://github.com/apache/airflow/commit/869ce7fc9bbcfa134c02b03f33232385321e5aa1","https://github.com/apache/airflow/commit/05b2bb8e0945be965b05f462fd3153930a9a3898"],"created_at":"2025-05-19T14:48:30Z","classification":"Efficiency"}
{"repo":"apache/airflow","pull_number":51735,"instance_id":"apache__airflow-51735","issue_numbers":[51123],"base_commit":"275a81a30a11d4484cdf227cd582fe467d195ca0","patch":"diff --git a/.github/CODEOWNERS b/.github/CODEOWNERS\nindex bb5815dac0b84..69f10c58301a7 100644\n--- a/.github/CODEOWNERS\n+++ b/.github/CODEOWNERS\n@@ -37,14 +37,14 @@\n \n # Translation Owners (i18n)\n # Note: Non committer engaged translators are listed in comments prevent making file syntax invalid\n-# See: https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/ui/src/i18n/README.md#43-engaged-translator\n-airflow-core/src/airflow/ui/src/i18n/locales/ar/ @shahar1 @hussein-awala # + @ahmadtfarhan\n-airflow-core/src/airflow/ui/src/i18n/locales/de/ @jscheffl # + @TJaniF @m1racoli\n-airflow-core/src/airflow/ui/src/i18n/locales/he/ @eladkal @shahar1 @romsharon98 # +@Dev-iL\n-airflow-core/src/airflow/ui/src/i18n/locales/nl/ @BasPH # + @DjVinnii\n-airflow-core/src/airflow/ui/src/i18n/locales/pl/ @potiuk @mobuchowski # + @kacpermuda\n-airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/ @Lee-W @jason810496 # + @RoyLee1224 @guan404ming\n-airflow-core/src/airflow/ui/src/i18n/locales/fr/ @pierrejeambrun @vincbeck\n+# See: https://github.com/apache/airflow/blob/main/airflow-core/src/airflow/ui/public/i18n/README.md#43-engaged-translator\n+airflow-core/src/airflow/ui/public/i18n/locales/ar/ @shahar1 @hussein-awala # + @ahmadtfarhan\n+airflow-core/src/airflow/ui/public/i18n/locales/de/ @jscheffl # + @TJaniF @m1racoli\n+airflow-core/src/airflow/ui/public/i18n/locales/he/ @eladkal @shahar1 @romsharon98 # +@Dev-iL\n+airflow-core/src/airflow/ui/public/i18n/locales/nl/ @BasPH # + @DjVinnii\n+airflow-core/src/airflow/ui/public/i18n/locales/pl/ @potiuk @mobuchowski # + @kacpermuda\n+airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/ @Lee-W @jason810496 # + @RoyLee1224 @guan404ming\n+airflow-core/src/airflow/ui/public/i18n/locales/fr/ @pierrejeambrun @vincbeck\n \n # Security/Permissions\n /airflow-core/src/airflow/security/permissions.py @vincbeck\ndiff --git a/.github/boring-cyborg.yml b/.github/boring-cyborg.yml\nindex 02954efbed283..484cfb333d047 100644\n--- a/.github/boring-cyborg.yml\n+++ b/.github/boring-cyborg.yml\n@@ -348,34 +348,34 @@ labelPRBasedOnFilePath:\n     - airflow-core/src/airflow/ui/**/*\n \n   area:translations:\n-    - airflow-core/src/airflow/ui/src/i18n/**/*\n+    - airflow-core/src/airflow/ui/public/i18n/**/*\n \n   translation:default:\n-    - airflow-core/src/airflow/ui/src/i18n/locales/en/*\n+    - airflow-core/src/airflow/ui/public/i18n/locales/en/*\n \n   translation:ar:\n-    - airflow-core/src/airflow/ui/src/i18n/locales/ar/*\n+    - airflow-core/src/airflow/ui/public/i18n/locales/ar/*\n \n   translation:de:\n-    - airflow-core/src/airflow/ui/src/i18n/locales/de/*\n+    - airflow-core/src/airflow/ui/public/i18n/locales/de/*\n \n   translation:fr:\n-    - airflow-core/src/airflow/ui/src/i18n/locales/fr/*\n+    - airflow-core/src/airflow/ui/public/i18n/locales/fr/*\n \n   translation:he:\n-    - airflow-core/src/airflow/ui/src/i18n/locales/he/*\n+    - airflow-core/src/airflow/ui/public/i18n/locales/he/*\n \n   translation:ko:\n-    - airflow-core/src/airflow/ui/src/i18n/locales/ko/*\n+    - airflow-core/src/airflow/ui/public/i18n/locales/ko/*\n \n   translation:nl:\n-    - airflow-core/src/airflow/ui/src/i18n/locales/nl/*\n+    - airflow-core/src/airflow/ui/public/i18n/locales/nl/*\n \n   translation:pl:\n-    - airflow-core/src/airflow/ui/src/i18n/locales/pl/*\n+    - airflow-core/src/airflow/ui/public/i18n/locales/pl/*\n \n   translation:zh-TW:\n-    - airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/*\n+    - airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/*\n \n   area:CLI:\n     - airflow-core/src/airflow/cli/**/*.py\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 36aa6dc0a5f06..b4a2d42fe8176 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -355,9 +355,7 @@ repos:\n           ^.*/kinglear\\.txt$|\n           ^.*pnpm-lock\\.yaml$|\n           .*/dist/.*|\n-          ^airflow-core/src/airflow/ui/src/i18n/locales/de/|\n-          ^airflow-core/src/airflow/ui/src/i18n/locales/fr/|\n-          ^airflow-core/src/airflow/ui/src/i18n/locales/pl/\n+          ^airflow-core/src/airflow/ui/public/i18n/locales/(?!en/).+/\n         args:\n           - --ignore-words=docs/spelling_wordlist.txt\n           - --skip=providers/.*/src/airflow/providers/*/*.rst,providers/*/docs/changelog.rst,docs/*/commits.rst,providers/*/docs/commits.rst,providers/*/*/docs/commits.rst,docs/apache-airflow/tutorial/pipeline_example.csv,*.min.js,*.lock,INTHEWILD.md,*.svg\n@@ -637,7 +635,7 @@ repos:\n           (?x)\n           ^airflow-core/src/airflow/ui/src/i18n/config\\.ts$|\n           ^airflow-core/src/airflow/ui/openapi-gen/|\n-          ^airflow-core/src/airflow/ui/src/i18n/locales/de/README\\.md$|\n+          ^airflow-core/src/airflow/ui/public/i18n/locales/de/README\\.md$|\n           ^airflow-core/src/airflow/cli/commands/local_commands/fastapi_api_command\\.py$|\n           ^airflow-core/src/airflow/config_templates/|\n           ^airflow-core/src/airflow/models/baseoperator\\.py$|\n@@ -1447,7 +1445,7 @@ repos:\n         name: Check i18n files validity\n         description: Check i18n files are valid json and have no TODOs\n         language: python\n-        files: ^airflow-core/src/airflow/ui/src/i18n/locales/.*\\.json\n+        files: ^airflow-core/src/airflow/ui/public/i18n/locales/.*\\.json\n         entry: ./scripts/ci/pre_commit/check_i18n_json.py\n         additional_dependencies: ['rich>=12.4.4']\n         pass_filenames: false\ndiff --git a/airflow-core/src/airflow/api_fastapi/core_api/app.py b/airflow-core/src/airflow/api_fastapi/core_api/app.py\nindex d378e009bc0c9..253ef206e13a9 100644\n--- a/airflow-core/src/airflow/api_fastapi/core_api/app.py\n+++ b/airflow-core/src/airflow/api_fastapi/core_api/app.py\n@@ -55,6 +55,13 @@ def init_views(app: FastAPI) -> None:\n \n     templates = Jinja2Templates(directory=directory)\n \n+    if dev_mode:\n+        app.mount(\n+            \"/static/i18n/locales\",\n+            StaticFiles(directory=Path(AIRFLOW_PATH) / \"airflow/ui/public/i18n/locales\"),\n+            name=\"dev_i18n_static\",\n+        )\n+\n     app.mount(\n         \"/static\",\n         StaticFiles(\ndiff --git a/airflow-core/src/airflow/ui/.prettierignore b/airflow-core/src/airflow/ui/.prettierignore\nindex f013813d71866..b90ad27ecef3d 100644\n--- a/airflow-core/src/airflow/ui/.prettierignore\n+++ b/airflow-core/src/airflow/ui/.prettierignore\n@@ -5,5 +5,5 @@ dist/\n *.yaml\n coverage/*\n .pnpm-store\n-src/i18n/locales/*\n+public/i18n/locales/*\n openapi-gen/\ndiff --git a/airflow-core/src/airflow/ui/package.json b/airflow-core/src/airflow/ui/package.json\nindex 7c6871eb433c8..2429adc35a0c7 100644\n--- a/airflow-core/src/airflow/ui/package.json\n+++ b/airflow-core/src/airflow/ui/package.json\n@@ -39,6 +39,7 @@\n     \"html-to-image\": \"^1.11.13\",\n     \"i18next\": \"^25.1.2\",\n     \"i18next-browser-languagedetector\": \"^8.1.0\",\n+    \"i18next-http-backend\": \"^3.0.2\",\n     \"next-themes\": \"^0.3.0\",\n     \"react\": \"^18.3.1\",\n     \"react-chartjs-2\": \"^5.3.0\",\n@@ -85,6 +86,7 @@\n     \"eslint-plugin-unicorn\": \"^55.0.0\",\n     \"globals\": \"^15.15.0\",\n     \"happy-dom\": \"^17.4.6\",\n+    \"jsonc-eslint-parser\": \"^2.4.0\",\n     \"msw\": \"^2.7.5\",\n     \"openapi-merge-cli\": \"^1.3.2\",\n     \"prettier\": \"^3.5.3\",\ndiff --git a/airflow-core/src/airflow/ui/pnpm-lock.yaml b/airflow-core/src/airflow/ui/pnpm-lock.yaml\nindex c7b0c7e6eebc2..04426f880c846 100644\n--- a/airflow-core/src/airflow/ui/pnpm-lock.yaml\n+++ b/airflow-core/src/airflow/ui/pnpm-lock.yaml\n@@ -77,6 +77,9 @@ importers:\n       i18next-browser-languagedetector:\n         specifier: ^8.1.0\n         version: 8.1.0\n+      i18next-http-backend:\n+        specifier: ^3.0.2\n+        version: 3.0.2\n       next-themes:\n         specifier: ^0.3.0\n         version: 0.3.0(react-dom@18.3.1(react@18.3.1))(react@18.3.1)\n@@ -210,6 +213,9 @@ importers:\n       happy-dom:\n         specifier: ^17.4.6\n         version: 17.4.6\n+      jsonc-eslint-parser:\n+        specifier: ^2.4.0\n+        version: 2.4.0\n       msw:\n         specifier: ^2.7.5\n         version: 2.7.6(@types/node@22.15.14)(typescript@5.8.3)\n@@ -2072,6 +2078,9 @@ packages:\n   cross-fetch@3.2.0:\n     resolution: {integrity: sha512-Q+xVJLoGOeIMXZmbUK4HYk+69cQH6LudR0Vu/pRm2YlU/hDV9CiS0gKUMaWY5f2NeUH9C1nV3bsTlCo0FsTV1Q==}\n \n+  cross-fetch@4.0.0:\n+    resolution: {integrity: sha512-e4a5N8lVvuLgAWgnCrLr2PP0YyDOTHa9H/Rj54dirp61qXnNq46m82bRhNqIA5VccJtWBvPTFRV3TtvHUKPB1g==}\n+\n   cross-spawn@7.0.6:\n     resolution: {integrity: sha512-uV2QOWP2nWzsy2aMp8aRibhi9dlzF5Hgh5SHaB9OiTGEyDTiJJyx0uy51QXdyWbtAHNua4XJzUKca3OzKUd3vA==}\n     engines: {node: '>= 8'}\n@@ -2419,6 +2428,10 @@ packages:\n     resolution: {integrity: sha512-0QYC8b24HWY8zjRnDTL6RiHfDbAWn63qb4LMj1Z4b076A4une81+z03Kg7l7mn/48PUTqoLptSXez8oknU8Clg==}\n     engines: {node: ^18.18.0 || ^20.9.0 || >=21.1.0}\n \n+  espree@9.6.1:\n+    resolution: {integrity: sha512-oruZaFkjorTpF32kDSI5/75ViwGeZginGGy2NoOSg3Q9bnwlnmDm4HLnkl0RE3n+njDXR037aY1+x58Z/zFdwQ==}\n+    engines: {node: ^12.22.0 || ^14.17.0 || >=16.0.0}\n+\n   esprima@4.0.1:\n     resolution: {integrity: sha512-eGuFFw7Upda+g4p+QHvnW0RyTX/SVeJBDM/gCtMARO0cLuT2HcEKnTPvhjV6aGeqrCB/sbNop0Kszm0jsaWU4A==}\n     engines: {node: '>=4'}\n@@ -2742,6 +2755,9 @@ packages:\n   i18next-browser-languagedetector@8.1.0:\n     resolution: {integrity: sha512-mHZxNx1Lq09xt5kCauZ/4bsXOEA2pfpwSoU11/QTJB+pD94iONFwp+ohqi///PwiFvjFOxe1akYCdHyFo1ng5Q==}\n \n+  i18next-http-backend@3.0.2:\n+    resolution: {integrity: sha512-PdlvPnvIp4E1sYi46Ik4tBYh/v/NbYfFFgTjkwFl0is8A18s7/bx9aXqsrOax9WUbeNS6mD2oix7Z0yGGf6m5g==}\n+\n   i18next@25.1.2:\n     resolution: {integrity: sha512-SP63m8LzdjkrAjruH7SCI3ndPSgjt4/wX7ouUUOzCW/eY+HzlIo19IQSfYA9X3qRiRP1SYtaTsg/Oz/PGsfD8w==}\n     peerDependencies:\n@@ -3013,6 +3029,10 @@ packages:\n   json-stable-stringify-without-jsonify@1.0.1:\n     resolution: {integrity: sha512-Bdboy+l7tA3OGW6FjyFHWkP5LuByj1Tk33Ljyq0axyzdk9//JSi2u3fP1QSmd1KNwq6VOKYGlAu87CisVir6Pw==}\n \n+  jsonc-eslint-parser@2.4.0:\n+    resolution: {integrity: sha512-WYDyuc/uFcGp6YtM2H0uKmUwieOuzeE/5YocFJLnLfclZ4inf3mRn8ZVy1s7Hxji7Jxm6Ss8gqpexD/GlKoGgg==}\n+    engines: {node: ^12.22.0 || ^14.17.0 || >=16.0.0}\n+\n   jsonpointer@5.0.1:\n     resolution: {integrity: sha512-p/nXbhSEcu3pZRdkW1OfJhpsVtW1gd4Wa1fnQc9YLiTfAjn0312eMKimbdIQzuZl9aa9xUGaRlP9T/CJE/ditQ==}\n     engines: {node: '>=0.10.0'}\n@@ -7051,6 +7071,12 @@ snapshots:\n     transitivePeerDependencies:\n       - encoding\n \n+  cross-fetch@4.0.0:\n+    dependencies:\n+      node-fetch: 2.7.0\n+    transitivePeerDependencies:\n+      - encoding\n+\n   cross-spawn@7.0.6:\n     dependencies:\n       path-key: 3.1.1\n@@ -7532,6 +7558,12 @@ snapshots:\n       acorn-jsx: 5.3.2(acorn@8.14.1)\n       eslint-visitor-keys: 4.2.0\n \n+  espree@9.6.1:\n+    dependencies:\n+      acorn: 8.14.1\n+      acorn-jsx: 5.3.2(acorn@8.14.1)\n+      eslint-visitor-keys: 3.4.3\n+\n   esprima@4.0.1: {}\n \n   esquery@1.6.0:\n@@ -7919,6 +7951,12 @@ snapshots:\n     dependencies:\n       '@babel/runtime': 7.26.10\n \n+  i18next-http-backend@3.0.2:\n+    dependencies:\n+      cross-fetch: 4.0.0\n+    transitivePeerDependencies:\n+      - encoding\n+\n   i18next@25.1.2(typescript@5.8.3):\n     dependencies:\n       '@babel/runtime': 7.26.10\n@@ -8179,6 +8217,13 @@ snapshots:\n \n   json-stable-stringify-without-jsonify@1.0.1: {}\n \n+  jsonc-eslint-parser@2.4.0:\n+    dependencies:\n+      acorn: 8.14.1\n+      eslint-visitor-keys: 3.4.3\n+      espree: 9.6.1\n+      semver: 7.7.1\n+\n   jsonpointer@5.0.1: {}\n \n   jsx-ast-utils@3.3.5:\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/README.md b/airflow-core/src/airflow/ui/public/i18n/README.md\nsimilarity index 95%\nrename from airflow-core/src/airflow/ui/src/i18n/README.md\nrename to airflow-core/src/airflow/ui/public/i18n/README.md\nindex 190ba07fbfdf2..4fa50122cf7a4 100644\n--- a/airflow-core/src/airflow/ui/src/i18n/README.md\n+++ b/airflow-core/src/airflow/ui/public/i18n/README.md\n@@ -28,7 +28,7 @@ This policy aims to avoid inconsistencies, maintenance issues, unclear ownership\n \n This policy applies to:\n \n-- Each supported locale included in `airflow-core/src/airflow/ui/src/i18n/locales`.\n+- Each supported locale included in `airflow-core/src/airflow/ui/public/i18n/locales`.\n - Contributors making changes in the default locale (English).\n - Contributors suggesting new locales to be added to the codebase.\n - Maintainers of supported locales in any role defined below.\n@@ -42,7 +42,7 @@ This policy applies to:\n \n **Internationalization (i18n)** - The process of designing a software application so that it can be adapted to various languages and regions without engineering changes (see also the [Wikipedia article](https://en.wikipedia.org/wiki/Internationalization_and_localization)).\n \n-**Supported locale** - An officially accepted locale in `airflow-core/src/airflow/ui/src/i18n/locales`.\n+**Supported locale** - An officially accepted locale in `airflow-core/src/airflow/ui/public/i18n/locales`.\n \n **Default locale** - English (`en`), the primary locale and fallback for all other locales.\n \n@@ -175,7 +175,7 @@ Translation conflicts MUST be resolved according to the procedures outlined in s\n The following steps outline the process for approving a new locale to be added to the supported locales:\n \n - Creating a PR for adding the suggested locale to the codebase ([see example](https://github.com/apache/airflow/pull/51258/files)), which includes:\n-    - The locale files (translated according to the guidelines) in the `airflow-core/src/airflow/ui/src/i18n/locales/<LOCALE_CODE>` directory, where `<LOCALE_CODE>` is the code of the language according to ISO 639-1 standard (e.g., `fr` for French). Languages with regional variants should be handled in separate directories, where the name is suffixed with `-<VARIANT>`, and `<VARIANT>` is the variant that follows ISO 3166-1 or UN M.49 codes in lowercase (e.g., `zh-tw` for Taiwanese Mandarin).\n+    - The locale files (translated according to the guidelines) in the `airflow-core/src/airflow/ui/public/i18n/locales/<LOCALE_CODE>` directory, where `<LOCALE_CODE>` is the code of the language according to ISO 639-1 standard (e.g., `fr` for French). Languages with regional variants should be handled in separate directories, where the name is suffixed with `-<VARIANT>`, and `<VARIANT>` is the variant that follows ISO 3166-1 or UN M.49 codes in lowercase (e.g., `zh-tw` for Taiwanese Mandarin).\n     - Making the required modifications in `airflow-core/src/airflow/ui/src/i18n/config.ts` ([see example](https://github.com/apache/airflow/pull/51258/files#diff-bfb4d5fafd26d206fb4a545a41ba303f33d15a479d21e0a726fd743bdf9717ff)).\n     - Changes to the `.github/CODEOWNERS` file to include the designated code owner(s) and translation owner(s) for the new locale, considering the following:\n         - A code owner who is also a translation sponsor should be indicated in a comment as well.\n@@ -234,13 +234,13 @@ Language proficiency for translation owners can be demonstrated through any of t\n All files:\n \n ```bash\n-uv run ./check_translations_completeness.py\n+uv run dev/i18n/check_translations_completeness.py\n ```\n \n Files for specific languages:\n \n ```bash\n-uv run ./check_translations_completeness.py --language <language_code>\n+uv run dev/i18n/check_translations_completeness.py --language <language_code>\n ```\n \n Where `<language_code>` is the code of the language you want to check, e.g., `en`, `fr`, `de`, etc.\n@@ -248,7 +248,7 @@ Where `<language_code>` is the code of the language you want to check, e.g., `en\n Adding missing translations (with `TODO: translate` prefix):\n \n ```bash\n-uv run ./check_translations_completeness.py --language <language_code> --add-missing\n+uv run dev/i18n/check_translations_completeness.py --language <language_code> --add-missing\n ```\n \n ## 9. Compliance & enforcement\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/ar/admin.json b/airflow-core/src/airflow/ui/public/i18n/locales/ar/admin.json\nsimilarity index 54%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/ar/admin.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/ar/admin.json\nindex ab5e6126974c1..b3396e6c75e14 100644\n--- a/airflow-core/src/airflow/ui/src/i18n/locales/ar/admin.json\n+++ b/airflow-core/src/airflow/ui/public/i18n/locales/ar/admin.json\n@@ -1,74 +1,74 @@\n {\n+  \"columns\": {\n+    \"description\": \"\",\n+    \"key\": \"\",\n+    \"name\": \"\",\n+    \"value\": \"\"\n+  },\n+  \"config\": {\n     \"columns\": {\n-        \"description\": \"\",\n-        \"key\": \"\",\n-        \"name\": \"\",\n-        \"value\": \"\"\n+      \"section\": \"\"\n     },\n-    \"config\": {\n-        \"columns\": {\n-            \"section\": \"\"\n-        },\n-        \"title\": \"Airflow \"\n+    \"title\": \"Airflow \"\n+  },\n+  \"connections\": {\n+    \"add\": \" \",\n+    \"columns\": {\n+      \"connectionId\": \" \",\n+      \"connectionType\": \" \",\n+      \"host\": \"\",\n+      \"port\": \"\"\n     },\n-    \"connections\": {\n-        \"add\": \" \",\n-        \"columns\": {\n-            \"connectionId\": \" \",\n-            \"connectionType\": \" \",\n-            \"host\": \"\",\n-            \"port\": \"\"\n-        },\n-        \"connection_one\": \"\",\n-        \"connection_other\": \"\",\n-        \"delete\": {\n-            \"deleteConnection_one\": \"  \",\n-            \"deleteConnection_other\": \" {{count}} \",\n-            \"firstConfirmMessage_one\": \":     \",\n-            \"firstConfirmMessage_other\": \":     \",\n-            \"title\": \" \"\n-        },\n-        \"edit\": \" \",\n-        \"form\": {\n-            \"connectionIdRequired\": \"  \",\n-            \"connectionIdRequirement\": \"        \",\n-            \"connectionTypeRequired\": \"  \",\n-            \"extraFields\": \" \",\n-            \"extraFieldsJson\": \"JSON   \",\n-            \"helperText\": \"       Airflow .\",\n-            \"selectConnectionType\": \"  \",\n-            \"standardFields\": \" \"\n-        },\n-        \"noRowMessage\": \"   \",\n-        \"searchPlaceholder\": \"  \",\n-        \"test\": \" \",\n-        \"testDisabled\": \"  .    .\",\n-        \"typeMeta\": {\n-            \"error\": \"    \",\n-            \"standardFields\": {\n-                \"description\": \"\",\n-                \"host\": \"\",\n-                \"login\": \" \",\n-                \"password\": \" \",\n-                \"port\": \"\",\n-                \"url_schema\": \"\"\n-            }\n-        }\n+    \"connection_one\": \"\",\n+    \"connection_other\": \"\",\n+    \"delete\": {\n+      \"deleteConnection_one\": \"  \",\n+      \"deleteConnection_other\": \" {{count}} \",\n+      \"firstConfirmMessage_one\": \":     \",\n+      \"firstConfirmMessage_other\": \":     \",\n+      \"title\": \" \"\n     },\n-    \"deleteActions\": {\n-        \"button\": \"\",\n-        \"modal\": {\n-            \"confirmButton\": \"  \",\n-            \"secondConfirmMessage\": \"       .\",\n-            \"thirdConfirmMessage\":  \"     \"\n-        },\n-        \"selected\": \"\",\n-        \"tooltip\": \"  \"\n+    \"edit\": \" \",\n+    \"form\": {\n+      \"connectionIdRequired\": \"  \",\n+      \"connectionIdRequirement\": \"        \",\n+      \"connectionTypeRequired\": \"  \",\n+      \"extraFields\": \" \",\n+      \"extraFieldsJson\": \"JSON   \",\n+      \"helperText\": \"       Airflow .\",\n+      \"selectConnectionType\": \"  \",\n+      \"standardFields\": \" \"\n     },\n-    \"formActions\": {\n-        \"reset\": \" \",\n-        \"save\": \"\"\n+    \"noRowMessage\": \"   \",\n+    \"searchPlaceholder\": \"  \",\n+    \"test\": \" \",\n+    \"testDisabled\": \"  .    .\",\n+    \"typeMeta\": {\n+      \"error\": \"    \",\n+      \"standardFields\": {\n+        \"description\": \"\",\n+        \"host\": \"\",\n+        \"login\": \" \",\n+        \"password\": \" \",\n+        \"port\": \"\",\n+        \"url_schema\": \"\"\n+      }\n+    }\n+  },\n+  \"deleteActions\": {\n+    \"button\": \"\",\n+    \"modal\": {\n+      \"confirmButton\": \"  \",\n+      \"secondConfirmMessage\": \"       .\",\n+      \"thirdConfirmMessage\": \"     \"\n     },\n+    \"selected\": \"\",\n+    \"tooltip\": \"  \"\n+  },\n+  \"formActions\": {\n+    \"reset\": \" \",\n+    \"save\": \"\"\n+  },\n   \"plugins\": {\n     \"columns\": {\n       \"source\": \"\"\n@@ -80,7 +80,7 @@\n   \"pools\": {\n     \"add\": \" \",\n     \"deferredSlotsIncluded\": \"  \",\n-    \"delete\":{\n+    \"delete\": {\n       \"title\": \" \",\n       \"warning\": \"              .\"\n     },\n@@ -122,7 +122,7 @@\n       \"title\": \" \",\n       \"tooltip\": \"  \"\n     },\n-    \"edit\":  \" \",\n+    \"edit\": \" \",\n     \"export\": \"\",\n     \"exportTooltip\": \"  \",\n     \"form\": {\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/ar/assets.json b/airflow-core/src/airflow/ui/public/i18n/locales/ar/assets.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/ar/assets.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/ar/assets.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/ar/browse.json b/airflow-core/src/airflow/ui/public/i18n/locales/ar/browse.json\nsimilarity index 87%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/ar/browse.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/ar/browse.json\nindex ff6abf0b81777..775230d598cc5 100644\n--- a/airflow-core/src/airflow/ui/src/i18n/locales/ar/browse.json\n+++ b/airflow-core/src/airflow/ui/public/i18n/locales/ar/browse.json\n@@ -1,10 +1,10 @@\n {\n-  \"auditLog\":{\n+  \"auditLog\": {\n     \"actions\": {\n       \"collapseAllExtra\": \"  JSON \",\n       \"expandAllExtra\": \"  JSON \"\n     },\n-    \"columns\":{\n+    \"columns\": {\n       \"event\": \"\",\n       \"extra\": \"\",\n       \"user\": \"\",\n@@ -12,8 +12,8 @@\n     },\n     \"title\": \" \"\n   },\n-  \"xcom\":{\n-    \"columns\":{\n+  \"xcom\": {\n+    \"columns\": {\n       \"dag\": \"Dag\",\n       \"key\": \"\",\n       \"value\": \"\"\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/ar/common.json b/airflow-core/src/airflow/ui/public/i18n/locales/ar/common.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/ar/common.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/ar/common.json\ndiff --git a/airflow-core/src/airflow/ui/public/i18n/locales/ar/components.json b/airflow-core/src/airflow/ui/public/i18n/locales/ar/components.json\nnew file mode 100644\nindex 0000000000000..7764a53c62edf\n--- /dev/null\n+++ b/airflow-core/src/airflow/ui/public/i18n/locales/ar/components.json\n@@ -0,0 +1,131 @@\n+{\n+  \"backfill\": {\n+    \"affected_one\": \"  .\",\n+    \"affected_other\": \"{{count}}  .\",\n+    \"affectedNone\": \"     .\",\n+    \"backwards\": \" \",\n+    \"dateRange\": \" \",\n+    \"dateRangeFrom\": \"\",\n+    \"dateRangeTo\": \"\",\n+    \"errorStartDateBeforeEndDate\": \"       \",\n+    \"maxRuns\": \"   \",\n+    \"reprocessBehavior\": \"  \",\n+    \"run\": \"  \",\n+    \"selectDescription\": \"  DAG   \",\n+    \"selectLabel\": \" \",\n+    \"title\": \"  \",\n+    \"toaster\": {\n+      \"success\": {\n+        \"description\": \"     .\",\n+        \"title\": \"   \"\n+      }\n+    },\n+    \"tooltip\": \"    \",\n+    \"unpause\": \"  {{dag_display_name}}  \",\n+    \"validation\": {\n+      \"datesRequired\": \"         .\",\n+      \"startBeforeEnd\": \"           . \"\n+    }\n+  },\n+  \"banner\": {\n+    \"backfillInProgress\": \"   \",\n+    \"cancel\": \"  \",\n+    \"pause\": \"  \",\n+    \"unpause\": \"   \"\n+  },\n+  \"clipboard\": {\n+    \"copy\": \"\"\n+  },\n+  \"close\": \"\",\n+  \"configForm\": {\n+    \"advancedOptions\": \" \",\n+    \"configJson\": \" JSON\",\n+    \"invalidJson\": \" JSON  : {{errorMessage}}\"\n+  },\n+  \"dagWarnings\": {\n+    \"error_one\": \"1 \",\n+    \"errorAndWarning\": \"1  {{warningText}}\",\n+    \"warning_one\": \"1 \",\n+    \"warning_other\": \"{{count}} \"\n+  },\n+  \"durationChart\": {\n+    \"duration\": \" ()\",\n+    \"lastDagRun_one\": \"  DAG\",\n+    \"lastDagRun_other\": \" {{count}}  DAG\",\n+    \"lastTaskInstance_one\": \"  \",\n+    \"lastTaskInstance_other\": \" {{count}}  \",\n+    \"queuedDuration\": \"   \",\n+    \"runAfter\": \" \",\n+    \"runDuration\": \" \"\n+  },\n+  \"fileUpload\": {\n+    \"files_other\": \"{{count}} \"\n+  },\n+  \"flexibleForm\": {\n+    \"placeholder\": \" \",\n+    \"placeholderArray\": \"     \",\n+    \"placeholderExamples\": \"   \",\n+    \"placeholderMulti\": \"   \",\n+    \"validationErrorArrayNotArray\": \"    .\",\n+    \"validationErrorArrayNotNumbers\": \"       .\",\n+    \"validationErrorArrayNotObject\": \"       .\",\n+    \"validationErrorRequired\": \"  \"\n+  },\n+  \"graph\": {\n+    \"directionDown\": \"   \",\n+    \"directionLeft\": \"   \",\n+    \"directionRight\": \"   \",\n+    \"directionUp\": \"   \",\n+    \"downloadImage\": \"   \",\n+    \"downloadImageError\": \"    .\",\n+    \"downloadImageErrorTitle\": \" \",\n+    \"otherDagRuns\": \"+ DAG \",\n+    \"taskCount_one\": \"{{count}} \",\n+    \"taskCount_other\": \"{{count}} \",\n+    \"taskGroup\": \" \"\n+  },\n+  \"limitedList\": \"+{{count}} \",\n+  \"logs\": {\n+    \"file\": \"\",\n+    \"location\": \" {{line}}  {{name}}\"\n+  },\n+  \"reparseDag\": \"  DAG\",\n+  \"sortedAscending\": \" \",\n+  \"sortedDescending\": \" \",\n+  \"sortedUnsorted\": \" \",\n+  \"taskTries\": \" \",\n+  \"toggleCardView\": \" \",\n+  \"toggleTableView\": \" \",\n+  \"triggerDag\": {\n+    \"button\": \"\",\n+    \"loading\": \"   DAG...\",\n+    \"loadingFailed\": \"   DAG.    .\",\n+    \"runIdHelp\": \" -       .\",\n+    \"selectDescription\": \"     DAG\",\n+    \"selectLabel\": \" \",\n+    \"title\": \" DAG\",\n+    \"toaster\": {\n+      \"success\": {\n+        \"description\": \"   DAG .\",\n+        \"title\": \"  DAG\"\n+      }\n+    },\n+    \"unpause\": \"  {{dagDisplayName}}  \"\n+  },\n+  \"trimText\": {\n+    \"details\": \"\",\n+    \"empty\": \"\",\n+    \"noContent\": \"   .\"\n+  },\n+  \"versionDetails\": {\n+    \"bundleLink\": \" \",\n+    \"bundleName\": \" \",\n+    \"bundleVersion\": \" \",\n+    \"createdAt\": \" \",\n+    \"versionId\": \" \"\n+  },\n+  \"versionSelect\": {\n+    \"dagVersion\": \" DAG\",\n+    \"versionCode\": \"v{{versionCode}}\"\n+  }\n+}\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/ar/dag.json b/airflow-core/src/airflow/ui/public/i18n/locales/ar/dag.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/ar/dag.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/ar/dag.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/ar/dags.json b/airflow-core/src/airflow/ui/public/i18n/locales/ar/dags.json\nsimilarity index 88%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/ar/dags.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/ar/dags.json\nindex 20964aff8e539..e535ee03ec670 100644\n--- a/airflow-core/src/airflow/ui/src/i18n/locales/ar/dags.json\n+++ b/airflow-core/src/airflow/ui/public/i18n/locales/ar/dags.json\n@@ -52,18 +52,18 @@\n       \"downstream\": \" \",\n       \"existingTasks\": \"  \",\n       \"future\": \"\",\n-        \"onlyFailed\": \"   \",\n-        \"past\": \"\",\n-        \"queueNew\": \"   \",\n-            \"upstream\": \" \"\n+      \"onlyFailed\": \"   \",\n+      \"past\": \"\",\n+      \"queueNew\": \"   \",\n+      \"upstream\": \" \"\n     }\n   },\n   \"search\": {\n-     \"advanced\": \" \",\n-     \"clear\": \" \",\n-     \"dags\": \"  Dags\",\n-     \"hotkey\": \"+K\",\n-     \"tasks\": \"  \"\n+    \"advanced\": \" \",\n+    \"clear\": \" \",\n+    \"dags\": \"  Dags\",\n+    \"hotkey\": \"+K\",\n+    \"tasks\": \"  \"\n   },\n   \"sort\": {\n     \"displayName\": {\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/ar/dashboard.json b/airflow-core/src/airflow/ui/public/i18n/locales/ar/dashboard.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/ar/dashboard.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/ar/dashboard.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/de/README.md b/airflow-core/src/airflow/ui/public/i18n/locales/de/README.md\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/de/README.md\nrename to airflow-core/src/airflow/ui/public/i18n/locales/de/README.md\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/de/admin.json b/airflow-core/src/airflow/ui/public/i18n/locales/de/admin.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/de/admin.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/de/admin.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/de/assets.json b/airflow-core/src/airflow/ui/public/i18n/locales/de/assets.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/de/assets.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/de/assets.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/de/browse.json b/airflow-core/src/airflow/ui/public/i18n/locales/de/browse.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/de/browse.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/de/browse.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/de/common.json b/airflow-core/src/airflow/ui/public/i18n/locales/de/common.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/de/common.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/de/common.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/de/components.json b/airflow-core/src/airflow/ui/public/i18n/locales/de/components.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/de/components.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/de/components.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/de/dag.json b/airflow-core/src/airflow/ui/public/i18n/locales/de/dag.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/de/dag.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/de/dag.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/de/dags.json b/airflow-core/src/airflow/ui/public/i18n/locales/de/dags.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/de/dags.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/de/dags.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/de/dashboard.json b/airflow-core/src/airflow/ui/public/i18n/locales/de/dashboard.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/de/dashboard.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/de/dashboard.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/en/admin.json b/airflow-core/src/airflow/ui/public/i18n/locales/en/admin.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/en/admin.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/en/admin.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/en/assets.json b/airflow-core/src/airflow/ui/public/i18n/locales/en/assets.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/en/assets.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/en/assets.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/en/browse.json b/airflow-core/src/airflow/ui/public/i18n/locales/en/browse.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/en/browse.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/en/browse.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/en/common.json b/airflow-core/src/airflow/ui/public/i18n/locales/en/common.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/en/common.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/en/common.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/en/components.json b/airflow-core/src/airflow/ui/public/i18n/locales/en/components.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/en/components.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/en/components.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/en/dag.json b/airflow-core/src/airflow/ui/public/i18n/locales/en/dag.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/en/dag.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/en/dag.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/en/dags.json b/airflow-core/src/airflow/ui/public/i18n/locales/en/dags.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/en/dags.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/en/dags.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/en/dashboard.json b/airflow-core/src/airflow/ui/public/i18n/locales/en/dashboard.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/en/dashboard.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/en/dashboard.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/fr/admin.json b/airflow-core/src/airflow/ui/public/i18n/locales/fr/admin.json\nsimilarity index 95%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/fr/admin.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/fr/admin.json\nindex 3626a7a2f1904..15edd6d9aad58 100644\n--- a/airflow-core/src/airflow/ui/src/i18n/locales/fr/admin.json\n+++ b/airflow-core/src/airflow/ui/public/i18n/locales/fr/admin.json\n@@ -1,19 +1,19 @@\n {\n-  \"columns\":{\n+  \"columns\": {\n     \"description\": \"Description\",\n     \"key\": \"Cl\",\n     \"name\": \"Nom\",\n     \"value\": \"Valeur\"\n   },\n-  \"config\":{\n-    \"columns\":{\n+  \"config\": {\n+    \"columns\": {\n       \"section\": \"Section\"\n     },\n     \"title\": \"Configuration d'Airflow\"\n   },\n-  \"connections\":{\n-    \"add\":\"Ajouter une Connexion\",\n-    \"columns\":{\n+  \"connections\": {\n+    \"add\": \"Ajouter une Connexion\",\n+    \"columns\": {\n       \"connectionId\": \"ID de la connexion\",\n       \"connectionType\": \"Type de la connexion\",\n       \"host\": \"Hte\",\n@@ -21,7 +21,7 @@\n     },\n     \"connection_one\": \"Connexion\",\n     \"connection_other\": \"Connexions\",\n-    \"delete\":{\n+    \"delete\": {\n       \"deleteConnection_one\": \"Supprimer 1 connexion\",\n       \"deleteConnection_other\": \"Supprimer {{count}} connexions\",\n       \"firstConfirmMessage_one\": \"Vous tes sur le point de supprimer la connexion suivante :\",\n@@ -29,7 +29,7 @@\n       \"title\": \"Supprimer la Connexion\"\n     },\n     \"edit\": \"Modifier la Connexion\",\n-    \"form\":{\n+    \"form\": {\n       \"connectionIdRequired\": \"L'ID de la connexion est requis\",\n       \"connectionIdRequirement\": \"L'ID de la connexion ne peut pas contenir uniquement des espaces\",\n       \"connectionTypeRequired\": \"Le type de la connexion est requis\",\n@@ -55,9 +55,9 @@\n       }\n     }\n   },\n-  \"deleteActions\":{\n+  \"deleteActions\": {\n     \"button\": \"Supprimer\",\n-    \"modal\":{\n+    \"modal\": {\n       \"confirmButton\": \"Oui, Supprimer\",\n       \"secondConfirmMessage\": \"Cette action est irrversible.\",\n       \"thirdConfirmMessage\": \" tes-vous sr de vouloir continuer ?\"\n@@ -65,7 +65,7 @@\n     \"selected\": \"Slectionn\",\n     \"tooltip\": \"Supprimer les connexions slectionnes\"\n   },\n-  \"formActions\":{\n+  \"formActions\": {\n     \"reset\": \"Rinitialiser\",\n     \"save\": \"Sauvegarder\"\n   },\n@@ -80,7 +80,7 @@\n   \"pools\": {\n     \"add\": \"Ajouter un Pool\",\n     \"deferredSlotsIncluded\": \"Slots Diffrs Inclus\",\n-    \"delete\":{\n+    \"delete\": {\n       \"title\": \"Supprimer le Pool\",\n       \"warning\": \"Cela supprimera toutes les mtadonnes lies au pool et peut affecter les tches utilisant ce pool.\"\n     },\n@@ -122,7 +122,7 @@\n       \"title\": \"Supprimer la Variable\",\n       \"tooltip\": \"Supprimer les variables slectionnes\"\n     },\n-    \"edit\":  \"Modifier la Variable\",\n+    \"edit\": \"Modifier la Variable\",\n     \"export\": \"Exporter\",\n     \"exportTooltip\": \"Exporter les variables slectionnes\",\n     \"form\": {\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/fr/assets.json b/airflow-core/src/airflow/ui/public/i18n/locales/fr/assets.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/fr/assets.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/fr/assets.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/fr/browse.json b/airflow-core/src/airflow/ui/public/i18n/locales/fr/browse.json\nsimilarity index 86%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/fr/browse.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/fr/browse.json\nindex 7aad5959a653f..09b7d3de68768 100644\n--- a/airflow-core/src/airflow/ui/src/i18n/locales/fr/browse.json\n+++ b/airflow-core/src/airflow/ui/public/i18n/locales/fr/browse.json\n@@ -1,10 +1,10 @@\n {\n-  \"auditLog\":{\n+  \"auditLog\": {\n     \"actions\": {\n       \"collapseAllExtra\": \"Rduire tous les extra json\",\n       \"expandAllExtra\": \"Ouvrir tous les extra json\"\n     },\n-    \"columns\":{\n+    \"columns\": {\n       \"event\": \"vnement\",\n       \"extra\": \"Extra\",\n       \"user\": \"Utilisateur\",\n@@ -12,8 +12,8 @@\n     },\n     \"title\": \"Journal d'Audit\"\n   },\n-  \"xcom\":{\n-    \"columns\":{\n+  \"xcom\": {\n+    \"columns\": {\n       \"dag\": \"Dag\",\n       \"key\": \"Cl\",\n       \"value\": \"Valeur\"\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/fr/common.json b/airflow-core/src/airflow/ui/public/i18n/locales/fr/common.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/fr/common.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/fr/common.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/fr/components.json b/airflow-core/src/airflow/ui/public/i18n/locales/fr/components.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/fr/components.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/fr/components.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/fr/dag.json b/airflow-core/src/airflow/ui/public/i18n/locales/fr/dag.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/fr/dag.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/fr/dag.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/fr/dags.json b/airflow-core/src/airflow/ui/public/i18n/locales/fr/dags.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/fr/dags.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/fr/dags.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/fr/dashboard.json b/airflow-core/src/airflow/ui/public/i18n/locales/fr/dashboard.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/fr/dashboard.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/fr/dashboard.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/he/admin.json b/airflow-core/src/airflow/ui/public/i18n/locales/he/admin.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/he/admin.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/he/admin.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/he/assets.json b/airflow-core/src/airflow/ui/public/i18n/locales/he/assets.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/he/assets.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/he/assets.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/he/browse.json b/airflow-core/src/airflow/ui/public/i18n/locales/he/browse.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/he/browse.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/he/browse.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/he/common.json b/airflow-core/src/airflow/ui/public/i18n/locales/he/common.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/he/common.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/he/common.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/he/components.json b/airflow-core/src/airflow/ui/public/i18n/locales/he/components.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/he/components.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/he/components.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/he/dag.json b/airflow-core/src/airflow/ui/public/i18n/locales/he/dag.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/he/dag.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/he/dag.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/he/dags.json b/airflow-core/src/airflow/ui/public/i18n/locales/he/dags.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/he/dags.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/he/dags.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/he/dashboard.json b/airflow-core/src/airflow/ui/public/i18n/locales/he/dashboard.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/he/dashboard.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/he/dashboard.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/ko/common.json b/airflow-core/src/airflow/ui/public/i18n/locales/ko/common.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/ko/common.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/ko/common.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/ko/dashboard.json b/airflow-core/src/airflow/ui/public/i18n/locales/ko/dashboard.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/ko/dashboard.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/ko/dashboard.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/nl/common.json b/airflow-core/src/airflow/ui/public/i18n/locales/nl/common.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/nl/common.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/nl/common.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/nl/dashboard.json b/airflow-core/src/airflow/ui/public/i18n/locales/nl/dashboard.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/nl/dashboard.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/nl/dashboard.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/pl/admin.json b/airflow-core/src/airflow/ui/public/i18n/locales/pl/admin.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/pl/admin.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/pl/admin.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/pl/assets.json b/airflow-core/src/airflow/ui/public/i18n/locales/pl/assets.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/pl/assets.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/pl/assets.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/pl/browse.json b/airflow-core/src/airflow/ui/public/i18n/locales/pl/browse.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/pl/browse.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/pl/browse.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/pl/common.json b/airflow-core/src/airflow/ui/public/i18n/locales/pl/common.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/pl/common.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/pl/common.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/pl/components.json b/airflow-core/src/airflow/ui/public/i18n/locales/pl/components.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/pl/components.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/pl/components.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/pl/dag.json b/airflow-core/src/airflow/ui/public/i18n/locales/pl/dag.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/pl/dag.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/pl/dag.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/pl/dags.json b/airflow-core/src/airflow/ui/public/i18n/locales/pl/dags.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/pl/dags.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/pl/dags.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/pl/dashboard.json b/airflow-core/src/airflow/ui/public/i18n/locales/pl/dashboard.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/pl/dashboard.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/pl/dashboard.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/admin.json b/airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/admin.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/admin.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/admin.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/assets.json b/airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/assets.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/assets.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/assets.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/browse.json b/airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/browse.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/browse.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/browse.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/common.json b/airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/common.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/common.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/common.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/components.json b/airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/components.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/components.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/components.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/dag.json b/airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/dag.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/dag.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/dag.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/dags.json b/airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/dags.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/dags.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/dags.json\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/dashboard.json b/airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/dashboard.json\nsimilarity index 100%\nrename from airflow-core/src/airflow/ui/src/i18n/locales/zh-TW/dashboard.json\nrename to airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/dashboard.json\ndiff --git a/airflow-core/src/airflow/ui/rules/i18n.js b/airflow-core/src/airflow/ui/rules/i18n.js\nindex f96e71b37b7d4..be2fbac79e7de 100644\n--- a/airflow-core/src/airflow/ui/rules/i18n.js\n+++ b/airflow-core/src/airflow/ui/rules/i18n.js\n@@ -20,6 +20,7 @@\n /* eslint-disable @typescript-eslint/no-unsafe-argument */\n \n /* eslint-disable @typescript-eslint/no-unsafe-assignment */\n+import jsoncParser from \"jsonc-eslint-parser\";\n import fs from \"node:fs\";\n import path from \"node:path\";\n import { fileURLToPath } from \"node:url\";\n@@ -50,7 +51,7 @@ const getKeys = (obj, prefix = \"\") => {\n };\n \n // Path to locales directory\n-const localesDir = path.resolve(path.dirname(fileURLToPath(import.meta.url)), \"../src/i18n/locales\");\n+const localesDir = path.resolve(path.dirname(fileURLToPath(import.meta.url)), \"../public/i18n/locales\");\n \n // Default language (English) as reference\n const defaultLanguage = \"en\";\n@@ -72,7 +73,7 @@ fs.readdirSync(defaultLanguageDir)\n   });\n \n export const i18nPlugin = {\n-  files: [\"**/i18n/locales/**/*.json\"],\n+  files: [\"public/i18n/locales/**/*.json\"],\n   rules: {\n     \"check-translation-completeness\": {\n       /** @param {import('@typescript-eslint/utils').TSESLint.RuleContext<'missingKeys' | 'fileError', []>} context */\n@@ -155,7 +156,13 @@ export const i18nPlugin = {\n \n /** @type {import(\"@typescript-eslint/utils/ts-eslint\").FlatConfig.Config} */\n export const i18nRules = {\n-  files: [\"**/i18n/locales/**/*.json\"],\n+  files: [\"public/i18n/locales/**/*.json\"],\n+  languageOptions: {\n+    parser: jsoncParser,\n+    parserOptions: {\n+      extraFileExtensions: [\".json\"],\n+    },\n+  },\n   plugins: {\n     [i18nNamespace]: i18nPlugin,\n   },\ndiff --git a/airflow-core/src/airflow/ui/rules/typescript.js b/airflow-core/src/airflow/ui/rules/typescript.js\nindex 2583b602e7264..ed05e15089799 100644\n--- a/airflow-core/src/airflow/ui/rules/typescript.js\n+++ b/airflow-core/src/airflow/ui/rules/typescript.js\n@@ -38,6 +38,7 @@ export const typescriptNamespace = \"@typescript-eslint\";\n  * @see [@typescript-eslint/eslint-plugin](https://typescript-eslint.io/rules/)\n  */\n export const typescriptRules = /** @type {const} @satisfies {FlatConfig.Config} */ ({\n+  files: [\"**/*.ts\", \"**/*.tsx\", \"**/*.js\", \"**/*.jsx\"],\n   languageOptions: {\n     parser: typescriptParser,\n     parserOptions: {\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/config.ts b/airflow-core/src/airflow/ui/src/i18n/config.ts\nindex 41bb37f5e8ace..9754236f0fd8b 100644\n--- a/airflow-core/src/airflow/ui/src/i18n/config.ts\n+++ b/airflow-core/src/airflow/ui/src/i18n/config.ts\n@@ -18,72 +18,9 @@\n  */\n import i18n from \"i18next\";\n import LanguageDetector from \"i18next-browser-languagedetector\";\n+import Backend from \"i18next-http-backend\";\n import { initReactI18next } from \"react-i18next\";\n \n-import arAdmin from \"./locales/ar/admin.json\";\n-import arAssets from \"./locales/ar/assets.json\";\n-import arBrowse from \"./locales/ar/browse.json\";\n-import arCommon from \"./locales/ar/common.json\";\n-import arComponents from \"./locales/ar/components.json\";\n-import arDag from \"./locales/ar/dag.json\";\n-import arDags from \"./locales/ar/dags.json\";\n-import arDashboard from \"./locales/ar/dashboard.json\";\n-import deAdmin from \"./locales/de/admin.json\";\n-import deAssets from \"./locales/de/assets.json\";\n-import deBrowse from \"./locales/de/browse.json\";\n-import deCommon from \"./locales/de/common.json\";\n-import deComponents from \"./locales/de/components.json\";\n-import deDag from \"./locales/de/dag.json\";\n-import deDags from \"./locales/de/dags.json\";\n-import deDashboard from \"./locales/de/dashboard.json\";\n-import enAdmin from \"./locales/en/admin.json\";\n-import enAssets from \"./locales/en/assets.json\";\n-import enBrowse from \"./locales/en/browse.json\";\n-import enCommon from \"./locales/en/common.json\";\n-import enComponents from \"./locales/en/components.json\";\n-import enDag from \"./locales/en/dag.json\";\n-import enDags from \"./locales/en/dags.json\";\n-import enDashboard from \"./locales/en/dashboard.json\";\n-import frAdmin from \"./locales/fr/admin.json\";\n-import frAssets from \"./locales/fr/assets.json\";\n-import frBrowse from \"./locales/fr/browse.json\";\n-import frCommon from \"./locales/fr/common.json\";\n-import frComponents from \"./locales/fr/components.json\";\n-import frDag from \"./locales/fr/dag.json\";\n-import frDags from \"./locales/fr/dags.json\";\n-import frDashboard from \"./locales/fr/dashboard.json\";\n-import heAdmin from \"./locales/he/admin.json\";\n-import heAsset from \"./locales/he/assets.json\";\n-import heBrowse from \"./locales/he/browse.json\";\n-import heCommon from \"./locales/he/common.json\";\n-import heComponents from \"./locales/he/components.json\";\n-import heDag from \"./locales/he/dag.json\";\n-import heDags from \"./locales/he/dags.json\";\n-import heDashboard from \"./locales/he/dashboard.json\";\n-import koCommon from \"./locales/ko/common.json\";\n-import koDashboard from \"./locales/ko/dashboard.json\";\n-import nlCommon from \"./locales/nl/common.json\";\n-import nlDashboard from \"./locales/nl/dashboard.json\";\n-import plAdmin from \"./locales/pl/admin.json\";\n-import plAssets from \"./locales/pl/assets.json\";\n-import plBrowse from \"./locales/pl/browse.json\";\n-import plCommon from \"./locales/pl/common.json\";\n-import plComponents from \"./locales/pl/components.json\";\n-import plDag from \"./locales/pl/dag.json\";\n-import plDags from \"./locales/pl/dags.json\";\n-import plDashboard from \"./locales/pl/dashboard.json\";\n-import zhTWAdmin from \"./locales/zh-TW/admin.json\";\n-import zhTWAssets from \"./locales/zh-TW/assets.json\";\n-import zhTWBrowse from \"./locales/zh-TW/browse.json\";\n-import zhTWCommon from \"./locales/zh-TW/common.json\";\n-import zhTWComponents from \"./locales/zh-TW/components.json\";\n-import zhTWDag from \"./locales/zh-TW/dag.json\";\n-import zhTWDags from \"./locales/zh-TW/dags.json\";\n-import zhTWDashboard from \"./locales/zh-TW/dashboard.json\";\n-\n-// TODO: Dynamically load translation files\n-// import Backend from 'i18next-http-backend';\n-\n export const supportedLanguages = [\n   { code: \"ar\", flag: \"\", name: \"\" },\n   { code: \"de\", flag: \"\", name: \"Deutsch\" },\n@@ -97,94 +34,16 @@ export const supportedLanguages = [\n ] as const;\n \n export const defaultLanguage = \"en\";\n-export const namespaces = [\"common\", \"dashboard\", \"dags\", \"admin\", \"browse\", \"assets\"] as const;\n-\n-const resources = {\n-  ar: {\n-    admin: arAdmin,\n-    assets: arAssets,\n-    browse: arBrowse,\n-    common: arCommon,\n-    components: arComponents,\n-    dag: arDag,\n-    dags: arDags,\n-    dashboard: arDashboard,\n-  },\n-  de: {\n-    admin: deAdmin,\n-    assets: deAssets,\n-    browse: deBrowse,\n-    common: deCommon,\n-    components: deComponents,\n-    dag: deDag,\n-    dags: deDags,\n-    dashboard: deDashboard,\n-  },\n-  en: {\n-    admin: enAdmin,\n-    assets: enAssets,\n-    browse: enBrowse,\n-    common: enCommon,\n-    components: enComponents,\n-    dag: enDag,\n-    dags: enDags,\n-    dashboard: enDashboard,\n-  },\n-  fr: {\n-    admin: frAdmin,\n-    assets: frAssets,\n-    browse: frBrowse,\n-    common: frCommon,\n-    components: frComponents,\n-    dag: frDag,\n-    dags: frDags,\n-    dashboard: frDashboard,\n-  },\n-  he: {\n-    admin: heAdmin,\n-    assets: heAsset,\n-    browse: heBrowse,\n-    common: heCommon,\n-    components: heComponents,\n-    dag: heDag,\n-    dags: heDags,\n-    dashboard: heDashboard,\n-  },\n-  ko: {\n-    common: koCommon,\n-    dashboard: koDashboard,\n-  },\n-  nl: {\n-    common: nlCommon,\n-    dashboard: nlDashboard,\n-  },\n-  pl: {\n-    admin: plAdmin,\n-    assets: plAssets,\n-    browse: plBrowse,\n-    common: plCommon,\n-    components: plComponents,\n-    dag: plDag,\n-    dags: plDags,\n-    dashboard: plDashboard,\n-  },\n-  \"zh-TW\": {\n-    admin: zhTWAdmin,\n-    assets: zhTWAssets,\n-    browse: zhTWBrowse,\n-    common: zhTWCommon,\n-    components: zhTWComponents,\n-    dag: zhTWDag,\n-    dags: zhTWDags,\n-    dashboard: zhTWDashboard,\n-  },\n-};\n+export const namespaces = [\"common\", \"dashboard\", \"dags\", \"admin\", \"browse\", \"assets\", \"components\"] as const;\n \n void i18n\n-  // .use(Backend) // TODO: Dynamically load translation files\n+  .use(Backend)\n   .use(LanguageDetector)\n   .use(initReactI18next)\n   .init({\n+    backend: {\n+      loadPath: \"/static/i18n/locales/{{lng}}/{{ns}}.json\",\n+    },\n     defaultNS: \"common\",\n     detection: {\n       caches: [\"localStorage\"],\n@@ -198,7 +57,6 @@ void i18n\n     react: {\n       useSuspense: false,\n     },\n-    resources,\n     supportedLngs: supportedLanguages.map((lang) => lang.code),\n   });\n \ndiff --git a/airflow-core/src/airflow/ui/src/i18n/locales/ar/components.json b/airflow-core/src/airflow/ui/src/i18n/locales/ar/components.json\ndeleted file mode 100644\nindex b5b9c1b43b372..0000000000000\n--- a/airflow-core/src/airflow/ui/src/i18n/locales/ar/components.json\n+++ /dev/null\n@@ -1,131 +0,0 @@\n-{\n-    \"backfill\": {\n-        \"affected_one\": \"  .\",\n-        \"affected_other\": \"{{count}}  .\",\n-        \"affectedNone\": \"     .\",\n-        \"backwards\": \" \",\n-        \"dateRange\": \" \",\n-        \"dateRangeFrom\": \"\",\n-        \"dateRangeTo\": \"\",\n-        \"errorStartDateBeforeEndDate\": \"       \",\n-        \"maxRuns\": \"   \",\n-        \"reprocessBehavior\": \"  \",\n-        \"run\": \"  \",\n-        \"selectDescription\": \"  DAG   \",\n-        \"selectLabel\": \" \",\n-        \"title\": \"  \",\n-        \"toaster\": {\n-            \"success\": {\n-                \"description\": \"     .\",\n-                \"title\": \"   \"\n-            }\n-        },\n-        \"tooltip\": \"    \",\n-        \"unpause\": \"  {{dag_display_name}}  \",\n-        \"validation\": {\n-            \"datesRequired\": \"         .\",\n-            \"startBeforeEnd\": \"           . \"\n-        }\n-    },\n-    \"banner\": {\n-        \"backfillInProgress\": \"   \",\n-        \"cancel\": \"  \",\n-        \"pause\": \"  \",\n-        \"unpause\": \"   \"\n-    },\n-    \"clipboard\": {\n-        \"copy\": \"\"\n-    },\n-    \"close\": \"\",\n-    \"configForm\": {\n-        \"advancedOptions\": \" \",\n-        \"configJson\": \" JSON\",\n-        \"invalidJson\": \" JSON  : {{errorMessage}}\"\n-    },\n-    \"dagWarnings\": {\n-        \"error_one\": \"1 \",\n-        \"errorAndWarning\": \"1  {{warningText}}\",\n-        \"warning_one\": \"1 \",\n-        \"warning_other\": \"{{count}} \"\n-    },\n-    \"durationChart\": {\n-        \"duration\": \" ()\",\n-        \"lastDagRun_one\": \"  DAG\",\n-        \"lastDagRun_other\": \" {{count}}  DAG\",\n-        \"lastTaskInstance_one\": \"  \",\n-        \"lastTaskInstance_other\": \" {{count}}  \",\n-        \"queuedDuration\": \"   \",\n-        \"runAfter\": \" \",\n-        \"runDuration\": \" \"\n-    },\n-    \"fileUpload\": {\n-        \"files_other\": \"{{count}} \"\n-    },\n-    \"flexibleForm\": {\n-        \"placeholder\": \" \",\n-        \"placeholderArray\": \"     \",\n-        \"placeholderExamples\": \"   \",\n-        \"placeholderMulti\": \"   \",\n-        \"validationErrorArrayNotArray\": \"    .\",\n-        \"validationErrorArrayNotNumbers\": \"       .\",\n-        \"validationErrorArrayNotObject\": \"       .\",\n-        \"validationErrorRequired\": \"  \"\n-    },\n-    \"graph\": {\n-        \"directionDown\": \"   \",\n-        \"directionLeft\": \"   \",\n-        \"directionRight\": \"   \",\n-        \"directionUp\": \"   \",\n-        \"downloadImage\": \"   \",\n-        \"downloadImageError\": \"    .\",\n-        \"downloadImageErrorTitle\": \" \",\n-        \"otherDagRuns\": \"+ DAG \",\n-        \"taskCount_one\": \"{{count}} \",\n-        \"taskCount_other\": \"{{count}} \",\n-        \"taskGroup\": \" \"\n-    },\n-    \"limitedList\": \"+{{count}} \",\n-    \"logs\": {\n-        \"file\": \"\",\n-        \"location\": \" {{line}}  {{name}}\"\n-    },\n-    \"reparseDag\": \"  DAG\",\n-    \"sortedAscending\": \" \",\n-    \"sortedDescending\": \" \",\n-    \"sortedUnsorted\": \" \",\n-    \"taskTries\": \" \",\n-    \"toggleCardView\": \" \",\n-    \"toggleTableView\": \" \",\n-    \"triggerDag\": {\n-        \"button\": \"\",\n-        \"loading\": \"   DAG...\",\n-        \"loadingFailed\": \"   DAG.    .\",\n-        \"runIdHelp\": \" -       .\",\n-        \"selectDescription\": \"     DAG\",\n-        \"selectLabel\": \" \",\n-        \"title\": \" DAG\",\n-        \"toaster\": {\n-            \"success\": {\n-                \"description\": \"   DAG .\",\n-                \"title\": \"  DAG\"\n-            }\n-        },\n-        \"unpause\": \"  {{dagDisplayName}}  \"\n-    },\n-    \"trimText\": {\n-        \"details\": \"\",\n-        \"empty\": \"\",\n-        \"noContent\": \"   .\"\n-    },\n-    \"versionDetails\": {\n-        \"bundleLink\": \" \",\n-        \"bundleName\": \" \",\n-        \"bundleVersion\": \" \",\n-        \"createdAt\": \" \",\n-        \"versionId\": \" \"\n-    },\n-    \"versionSelect\": {\n-        \"dagVersion\": \" DAG\",\n-        \"versionCode\": \"v{{versionCode}}\"\n-    }\n-}\ndiff --git a/dev/README_RELEASE_AIRFLOW.md b/dev/README_RELEASE_AIRFLOW.md\nindex 0b5b538fcc188..268fdb1d1ada7 100644\n--- a/dev/README_RELEASE_AIRFLOW.md\n+++ b/dev/README_RELEASE_AIRFLOW.md\n@@ -75,7 +75,7 @@ The first step of a release is to work out what is being included. This differs\n \n ## Validating completeness of i18n locale files\n \n-At this point you should validate the completeness of the i18n locale files - follow the instructions in section 8.1 of the [internationalization (i18n) policy](../airflow-core/src/airflow/ui/src/i18n/README.md) for doing so.\n+At this point you should validate the completeness of the i18n locale files - follow the instructions in section 8.1 of the [internationalization (i18n) policy](../airflow-core/src/airflow/ui/public/i18n/README.md) for doing so.\n If there are any incomplete locales, copy the names of the incomplete locales and send out a reminder to the code owners to ensure completion of the translation by a due date of your choice\n before cutting the release candidate (RC).\n The reminder should be sent via dev@airflow.apache.org mailing list, preferably with an accompanying GitHub issue for tracking purposes.\ndiff --git a/airflow-core/src/airflow/ui/src/i18n/check_translations_completeness.py b/dev/i18n/check_translations_completeness.py\nsimilarity index 99%\nrename from airflow-core/src/airflow/ui/src/i18n/check_translations_completeness.py\nrename to dev/i18n/check_translations_completeness.py\nindex d001091f606da..35d0583b53160 100755\n--- a/airflow-core/src/airflow/ui/src/i18n/check_translations_completeness.py\n+++ b/dev/i18n/check_translations_completeness.py\n@@ -40,7 +40,9 @@\n click.rich_click.MAX_WIDTH = 120\n click.rich_click.USE_RICH_MARKUP = True\n \n-LOCALES_DIR = Path(__file__).parent / \"locales\"\n+LOCALES_DIR = (\n+    Path(__file__).parents[2] / \"airflow-core\" / \"src\" / \"airflow\" / \"ui\" / \"public\" / \"i18n\" / \"locales\"\n+)\n \n \n class LocaleSummary(NamedTuple):\ndiff --git a/scripts/ci/pre_commit/boring_cyborg.py b/scripts/ci/pre_commit/boring_cyborg.py\nindex 22d82d736cb9f..95fb1f057c032 100755\n--- a/scripts/ci/pre_commit/boring_cyborg.py\n+++ b/scripts/ci/pre_commit/boring_cyborg.py\n@@ -63,7 +63,7 @@\n \n # Check for missing translations\n EXCEPTIONS = [\"en\"]\n-for p in repo_root.glob(\"airflow-core/src/airflow/ui/src/i18n/locales/*\"):\n+for p in repo_root.glob(\"airflow-core/src/airflow/ui/public/i18n/locales/*\"):\n     if p.is_dir():\n         lang_id = p.name\n         expected_key = f\"translation:{lang_id}\"\ndiff --git a/scripts/ci/pre_commit/check_i18n_json.py b/scripts/ci/pre_commit/check_i18n_json.py\nindex 9c4f1af4a77d3..31051092e4edb 100755\n--- a/scripts/ci/pre_commit/check_i18n_json.py\n+++ b/scripts/ci/pre_commit/check_i18n_json.py\n@@ -17,7 +17,7 @@\n # under the License.\n \n \"\"\"\n-Pre-commit script to check that all .json files in airflow-core/src/airflow/ui/src/i18n/locales/\n+Pre-commit script to check that all .json files in airflow-core/src/airflow/ui/public/i18n/locales/\n are valid JSON and do not contain any 'TODO:' entries.\n \"\"\"\n \n@@ -32,7 +32,7 @@\n sys.path.insert(0, COMMON_PRECOMMIT_PATH.as_posix())  # make sure common_precommit_utils is imported\n from common_precommit_utils import AIRFLOW_ROOT_PATH, console\n \n-LOCALES_DIR = AIRFLOW_ROOT_PATH / \"airflow-core\" / \"src\" / \"airflow\" / \"ui\" / \"src\" / \"i18n\" / \"locales\"\n+LOCALES_DIR = AIRFLOW_ROOT_PATH / \"airflow-core\" / \"src\" / \"airflow\" / \"ui\" / \"public\" / \"i18n\" / \"locales\"\n \n \n def main():\n","test_patch":"diff --git a/airflow-core/src/airflow/ui/src/pages/DagsList/DagCard.test.tsx b/airflow-core/src/airflow/ui/src/pages/DagsList/DagCard.test.tsx\nindex d95e4466be5cf..c5ed905568f95 100644\n--- a/airflow-core/src/airflow/ui/src/pages/DagsList/DagCard.test.tsx\n+++ b/airflow-core/src/airflow/ui/src/pages/DagsList/DagCard.test.tsx\n@@ -19,8 +19,9 @@\n  * under the License.\n  */\n import { render, screen } from \"@testing-library/react\";\n+import i18n from \"i18next\";\n import type { DagTagResponse, DAGWithLatestDagRunsResponse } from \"openapi-gen/requests/types.gen\";\n-import { afterEach, describe, it, vi, expect } from \"vitest\";\n+import { afterEach, describe, it, vi, expect, beforeAll } from \"vitest\";\n \n import { Wrapper } from \"src/utils/Wrapper\";\n \n@@ -58,6 +59,23 @@ const mockDag = {\n   timetable_summary: \"\",\n } satisfies DAGWithLatestDagRunsResponse;\n \n+beforeAll(async () => {\n+  await i18n.init({\n+    defaultNS: \"components\",\n+    fallbackLng: \"en\",\n+    interpolation: { escapeValue: false },\n+    lng: \"en\",\n+    ns: [\"components\"],\n+    resources: {\n+      en: {\n+        components: {\n+          limitedList: \"+{{count}} more\",\n+        },\n+      },\n+    },\n+  });\n+});\n+\n afterEach(() => {\n   vi.restoreAllMocks();\n });\ndiff --git a/airflow-core/src/airflow/ui/testsSetup.ts b/airflow-core/src/airflow/ui/testsSetup.ts\nindex 676ffc3d82e0d..09dbe7bce31e3 100644\n--- a/airflow-core/src/airflow/ui/testsSetup.ts\n+++ b/airflow-core/src/airflow/ui/testsSetup.ts\n@@ -16,19 +16,15 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-import * as matchers from \"@testing-library/jest-dom/matchers\";\n import \"@testing-library/jest-dom/vitest\";\n import type { HttpHandler } from \"msw\";\n import { setupServer, type SetupServerApi } from \"msw/node\";\n-import { beforeEach, expect, beforeAll, afterAll } from \"vitest\";\n+import { beforeEach, beforeAll, afterAll } from \"vitest\";\n \n import { handlers } from \"src/mocks/handlers\";\n \n let server: SetupServerApi;\n \n-// extends vitest matchers with react-testing-library's ones\n-expect.extend(matchers);\n-\n beforeAll(() => {\n   server = setupServer(...(handlers as Array<HttpHandler>));\n   server.listen({ onUnhandledRequest: \"bypass\" });\n","problem_statement":"i18n - Load translation files from a backend\n### Description\n\nRight now all of the translation json files are in the UI js bundle at runtime. We should move them to the server and only load the translation currently being used.\n\nThe initial PR for i18n left TODO comments for the places we need to change in the UI to handle this.\n\n### Use case/motivation\n\nPrevent unnecessary bundle size bloat.\n\n### Related issues\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n","hints_text":"I'd like to help on this issue. Could you assign it to me? \n@choo121600 go for it!\nI'm planning to move the i18n translation files from the UI bundle (airflow-core/src/airflow/ui/src/i18n/locales/{lang}/{ns}.json) to the backend.\nId like to get input on the best place to put the directory.\n\nFor example:\n1. `api_fastapi/core_api/i18n/locales/{lang}/{ns}.json`\n2. `api_fastapi/common/static/i18n/locales/{lang}/{ns}.json`\n\nOr would it be better to manage them as a core Airflow resource like `airflow-core/src/airflow/i18n/locales/`?\n@bbovenzi Do you already have any preferred location in mind?\nGood idea.\nIf I also may suggest - should we use `yaml` maybe instead of json? It's simple enough to fix all the scripting we have, and I Find json in this context overly verbose and prone to errors (especially when you copy lines and there is a dangling comma).\nThey are just static files so I think option 1 makes sense to me. What do you think @pierrejeambrun ?\nThanks for the suggestion @potiuk ! \nI totally agree that `yaml` is more human-friendly and reduces the risk of syntax errors compared to `json`, especially when translators manually edit the files.\n\nCurrently, the reason we use `json` is that i18next by default expects translation files in **json format** on the client side, and loading yaml directly in the browser would require bundling a yaml parser, which would increase the frontend bundle size something we want to avoid for performance reasons.\n\nTo get the best of both worlds, Im considering the following approach:\n- **Store and maintain translation files in yaml format on the backend** (making it easier for contributors to edit and review)\n- Serve the translations through an API endpoint that loads yaml files, **parses them on the server, and returns json responses**\n- Let the frontend continue to load translations as JSON via i18nexts standard HTTP backend without extra yaml parsing in the client\n\nThis way, we keep the translation source files developer and contributor-friendly with yaml, while preserving frontend performance and compatibility with existing i18next tooling.\n\nWould love to hear your thoughts or concerns on this approach!\nYep. 100%. I'd do it exactly the same  I figured that Json was because of the client side\nI think you can simply move the translation files to `airflow/airflow-core/src/airflow/ui/public`. When built, that will not be part of the bundle, and the output `dist` folder is already served from the backend as a static folder.\n\nFor example the `pin_100.png` image is already accessible at `/static/pin_32.png` (dev mode needs to be handled slightly differently, but that already works for other public assets, should be similar for translations)\nThanks @pierrejeambrun ! That's a great point using `airflow/airflow-core/src/airflow/ui/public` definitely keeps things simple and fits nicely with how other frontend assets are handled.\n\nIf we werent aiming to serve translation files from the backend as the issue title suggests I would have gone with that approach too. \n\nI also think the discussion around improving contributor experience (DX) with YAML, as @potiuk suggested, is very valuable. To achieve both goals, how about we take a two-step approach?\n\n1. First (For 3.1.0): We implement your simple solution now to fix the urgent bundle size problem and meet the milestone.\n2. Then (As a follow-up): I'll create a new issue to tackle the DX improvements, like YAML processing, for a future release.\n\nThis allows us to meet the 3.1.0 milestone while still planning for long-term enhancements. \nWhat does everyone think of this plan?\nSure. Switching to yaml is just a small improvement, it is not \"necessary\". And if it makes things easier/less dependencies with serving files directly it can stay like that even.\nSounds good to me.\n\n","all_hints_text":"I'd like to help on this issue. Could you assign it to me? \n@choo121600 go for it!\nI'm planning to move the i18n translation files from the UI bundle (airflow-core/src/airflow/ui/src/i18n/locales/{lang}/{ns}.json) to the backend.\nId like to get input on the best place to put the directory.\n\nFor example:\n1. `api_fastapi/core_api/i18n/locales/{lang}/{ns}.json`\n2. `api_fastapi/common/static/i18n/locales/{lang}/{ns}.json`\n\nOr would it be better to manage them as a core Airflow resource like `airflow-core/src/airflow/i18n/locales/`?\n@bbovenzi Do you already have any preferred location in mind?\nGood idea.\nIf I also may suggest - should we use `yaml` maybe instead of json? It's simple enough to fix all the scripting we have, and I Find json in this context overly verbose and prone to errors (especially when you copy lines and there is a dangling comma).\nThey are just static files so I think option 1 makes sense to me. What do you think @pierrejeambrun ?\nThanks for the suggestion @potiuk ! \nI totally agree that `yaml` is more human-friendly and reduces the risk of syntax errors compared to `json`, especially when translators manually edit the files.\n\nCurrently, the reason we use `json` is that i18next by default expects translation files in **json format** on the client side, and loading yaml directly in the browser would require bundling a yaml parser, which would increase the frontend bundle size something we want to avoid for performance reasons.\n\nTo get the best of both worlds, Im considering the following approach:\n- **Store and maintain translation files in yaml format on the backend** (making it easier for contributors to edit and review)\n- Serve the translations through an API endpoint that loads yaml files, **parses them on the server, and returns json responses**\n- Let the frontend continue to load translations as JSON via i18nexts standard HTTP backend without extra yaml parsing in the client\n\nThis way, we keep the translation source files developer and contributor-friendly with yaml, while preserving frontend performance and compatibility with existing i18next tooling.\n\nWould love to hear your thoughts or concerns on this approach!\nYep. 100%. I'd do it exactly the same  I figured that Json was because of the client side\nI think you can simply move the translation files to `airflow/airflow-core/src/airflow/ui/public`. When built, that will not be part of the bundle, and the output `dist` folder is already served from the backend as a static folder.\n\nFor example the `pin_100.png` image is already accessible at `/static/pin_32.png` (dev mode needs to be handled slightly differently, but that already works for other public assets, should be similar for translations)\nThanks @pierrejeambrun ! That's a great point using `airflow/airflow-core/src/airflow/ui/public` definitely keeps things simple and fits nicely with how other frontend assets are handled.\n\nIf we werent aiming to serve translation files from the backend as the issue title suggests I would have gone with that approach too. \n\nI also think the discussion around improving contributor experience (DX) with YAML, as @potiuk suggested, is very valuable. To achieve both goals, how about we take a two-step approach?\n\n1. First (For 3.1.0): We implement your simple solution now to fix the urgent bundle size problem and meet the milestone.\n2. Then (As a follow-up): I'll create a new issue to tackle the DX improvements, like YAML processing, for a future release.\n\nThis allows us to meet the 3.1.0 milestone while still planning for long-term enhancements. \nWhat does everyone think of this plan?\nSure. Switching to yaml is just a small improvement, it is not \"necessary\". And if it makes things easier/less dependencies with serving files directly it can stay like that even.\nSounds good to me.\n\n","commit_urls":["https://github.com/apache/airflow/commit/13ab18227be2260a1cdfbafaa1c9ab1c43af6dde","https://github.com/apache/airflow/commit/0a906231f083ce173b040c0a03120e9b91c48fc2","https://github.com/apache/airflow/commit/2b8be2a800b3f58f06b7c0ff2ffe77250906a488","https://github.com/apache/airflow/commit/40ed727d9f90b0dc1a41dde9406808c7c664dcbc","https://github.com/apache/airflow/commit/a28bb14ce6ff4facc5d4d04a1ae3fcb32623f54d","https://github.com/apache/airflow/commit/6ad821d0cc586cc7241358af546f687cedd9857e","https://github.com/apache/airflow/commit/a70a358c3ed38da936f3e9eb576af667a25faf12","https://github.com/apache/airflow/commit/48fcb91598f13fafac8dcecb8600e4d2beb6bcf9","https://github.com/apache/airflow/commit/50267c34d5cd661936be278cfd3c9a7f96b5f473","https://github.com/apache/airflow/commit/2010dcfaef38093178674c44cae00736acf45f23","https://github.com/apache/airflow/commit/71cfb8dec35efd2016f24236518fff77b6a6e503","https://github.com/apache/airflow/commit/fda512b1900262dcf4dce8d990f101d0cadc4a97","https://github.com/apache/airflow/commit/0a0818e035a10936293bc2e7fd686a6421bac36a","https://github.com/apache/airflow/commit/efb2a730d19e9d774e30db03c90bc9a02260ea16","https://github.com/apache/airflow/commit/6376d32ec90d835453a097f96452c506d34ae286","https://github.com/apache/airflow/commit/636cfcbb404aa5d0cd592890ea74049585b404e8","https://github.com/apache/airflow/commit/ee055712ad6dd4cb41e91ae42271f6a3c0d68865","https://github.com/apache/airflow/commit/fea261a5544d67ee2bd02906a6236c3ae43cf69a","https://github.com/apache/airflow/commit/f36856d46c23828df97537e19be4643b746be2e2"],"created_at":"2025-06-14T15:46:21Z","classification":"Efficiency"}
{"repo":"apache/airflow","pull_number":51268,"instance_id":"apache__airflow-51268","issue_numbers":[51156],"base_commit":"1088a9ce71fe927caf8240d2ad5605f1aedf7da5","patch":"diff --git a/airflow-core/src/airflow/utils/db_cleanup.py b/airflow-core/src/airflow/utils/db_cleanup.py\nindex 48df00ef6ab5b..77fa2a2147382 100644\n--- a/airflow-core/src/airflow/utils/db_cleanup.py\n+++ b/airflow-core/src/airflow/utils/db_cleanup.py\n@@ -160,7 +160,11 @@ def _dump_table_to_file(*, target_table: str, file_path: str, export_format: str\n             csv_writer = csv.writer(f)\n             cursor = session.execute(text(f\"SELECT * FROM {target_table}\"))\n             csv_writer.writerow(cursor.keys())\n-            csv_writer.writerows(cursor.fetchall())\n+            BATCH_SIZE = 500\n+            rows = cursor.fetchmany(BATCH_SIZE)\n+            while rows:\n+                csv_writer.writerows(rows)\n+                rows = cursor.fetchmany(BATCH_SIZE)\n     else:\n         raise AirflowException(f\"Export format {export_format} is not supported.\")\n \n","test_patch":"diff --git a/airflow-core/tests/unit/utils/test_db_cleanup.py b/airflow-core/tests/unit/utils/test_db_cleanup.py\nindex de78e0f938a24..e479d621c1759 100644\n--- a/airflow-core/tests/unit/utils/test_db_cleanup.py\n+++ b/airflow-core/tests/unit/utils/test_db_cleanup.py\n@@ -538,15 +538,25 @@ def test_export_archived_no_confirm_if_no_tables(\n     @patch(\"airflow.utils.db_cleanup.csv\")\n     def test_dump_table_to_file_function_for_csv(self, mock_csv):\n         mockopen = mock_open()\n+        mock_cursor = MagicMock()\n+        mock_session = MagicMock()\n+        mock_session.execute.return_value = mock_cursor\n+        mock_cursor.keys.return_value = [\"test-col-1\", \"test-col-2\"]\n+        mock_cursor.fetchmany.side_effect = [\n+            [(\"testval-1.1\", \"testval-1.2\"), (\"testval-2.1\", \"testval-2.2\")],\n+            [],\n+        ]\n         with patch(\"airflow.utils.db_cleanup.open\", mockopen, create=True):\n             _dump_table_to_file(\n-                target_table=\"mytable\", file_path=\"dags/myfile.csv\", export_format=\"csv\", session=MagicMock()\n+                target_table=\"mytable\", file_path=\"dags/myfile.csv\", export_format=\"csv\", session=mock_session\n             )\n             mockopen.assert_called_once_with(\"dags/myfile.csv\", \"w\")\n             writer = mock_csv.writer\n             writer.assert_called_once()\n-            writer.return_value.writerow.assert_called_once()\n-            writer.return_value.writerows.assert_called_once()\n+            writer.return_value.writerow.assert_called_once_with([\"test-col-1\", \"test-col-2\"])\n+            writer.return_value.writerows.assert_called_once_with(\n+                [(\"testval-1.1\", \"testval-1.2\"), (\"testval-2.1\", \"testval-2.2\")]\n+            )\n \n     def test_dump_table_to_file_raises_if_format_not_supported(self):\n         with pytest.raises(AirflowException) as exc_info:\n","problem_statement":"airflow db export-archived doesn't page results\n### Apache Airflow version\n\n3.0.1\n\n### If \"Other Airflow 2 version\" selected, which one?\n\nAll 2.x\n\n### What happened?\n\nWhile using [airflow db export-archived](https://airflow.apache.org/docs/apache-airflow/stable/howto/usage-cli.html#export-the-purged-records-from-the-archive-tables) command it fetches all the results at once instead of paging them. \n\nIn case of big instances and archiving a lot of data a lot of memory is needed to use this command, othervise it ends up with OOMErrors\n\nExact line:\nhttps://github.com/apache/airflow/blob/3.0.1/airflow-core/src/airflow/utils/db_cleanup.py#L163\n\n### What you think should happen instead?\n\nuse server-side cursor for paging if available. \n\n### How to reproduce\n\nCreate a database that has tables with the size bigger than your memory and then try to archive data with use of \nairflow db export-archived command. \n\n\n\n### Operating System\n\nK8s - pods with Ubuntu\n\n### Versions of Apache Airflow Providers\n\n_No response_\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n","hints_text":"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\n@potiuk What is your take on this. I was thinking of using a buffer including n rows and using `.fetchmany(n)` to append the rows batch by batch to the `csv_writer`. With this approach, we would only hold one batch at a time in memory. \n\nI can come up with a PR for this.\nYep. sounds like that's the right approach.\n\n","all_hints_text":"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\n@potiuk What is your take on this. I was thinking of using a buffer including n rows and using `.fetchmany(n)` to append the rows batch by batch to the `csv_writer`. With this approach, we would only hold one batch at a time in memory. \n\nI can come up with a PR for this.\nYep. sounds like that's the right approach.\n\n","commit_urls":["https://github.com/apache/airflow/commit/86c0d4170eb907405c8e2fac835eefaa61f33405"],"created_at":"2025-05-31T22:10:40Z","classification":"Efficiency"}
{"repo":"apache/airflow","pull_number":52119,"instance_id":"apache__airflow-52119","issue_numbers":[51910],"base_commit":"ac9e968fe85cf43717d280f303470eb5268ca9ca","patch":"diff --git a/providers/databricks/src/airflow/providers/databricks/hooks/databricks_base.py b/providers/databricks/src/airflow/providers/databricks/hooks/databricks_base.py\nindex b2e08ecaad989..2829c7012d32b 100644\n--- a/providers/databricks/src/airflow/providers/databricks/hooks/databricks_base.py\n+++ b/providers/databricks/src/airflow/providers/databricks/hooks/databricks_base.py\n@@ -353,14 +353,15 @@ async def _a_get_aad_token(self, resource: str) -> str:\n             async for attempt in self._a_get_retry_object():\n                 with attempt:\n                     if self.databricks_conn.extra_dejson.get(\"use_azure_managed_identity\", False):\n-                        token = await AsyncManagedIdentityCredential().get_token(f\"{resource}/.default\")\n+                        async with AsyncManagedIdentityCredential() as credential:\n+                            token = await credential.get_token(f\"{resource}/.default\")\n                     else:\n-                        credential = AsyncClientSecretCredential(\n+                        async with AsyncClientSecretCredential(\n                             client_id=self.databricks_conn.login,\n                             client_secret=self.databricks_conn.password,\n                             tenant_id=self.databricks_conn.extra_dejson[\"azure_tenant_id\"],\n-                        )\n-                        token = await credential.get_token(f\"{resource}/.default\")\n+                        ) as credential:\n+                            token = await credential.get_token(f\"{resource}/.default\")\n                     jsn = {\n                         \"access_token\": token.token,\n                         \"token_type\": \"Bearer\",\n","test_patch":"diff --git a/providers/databricks/tests/unit/databricks/hooks/test_databricks.py b/providers/databricks/tests/unit/databricks/hooks/test_databricks.py\nindex 2698a12b62946..c4b0b427d7dd6 100644\n--- a/providers/databricks/tests/unit/databricks/hooks/test_databricks.py\n+++ b/providers/databricks/tests/unit/databricks/hooks/test_databricks.py\n@@ -1907,7 +1907,7 @@ async def test_get_run_state(self, mock_azure_identity, mock_get):\n @pytest.mark.db_test\n class TestDatabricksHookAsyncAadTokenOtherClouds:\n     \"\"\"\n-    Tests for DatabricksHook using async methodswhen auth is done with AAD token\n+    Tests for DatabricksHook using async methods when auth is done with AAD token\n     for SP as user inside workspace and using non-global Azure cloud (China, GovCloud, Germany)\n     \"\"\"\n \n@@ -1934,12 +1934,18 @@ def setup_connections(self, create_connection_without_db):\n         self.hook = DatabricksHook(retry_args=DEFAULT_RETRY_ARGS)\n \n     @pytest.mark.asyncio\n+    @mock.patch(\"azure.identity.aio.ClientSecretCredential\")\n     @mock.patch(\"airflow.providers.databricks.hooks.databricks_base.aiohttp.ClientSession.get\")\n-    @mock.patch(\"azure.identity.aio.ClientSecretCredential.__init__\")\n-    @mock.patch(\"azure.identity.aio.ClientSecretCredential.get_token\")\n-    async def test_get_run_state(self, mock_azure_identity_get_token, mock_azure_identity, mock_get):\n-        mock_azure_identity.return_value = None\n-        mock_azure_identity_get_token.return_value = create_aad_token_for_resource()\n+    async def test_get_run_state(self, mock_get, mock_client_secret_credential_class):\n+        mock_credential = mock.Mock()\n+        mock_credential.get_token = AsyncMock(return_value=create_aad_token_for_resource())\n+\n+        mock_context_manager = mock.AsyncMock()\n+        mock_context_manager.__aenter__.return_value = mock_credential\n+        mock_context_manager.__aexit__.return_value = AsyncMock()\n+\n+        mock_client_secret_credential_class.return_value = mock_context_manager\n+\n         mock_get.return_value.__aenter__.return_value.json = AsyncMock(return_value=GET_RUN_RESPONSE)\n \n         async with self.hook:\n@@ -1947,12 +1953,11 @@ async def test_get_run_state(self, mock_azure_identity_get_token, mock_azure_ide\n \n         assert run_state == RunState(LIFE_CYCLE_STATE, RESULT_STATE, STATE_MESSAGE)\n \n-        azure_identity_args = mock_azure_identity.call_args.kwargs\n-        assert azure_identity_args[\"tenant_id\"] == self.tenant_id\n-        assert azure_identity_args[\"client_id\"] == self.client_id\n+        credential_call_kwargs = mock_client_secret_credential_class.call_args.kwargs\n+        assert credential_call_kwargs[\"tenant_id\"] == self.tenant_id\n+        assert credential_call_kwargs[\"client_id\"] == self.client_id\n \n-        get_token_args = mock_azure_identity_get_token.call_args_list\n-        assert get_token_args == [mock.call(f\"{DEFAULT_DATABRICKS_SCOPE}/.default\")]\n+        mock_credential.get_token.assert_called_once_with(f\"{DEFAULT_DATABRICKS_SCOPE}/.default\")\n \n         mock_get.assert_called_once_with(\n             get_run_endpoint(HOST),\n@@ -1992,11 +1997,21 @@ def setup_connections(self, create_connection_without_db):\n \n     @pytest.mark.asyncio\n     @mock.patch(\"airflow.providers.databricks.hooks.databricks_base.aiohttp.ClientSession.get\")\n-    @mock.patch(\"azure.identity.aio.ClientSecretCredential.__init__\")\n-    @mock.patch(\"azure.identity.aio.ClientSecretCredential.get_token\")\n-    async def test_get_run_state(self, mock_azure_identity_get_token, mock_azure_identity, mock_get):\n-        mock_azure_identity.return_value = None\n-        mock_azure_identity_get_token.return_value = create_aad_token_for_resource()\n+    @mock.patch(\"azure.identity.aio.ClientSecretCredential\")\n+    async def test_get_run_state(self, mock_client_secret_credential_class, mock_get):\n+        mock_credential = mock.Mock()\n+        mock_credential.get_token = AsyncMock(\n+            side_effect=[\n+                create_aad_token_for_resource(),\n+                create_aad_token_for_resource(),\n+            ]\n+        )\n+\n+        mock_cm = mock.AsyncMock()\n+        mock_cm.__aenter__.return_value = mock_credential\n+        mock_cm.__aexit__.return_value = AsyncMock()\n+        mock_client_secret_credential_class.return_value = mock_cm\n+\n         mock_get.return_value.__aenter__.return_value.json = AsyncMock(return_value=GET_RUN_RESPONSE)\n \n         async with self.hook:\n@@ -2004,12 +2019,11 @@ async def test_get_run_state(self, mock_azure_identity_get_token, mock_azure_ide\n \n         assert run_state == RunState(LIFE_CYCLE_STATE, RESULT_STATE, STATE_MESSAGE)\n \n-        azure_identity_args = mock_azure_identity.call_args.kwargs\n-        assert azure_identity_args[\"tenant_id\"] == self.tenant_id\n-        assert azure_identity_args[\"client_id\"] == self.client_id\n+        credential_call_kwargs = mock_client_secret_credential_class.call_args.kwargs\n+        assert credential_call_kwargs[\"tenant_id\"] == self.tenant_id\n+        assert credential_call_kwargs[\"client_id\"] == self.client_id\n \n-        get_token_args = mock_azure_identity_get_token.call_args_list\n-        assert get_token_args == [\n+        assert mock_credential.get_token.await_args_list == [\n             mock.call(f\"{AZURE_MANAGEMENT_ENDPOINT}/.default\"),\n             mock.call(f\"{DEFAULT_DATABRICKS_SCOPE}/.default\"),\n         ]\n","problem_statement":"Unclosed aiohttp ClientSession and TCPConnector in DatabricksRunNowOperator with deferrable=True (Airflow 3.0.2, Databricks Provider 7.4.0)\n### Apache Airflow Provider(s)\n\ndatabricks\n\n### Versions of Apache Airflow Providers\n\n7.4.0\n\n### Apache Airflow version\n\n3.0.2\n\n### Operating System\n\nUbuntu 24.04.2 LTS\n\n### Deployment\n\nVirtualenv installation\n\n### Deployment details\n\n- **Deployment Type**: Virtualenv installation\n- **Operating System**: Ubuntu 24.04.2 LTS\n- **Python Version**: 3.12.3\n- **Airflow Version**: 3.0.2\n- **Databricks Provider Version**: 7.4.0\n- **Database Backend**: PostgreSQL 16\n- **Secrets Backend**: Microsoft Azure Key Vault\n- **Authentication**: Flask AppBuilder (FAB) with Microsoft Entra ID (SSO)\n- **SSL Configuration**: Enabled with custom certificates\n- **Timezone**: Pacific/Auckland\n- **Airflow Services Management**: systemd unit files for `api-server`, `scheduler`, `dag-processor`, and `triggerer`\n- **Custom Configuration Highlights**:\n  - Airflow configuration (`airflow.cfg`) includes:\n    - `sql_alchemy_conn_secret` for DB connection string\n    - Azure Key Vault integration for secrets\n    - SSL cert/key paths\n    - FAB auth manager\n  - Environment variables for Azure credentials (`AZURE_CLIENT_ID`, `AZURE_TENANT_ID`, `AZURE_CLIENT_SECRET`)\n  - Custom `webserver_config.py` for SSO\n  - Firewall configured to allow port 8443\n\nInstallation followed a manual setup process using a Python virtual environment and systemd for service orchestration. All dependencies were installed using pip with Airflow constraints for version compatibility.\n\n### What happened\n\nWhile running a DAG using `DatabricksRunNowOperator` with `deferrable=True` in Airflow 3.0.2 and Databricks Provider 7.4.0, the task successfully triggered a Databricks job and completed with a `SUCCESS` state. However, during the execution, the Airflow logs showed repeated warnings and errors related to unclosed `aiohttp` client sessions and connectors:\n\n```\n[2025-06-19, 02:21:23] ERROR - Unclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x7784a33f1250>: source=\"asyncio\"\n[2025-06-19, 02:21:23] ERROR - Unclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7784a33c6990>, 341232.543125349)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x7784a33f1880>: source=\"asyncio\"\n```\n\nThese errors appeared shortly after the task was deferred and while the `DatabricksExecutionTrigger` was polling the job status. Despite the job completing successfully, the presence of these warnings suggests a potential resource leak or improper cleanup of async HTTP sessions in the provider code.\n\nThis issue occurred in a production-like environment using:\n- Airflow 3.0.2 (virtualenv installation)\n- Python 3.12.3\n- Ubuntu 24.04.2 LTS\n- Databricks Provider 7.4.0\n- PostgreSQL 16 as backend\n- Azure Key Vault as secrets backend\n\nNo custom modifications were made to the operator or trigger logic. The DAG was manually triggered and ran without retries.\n\n### What you think should happen instead\n\nThe task completes successfully, but the error messages in the logs indicates improper resource management in the Databricks provider:\n\n```\n[2025-06-19, 02:21:23] ERROR - Unclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x7784a33f1250>: source=\"asyncio\"\n[2025-06-19, 02:21:23] ERROR - Unclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7784a33c6990>, 341232.543125349)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x7784a33f1880>: source=\"asyncio\"\n```\n\nThese errors suggest that the `aiohttp.ClientSession` and `TCPConnector` objects used during the async polling in `DatabricksExecutionTrigger` are not being properly closed or garbage collected. This could lead to memory leaks or degraded performance over time, especially in long-running Airflow environments.\n\nSince the operator is marked as `deferrable=True`, it should follow best practices for async resource cleanup. I expect that:\n- All async HTTP sessions and connectors are properly closed after use.\n- No warnings or errors related to unclosed resources appear in the logs.\n- The operator and trigger behave cleanly and efficiently without leaving behind dangling resources.\n\nThis behavior is erroneous because it violates expected resource lifecycle management in asynchronous Python code and could impact system stability.\n\n### How to reproduce\n\nTo reproduce the issue, follow these steps in a clean Airflow 3.0.2 environment with the Databricks Provider version 7.4.0:\n\n1. **Set up Airflow**:\n   - Install Airflow 3.0.2 in a Python 3.12 virtual environment.\n   - Use PostgreSQL 16 as the metadata database.\n   - Configure Airflow to use Microsoft Azure Key Vault as the secrets backend.\n   - Enable SSL and set up Flask AppBuilder (FAB) authentication.\n\n2. **Create a DAG** with the following characteristics:\n   - Uses `DatabricksRunNowOperator`\n   - Sets `deferrable=True`\n   - Specifies a valid `job_id` for a Databricks job\n   - Uses a valid `databricks_conn_id` pointing to an Azure Databricks workspace\n\n  My DAG snippet:\n```python\nfrom datetime import datetime, timedelta\n\nfrom airflow.models import DAG\n\nfrom airflow.providers.databricks.operators.databricks import (\n    DatabricksRunNowOperator,\n)\n\nwith DAG(\n    dag_id=\"sit_trigger_databricks_job_run\",\n    description=\"System Integration Testing - Trigger a Databricks job run\",\n    start_date=datetime.now() - timedelta(days=1),\n    schedule=None,  # timedelta(days=1),\n    catchup=False,\n    default_args={\n        \"retries\": 0,\n        \"retry_delay\": timedelta(minutes=1),\n        \"databricks_conn_id\": \"databricks-default2\",\n    },\n    tags=[\"sit\", \"databricks\"],\n) as dag:\n\n    trigger_databricks_job_run = DatabricksRunNowOperator(\n        task_id=\"trigger_databricks_job_run\",\n        job_id=604918372746183,\n        deferrable=True,\n        execution_timeout=timedelta(hours=3),\n    )\n```\n\n3. **Trigger the DAG manually** via the Airflow UI or CLI.\n\n4. **Observe the logs** of the task. You will see the following error messages even though the job completes successfully:\n   ```\n   ERROR - Unclosed client session\n   ERROR - Unclosed connector\n   ```\n\nThis issue is reproducible consistently in environments using async deferrable operators with the Databricks provider.\n\n### Anything else\n\nThis issue occurs **every time** the DAG is triggered using `DatabricksRunNowOperator` with `deferrable=True`. The error messages consistently appear during the polling phase managed by `DatabricksExecutionTrigger`.\n\nHere are the relevant full log:\n\n```log\n[2025-06-19, 02:21:16] INFO - DAG bundles loaded: dags-folder: source=\"airflow.dag_processing.bundles.manager.DagBundlesManager\"\n[2025-06-19, 02:21:16] INFO - Filling up the DagBag from /home/airflow/airflow/dags/sit_trigger_databricks_job_run.py: source=\"airflow.models.dagbag.DagBag\"\n[2025-06-19, 02:21:16] INFO - Environment is configured for ClientSecretCredential: source=\"azure.identity._credentials.environment\"\n[2025-06-19, 02:21:16] INFO - ManagedIdentityCredential will use IMDS with client_id: 3a9f1c84-7b3d-4e2a-a9d1-8f6c3e2b7f90: source=\"azure.identity._credentials.managed_identity\"\n[2025-06-19, 02:21:17] INFO - DefaultAzureCredential acquired a token from EnvironmentCredential: source=\"azure.identity._credentials.chained\"\n[2025-06-19, 02:21:17] INFO - Connection Retrieved 'databricks-default2': source=\"airflow.hooks.base\"\n[2025-06-19, 02:21:17] INFO - Existing AAD token is expired, or going to expire soon. Refreshing...: source=\"airflow.task.hooks.airflow.providers.databricks.hooks.databricks.DatabricksHook\"\n[2025-06-19, 02:21:18] INFO - ClientSecretCredential.get_token succeeded: source=\"azure.identity._internal.get_token_mixin\"\n[2025-06-19, 02:21:19] INFO - Run submitted with run_id: 748193847562019: source=\"airflow.task.operators.airflow.providers.databricks.operators.databricks.DatabricksRunNowOperator\"\n[2025-06-19, 02:21:19] INFO - View run status, Spark UI, and logs at https://adb-8274910385627419.47.azuredatabricks.net/?o=9182736450192837#job/604918372746183/run/748193847562019: source=\"airflow.task.operators.airflow.providers.databricks.operators.databricks.DatabricksRunNowOperator\"\n[2025-06-19, 02:21:20] INFO - Pausing task as DEFERRED. : dag_id=\"sit_trigger_databricks_job_run\": task_id=\"trigger_databricks_job_run\": run_id=\"manual__2025-06-19T02:21:11.927855+00:00\": source=\"task\"\n[2025-06-19, 02:21:20] INFO - trigger sit_trigger_databricks_job_run/manual__2025-06-19T02:21:11.927855+00:00/trigger_databricks_job_run/-1/1 (ID 15) starting\n[2025-06-19, 02:21:20] INFO - Environment is configured for ClientSecretCredential: source=\"azure.identity._credentials.environment\"\n[2025-06-19, 02:21:20] INFO - ManagedIdentityCredential will use IMDS with client_id: 3a9f1c84-7b3d-4e2a-a9d1-8f6c3e2b7f90: source=\"azure.identity._credentials.managed_identity\"\n[2025-06-19, 02:21:21] INFO - DefaultAzureCredential acquired a token from EnvironmentCredential: source=\"azure.identity._credentials.chained\"\n[2025-06-19, 02:21:23] INFO - Connection Retrieved 'databricks-default2': source=\"airflow.hooks.base\"\n[2025-06-19, 02:21:23] INFO - Existing AAD token is expired, or going to expire soon. Refreshing...: source=\"airflow.task.hooks.airflow.providers.databricks.hooks.databricks.DatabricksHook\"\n[2025-06-19, 02:21:23] INFO - ClientSecretCredential.get_token succeeded: source=\"azure.identity.aio._internal.get_token_mixin\"\n[2025-06-19, 02:21:23] ERROR - Unclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x7784a33f1250>: source=\"asyncio\"\n[2025-06-19, 02:21:23] ERROR - Unclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7784a33c6990>, 341232.543125349)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x7784a33f1880>: source=\"asyncio\"\n[2025-06-19, 02:21:23] INFO - run-id 748193847562019 in run state {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}. sleeping for 30 seconds: source=\"airflow.providers.databricks.triggers.databricks.DatabricksExecutionTrigger\"\n[2025-06-19, 02:21:54] INFO - run-id 748193847562019 in run state {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}. sleeping for 30 seconds: source=\"airflow.providers.databricks.triggers.databricks.DatabricksExecutionTrigger\"\n[2025-06-19, 02:22:24] INFO - run-id 748193847562019 in run state {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}. sleeping for 30 seconds: source=\"airflow.providers.databricks.triggers.databricks.DatabricksExecutionTrigger\"\n[2025-06-19, 02:22:54] INFO - run-id 748193847562019 in run state {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}. sleeping for 30 seconds: source=\"airflow.providers.databricks.triggers.databricks.DatabricksExecutionTrigger\"\n[2025-06-19, 02:23:25] INFO - run-id 748193847562019 in run state {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}. sleeping for 30 seconds: source=\"airflow.providers.databricks.triggers.databricks.DatabricksExecutionTrigger\"\n[2025-06-19, 02:23:55] INFO - run-id 748193847562019 in run state {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}. sleeping for 30 seconds: source=\"airflow.providers.databricks.triggers.databricks.DatabricksExecutionTrigger\"\n[2025-06-19, 02:24:26] INFO - run-id 748193847562019 in run state {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}. sleeping for 30 seconds: source=\"airflow.providers.databricks.triggers.databricks.DatabricksExecutionTrigger\"\n[2025-06-19, 02:24:56] INFO - run-id 748193847562019 in run state {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}. sleeping for 30 seconds: source=\"airflow.providers.databricks.triggers.databricks.DatabricksExecutionTrigger\"\n[2025-06-19, 02:25:26] INFO - run-id 748193847562019 in run state {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': ''}. sleeping for 30 seconds: source=\"airflow.providers.databricks.triggers.databricks.DatabricksExecutionTrigger\"\n[2025-06-19, 02:25:56] INFO - Trigger fired event: name=\"sit_trigger_databricks_job_run/manual__2025-06-19T02:21:11.927855+00:00/trigger_databricks_job_run/-1/1 (ID 15)\": result=\"TriggerEvent<{'run_id': 748193847562019, 'run_page_url': 'https://adb-8274910385627419.47.azuredatabricks.net/?o=9182736450192837#job/604918372746183/run/748193847562019', 'run_state': '{\\\"life_cycle_state\\\": \\\"TERMINATED\\\", \\\"result_state\\\": \\\"SUCCESS\\\", \\\"state_message\\\": \\\"\\\"}', 'repair_run': False, 'errors': []}>\"\n```\n\nThese errors appear shortly after the task is deferred and while the trigger is actively polling the Databricks job status. The job itself completes successfully, but the logs indicate a potential issue with async resource cleanup.\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n","hints_text":"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\nHi team, I was able to reproduce this issue using the latest stable versions:\n\napache-airflow==3.0.2\n\napache-airflow-providers-databricks==7.4.0\n\nAuthentication: Azure AAD with ClientSecretCredential\n\nOperator: DatabricksRunNowOperator(deferrable=True)\n\nWhen the task is deferred, I still see the following warning right after token refresh succeeds:\n\n```\n[2025-06-20, 22:53:02] INFO - ClientSecretCredential.get_token succeeded: source=\"azure.identity.aio._internal.get_token_mixin\"\n[2025-06-20, 22:53:02] ERROR - Unclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x7f8db1fac590>: source=\"asyncio\"\n[20[25](http://localhost:8080/dags/test_deferrable_databricks/runs/manual__2025-06-21T03:52:54.867109+00:00/tasks/run_now_task?try_number=1#25)-06-20, 22:53:02] \nERROR - Unclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7f8db1f0bb90>, 784043.395003365)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x7f8db11cab10>: source=\"asyncio\"\n```\n\nThe run-id is generated successfully, and the job proceeds, but this warning appears right after:\n```\nPausing task as DEFERRED...\nClientSecretCredential.get_token succeeded...\n<unclosed session warning appears here>\n```\n\nI am working on a potential fix for this issue  it would be my first contribution to Airflow.\nCould you please assign it to me?  I'd love to contribute! @albertwangnz \nHi @potiuk,\n\nI tried but failed to fix the issue. @SalikramPaudel would like to help. Can I or you assign the issue to him?\n\nCheers.\n\nRegards,\nAlbert\nassigned\n\n","all_hints_text":"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\nHi team, I was able to reproduce this issue using the latest stable versions:\n\napache-airflow==3.0.2\n\napache-airflow-providers-databricks==7.4.0\n\nAuthentication: Azure AAD with ClientSecretCredential\n\nOperator: DatabricksRunNowOperator(deferrable=True)\n\nWhen the task is deferred, I still see the following warning right after token refresh succeeds:\n\n```\n[2025-06-20, 22:53:02] INFO - ClientSecretCredential.get_token succeeded: source=\"azure.identity.aio._internal.get_token_mixin\"\n[2025-06-20, 22:53:02] ERROR - Unclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x7f8db1fac590>: source=\"asyncio\"\n[20[25](http://localhost:8080/dags/test_deferrable_databricks/runs/manual__2025-06-21T03:52:54.867109+00:00/tasks/run_now_task?try_number=1#25)-06-20, 22:53:02] \nERROR - Unclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7f8db1f0bb90>, 784043.395003365)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x7f8db11cab10>: source=\"asyncio\"\n```\n\nThe run-id is generated successfully, and the job proceeds, but this warning appears right after:\n```\nPausing task as DEFERRED...\nClientSecretCredential.get_token succeeded...\n<unclosed session warning appears here>\n```\n\nI am working on a potential fix for this issue  it would be my first contribution to Airflow.\nCould you please assign it to me?  I'd love to contribute! @albertwangnz \nHi @potiuk,\n\nI tried but failed to fix the issue. @SalikramPaudel would like to help. Can I or you assign the issue to him?\n\nCheers.\n\nRegards,\nAlbert\nassigned\n\n","commit_urls":["https://github.com/apache/airflow/commit/8f506e2a0837600a81632505063c378fd6f89a62","https://github.com/apache/airflow/commit/8b263af7105a86d7709eda53b479a88d431ac62a","https://github.com/apache/airflow/commit/0e015f65b6d30d46793a25bdc5ea3ec5b455ac0a","https://github.com/apache/airflow/commit/1d1b4e46f9c3d411e6e2ca2f5ce2441b67d55ff9"],"created_at":"2025-06-23T22:59:34Z","classification":"Efficiency"}
{"repo":"apache/airflow","pull_number":52952,"instance_id":"apache__airflow-52952","issue_numbers":[52922],"base_commit":"e311c6bf990bb9ab91afe1620e62baf19866d493","patch":"diff --git a/providers/edge3/src/airflow/providers/edge3/plugins/edge_executor_plugin.py b/providers/edge3/src/airflow/providers/edge3/plugins/edge_executor_plugin.py\nindex 9c8b32eadea33..bbae741062384 100644\n--- a/providers/edge3/src/airflow/providers/edge3/plugins/edge_executor_plugin.py\n+++ b/providers/edge3/src/airflow/providers/edge3/plugins/edge_executor_plugin.py\n@@ -17,6 +17,7 @@\n \n from __future__ import annotations\n \n+import sys\n from typing import TYPE_CHECKING, Any\n \n from airflow.configuration import conf\n@@ -213,12 +214,22 @@ def change_maintenance_comment(self, worker_name: str):\n except AirflowConfigException:\n     EDGE_EXECUTOR_ACTIVE = False\n \n+# Load the API endpoint only on api-server (Airflow 3.x) or webserver (Airflow 2.x)\n+# todo(jscheffl): Remove this check when the discussion in\n+#                 https://lists.apache.org/thread/w170czq6r7bslkqp1tk6bjjjo0789wgl\n+#                 resulted in a proper API to selective initialize. Maybe backcompat-shim\n+#                 is also needed to support Airflow-versions prior the rework.\n+if AIRFLOW_V_3_0_PLUS:\n+    RUNNING_ON_APISERVER = sys.argv[1] in [\"api-server\"] if len(sys.argv) > 1 else False\n+else:\n+    RUNNING_ON_APISERVER = \"gunicorn\" in sys.argv[0] and \"airflow-webserver\" in sys.argv\n+\n \n class EdgeExecutorPlugin(AirflowPlugin):\n     \"\"\"EdgeExecutor Plugin - provides API endpoints for Edge Workers in Webserver.\"\"\"\n \n     name = \"edge_executor\"\n-    if EDGE_EXECUTOR_ACTIVE:\n+    if EDGE_EXECUTOR_ACTIVE and RUNNING_ON_APISERVER:\n         if AIRFLOW_V_3_0_PLUS:\n             fastapi_apps = [_get_api_endpoint()]\n         else:\n","test_patch":"diff --git a/providers/edge3/tests/unit/edge3/plugins/test_edge_executor_plugin.py b/providers/edge3/tests/unit/edge3/plugins/test_edge_executor_plugin.py\nindex 99e5e24722e77..c5a4c6782f910 100644\n--- a/providers/edge3/tests/unit/edge3/plugins/test_edge_executor_plugin.py\n+++ b/providers/edge3/tests/unit/edge3/plugins/test_edge_executor_plugin.py\n@@ -17,6 +17,7 @@\n from __future__ import annotations\n \n import importlib\n+from unittest.mock import patch\n \n import pytest\n import time_machine\n@@ -44,17 +45,20 @@ def test_plugin_inactive():\n \n \n @pytest.mark.db_test\n-def test_plugin_active():\n-    with conf_vars({(\"edge\", \"api_enabled\"): \"true\"}):\n+def test_plugin_active_apiserver():\n+    mock_cli = [\"airflow\", \"api-server\"] if AIRFLOW_V_3_0_PLUS else [\"gunicorn\", \"airflow-webserver\"]\n+    with conf_vars({(\"edge\", \"api_enabled\"): \"true\"}), patch(\"sys.argv\", mock_cli):\n         importlib.reload(edge_executor_plugin)\n \n         from airflow.providers.edge3.plugins.edge_executor_plugin import (\n             EDGE_EXECUTOR_ACTIVE,\n+            RUNNING_ON_APISERVER,\n             EdgeExecutorPlugin,\n         )\n \n         rep = EdgeExecutorPlugin()\n         assert EDGE_EXECUTOR_ACTIVE\n+        assert RUNNING_ON_APISERVER\n         if AIRFLOW_V_3_0_PLUS:\n             assert len(rep.appbuilder_views) == 0\n             assert len(rep.flask_blueprints) == 0\n@@ -64,6 +68,27 @@ def test_plugin_active():\n             assert len(rep.flask_blueprints) == 2\n \n \n+@patch(\"sys.argv\", [\"airflow\", \"some-other-command\"])\n+def test_plugin_active_non_apiserver():\n+    with conf_vars({(\"edge\", \"api_enabled\"): \"true\"}):\n+        importlib.reload(edge_executor_plugin)\n+\n+        from airflow.providers.edge3.plugins.edge_executor_plugin import (\n+            EDGE_EXECUTOR_ACTIVE,\n+            RUNNING_ON_APISERVER,\n+            EdgeExecutorPlugin,\n+        )\n+\n+        rep = EdgeExecutorPlugin()\n+        assert EDGE_EXECUTOR_ACTIVE\n+        assert not RUNNING_ON_APISERVER\n+        assert len(rep.appbuilder_views) == 0\n+        assert len(rep.flask_blueprints) == 0\n+        assert len(rep.appbuilder_views) == 0\n+        if AIRFLOW_V_3_0_PLUS:\n+            assert len(rep.fastapi_apps) == 0\n+\n+\n @pytest.fixture\n def plugin():\n     from airflow.providers.edge3.plugins.edge_executor_plugin import EdgeExecutorPlugin\n","problem_statement":"Task Runner loads Plugins and fails in DB Connections\n### Apache Airflow version\n\nmain (development) as well as 3.0.3rc3\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWhen running providers/edge3/src/airflow/providers/edge3/example_dags/integration_test.py Dag with EdgeExecutor for every task it seems the plugin manager is initialized and implicitly when loading plugins a stack trace is generated faailing DB connections. Stack trace in every task execution  seen is:\n```\nLog message source details: sources=[\"/root/airflow/logs/dag_id=integration_test/run_id=manual__2025-07-05T19-37-25.606441+00-00/task_id=classic_python/attempt=1.log\"]\n[2025-07-05, 21:37:53] ERROR - Failed to import plugin edge_executor: source=\"airflow.plugins_manager\"\nRuntimeError: Direct database access via the ORM is not allowed in Airflow 3.0\nFile \"/opt/airflow/airflow-core/src/airflow/plugins_manager.py\", line 260 in load_entrypoint_plugins\nFile \"/usr/local/lib/python3.12/importlib/metadata/__init__.py\", line 205 in load\nFile \"/usr/local/lib/python3.12/importlib/__init__.py\", line 90 in import_module\nFile \"<frozen importlib._bootstrap>\", line 1387 in _gcd_import\nFile \"<frozen importlib._bootstrap>\", line 1360 in _find_and_load\nFile \"<frozen importlib._bootstrap>\", line 1331 in _find_and_load_unlocked\nFile \"<frozen importlib._bootstrap>\", line 935 in _load_unlocked\nFile \"<frozen importlib._bootstrap_external>\", line 999 in exec_module\nFile \"<frozen importlib._bootstrap>\", line 488 in _call_with_frames_removed\nFile \"/opt/airflow/providers/edge3/src/airflow/providers/edge3/plugins/edge_executor_plugin.py\", line 217 in <module>\nFile \"/opt/airflow/providers/edge3/src/airflow/providers/edge3/plugins/edge_executor_plugin.py\", line 223 in EdgeExecutorPlugin\nFile \"/opt/airflow/airflow-core/src/airflow/utils/session.py\", line 100 in wrapper\nFile \"/usr/local/lib/python3.12/contextlib.py\", line 137 in __enter__\nFile \"/opt/airflow/airflow-core/src/airflow/utils/session.py\", line 41 in create_session\nFile \"/opt/airflow/task-sdk/src/airflow/sdk/execution_time/supervisor.py\", line 251 in __init__\n```\n\nNote: This is not happening in LocalExecutor, seems the plugins are initialized still when DB connections are not dropped.\n\n### What you think should happen instead?\n\nNo stack trace produced as well as I am wondering that plugins manager is initialized at-all because this will eat up a lot of runtime to load all hooks and plugins from all providers.\n\n### How to reproduce\n\nStarted breeze with `breeze start-airflow --python 3.12 --backend postgres --executor EdgeExecutor --load-example-dags`\nAdded integration_test Dag to files/dags\nStart a run and any task will have this exception\n\n### Operating System\n\nUbuntu 22.04\n\n### Versions of Apache Airflow Providers\n\nAll from main\n\n### Deployment\n\nOther\n\n### Deployment details\n\nBreeze\n\n### Anything else?\n\nIs this also related to #51873 ?\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n","hints_text":"Note: Also checked and saw the same error on 3.0.3rc3\nOkay, tested with 3.0.2 and this is not related to Task-SDK actually. Seems to be a regression of #52000 as DB stuff is now explicitly initialized.\n\n","all_hints_text":"Note: Also checked and saw the same error on 3.0.3rc3\nOkay, tested with 3.0.2 and this is not related to Task-SDK actually. Seems to be a regression of #52000 as DB stuff is now explicitly initialized.\n\n","commit_urls":["https://github.com/apache/airflow/commit/073d47b7652dcd408bf6739652153fa7119e9361","https://github.com/apache/airflow/commit/2266fef982a3e65aeef16ae464b800c165facc17","https://github.com/apache/airflow/commit/67791b72279c20ecfaa4174f62f4592f093ebbb1"],"created_at":"2025-07-06T17:39:12Z","classification":"Efficiency"}
{"repo":"ray-project/ray","pull_number":52102,"instance_id":"ray-project__ray-52102","issue_numbers":[45755],"base_commit":"ff61ea3eafa728f6e5ef2a29ae9e5855567b006b","patch":"diff --git a/ci/lint/pydoclint-baseline.txt b/ci/lint/pydoclint-baseline.txt\nindex 30c316914ba9..fba958df458b 100644\n--- a/ci/lint/pydoclint-baseline.txt\n+++ b/ci/lint/pydoclint-baseline.txt\n@@ -973,7 +973,6 @@ python/ray/dashboard/modules/reporter/profile_manager.py\n     DOC111: Method `MemoryProfilingManager.detach_profiler`: The option `--arg-type-hints-in-docstring` is `False` but there are type hints in the docstring arg list\n --------------------\n python/ray/dashboard/modules/reporter/reporter_agent.py\n-    DOC103: Method `ReporterAgent.generate_worker_stats_record`: Docstring arguments are different from function arguments. (Or could be other formatting issues: https://jsh9.github.io/pydoclint/violation_codes.html#notes-on-doc103 ). Arguments in the function signature but not in the docstring: [worker_stats: List[dict]]. Arguments in the docstring but not in the function signature: [stats: ].\n     DOC201: Method `ReporterAgent.generate_worker_stats_record` does not have a return section in docstring\n --------------------\n python/ray/dashboard/modules/reporter/reporter_head.py\ndiff --git a/python/ray/_private/ray_constants.py b/python/ray/_private/ray_constants.py\nindex 0bf4f4952ce6..0816783336bd 100644\n--- a/python/ray/_private/ray_constants.py\n+++ b/python/ray/_private/ray_constants.py\n@@ -578,6 +578,11 @@ def gcs_actor_scheduling_enabled():\n #   WorkerId will be removed from all metrics.\n RAY_METRIC_CARDINALITY_LEVEL = os.environ.get(\"RAY_metric_cardinality_level\", \"legacy\")\n \n+# Whether GPU metrics collection via `nvidia-smi` is enabled.\n+# Controlled by the environment variable `RAY_metric_enable_gpu_nvsmi`.\n+# Defaults to False to use pynvml to collect usage.\n+RAY_METRIC_ENABLE_GPU_NVSMI = env_bool(\"RAY_metric_enable_gpu_nvsmi\", False)\n+\n # Whether enable OpenTelemetry as the metrics collection backend on the driver\n # component. This flag is only used during the migration of the  metric collection\n # backend from OpenCensus to OpenTelemetry. It will be removed in the future.\ndiff --git a/python/ray/dashboard/client/src/pages/metrics/Metrics.tsx b/python/ray/dashboard/client/src/pages/metrics/Metrics.tsx\nindex 78d3fd7bddba..07e7af851266 100644\n--- a/python/ray/dashboard/client/src/pages/metrics/Metrics.tsx\n+++ b/python/ray/dashboard/client/src/pages/metrics/Metrics.tsx\n@@ -201,6 +201,14 @@ const METRICS_CONFIG: MetricsSectionConfig[] = [\n         title: \"Node Memory by Component\",\n         pathParams: \"theme=light&panelId=34\",\n       },\n+      {\n+        title: \"Node GPU by Component\",\n+        pathParams: \"orgId=1&theme=light&panelId=45\",\n+      },\n+      {\n+        title: \"Node GPU Memory by Component\",\n+        pathParams: \"orgId=1&theme=light&panelId=46\",\n+      },\n     ],\n   },\n ];\ndiff --git a/python/ray/dashboard/consts.py b/python/ray/dashboard/consts.py\nindex bbd1a2f57361..1c5fdb9386b2 100644\n--- a/python/ray/dashboard/consts.py\n+++ b/python/ray/dashboard/consts.py\n@@ -74,6 +74,7 @@\n TPU_TAG_KEYS = NODE_TAG_KEYS + [\"TpuDeviceName\", \"TpuIndex\", \"TpuType\", \"TpuTopology\"]\n CLUSTER_TAG_KEYS = [\"node_type\", \"Version\", \"SessionName\"]\n COMPONENT_METRICS_TAG_KEYS = [\"ip\", \"pid\", \"Version\", \"Component\", \"SessionName\"]\n+COMPONENT_GPU_TAG_KEYS = GPU_TAG_KEYS + COMPONENT_METRICS_TAG_KEYS\n \n # Dashboard metrics are tracked separately at the dashboard. TODO(sang): Support GCS.\n # Note that for dashboard subprocess module, the component name is \"dashboard_[module_name]\".\ndiff --git a/python/ray/dashboard/modules/metrics/dashboards/default_dashboard_panels.py b/python/ray/dashboard/modules/metrics/dashboards/default_dashboard_panels.py\nindex 051e5568ec9f..dd4e702b34d7 100644\n--- a/python/ray/dashboard/modules/metrics/dashboards/default_dashboard_panels.py\n+++ b/python/ray/dashboard/modules/metrics/dashboards/default_dashboard_panels.py\n@@ -508,6 +508,34 @@ def max_plus_pending(max_resource, pending_resource):\n         fill=0,\n         stack=False,\n     ),\n+    Panel(\n+        id=45,\n+        title=\"Node GPU by Component\",\n+        description=\"The physical (hardware) GPU usage across the cluster, broken down by component. This reports the summed GPU usage per Ray component.\",\n+        unit=\"GPUs\",\n+        targets=[\n+            Target(\n+                expr=\"sum(ray_component_gpu_percentage{{{global_filters}}} / 100) by (Component)\",\n+                legend=\"{{Component}}\",\n+            ),\n+        ],\n+    ),\n+    Panel(\n+        id=46,\n+        title=\"Node GPU Memory by Component\",\n+        description=\"The physical (hardware) GPU memory usage across the cluster, broken down by component. This reports the summed GPU memory usage per Ray component.\",\n+        unit=\"bytes\",\n+        targets=[\n+            Target(\n+                expr=\"sum(ray_component_gpu_memory_mb{{{global_filters}}}) by (Component)\",\n+                legend=\"{{Component}}\",\n+            ),\n+            Target(\n+                expr='(sum(ray_node_gram_available{{instance=~\"$Instance\",{global_filters}}}) + sum(ray_node_gram_used{{instance=~\"$Instance\",{global_filters}}}))*1024*1024',\n+                legend=\"MAX\",\n+            ),\n+        ],\n+    ),\n ]\n \n \ndiff --git a/python/ray/dashboard/modules/reporter/gpu_providers.py b/python/ray/dashboard/modules/reporter/gpu_providers.py\nindex 69f9f39ba7b2..b50a417c1c48 100644\n--- a/python/ray/dashboard/modules/reporter/gpu_providers.py\n+++ b/python/ray/dashboard/modules/reporter/gpu_providers.py\n@@ -8,7 +8,10 @@\n import enum\n import logging\n import subprocess\n-from typing import List, Optional, Union, TypedDict\n+from typing import Dict, List, Optional, Union, TypedDict\n+from collections import defaultdict\n+\n+from ray._private.ray_constants import RAY_METRIC_ENABLE_GPU_NVSMI\n \n logger = logging.getLogger(__name__)\n \n@@ -33,6 +36,7 @@ class ProcessGPUInfo(TypedDict):\n \n     pid: int\n     gpu_memory_usage: Megabytes\n+    gpu_utilization: Optional[Percentage]\n \n \n class GpuUtilizationInfo(TypedDict):\n@@ -44,7 +48,7 @@ class GpuUtilizationInfo(TypedDict):\n     utilization_gpu: Optional[Percentage]\n     memory_used: Megabytes\n     memory_total: Megabytes\n-    processes_pids: Optional[List[ProcessGPUInfo]]\n+    processes_pids: Optional[Dict[int, ProcessGPUInfo]]\n \n \n # tpu utilization for google tpu\n@@ -105,6 +109,7 @@ class NvidiaGpuProvider(GpuProvider):\n     def __init__(self):\n         super().__init__()\n         self._pynvml = None\n+        self._using_nvidia_smi = RAY_METRIC_ENABLE_GPU_NVSMI\n \n     def get_provider_name(self) -> GpuProviderType:\n         return GpuProviderType.NVIDIA\n@@ -149,6 +154,131 @@ def _shutdown(self):\n \n     def get_gpu_utilization(self) -> List[GpuUtilizationInfo]:\n         \"\"\"Get GPU utilization information for all NVIDIA GPUs and MIG devices.\"\"\"\n+\n+        return (\n+            self._get_nvsmi_gpu_usage()\n+            if self._using_nvidia_smi\n+            else self._get_pynvml_gpu_usage()\n+        )\n+\n+    def _get_nvsmi_gpu_usage(self) -> List[GpuUtilizationInfo]:\n+        try:\n+            gpu_info = subprocess.run(\n+                [\n+                    \"nvidia-smi\",\n+                    \"--query-gpu=index,name,uuid,utilization.gpu,memory.used,memory.total\",\n+                    \"--format=csv,noheader,nounits\",\n+                ],\n+                check=True,\n+                capture_output=True,\n+                text=True,\n+            )\n+            \"\"\"Sample output:\n+            0, GPU-0, GPU-36e1567d-37ed-051e-f8ff-df807517b396, 0, 73348, 81559\n+            1, GPU-1, GPU-4a2c89ef-1b3d-492c-a8d5-e9c614f82d73, 0, 73444, 81559\n+            2, GPU-2, GPU-7f15d234-9c6a-4e8b-b3f2-c982a5d91b48, 0, 73444, 81559\n+            3, GPU-3, GPU-2b8d6f91-5e4c-47a3-96d7-8b31c4f9ae52, 0, 73332, 81559\n+            4, GPU-4, GPU-9d3a7c82-6b5f-4d1e-ae94-3f5c8d2e9b14, 0, 73344, 81559\n+            5, GPU-5, GPU-c4e6b853-2a9d-48f6-b1c7-d4f982e6a795, 0, 73440, 81559\n+            6, GPU-6, GPU-1f9b4c75-8e3a-4d2b-95c8-6a7d3b8f4e21, 0, 73440, 81559\n+            7, GPU-7, GPU-5d2e9f36-4c7b-483a-b9e1-2f8ac4d5b963, 0, 73328, 81559\n+            \"\"\"\n+            gpus = []\n+            for line in sorted(gpu_info.stdout.strip().split(\"\\n\")):  # Sort by index\n+                index, name, uuid, util, mem_used, mem_total = line.split(\", \")\n+                gpus.append(\n+                    GpuUtilizationInfo(\n+                        index=int(index),\n+                        name=name,\n+                        uuid=uuid,\n+                        utilization_gpu=int(util),\n+                        memory_used=int(mem_used),\n+                        memory_total=int(mem_total),\n+                        processes_pids={},\n+                    )\n+                )\n+\n+            processes_info = subprocess.run(\n+                [\"nvidia-smi\", \"pmon\", \"-c\", \"1\"],\n+                stdout=subprocess.PIPE,\n+                stderr=subprocess.PIPE,\n+                check=True,\n+                text=True,\n+            )\n+            processes_info = self._parse_nvsmi_pmon_output(processes_info.stdout, gpus)\n+            for gpu in gpus:\n+                gpu_id = gpu[\"index\"]\n+                if gpu_id in processes_info:\n+                    gpu[\"processes_pids\"] = processes_info[gpu_id]\n+            return gpus\n+        except (subprocess.CalledProcessError, ValueError) as e:\n+            logger.warning(f\"nvidia-smi failed to call: {e}. Falling back to pynvml.\")\n+            self._using_nvidia_smi = False\n+            return self._get_pynvml_gpu_usage()\n+\n+    @staticmethod\n+    def _parse_nvsmi_pmon_output(\n+        nvsmi_stdout: str,\n+        gpus: List[GpuUtilizationInfo],\n+    ) -> Dict[int, List[ProcessGPUInfo]]:\n+        \"\"\"Parse the output of nvidia-smi pmon -c 1.\n+\n+        Sample output of 'nvidia-smi pmon -c 1':\n+        # gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command\n+        # Idx           #    C/G      %      %      %      %      %      %    name\n+            0       7175     C     84     26      -      -      -      -    ray::TorchGPUWo\n+            1       7175     C     86     26      -      -      -      -    ray::TorchGPUWo\n+            2          -     -      -      -      -      -      -      -    -\n+\n+        Returns a dict mapping GPU index to list of ProcessGPUInfo.\n+        \"\"\"\n+        process_utilizations = defaultdict(list)\n+        lines = nvsmi_stdout.splitlines()\n+        # Get the first line that is started with #\n+        table_header = None\n+        for line in lines:\n+            if line.startswith(\"#\"):\n+                table_header = line\n+                break\n+        if not table_header:\n+            raise ValueError(\n+                \"nvidia-smi pmon output is not supported. Please upgrade to a newer version of nvidia-smi.\"\n+            )\n+        table_header = table_header.lower().split()[1:]\n+        # Base on different versions, the header may be different.\n+        # ValueError will be raised if the header is not found by the index function.\n+        gpu_id_index = table_header.index(\"gpu\")\n+        pid_index = table_header.index(\"pid\")\n+        sm_index = table_header.index(\"sm\")\n+        mem_index = table_header.index(\"mem\")\n+\n+        for line in lines:\n+            if line.startswith(\"#\") or not line.strip():\n+                continue\n+\n+            columns = line.split()\n+            if len(columns) < max(gpu_id_index, pid_index, sm_index, mem_index) + 1:\n+                continue\n+\n+            gpu_id, pid, sm, mem = (\n+                int(columns[gpu_id_index]),\n+                0 if columns[pid_index] == \"-\" else int(columns[pid_index]),\n+                0 if columns[sm_index] == \"-\" else int(columns[sm_index]),\n+                0 if columns[mem_index] == \"-\" else int(columns[mem_index]),\n+            )\n+            if pid == 0:  # no process on this GPU\n+                continue\n+            process_info = ProcessGPUInfo(\n+                pid=pid,\n+                gpu_memory_usage=int(\n+                    gpus[gpu_id][\"memory_total\"] * mem / 100\n+                ),  # Convert percentage to MB\n+                gpu_utilization=sm,\n+            )\n+            process_utilizations[gpu_id].append(process_info)\n+        return process_utilizations\n+\n+    def _get_pynvml_gpu_usage(self) -> List[GpuUtilizationInfo]:\n         if not self._initialized:\n             if not self._initialize():\n                 return []\n@@ -232,7 +362,7 @@ def _get_mig_device_info(\n                 logger.debug(f\"Failed to retrieve MIG device utilization: {e}\")\n \n             # Get running processes on MIG device\n-            processes_pids = []\n+            processes_pids = {}\n             try:\n                 nv_comp_processes = self._pynvml.nvmlDeviceGetComputeRunningProcesses(\n                     mig_handle\n@@ -241,17 +371,16 @@ def _get_mig_device_info(\n                     self._pynvml.nvmlDeviceGetGraphicsRunningProcesses(mig_handle)\n                 )\n \n-                processes_pids = [\n-                    ProcessGPUInfo(\n+                for nv_process in nv_comp_processes + nv_graphics_processes:\n+                    processes_pids[int(nv_process.pid)] = ProcessGPUInfo(\n                         pid=int(nv_process.pid),\n                         gpu_memory_usage=(\n                             int(nv_process.usedGpuMemory) // MB\n                             if nv_process.usedGpuMemory\n                             else 0\n                         ),\n+                        gpu_utilization=None,  # Not available in pynvml\n                     )\n-                    for nv_process in (nv_comp_processes + nv_graphics_processes)\n-                ]\n             except self._pynvml.NVMLError as e:\n                 logger.debug(f\"Failed to retrieve MIG device processes: {e}\")\n \n@@ -303,7 +432,7 @@ def _get_gpu_info(self, gpu_handle, gpu_index: int) -> Optional[GpuUtilizationIn\n                 logger.debug(f\"Failed to retrieve GPU utilization: {e}\")\n \n             # Get running processes\n-            processes_pids = []\n+            processes_pids = {}\n             try:\n                 nv_comp_processes = self._pynvml.nvmlDeviceGetComputeRunningProcesses(\n                     gpu_handle\n@@ -312,17 +441,16 @@ def _get_gpu_info(self, gpu_handle, gpu_index: int) -> Optional[GpuUtilizationIn\n                     self._pynvml.nvmlDeviceGetGraphicsRunningProcesses(gpu_handle)\n                 )\n \n-                processes_pids = [\n-                    ProcessGPUInfo(\n+                for nv_process in nv_comp_processes + nv_graphics_processes:\n+                    processes_pids[int(nv_process.pid)] = ProcessGPUInfo(\n                         pid=int(nv_process.pid),\n                         gpu_memory_usage=(\n                             int(nv_process.usedGpuMemory) // MB\n                             if nv_process.usedGpuMemory\n                             else 0\n                         ),\n+                        gpu_utilization=None,  # Not available in pynvml\n                     )\n-                    for nv_process in (nv_comp_processes + nv_graphics_processes)\n-                ]\n             except self._pynvml.NVMLError as e:\n                 logger.debug(f\"Failed to retrieve GPU processes: {e}\")\n \n@@ -407,16 +535,15 @@ def get_gpu_utilization(self) -> List[GpuUtilizationInfo]:\n                     utilization = -1\n \n                 # Get running processes\n-                processes_pids = []\n+                processes_pids = {}\n                 for process in self._pyamdsmi.smi_get_compute_process_info_by_device(\n                     i, processes\n                 ):\n                     if process.vram_usage:\n-                        processes_pids.append(\n-                            ProcessGPUInfo(\n-                                pid=int(process.process_id),\n-                                gpu_memory_usage=int(process.vram_usage) // MB,\n-                            )\n+                        processes_pids[int(process.process_id)] = ProcessGPUInfo(\n+                            pid=int(process.process_id),\n+                            gpu_memory_usage=int(process.vram_usage) // MB,\n+                            gpu_utilization=None,\n                         )\n \n                 info = GpuUtilizationInfo(\ndiff --git a/python/ray/dashboard/modules/reporter/reporter_agent.py b/python/ray/dashboard/modules/reporter/reporter_agent.py\nindex 59ab2bb5adbe..c82d7cfab88b 100644\n--- a/python/ray/dashboard/modules/reporter/reporter_agent.py\n+++ b/python/ray/dashboard/modules/reporter/reporter_agent.py\n@@ -33,6 +33,7 @@\n from ray._private.utils import get_system_memory\n from ray.dashboard.modules.reporter.gpu_providers import (\n     GpuMetricProvider,\n+    GpuUtilizationInfo,\n     TpuUtilizationInfo,\n )\n from ray._private import utils\n@@ -51,6 +52,7 @@\n from ray.dashboard import k8s_utils\n from ray.dashboard.consts import (\n     CLUSTER_TAG_KEYS,\n+    COMPONENT_GPU_TAG_KEYS,\n     COMPONENT_METRICS_TAG_KEYS,\n     GCS_RPC_TIMEOUT_SECONDS,\n     GPU_TAG_KEYS,\n@@ -352,6 +354,18 @@ def jsonify_asdict(o) -> str:\n         \"count\",\n         CLUSTER_TAG_KEYS,\n     ),\n+    \"component_gpu_percentage\": Gauge(\n+        \"component_gpu_percentage\",\n+        \"GPU usage of all components on the node.\",\n+        \"percentage\",\n+        COMPONENT_GPU_TAG_KEYS,\n+    ),\n+    \"component_gpu_memory_mb\": Gauge(\n+        \"component_gpu_memory_mb\",\n+        \"GPU memory usage of all components on the node.\",\n+        \"MB\",\n+        COMPONENT_GPU_TAG_KEYS,\n+    ),\n }\n \n PSUTIL_PROCESS_ATTRS = (\n@@ -413,6 +427,7 @@ def __init__(self, dashboard_agent):\n         self._agent_proc = None\n         # The last reported worker proc names (e.g., ray::*).\n         self._latest_worker_proc_names = set()\n+        self._latest_gpu_worker_proc_names = set()\n         self._network_stats_hist = [(0, (0.0, 0.0))]  # time, (sent, recv)\n         self._disk_io_stats_hist = [\n             (0, (0.0, 0.0, 0, 0))\n@@ -867,9 +882,8 @@ def _get_agent_proc(self) -> psutil.Process:\n     def _generate_worker_key(self, proc: psutil.Process) -> Tuple[int, float]:\n         return (proc.pid, proc.create_time())\n \n-    def _get_workers(self):\n+    def _get_workers(self, gpus: Optional[List[GpuUtilizationInfo]] = None):\n         raylet_proc = self._get_raylet_proc()\n-\n         if raylet_proc is None:\n             return []\n         else:\n@@ -905,13 +919,41 @@ def _get_workers(self):\n             # Remove the current process (reporter agent), which is also a child of\n             # the Raylet.\n             self._workers.pop(self._generate_worker_key(self._get_agent_proc()))\n+            # Build process ID -> GPU info mapping for faster lookups\n+            gpu_pid_mapping = defaultdict(list)\n+            if gpus is not None:\n+                for gpu in gpus:\n+                    processes = gpu.get(\"processes_pids\")\n+                    if processes:\n+                        for proc in processes.values():\n+                            gpu_pid_mapping[proc.pid].append(proc)\n \n             result = []\n             for w in self._workers.values():\n                 try:\n                     if w.status() == psutil.STATUS_ZOMBIE:\n                         continue\n-                    result.append(w.as_dict(attrs=PSUTIL_PROCESS_ATTRS))\n+\n+                    # Get basic process info\n+                    worker_info = w.as_dict(attrs=PSUTIL_PROCESS_ATTRS)\n+\n+                    # Add GPU information if available\n+                    worker_pid = worker_info[\"pid\"]\n+                    gpu_memory_usage = 0\n+                    gpu_utilization = 0\n+\n+                    if worker_pid in gpu_pid_mapping:\n+                        # Aggregate GPU memory and utilization across all GPUs for this process\n+                        for gpu_proc in gpu_pid_mapping[worker_pid]:\n+                            gpu_memory_usage += gpu_proc[\"gpu_memory_usage\"]\n+                            utilization = gpu_proc[\"gpu_utilization\"] or 0\n+                            gpu_utilization += utilization\n+\n+                    # Add GPU information to worker info\n+                    worker_info[\"gpu_memory_usage\"] = gpu_memory_usage  # in MB\n+                    worker_info[\"gpu_utilization\"] = gpu_utilization  # percentage\n+\n+                    result.append(worker_info)\n                 except psutil.NoSuchProcess:\n                     # the process may have terminated due to race condition.\n                     continue\n@@ -1004,6 +1046,7 @@ def _collect_stats(self):\n         self._disk_io_stats_hist.append((now, disk_stats))\n         disk_speed_stats = self._compute_speed_from_hist(self._disk_io_stats_hist)\n \n+        gpus = self._get_gpu_usage()\n         stats = {\n             \"now\": now,\n             \"hostname\": self._hostname,\n@@ -1013,7 +1056,7 @@ def _collect_stats(self):\n             \"mem\": self._get_mem_usage(),\n             # Unit is in bytes. None if\n             \"shm\": self._get_shm_usage(),\n-            \"workers\": self._get_workers(),\n+            \"workers\": self._get_workers(gpus),\n             \"raylet\": self._get_raylet(),\n             \"agent\": self._get_agent(),\n             \"bootTime\": self._get_boot_time(),\n@@ -1021,7 +1064,7 @@ def _collect_stats(self):\n             \"disk\": self._get_disk_usage(),\n             \"disk_io\": disk_stats,\n             \"disk_io_speed\": disk_speed_stats,\n-            \"gpus\": self._get_gpu_usage(),\n+            \"gpus\": gpus,\n             \"tpus\": self._get_tpu_usage(),\n             \"network\": network_stats,\n             \"network_speed\": network_speed_stats,\n@@ -1080,6 +1123,7 @@ def _generate_reseted_stats_record(self, component_name: str) -> List[Record]:\n                 tags=tags,\n             )\n         )\n+\n         return records\n \n     def _generate_system_stats_record(\n@@ -1098,13 +1142,19 @@ def _generate_system_stats_record(\n             a list of Record class that will be exposed to Prometheus.\n         \"\"\"\n         total_cpu_percentage = 0.0\n+        total_gpu_percentage = 0.0\n+        total_gpu_memory = 0.0\n         total_rss = 0.0\n         total_uss = 0.0\n         total_shm = 0.0\n         total_num_fds = 0\n-\n         for stat in stats:\n             total_cpu_percentage += float(stat.get(\"cpu_percent\", 0.0))  # noqa\n+\n+            # Aggregate GPU stats if available\n+            total_gpu_percentage += float(stat.get(\"gpu_utilization\", 0.0))\n+            total_gpu_memory += float(stat.get(\"gpu_memory_usage\", 0.0))\n+\n             memory_info = stat.get(\"memory_info\")\n             if memory_info:\n                 mem = stat[\"memory_info\"]\n@@ -1158,32 +1208,92 @@ def _generate_system_stats_record(\n             )\n         )\n \n+        # Add GPU records if there's GPU usage\n+        if total_gpu_memory > 0.0:\n+            records.append(\n+                Record(\n+                    gauge=METRICS_GAUGES[\"component_gpu_memory_mb\"],\n+                    value=total_gpu_memory,\n+                    tags=tags,\n+                )\n+            )\n+\n+        if total_gpu_percentage > 0.0:\n+            records.append(\n+                Record(\n+                    gauge=METRICS_GAUGES[\"component_gpu_percentage\"],\n+                    value=total_gpu_percentage,\n+                    tags=tags,\n+                )\n+            )\n+\n+        return records\n+\n+    def _generate_reseted_gpu_stats_record(self, component_name: str) -> List[Record]:\n+        \"\"\"Return a list of Record that will reset\n+        the GPU metrics of a given component name.\n+\n+        Args:\n+            component_name: a component name for a given stats.\n+\n+        Returns:\n+            a list of Record instances of GPU metrics with all values 0.\n+        \"\"\"\n+        tags = {\"ip\": self._ip, \"Component\": component_name}\n+\n+        records = []\n+        records.append(\n+            Record(\n+                gauge=METRICS_GAUGES[\"component_gpu_memory_mb\"],\n+                value=0.0,\n+                tags=tags,\n+            )\n+        )\n+        records.append(\n+            Record(\n+                gauge=METRICS_GAUGES[\"component_gpu_percentage\"],\n+                value=0.0,\n+                tags=tags,\n+            )\n+        )\n+\n         return records\n \n     def generate_worker_stats_record(self, worker_stats: List[dict]) -> List[Record]:\n-        \"\"\"Generate a list of Record class for worker proceses.\n+        \"\"\"Generate a list of Record class for worker processes.\n \n         This API automatically sets the component_name of record as\n         the name of worker processes. I.e., ray::* so that we can report\n         per task/actor (grouped by a func/class name) resource usages.\n \n         Args:\n-            stats: a list of stats dict generated by `psutil.as_dict`\n-                for worker processes.\n+            worker_stats: a list of stats dict generated by `psutil.as_dict`\n+                for worker processes. Now with gpu usage information.\n         \"\"\"\n-        # worekr cmd name (ray::*) -> stats dict.\n+        # worker cmd name (ray::*) -> stats dict.\n         proc_name_to_stats = defaultdict(list)\n+        gpu_worker_proc_names = set()  # Track processes with GPU usage\n+\n         for stat in worker_stats:\n             cmdline = stat.get(\"cmdline\")\n             # All ray processes start with ray::\n             if cmdline and len(cmdline) > 0 and cmdline[0].startswith(\"ray::\"):\n                 proc_name = cmdline[0]\n                 proc_name_to_stats[proc_name].append(stat)\n+\n+                # Track if this process has GPU usage\n+                if (\n+                    stat.get(\"gpu_memory_usage\", 0) > 0\n+                    or stat.get(\"gpu_utilization\", 0) > 0\n+                ):\n+                    gpu_worker_proc_names.add(proc_name)\n             # We will lose worker stats that don't follow the ray worker proc\n             # naming convention. Theoretically, there should be no data loss here\n             # because all worker processes are renamed to ray::.\n \n         records = []\n+\n+        # Generate system stats records (now includes GPU stats)\n         for proc_name, stats in proc_name_to_stats.items():\n             records.extend(self._generate_system_stats_record(stats, proc_name))\n \n@@ -1195,6 +1305,15 @@ def generate_worker_stats_record(self, worker_stats: List[dict]) -> List[Record]\n         for stale_proc_name in stale_procs:\n             records.extend(self._generate_reseted_stats_record(stale_proc_name))\n \n+        # Reset GPU metrics for processes that no longer use GPU\n+        stale_gpu_worker_proc_names = (\n+            self._latest_gpu_worker_proc_names - gpu_worker_proc_names\n+        )\n+        self._latest_gpu_worker_proc_names = gpu_worker_proc_names\n+\n+        for stale_gpu_proc in stale_gpu_worker_proc_names:\n+            records.extend(self._generate_reseted_gpu_stats_record(stale_gpu_proc))\n+\n         return records\n \n     def _to_records(self, stats, cluster_stats) -> List[Record]:\n","test_patch":"diff --git a/python/ray/dashboard/modules/reporter/tests/test_gpu_providers.py b/python/ray/dashboard/modules/reporter/tests/test_gpu_providers.py\nindex 4b6dfbe816b9..a49e97d421ab 100644\n--- a/python/ray/dashboard/modules/reporter/tests/test_gpu_providers.py\n+++ b/python/ray/dashboard/modules/reporter/tests/test_gpu_providers.py\n@@ -20,10 +20,13 @@ class TestProcessGPUInfo(unittest.TestCase):\n \n     def test_creation(self):\n         \"\"\"Test ProcessGPUInfo creation.\"\"\"\n-        process_info = ProcessGPUInfo(pid=1234, gpu_memory_usage=256)\n+        process_info = ProcessGPUInfo(\n+            pid=1234, gpu_memory_usage=256, gpu_utilization=None\n+        )\n \n         self.assertEqual(process_info[\"pid\"], 1234)\n         self.assertEqual(process_info[\"gpu_memory_usage\"], 256)\n+        self.assertIsNone(process_info[\"gpu_utilization\"])\n \n \n class TestGpuUtilizationInfo(unittest.TestCase):\n@@ -31,8 +34,8 @@ class TestGpuUtilizationInfo(unittest.TestCase):\n \n     def test_creation_with_processes(self):\n         \"\"\"Test GpuUtilizationInfo with process information.\"\"\"\n-        process1 = ProcessGPUInfo(pid=1234, gpu_memory_usage=256)\n-        process2 = ProcessGPUInfo(pid=5678, gpu_memory_usage=512)\n+        process1 = ProcessGPUInfo(pid=1234, gpu_memory_usage=256, gpu_utilization=None)\n+        process2 = ProcessGPUInfo(pid=5678, gpu_memory_usage=512, gpu_utilization=None)\n \n         gpu_info = GpuUtilizationInfo(\n             index=0,\n@@ -41,7 +44,7 @@ def test_creation_with_processes(self):\n             utilization_gpu=75,\n             memory_used=8192,\n             memory_total=10240,\n-            processes_pids=[process1, process2],\n+            processes_pids={1234: process1, 5678: process2},\n         )\n \n         self.assertEqual(gpu_info[\"index\"], 0)\n@@ -51,6 +54,12 @@ def test_creation_with_processes(self):\n         self.assertEqual(gpu_info[\"memory_used\"], 8192)\n         self.assertEqual(gpu_info[\"memory_total\"], 10240)\n         self.assertEqual(len(gpu_info[\"processes_pids\"]), 2)\n+        self.assertIn(1234, gpu_info[\"processes_pids\"])\n+        self.assertIn(5678, gpu_info[\"processes_pids\"])\n+        self.assertEqual(gpu_info[\"processes_pids\"][1234][\"pid\"], 1234)\n+        self.assertEqual(gpu_info[\"processes_pids\"][1234][\"gpu_memory_usage\"], 256)\n+        self.assertEqual(gpu_info[\"processes_pids\"][5678][\"pid\"], 5678)\n+        self.assertEqual(gpu_info[\"processes_pids\"][5678][\"gpu_memory_usage\"], 512)\n \n     def test_creation_without_processes(self):\n         \"\"\"Test GpuUtilizationInfo without process information.\"\"\"\n@@ -263,8 +272,8 @@ def test_get_gpu_utilization_success(self, mock_pynvml):\n         self.assertEqual(gpu_info[\"memory_used\"], 8 * 1024)  # 8GB in MB\n         self.assertEqual(gpu_info[\"memory_total\"], 12 * 1024)  # 12GB in MB\n         self.assertEqual(len(gpu_info[\"processes_pids\"]), 1)\n-        self.assertEqual(gpu_info[\"processes_pids\"][0][\"pid\"], 1234)\n-        self.assertEqual(gpu_info[\"processes_pids\"][0][\"gpu_memory_usage\"], 256)\n+        self.assertEqual(gpu_info[\"processes_pids\"][1234][\"pid\"], 1234)\n+        self.assertEqual(gpu_info[\"processes_pids\"][1234][\"gpu_memory_usage\"], 256)\n \n     @patch(\"ray._private.thirdparty.pynvml\", create=True)\n     def test_get_gpu_utilization_with_errors(self, mock_pynvml):\n@@ -313,8 +322,8 @@ class MockNVMLError(Exception):\n         self.assertEqual(gpu_info[\"name\"], \"NVIDIA Tesla V100\")\n         self.assertEqual(gpu_info[\"utilization_gpu\"], -1)  # Should be -1 due to error\n         self.assertEqual(\n-            gpu_info[\"processes_pids\"], []\n-        )  # Should be empty list due to error\n+            gpu_info[\"processes_pids\"], {}\n+        )  # Should be empty dict due to error\n \n     @patch(\"ray._private.thirdparty.pynvml\", create=True)\n     def test_get_gpu_utilization_with_mig(self, mock_pynvml):\n@@ -378,7 +387,7 @@ def test_get_gpu_utilization_with_mig(self, mock_pynvml):\n         self.assertEqual(gpu_info[\"utilization_gpu\"], 80)\n         self.assertEqual(gpu_info[\"memory_used\"], 2 * 1024)  # 2GB in MB\n         self.assertEqual(gpu_info[\"memory_total\"], 4 * 1024)  # 4GB in MB\n-        self.assertEqual(gpu_info[\"processes_pids\"], [])\n+        self.assertEqual(gpu_info[\"processes_pids\"], {})\n \n \n class TestAmdGpuProvider(unittest.TestCase):\n@@ -456,8 +465,8 @@ def test_get_gpu_utilization_success(self, mock_pyamdsmi):\n         self.assertEqual(gpu_info[\"memory_used\"], 6 * 1024)  # 6GB in MB\n         self.assertEqual(gpu_info[\"memory_total\"], 16 * 1024)  # 16GB in MB\n         self.assertEqual(len(gpu_info[\"processes_pids\"]), 1)\n-        self.assertEqual(gpu_info[\"processes_pids\"][0][\"pid\"], 5678)\n-        self.assertEqual(gpu_info[\"processes_pids\"][0][\"gpu_memory_usage\"], 512)\n+        self.assertEqual(gpu_info[\"processes_pids\"][5678][\"pid\"], 5678)\n+        self.assertEqual(gpu_info[\"processes_pids\"][5678][\"gpu_memory_usage\"], 512)\n \n \n class TestGpuMetricProvider(unittest.TestCase):\n@@ -576,7 +585,11 @@ def test_get_gpu_usage_success(self, mock_detect):\n                 utilization_gpu=50,\n                 memory_used=1024,\n                 memory_total=2048,\n-                processes_pids=None,\n+                processes_pids={\n+                    1234: ProcessGPUInfo(\n+                        pid=1234, gpu_memory_usage=1024, gpu_utilization=None\n+                    )\n+                },\n             )\n         ]\n         mock_detect.return_value = mock_provider\ndiff --git a/python/ray/dashboard/modules/reporter/tests/test_reporter.py b/python/ray/dashboard/modules/reporter/tests/test_reporter.py\nindex 0f1b9fc755f2..1247dad30568 100644\n--- a/python/ray/dashboard/modules/reporter/tests/test_reporter.py\n+++ b/python/ray/dashboard/modules/reporter/tests/test_reporter.py\n@@ -23,6 +23,7 @@\n     wait_until_server_available,\n )\n from ray.core.generated.metrics_pb2 import Metric\n+from ray.dashboard.modules.reporter.gpu_providers import NvidiaGpuProvider, MB\n from ray.dashboard.modules.reporter.reporter_agent import (\n     ReporterAgent,\n     TpuUtilizationInfo,\n@@ -122,6 +123,7 @@\n         ),\n     },\n     \"gpus\": [],\n+    \"gpu_processes\": {},\n     \"tpus\": [],\n     \"network\": (13621160960, 11914936320),\n     \"network_speed\": (8.435062128545095, 7.378462703142336),\n@@ -481,6 +483,142 @@ def test_report_stats_gpu():\n     assert gpu_metrics_aggregatd[\"node_gram_available\"] == GPU_MEMORY * 4 - 6\n \n \n+def test_report_per_component_stats_gpu():\n+    dashboard_agent = MagicMock()\n+    agent = ReporterAgent(dashboard_agent)\n+    # Assume it is a head node.\n+    agent._is_head_node = True\n+    # GPUstats query output example.\n+    \"\"\"\n+    {'index': 0,\n+    'uuid': 'GPU-36e1567d-37ed-051e-f8ff-df807517b396',\n+    'name': 'NVIDIA A10G',\n+    'utilization_gpu': 1,\n+    'memory_used': 0,\n+    'memory_total': 22731,\n+    'processes': []}\n+    \"\"\"\n+    GPU_MEMORY = 22731\n+\n+    STATS_TEMPLATE[\"gpus\"] = [\n+        {\n+            \"index\": 0,\n+            \"uuid\": \"GPU-36e1567d-37ed-051e-f8ff-df807517b396\",\n+            \"name\": \"NVIDIA A10G\",\n+            \"utilization_gpu\": 0,  # NOTE: this is a dummy value\n+            \"memory_used\": 0,\n+            \"memory_total\": GPU_MEMORY,\n+            \"processes_pids\": {\n+                2297322: {\n+                    \"pid\": 2297322,\n+                    \"gpu_memory_usage\": 26,\n+                    \"gpu_utilization\": None,\n+                }\n+            },\n+        },\n+        {\n+            \"index\": 1,\n+            \"uuid\": \"GPU-36e1567d-37ed-051e-f8ff-df807517b397\",\n+            \"name\": \"NVIDIA A10G\",\n+            \"utilization_gpu\": 1,\n+            \"memory_used\": 1,\n+            \"memory_total\": GPU_MEMORY,\n+            \"processes_pids\": {\n+                2297332: {\n+                    \"pid\": 2297332,\n+                    \"gpu_memory_usage\": 26,\n+                    \"gpu_utilization\": None,\n+                }\n+            },\n+        },\n+    ]\n+    gpu_worker = STATS_TEMPLATE[\"workers\"][0].copy()\n+    gpu_worker.update(\n+        {\"pid\": 7175, \"cmdline\": [\"ray::TorchGPUWorker.dummy_method\", \"\"]}\n+    )\n+    gpu_metrics_aggregatd = {\n+        \"component_gpu_utilization\": 0,\n+        \"component_gpu_memory_usage\": 0,\n+    }\n+    STATS_TEMPLATE[\"workers\"].append(gpu_worker)\n+\n+    NVSMI_OUTPUT_TWO_TASK_ON_TWO_GPUS = (\n+        \"# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command \\n\"\n+        \"# Idx           #    C/G      %      %      %      %      %      %    name \\n\"\n+        \"    0       7175     C     84     26      -      -      -      -    ray::TorchGPUWo\\n\"\n+        \"    1       7175     C     86     26      -      -      -      -    ray::TorchGPUWo\\n\"\n+    )\n+    STATS_TEMPLATE[\"gpu_processes\"] = NvidiaGpuProvider._parse_nvsmi_pmon_output(\n+        NVSMI_OUTPUT_TWO_TASK_ON_TWO_GPUS, STATS_TEMPLATE[\"gpus\"]\n+    )\n+    records = agent._to_records(STATS_TEMPLATE, {})\n+\n+    gpu_component_records = defaultdict(list)\n+\n+    for record in records:\n+        if record.gauge.name in gpu_metrics_aggregatd:\n+            gpu_component_records[record.gauge.name].append(record)\n+    for name, records in gpu_component_records.items():\n+        assert len(records) == 2  # Each matric should have 2 records\n+\n+    for record in gpu_component_records[\"component_gpu_memory_usage\"]:\n+        assert record.value == int(0.26 * GPU_MEMORY * MB)\n+        assert record.tags[\"Component\"] == \"ray::TorchGPUWorker.dummy_method\"\n+    for record in gpu_component_records[\"component_gpu_utilization\"]:\n+        if record.tags[\"GpuIndex\"] == \"0\":\n+            assert record.value == 84\n+        else:\n+            assert record.value == 86\n+\n+    # Test stats with two tasks on one GPU.\n+    NVSMI_OUTPUT_TWO_TASK_ON_ONE_GPUS = (\n+        \"# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command \\n\"\n+        \"# Idx           #    C/G      %      %      %      %      %      %    name \\n\"\n+        \"    0       7175     C     22      6      -      -      -      -    ray::TorchGPUWo\\n\"\n+        \"    0       7176     C     77     22      -      -      -      -    ray::TorchGPUWo\\n\"\n+        \"    1          -     -      -      -      -      -      -      -    -      \\n\"\n+    )\n+    STATS_TEMPLATE[\"gpu_processes\"] = NvidiaGpuProvider._parse_nvsmi_pmon_output(\n+        NVSMI_OUTPUT_TWO_TASK_ON_ONE_GPUS, STATS_TEMPLATE[\"gpus\"]\n+    )\n+    # Move process from GPU 1 to GPU 0\n+    gpu1_process = STATS_TEMPLATE[\"gpus\"][1][\"processes_pids\"][2297332]\n+    STATS_TEMPLATE[\"gpus\"][0][\"processes_pids\"][2297332] = gpu1_process\n+    STATS_TEMPLATE[\"gpus\"][1][\"processes_pids\"] = {}\n+\n+    gpu_worker = gpu_worker.copy()\n+    gpu_worker.update(\n+        {\"pid\": 7176, \"cmdline\": [\"ray::TorchGPUWorker.dummy_method_2\", \"\"]}\n+    )\n+    STATS_TEMPLATE[\"workers\"].append(gpu_worker)\n+\n+    records = agent._to_records(STATS_TEMPLATE, {})\n+\n+    gpu_component_records = defaultdict(list)\n+    for record in records:\n+        if record.gauge.name in gpu_metrics_aggregatd:\n+            gpu_component_records[record.gauge.name].append(record)\n+    for name, records in gpu_component_records.items():\n+        assert len(records) == 2\n+\n+    for record in gpu_component_records[\"component_gpu_memory_usage\"]:\n+        assert record.tags[\"GpuIndex\"] == \"0\"\n+        if record.tags[\"Component\"] == \"ray::TorchGPUWorker.dummy_method\":\n+            assert record.value == int(0.06 * GPU_MEMORY * MB)\n+            assert record.tags[\"pid\"] == \"7175\"\n+        else:\n+            assert record.value == int(0.22 * GPU_MEMORY * MB)\n+            assert record.tags[\"pid\"] == \"7176\"\n+    for record in gpu_component_records[\"component_gpu_utilization\"]:\n+        assert record.tags[\"GpuIndex\"] == \"0\"\n+        if record.tags[\"Component\"] == \"ray::TorchGPUWorker.dummy_method\":\n+            assert record.value == 22\n+            assert record.tags[\"pid\"] == \"7175\"\n+        else:\n+            assert record.value == 77\n+            assert record.tags[\"pid\"] == \"7176\"\n+\n+\n def test_get_tpu_usage():\n     dashboard_agent = MagicMock()\n     agent = ReporterAgent(dashboard_agent)\n","problem_statement":"[Core] Show per task/actor GPU usage metric\n### Description\n\ntoday the _ray_component_cpu_percentage_ and the _ray_component_rss_mb_ metrics reports a label/tag named Component which carries the task/actors name and it helps to get granular CPU and memory usage at the task/actor level.\r\nso, we would like to have the similar metrics for GPU at task/actor level too.\n\n### Use case\n\n_No response_\n","hints_text":"Ray Slack thread in this context https://ray-distributed.slack.com/archives/CNCKBBRJL/p1715026231056019\n\n","all_hints_text":"Ray Slack thread in this context https://ray-distributed.slack.com/archives/CNCKBBRJL/p1715026231056019\n\n","commit_urls":["https://github.com/ray-project/ray/commit/e297abcf7021b4c5b4135b242153df275d958438","https://github.com/ray-project/ray/commit/558d39d768c40ead267b535aa6abbce9fc4359d9","https://github.com/ray-project/ray/commit/0625819e2c0c6c5d879c4da96fbb38c1b3e4493a","https://github.com/ray-project/ray/commit/b80a862bf9ed8b93fe6df35c300e2fb9bd3ba540","https://github.com/ray-project/ray/commit/de3e1ae2d1522848c3d020724be135768b7d79a1","https://github.com/ray-project/ray/commit/756a9cd10cbf37987916bea6547483f87f52093b","https://github.com/ray-project/ray/commit/b8fcd2bf0db335476bc5e2646c55c48b94cee827","https://github.com/ray-project/ray/commit/296fa0851cadeb5ecc2f803a993fc53db4c5ea5d","https://github.com/ray-project/ray/commit/958c9debd0a0fc005cd887cd0713ceb966b89ec4","https://github.com/ray-project/ray/commit/fbceef9bf269b61c28b2cacb1303fe6c6fe77b75","https://github.com/ray-project/ray/commit/a361ee6e241beb6803f8dc7fd6062fdc42744bc3","https://github.com/ray-project/ray/commit/e045916b482807766f4178fde8446b2a5baef4d5","https://github.com/ray-project/ray/commit/59cf4e91c404a1a71b7f62e4e39c90fab676eda5","https://github.com/ray-project/ray/commit/6662fa19fb518ececb9594dc2324f2ccec7eef02","https://github.com/ray-project/ray/commit/49f35b5627d2eabc99451d2e6fccd0a235ebaf16","https://github.com/ray-project/ray/commit/1d7f2586b29ec53564530f110c96b7f1708d4117","https://github.com/ray-project/ray/commit/8f5b0e9ad8fa2266dc96c5330e28be45f37bb2c3","https://github.com/ray-project/ray/commit/8e61a83c1b21fd640be3e5da55ba7a102956ade3","https://github.com/ray-project/ray/commit/396ef49941b9408670ee70dcbef4c905bebb02c1","https://github.com/ray-project/ray/commit/68e3fa570a543f4eca91c050a0eb4c67daf97808","https://github.com/ray-project/ray/commit/ffcd4c4f813facfb8a98378b50b3f82ee3787259","https://github.com/ray-project/ray/commit/639d86a1a4218a5bcc3eaf600477a4df7a3aa178","https://github.com/ray-project/ray/commit/5e556e20e9692b5af892f0b4fb5bfb1737e59b56","https://github.com/ray-project/ray/commit/5aaf27119698d1178e2f5b4ef0b356240d703fe7","https://github.com/ray-project/ray/commit/8b99752975f01dff16df0ddd78636d5d55bb452e","https://github.com/ray-project/ray/commit/13aed357eecb8ad9ec75f8b5f53c0464fb74be06","https://github.com/ray-project/ray/commit/28f10dc2515d01fefca150ba108f492a5cdf77a5","https://github.com/ray-project/ray/commit/1384c2e1844f6b814a8129f20f0e873b43c161f1","https://github.com/ray-project/ray/commit/a6c48bceb0c7359c7643be1a320fd44c7e8522a2","https://github.com/ray-project/ray/commit/262c6af8d9a8791b37395b4a747f63fbb3d08e16","https://github.com/ray-project/ray/commit/08b3bc4d3e966fd461dac2a23e27888f3ef57c0f","https://github.com/ray-project/ray/commit/df382b551092eccf91210423f6c35afcdb3bbddd","https://github.com/ray-project/ray/commit/42f750342894ba0c97dd7b6e9fcbc9010e64d978","https://github.com/ray-project/ray/commit/594d5940ac3d27656063b55338c5ac5e35e7ddfa","https://github.com/ray-project/ray/commit/22c1c9d2c8a4ded23cd8c63374eb97efceaede05","https://github.com/ray-project/ray/commit/fa5e4d5dc68c7fc9171bd01d341baeccd07242d6","https://github.com/ray-project/ray/commit/b92f9274395a5dd33aeb93eca6cb2a24c874a7f0","https://github.com/ray-project/ray/commit/d04263f01fdb9839f042e723e03119ec7aabce61","https://github.com/ray-project/ray/commit/60456b3b7956d03da0519088abe40cf290806f7f","https://github.com/ray-project/ray/commit/1ed8af858e04e6e55fdc3b6cde28a793d4ae9338","https://github.com/ray-project/ray/commit/33e9c00c52bacd4414210ac449a5adf8d5aa8047","https://github.com/ray-project/ray/commit/11ced140c76c3127410ed689bf5fd16a00344d48","https://github.com/ray-project/ray/commit/58053060bfcd936c12ab9d25ae9d2c84feba3ec5","https://github.com/ray-project/ray/commit/bb339c2f9f8ba2787e0da602fb8481fd9db116e4","https://github.com/ray-project/ray/commit/baa80f1c08fbd2a5810b92f773b8bd526edae581","https://github.com/ray-project/ray/commit/01826fc86776518c365d425965b45886c60b7f4d","https://github.com/ray-project/ray/commit/f146a8fb96c943e4a7c1c03a7fc5f30104c8881b","https://github.com/ray-project/ray/commit/489722c523ba6c19f64146979d71bc28526ce8ea","https://github.com/ray-project/ray/commit/9cfd7dcbaccd49fa19690cfa4b289f67b81a004a","https://github.com/ray-project/ray/commit/26e57af854a3bb18b24ed142a55b9c80d0817f49","https://github.com/ray-project/ray/commit/61d3d47a48a972da707d20af2fd07592a711b662"],"created_at":"2025-04-08T18:13:58Z","classification":"Efficiency"}
{"repo":"ray-project/ray","pull_number":47243,"instance_id":"ray-project__ray-47243","issue_numbers":[47242],"base_commit":"a8722868f951e59e2d61b90fa8f72a2fe12a62e7","patch":"diff --git a/doc/source/ray-contribute/profiling.rst b/doc/source/ray-contribute/profiling.rst\nindex 92c26d441099..ee499f04ee1a 100644\n--- a/doc/source/ray-contribute/profiling.rst\n+++ b/doc/source/ray-contribute/profiling.rst\n@@ -88,11 +88,12 @@ Ray supports environment variables that override `LD_PRELOAD` on core components\n You can find the component name from `ray_constants.py`. For example, if you'd like to profile gcs_server, \n search `PROCESS_TYPE_GCS_SERVER` in `ray_constants.py`. You can see the value is `gcs_server`.\n \n-Users are supposed to provide 3 env vars for memory profiling.\n+Users are supposed to provide 4 env vars for memory profiling.\n \n * `RAY_JEMALLOC_LIB_PATH`: The path to the jemalloc shared library `libjemalloc.so`\n * `RAY_JEMALLOC_CONF`: The MALLOC_CONF configuration for jemalloc, using comma-separated values. Read `jemalloc docs <http://jemalloc.net/jemalloc.3.html>`_ for more details.\n * `RAY_JEMALLOC_PROFILE`: Comma separated Ray components to run Jemalloc `.so`. e.g., (\"raylet,gcs_server\"). Note that the components should match the process type in `ray_constants.py`. (It means \"RAYLET,GCS_SERVER\" won't work).\n+* `RAY_LD_PRELOAD_ON_WORKERS`: Default value is `0`, which means Ray doesn't preload Jemalloc for workers if a library is incompatible with Jemalloc. Set to `1` to instruct Ray to preload Jemalloc for a worker using values configured by `RAY_JEMALLOC_LIB_PATH` and `RAY_JEMALLOC_PROFILE`.\n \n .. code-block:: bash\n \ndiff --git a/python/ray/_private/services.py b/python/ray/_private/services.py\nindex 3c273027eca0..551046a678ab 100644\n--- a/python/ray/_private/services.py\n+++ b/python/ray/_private/services.py\n@@ -227,7 +227,10 @@ def propagate_jemalloc_env_var(\n     if not jemalloc_path:\n         return {}\n \n-    env_vars = {\"LD_PRELOAD\": jemalloc_path, \"RAY_LD_PRELOAD\": \"1\"}\n+    env_vars = {\n+        \"LD_PRELOAD\": jemalloc_path,\n+        \"RAY_LD_PRELOAD_ON_WORKERS\": os.environ.get(\"RAY_LD_PRELOAD_ON_WORKERS\", \"0\"),\n+    }\n     if process_type in jemalloc_comps and jemalloc_conf:\n         env_vars.update({\"MALLOC_CONF\": jemalloc_conf})\n     return env_vars\ndiff --git a/src/ray/raylet/main.cc b/src/ray/raylet/main.cc\nindex d13a9eea9005..91b1d0fa0861 100644\n--- a/src/ray/raylet/main.cc\n+++ b/src/ray/raylet/main.cc\n@@ -178,8 +178,8 @@ int main(int argc, char *argv[]) {\n \n #ifdef __linux__\n   // Reset LD_PRELOAD if it's loaded with ray jemalloc\n-  auto ray_ld_preload = std::getenv(\"RAY_LD_PRELOAD\");\n-  if (ray_ld_preload != nullptr && std::string(ray_ld_preload) == \"1\") {\n+  auto ray_ld_preload = std::getenv(\"RAY_LD_PRELOAD_ON_WORKERS\");\n+  if (ray_ld_preload != nullptr && std::string(ray_ld_preload) == \"0\") {\n     unsetenv(\"LD_PRELOAD\");\n   }\n #endif\n","test_patch":"diff --git a/python/ray/tests/test_advanced_4.py b/python/ray/tests/test_advanced_4.py\nindex 8a01fa4218b2..6df23c6c2254 100644\n--- a/python/ray/tests/test_advanced_4.py\n+++ b/python/ray/tests/test_advanced_4.py\n@@ -1,5 +1,7 @@\n+import os\n import subprocess\n import sys\n+from unittest.mock import patch\n \n import pytest\n \n@@ -67,7 +69,7 @@ def test_jemalloc_env_var_propagate():\n     When the shared library is specified\n     \"\"\"\n     library_path = \"/abc\"\n-    expected = {\"LD_PRELOAD\": library_path, \"RAY_LD_PRELOAD\": \"1\"}\n+    expected = {\"LD_PRELOAD\": library_path, \"RAY_LD_PRELOAD_ON_WORKERS\": \"0\"}\n     actual = ray._private.services.propagate_jemalloc_env_var(\n         jemalloc_path=library_path,\n         jemalloc_conf=\"\",\n@@ -85,14 +87,15 @@ def test_jemalloc_env_var_propagate():\n             process_type=gcs_ptype,\n         )\n \n-    # When comps don't match the process_type, it should return an empty dict.\n-    expected = {}\n+    # When comps don't match the process_type, it should not contain MALLOC_CONF.\n     actual = ray._private.services.propagate_jemalloc_env_var(\n         jemalloc_path=library_path,\n         jemalloc_conf=\"\",\n         jemalloc_comps=[ray._private.ray_constants.PROCESS_TYPE_RAYLET],\n         process_type=gcs_ptype,\n     )\n+    assert \"MALLOC_CONF\" not in actual\n+\n     \"\"\"\n     When the malloc config is specified\n     \"\"\"\n@@ -101,7 +104,7 @@ def test_jemalloc_env_var_propagate():\n     expected = {\n         \"LD_PRELOAD\": library_path,\n         \"MALLOC_CONF\": malloc_conf,\n-        \"RAY_LD_PRELOAD\": \"1\",\n+        \"RAY_LD_PRELOAD_ON_WORKERS\": \"0\",\n     }\n     actual = ray._private.services.propagate_jemalloc_env_var(\n         jemalloc_path=library_path,\n@@ -112,6 +115,24 @@ def test_jemalloc_env_var_propagate():\n     assert actual == expected\n \n \n+@patch.dict(os.environ, {\"RAY_LD_PRELOAD_ON_WORKERS\": \"1\"})\n+def test_enable_jemallc_for_workers():\n+    library_path = \"/abc\"\n+    malloc_conf = \"a,b,c\"\n+    expected = {\n+        \"LD_PRELOAD\": library_path,\n+        \"MALLOC_CONF\": malloc_conf,\n+        \"RAY_LD_PRELOAD_ON_WORKERS\": \"1\",\n+    }\n+    actual = ray._private.services.propagate_jemalloc_env_var(\n+        jemalloc_path=library_path,\n+        jemalloc_conf=malloc_conf,\n+        jemalloc_comps=[ray._private.ray_constants.PROCESS_TYPE_WORKER],\n+        process_type=ray._private.ray_constants.PROCESS_TYPE_WORKER,\n+    )\n+    assert actual == expected\n+\n+\n def test_back_pressure(shutdown_only_with_initialization_check):\n     ray.init()\n \n","problem_statement":"[Core] We should make preloading Jemalloc configurable for worker \n### Description\n\nThe PR [#39446](https://github.com/ray-project/ray/pull/39446) disables preloading Jemalloc for workers totally. However, Jemalloc is still useful in some cases, and we could make it configurable if user setting env `RAY_LD_PRELOAD` as `0`.\r\n\r\nI did a inference test with limited memory, and we can see the OOM counts decrease from 900+ to 700.\r\n<img width=\"1839\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f8615757-9ed4-45a6-98a2-9d35c7d97fb9\">\r\n\r\n\n\n### Use case\n\n_No response_\n","hints_text":"Could you share your inference example? We might need some public benchmark to move forward.\n\n","all_hints_text":"Could you share your inference example? We might need some public benchmark to move forward.\n\n","commit_urls":["https://github.com/ray-project/ray/commit/87b00b8233e31d7374cc3299b1e105b2d6e5779b"],"created_at":"2024-08-21T09:28:25Z","classification":"Efficiency"}
{"repo":"ray-project/ray","pull_number":50414,"instance_id":"ray-project__ray-50414","issue_numbers":[50259],"base_commit":"2f3df505506ca2b0acb7a4771ec18a6c0ed73932","patch":"diff --git a/python/ray/autoscaler/_private/constants.py b/python/ray/autoscaler/_private/constants.py\nindex 874e5af23993..304625480d20 100644\n--- a/python/ray/autoscaler/_private/constants.py\n+++ b/python/ray/autoscaler/_private/constants.py\n@@ -88,6 +88,9 @@ def env_integer(key, default):\n # Port that autoscaler prometheus metrics will be exported to\n AUTOSCALER_METRIC_PORT = env_integer(\"AUTOSCALER_METRIC_PORT\", 44217)\n \n+# The minimum number of nodes to launch concurrently.\n+AUTOSCALER_UPSCALING_INITIAL_NUM_NODES = 5\n+\n # Max number of retries to AWS (default is 5, time increases exponentially)\n BOTO_MAX_RETRIES = env_integer(\"BOTO_MAX_RETRIES\", 12)\n # Max number of retries to create an EC2 node (retry different subnet)\ndiff --git a/python/ray/autoscaler/_private/resource_demand_scheduler.py b/python/ray/autoscaler/_private/resource_demand_scheduler.py\nindex 1983b3896564..a24c19c94ebb 100644\n--- a/python/ray/autoscaler/_private/resource_demand_scheduler.py\n+++ b/python/ray/autoscaler/_private/resource_demand_scheduler.py\n@@ -19,6 +19,7 @@\n from ray._private.gcs_utils import PlacementGroupTableData\n from ray.autoscaler._private.constants import (\n     AUTOSCALER_CONSERVE_GPU_NODES,\n+    AUTOSCALER_UPSCALING_INITIAL_NUM_NODES,\n     AUTOSCALER_UTILIZATION_SCORER_KEY,\n )\n from ray.autoscaler._private.loader import load_function_or_class\n@@ -45,9 +46,6 @@\n \n logger = logging.getLogger(__name__)\n \n-# The minimum number of nodes to launch concurrently.\n-UPSCALING_INITIAL_NUM_NODES = 5\n-\n NodeResources = ResourceDict\n ResourceDemands = List[ResourceDict]\n \n@@ -437,7 +435,7 @@ def _get_concurrent_resource_demand_to_launch(\n             # Enforce here max allowed pending nodes to be frac of total\n             # running nodes.\n             max_allowed_pending_nodes = max(\n-                UPSCALING_INITIAL_NUM_NODES,\n+                AUTOSCALER_UPSCALING_INITIAL_NUM_NODES,\n                 int(self.upscaling_speed * max(running_nodes[node_type], 1)),\n             )\n             total_pending_nodes = (\ndiff --git a/python/ray/autoscaler/v2/instance_manager/reconciler.py b/python/ray/autoscaler/v2/instance_manager/reconciler.py\nindex 4e1f2788259f..887a89aecd48 100644\n--- a/python/ray/autoscaler/v2/instance_manager/reconciler.py\n+++ b/python/ray/autoscaler/v2/instance_manager/reconciler.py\n@@ -743,15 +743,15 @@ def _handle_instances_launch(\n \n         queued_instances = []\n         requested_instances = []\n-        allocated_instances = []\n+        running_instances = []\n \n         for instance in instances:\n             if instance.status == IMInstance.QUEUED:\n                 queued_instances.append(instance)\n             elif instance.status == IMInstance.REQUESTED:\n                 requested_instances.append(instance)\n-            elif instance.cloud_instance_id:\n-                allocated_instances.append(instance)\n+            elif instance.status == IMInstance.RAY_RUNNING:\n+                running_instances.append(instance)\n \n         if not queued_instances:\n             # No QUEUED instances\n@@ -760,7 +760,7 @@ def _handle_instances_launch(\n         to_launch = Reconciler._compute_to_launch(\n             queued_instances,\n             requested_instances,\n-            allocated_instances,\n+            running_instances,\n             autoscaling_config.get_upscaling_speed(),\n             autoscaling_config.get_max_concurrent_launches(),\n         )\n@@ -795,7 +795,7 @@ def _handle_instances_launch(\n     def _compute_to_launch(\n         queued_instances: List[IMInstance],\n         requested_instances: List[IMInstance],\n-        allocated_instances: List[IMInstance],\n+        running_instances: List[IMInstance],\n         upscaling_speed: float,\n         max_concurrent_launches: int,\n     ) -> Dict[NodeType, List[IMInstance]]:\n@@ -813,8 +813,7 @@ def _sort_by_earliest_queued(instance: IMInstance) -> List[int]:\n             return sorted(queue_times)\n \n         queued_instances_by_type = _group_by_type(queued_instances)\n-        requested_instances_by_type = _group_by_type(requested_instances)\n-        allocated_instances_by_type = _group_by_type(allocated_instances)\n+        running_instances_by_type = _group_by_type(running_instances)\n \n         total_num_requested_to_launch = len(requested_instances)\n         all_to_launch: Dict[NodeType : List[IMInstance]] = defaultdict(list)\n@@ -823,22 +822,14 @@ def _sort_by_earliest_queued(instance: IMInstance) -> List[int]:\n             instance_type,\n             queued_instances_for_type,\n         ) in queued_instances_by_type.items():\n-            requested_instances_for_type = requested_instances_by_type.get(\n-                instance_type, []\n-            )\n-            allocated_instances_for_type = allocated_instances_by_type.get(\n+            running_instances_for_type = running_instances_by_type.get(\n                 instance_type, []\n             )\n \n+            # Enforce the max allowed pending nodes based on current running nodes\n             num_desired_to_upscale = max(\n                 1,\n-                math.ceil(\n-                    upscaling_speed\n-                    * (\n-                        len(requested_instances_for_type)\n-                        + len(allocated_instances_for_type)\n-                    )\n-                ),\n+                math.ceil(upscaling_speed * len(running_instances_for_type)),\n             )\n \n             # Enforce global limit, at most we can launch `max_concurrent_launches`\n","test_patch":"diff --git a/python/ray/autoscaler/v2/tests/test_reconciler.py b/python/ray/autoscaler/v2/tests/test_reconciler.py\nindex 38719ae12175..1092daba17f4 100644\n--- a/python/ray/autoscaler/v2/tests/test_reconciler.py\n+++ b/python/ray/autoscaler/v2/tests/test_reconciler.py\n@@ -1,3 +1,4 @@\n+import math\n import os\n import sys\n import time\n@@ -628,15 +629,15 @@ def test_draining_ray_node_also_terminated(setup):\n \n     @staticmethod\n     @pytest.mark.parametrize(\n-        \"max_concurrent_launches,num_allocated,num_requested\",\n+        \"max_concurrent_launches,num_allocated,num_requested,num_running\",\n         [\n-            (1, 0, 0),\n-            (10, 0, 0),\n-            (1, 0, 1),\n-            (1, 1, 0),\n-            (10, 1, 0),\n-            (10, 0, 1),\n-            (10, 5, 5),\n+            (1, 0, 0, 0),\n+            (10, 0, 0, 0),\n+            (1, 0, 1, 1),\n+            (1, 1, 0, 1),\n+            (10, 1, 0, 1),\n+            (10, 0, 1, 1),\n+            (10, 5, 5, 5),\n         ],\n     )\n     @pytest.mark.parametrize(\n@@ -644,7 +645,12 @@ def test_draining_ray_node_also_terminated(setup):\n         [0.0, 0.1, 0.5, 1.0, 100.0],\n     )\n     def test_max_concurrent_launches(\n-        max_concurrent_launches, num_allocated, num_requested, upscaling_speed, setup\n+        max_concurrent_launches,\n+        num_allocated,\n+        num_requested,\n+        num_running,\n+        upscaling_speed,\n+        setup,\n     ):\n         instance_manager, instance_storage, subscriber = setup\n         next_id = 0\n@@ -684,7 +690,18 @@ def test_max_concurrent_launches(\n         ]\n         TestReconciler._add_instances(instance_storage, queued_instances)\n \n-        num_desired_upscale = max(1, upscaling_speed * (num_requested + num_allocated))\n+        # Add some running instances.\n+        for _ in range(num_running):\n+            instance = create_instance(\n+                str(next_id),\n+                status=Instance.RAY_RUNNING,\n+                instance_type=\"type-1\",\n+                launch_request_id=\"l-1\",\n+            )\n+            TestReconciler._add_instances(instance_storage, [instance])\n+            next_id += 1\n+\n+        num_desired_upscale = max(1, math.ceil(upscaling_speed * (num_running)))\n         expected_launch_num = min(\n             num_desired_upscale,\n             max(0, max_concurrent_launches - num_requested),  # global limit\n","problem_statement":"[Autoscaler][V2] Autoscaler V2 does not honor 'Conservative' upscaling mode\n### What happened + What you expected to happen\n\nWith KubeRay, setting `upscalingMode: Conservative` in the autoscaling config of a RayCluster limits the number of allowed pending launches at any time to the size of the RayCluster (i.e. the number of running worker Pods), with a minimum of 5. The V2 autoscaler determines the number of nodes to launch as follows in [reconciler.py](https://github.com/ray-project/ray/blob/eb80f8ad76db250ca6fa42448cd1adc82232c8b4/python/ray/autoscaler/v2/instance_manager/reconciler.py#L833):\n```\nnum_desired_to_upscale = max(\n                1,\n                math.ceil(\n                    upscaling_speed\n                    * (\n                        len(requested_instances_for_type)\n                        + len(allocated_instances_for_type)\n                    )\n                ),\n            )\n```\nwhereas the v1 autoscaler has this block of code in [resource_demand_scheduler.py](https://github.com/ray-project/ray/blob/85733ab60f2f5a16e92fcf046c7af485fa11c3f2/python/ray/autoscaler/_private/resource_demand_scheduler.py#L437) which determines max concurrent workers to launch for each node type:\n```\nmax_allowed_pending_nodes = max(\n                UPSCALING_INITIAL_NUM_NODES,\n                int(self.upscaling_speed * max(running_nodes[node_type], 1)),\n            )\n```\nit seems like the V2 autoscaler should check for the number of running nodes (so just `allocated_instances_for_type`), rather than including `requested_instances_for_type`. I believe this is the cause of the test failure (passes for V1 but not for V2 when checking if the # pending Pods is ever > 5 and > # running Pods) in this PR: https://buildkite.com/ray-project/ray-ecosystem-ci-kuberay-ci/builds/6525#0194d25f-22ea-4d08-87d3-e035d25eb9f4\n\n### Versions / Dependencies\n\n- Ray 2.41\n- KubeRay nightly image\n\n### Reproduction script\n\nCreate a RayCluster with the v2 autoscaler and a ConfigMap:\n```\napiVersion: ray.io/v1\nkind: RayCluster\nmetadata:\n  name: raycluster-autoscaler\nspec:\n  rayVersion: '2.41.0'\n  enableInTreeAutoscaling: true\n  autoscalerOptions:\n    upscalingMode: Conservative\n    idleTimeoutSeconds: 60\n    imagePullPolicy: IfNotPresent\n    # Optionally specify the Autoscaler container's securityContext.\n    securityContext: {}\n    env: []\n    envFrom: []\n    resources:\n      limits:\n        cpu: \"500m\"\n        memory: \"512Mi\"\n      requests:\n        cpu: \"500m\"\n        memory: \"512Mi\"\n  headGroupSpec:\n    rayStartParams:\n      num-cpus: \"0\"\n    template:\n      spec:\n        containers:\n        # The Ray head container\n        - name: ray-head\n          image: rayproject/ray:2.41.0\n          ports:\n          - containerPort: 6379\n            name: gcs\n          - containerPort: 8265\n            name: dashboard\n          - containerPort: 10001\n            name: client\n          resources:\n            limits:\n              cpu: \"1\"\n              memory: \"2G\"\n            requests:\n              cpu: \"1\"\n              memory: \"2G\"\n          env:\n            - name: RAY_enable_autoscaler_v2 # Pass env var for the autoscaler v2.\n              value: \"1\"\n          volumeMounts:\n            - mountPath: /home/ray/samples\n              name: ray-example-configmap\n        volumes:\n          - name: ray-example-configmap\n            configMap:\n              name: ray-example\n              defaultMode: 0777\n              items:\n                - key: detached_actor.py\n                  path: detached_actor.py\n                - key: terminate_detached_actor.py\n                  path: terminate_detached_actor.py\n        restartPolicy: Never # No restart to avoid reuse of pod for different ray nodes.\n  workerGroupSpecs:\n  - replicas: 0\n    minReplicas: 0\n    maxReplicas: 10\n    groupName: small-group\n    rayStartParams: {}\n    # Pod template\n    template:\n      spec:\n        containers:\n        - name: ray-worker\n          image: rayproject/ray:2.41.0\n          resources:\n            limits:\n              cpu: \"1\"\n              memory: \"1G\"\n            requests:\n              cpu: \"1\"\n              memory: \"1G\"\n        restartPolicy: Never # Never restart a pod to avoid pod reuse\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ray-example\ndata:\n  detached_actor.py: |\n    import ray\n    import sys\n\n    @ray.remote(num_cpus=1)\n    class Actor:\n      pass\n\n    ray.init(namespace=\"default_namespace\")\n    for i in range (10): \n      Actor.options(name=sys.argv[1]+str(i), lifetime=\"detached\").remote()\n\n  terminate_detached_actor.py: |\n    import ray\n    import sys\n\n    ray.init(namespace=\"default_namespace\")\n    for i in range (10): \n      detached_actor = ray.get_actor(sys.argv[1]+str(i))\n      ray.kill(detached_actor)\n```\n\nScale up multiple workers to test upscaling is rate limited to size of RayCluster:\n```\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- python3 /home/ray/samples/detached_actor.py actor\n```\n\n### Issue Severity\n\nNone\n","hints_text":"The V2 autoscaler currently rate limits the number of nodes to launch per node type as follows:\n```\nnum_desired_to_upscale = max(1, upscaling_speed * len(requested_instances_for_type) + len(allocated_instances_for_type))\nnum_to_launch = min(max_concurrent_launches - total_num_requested_to_launch, num_desired_to_upscale)\n\nnum_to_launch = max(0, num_to_launch)\nnum_to_launch = min(len(queued_instances_for_type), num_to_launch)\n```\n`upscaling_speed` is retrieved from the autoscaling config and either set directly or according to `upscalingMode`, with 1 for `Conservative` and 1000 otherwise. The [docs](https://docs.ray.io/en/latest/cluster/vms/user-guides/configuring-autoscaling.html#upscaling-and-downscaling-speed) specify that the minimum number of pending launches should be 5. `max_concurrent_launches` is set to `AUTOSCALER_MAX_CONCURRENT_LAUNCHES`  which defaults to 10.  This means the number of nodes to launch (i.e. move from `QUEUED` to `REQUESTED`) for each type is set to:\n```\n0 <= max(10 - total_num_requested_to_launch, max(upscaling_speed * (len(requested_instances_for_type)+len(allocated_instances_for_type))) <= number of queued nodes of that type\n```\n`allocated_instances_for_type` includes all nodes with a `cloud_instance_id`, this can include instances that are [not running Ray yet](https://github.com/ray-project/ray/blob/13ba63cdfbbb529e3c35f91368bfd31086165f1f/python/ray/autoscaler/v2/schema.py#L259). To be consistent with the V1 autoscaler and the docs on `upscaling_speed`, it seems as though `num_to_launch` should be computed as follows:\n```\nnum_desired_to_upscale = max(AUTOSCALER_UPSCALING_INITIAL_NUM_NODES, upscaling_speed * len(running_instances_for_type))\nnum_to_launch = min(max_concurrent_launches - total_num_requested_to_launch, num_desired_to_upscale)\n\nnum_to_launch = max(0, num_to_launch)\nnum_to_launch = min(len(queued_instances_for_type), num_to_launch)\n```\nwhere `running_instances_for_type` is the number of instances per node type with `IMInstance.RAY_RUNNING` and `AUTOSCALER_UPSCALING_INITIAL_NUM_NODES` is a constant set to 5. Currently the V2 autoscaler rate limits the number of nodes to launch at once based on both the allocated and requested nodes, either of which could include Pods which are 'pending' (i.e. not yet connected to the RayCluster) or terminating. Does this change make sense @rickyyx?\nThabks for the detailed write-up!\n\nIIUC, the main issue is the upscaling in v2 takes into account some non running nodes, while in V1 its only the actual running nodes?\n\nI think either behavior is fine but for consistency, sticking with V1 behaviour if thats not too hard definitely sound reasonable to me! \n> IIUC, the main issue is the upscaling in v2 takes into account some non running nodes, while in V1 its only the actual running nodes?\n\nYeah that's correct, I was trying to figure out why only the V2 autoscaler fails this [test](https://buildkite.com/ray-project/ray-ecosystem-ci-kuberay-ci/builds/6525#0194d25f-22ea-4d08-87d3-e035d25eb9f4).\nhttps://github.com/ray-project/kuberay/issues/2600\n\n","all_hints_text":"The V2 autoscaler currently rate limits the number of nodes to launch per node type as follows:\n```\nnum_desired_to_upscale = max(1, upscaling_speed * len(requested_instances_for_type) + len(allocated_instances_for_type))\nnum_to_launch = min(max_concurrent_launches - total_num_requested_to_launch, num_desired_to_upscale)\n\nnum_to_launch = max(0, num_to_launch)\nnum_to_launch = min(len(queued_instances_for_type), num_to_launch)\n```\n`upscaling_speed` is retrieved from the autoscaling config and either set directly or according to `upscalingMode`, with 1 for `Conservative` and 1000 otherwise. The [docs](https://docs.ray.io/en/latest/cluster/vms/user-guides/configuring-autoscaling.html#upscaling-and-downscaling-speed) specify that the minimum number of pending launches should be 5. `max_concurrent_launches` is set to `AUTOSCALER_MAX_CONCURRENT_LAUNCHES`  which defaults to 10.  This means the number of nodes to launch (i.e. move from `QUEUED` to `REQUESTED`) for each type is set to:\n```\n0 <= max(10 - total_num_requested_to_launch, max(upscaling_speed * (len(requested_instances_for_type)+len(allocated_instances_for_type))) <= number of queued nodes of that type\n```\n`allocated_instances_for_type` includes all nodes with a `cloud_instance_id`, this can include instances that are [not running Ray yet](https://github.com/ray-project/ray/blob/13ba63cdfbbb529e3c35f91368bfd31086165f1f/python/ray/autoscaler/v2/schema.py#L259). To be consistent with the V1 autoscaler and the docs on `upscaling_speed`, it seems as though `num_to_launch` should be computed as follows:\n```\nnum_desired_to_upscale = max(AUTOSCALER_UPSCALING_INITIAL_NUM_NODES, upscaling_speed * len(running_instances_for_type))\nnum_to_launch = min(max_concurrent_launches - total_num_requested_to_launch, num_desired_to_upscale)\n\nnum_to_launch = max(0, num_to_launch)\nnum_to_launch = min(len(queued_instances_for_type), num_to_launch)\n```\nwhere `running_instances_for_type` is the number of instances per node type with `IMInstance.RAY_RUNNING` and `AUTOSCALER_UPSCALING_INITIAL_NUM_NODES` is a constant set to 5. Currently the V2 autoscaler rate limits the number of nodes to launch at once based on both the allocated and requested nodes, either of which could include Pods which are 'pending' (i.e. not yet connected to the RayCluster) or terminating. Does this change make sense @rickyyx?\nThabks for the detailed write-up!\n\nIIUC, the main issue is the upscaling in v2 takes into account some non running nodes, while in V1 its only the actual running nodes?\n\nI think either behavior is fine but for consistency, sticking with V1 behaviour if thats not too hard definitely sound reasonable to me! \n> IIUC, the main issue is the upscaling in v2 takes into account some non running nodes, while in V1 its only the actual running nodes?\n\nYeah that's correct, I was trying to figure out why only the V2 autoscaler fails this [test](https://buildkite.com/ray-project/ray-ecosystem-ci-kuberay-ci/builds/6525#0194d25f-22ea-4d08-87d3-e035d25eb9f4).\nhttps://github.com/ray-project/kuberay/issues/2600\n\n","commit_urls":["https://github.com/ray-project/ray/commit/699d88ac477ba429e2a9d24644c7d76fa29730e6","https://github.com/ray-project/ray/commit/36770083902cbc9eee6ba295badaf0bbd4bbcf16","https://github.com/ray-project/ray/commit/f9bf403ca7208e9731a50db8939848ba157ad0d9","https://github.com/ray-project/ray/commit/934dfe74172d458efd33645c1cee2652020cb28d","https://github.com/ray-project/ray/commit/1d8aa482a68123db1fcd3a9eb1e6b64bd6931308","https://github.com/ray-project/ray/commit/40335b5c88c0640a78c9a2f5f6f34f20328b4924","https://github.com/ray-project/ray/commit/58a0800e1c5a38153dc6f2ec906e9a6f9782968d","https://github.com/ray-project/ray/commit/cc411f11c76e92cfb84c41f81804d2507c6c447a","https://github.com/ray-project/ray/commit/d487cdd577a7b6352f8a87fd457c932a3dfacfa6","https://github.com/ray-project/ray/commit/86f0b2539d679993b7b8e67742b92dc17491b705","https://github.com/ray-project/ray/commit/58ec118e2ef7f59df16f73c09afd3e400292e822","https://github.com/ray-project/ray/commit/4ec5571882386f34afcde53787920cfa37cd5dcb","https://github.com/ray-project/ray/commit/d739bf07075804324a958196859e9c0d875449e7"],"created_at":"2025-02-11T06:44:16Z","classification":"Efficiency"}
{"repo":"ray-project/ray","pull_number":53911,"instance_id":"ray-project__ray-53911","issue_numbers":[51262,51273],"base_commit":"263c7e1e66746c03f16e8ee20753d05a9936f6f0","patch":"diff --git a/python/ray/_private/serialization.py b/python/ray/_private/serialization.py\nindex c59b55bf970a..41164f66cee5 100644\n--- a/python/ray/_private/serialization.py\n+++ b/python/ray/_private/serialization.py\n@@ -292,8 +292,6 @@ def _deserialize_pickle5_data(\n                 gpu_object_manager.fetch_gpu_object(object_id)\n             tensors = gpu_object_manager.gpu_object_store.get_gpu_object(object_id)\n             ctx.reset_out_of_band_tensors(tensors)\n-            # TODO(kevin85421): The current garbage collection implementation for the in-actor object store\n-            # is naive. We garbage collect each object after it is consumed once.\n             gpu_object_manager.gpu_object_store.remove_gpu_object(object_id)\n \n         try:\ndiff --git a/python/ray/_raylet.pyx b/python/ray/_raylet.pyx\nindex 3dac73d9b192..02233a34e829 100644\n--- a/python/ray/_raylet.pyx\n+++ b/python/ray/_raylet.pyx\n@@ -2287,6 +2287,12 @@ cdef execute_task_with_cancellation_handler(\n                 f\"Exited because worker reached max_calls={execution_info.max_calls}\"\n                 \" for this method.\")\n \n+cdef void free_actor_object_callback(const CObjectID &c_object_id) nogil:\n+    with gil:\n+        object_id = c_object_id.Hex().decode()\n+        gpu_object_manager = ray._private.worker.global_worker.gpu_object_manager\n+        gpu_object_manager.gpu_object_store.remove_gpu_object(object_id)\n+\n cdef shared_ptr[LocalMemoryBuffer] ray_error_to_memory_buf(ray_error):\n     cdef bytes py_bytes = ray_error.to_bytes()\n     return make_shared[LocalMemoryBuffer](\n@@ -2998,6 +3004,7 @@ cdef class CoreWorker:\n         options.driver_name = driver_name\n         options.initialize_thread_callback = initialize_pygilstate_for_thread\n         options.task_execution_callback = task_execution_handler\n+        options.free_actor_object_callback = free_actor_object_callback\n         options.check_signals = check_signals\n         options.gc_collect = gc_collect\n         options.spill_objects = spill_objects_handler\ndiff --git a/python/ray/experimental/gpu_object_manager/gpu_object_manager.py b/python/ray/experimental/gpu_object_manager/gpu_object_manager.py\nindex a46c303faedd..9962d22f4976 100644\n--- a/python/ray/experimental/gpu_object_manager/gpu_object_manager.py\n+++ b/python/ray/experimental/gpu_object_manager/gpu_object_manager.py\n@@ -44,10 +44,6 @@ def __ray_fetch_gpu_object__(self, obj_id: str):\n         obj_id\n     ), f\"obj_id={obj_id} not found in GPU object store\"\n     tensors = gpu_object_store.get_gpu_object(obj_id)\n-    # TODO(kevin85421): The current garbage collection implementation for the\n-    # in-actor object store is naive. We garbage collect each object after it\n-    # is consumed once.\n-    gpu_object_store.remove_gpu_object(obj_id)\n     return tensors\n \n \ndiff --git a/python/ray/experimental/gpu_object_manager/gpu_object_store.py b/python/ray/experimental/gpu_object_manager/gpu_object_store.py\nindex 54aa7a56ab49..e6dc6d867681 100644\n--- a/python/ray/experimental/gpu_object_manager/gpu_object_store.py\n+++ b/python/ray/experimental/gpu_object_manager/gpu_object_store.py\n@@ -56,10 +56,6 @@ def __ray_send__(self, communicator_name: str, obj_id: str, dst_rank: int):\n                 f\"tensor device {tensor.device} does not match device {device}\"\n             )\n         collective.send(tensor, dst_rank, group_name=communicator_name)\n-    # TODO(kevin85421): The current garbage collection implementation for the\n-    # in-actor object store is naive. We garbage collect each object after it\n-    # is consumed once.\n-    gpu_object_store.remove_gpu_object(obj_id)\n \n \n def __ray_recv__(\n@@ -94,10 +90,6 @@ def __ray_fetch_gpu_object__(self, obj_id: str):\n         obj_id\n     ), f\"obj_id={obj_id} not found in GPU object store\"\n     tensors = gpu_object_store.get_gpu_object(obj_id)\n-    # TODO(kevin85421): The current garbage collection implementation for the\n-    # in-actor object store is naive. We garbage collect each object after it\n-    # is consumed once.\n-    gpu_object_store.remove_gpu_object(obj_id)\n     return tensors\n \n \ndiff --git a/python/ray/includes/libcoreworker.pxd b/python/ray/includes/libcoreworker.pxd\nindex fd90851311a3..b485abff9074 100644\n--- a/python/ray/includes/libcoreworker.pxd\n+++ b/python/ray/includes/libcoreworker.pxd\n@@ -406,6 +406,7 @@ cdef extern from \"ray/core_worker/core_worker.h\" nogil:\n             int64_t generator_backpressure_num_objects,\n             CTensorTransport tensor_transport\n         ) nogil) task_execution_callback\n+        (void(const CObjectID &) nogil) free_actor_object_callback\n         (function[void()]() nogil) initialize_thread_callback\n         (CRayStatus() nogil) check_signals\n         (void(c_bool) nogil) gc_collect\ndiff --git a/src/ray/core_worker/core_worker.cc b/src/ray/core_worker/core_worker.cc\nindex 783eb88064f4..522b81d2bfb5 100644\n--- a/src/ray/core_worker/core_worker.cc\n+++ b/src/ray/core_worker/core_worker.cc\n@@ -734,7 +734,13 @@ CoreWorker::CoreWorker(CoreWorkerOptions options, const WorkerID &worker_id)\n       },\n       push_error_callback,\n       RayConfig::instance().max_lineage_bytes(),\n-      *task_event_buffer_);\n+      *task_event_buffer_,\n+      /*get_actor_rpc_client_callback=*/\n+      [this](const ActorID &actor_id) {\n+        auto addr = actor_task_submitter_->GetActorAddress(actor_id);\n+        RAY_CHECK(addr.has_value()) << \"Actor address not found for actor \" << actor_id;\n+        return core_worker_client_pool_->GetOrConnect(addr.value());\n+      });\n \n   // Create an entry for the driver task in the task table. This task is\n   // added immediately with status RUNNING. This allows us to push errors\n@@ -4952,6 +4958,14 @@ void CoreWorker::HandlePlasmaObjectReady(rpc::PlasmaObjectReadyRequest request,\n   send_reply_callback(Status::OK(), nullptr, nullptr);\n }\n \n+void CoreWorker::HandleFreeActorObject(rpc::FreeActorObjectRequest request,\n+                                       rpc::FreeActorObjectReply *reply,\n+                                       rpc::SendReplyCallback send_reply_callback) {\n+  ObjectID object_id = ObjectID::FromBinary(request.object_id());\n+  options_.free_actor_object_callback(object_id);\n+  send_reply_callback(Status::OK(), nullptr, nullptr);\n+}\n+\n void CoreWorker::SetActorId(const ActorID &actor_id) {\n   absl::MutexLock lock(&mutex_);\n   if (!options_.is_local_mode) {\ndiff --git a/src/ray/core_worker/core_worker.h b/src/ray/core_worker/core_worker.h\nindex dbda41049df4..398e1dc3cfbe 100644\n--- a/src/ray/core_worker/core_worker.h\n+++ b/src/ray/core_worker/core_worker.h\n@@ -1268,6 +1268,12 @@ class CoreWorker : public rpc::CoreWorkerServiceHandler {\n   void HandleNumPendingTasks(rpc::NumPendingTasksRequest request,\n                              rpc::NumPendingTasksReply *reply,\n                              rpc::SendReplyCallback send_reply_callback) override;\n+\n+  // Free GPU objects from the in-actor GPU object store.\n+  void HandleFreeActorObject(rpc::FreeActorObjectRequest request,\n+                             rpc::FreeActorObjectReply *reply,\n+                             rpc::SendReplyCallback send_reply_callback) override;\n+\n   ///\n   /// Public methods related to async actor call. This should only be used when\n   /// the actor is (1) direct actor and (2) using async mode.\ndiff --git a/src/ray/core_worker/core_worker_options.h b/src/ray/core_worker/core_worker_options.h\nindex 20bb7f485c2a..6cfd2978398e 100644\n--- a/src/ray/core_worker/core_worker_options.h\n+++ b/src/ray/core_worker/core_worker_options.h\n@@ -87,6 +87,7 @@ struct CoreWorkerOptions {\n         raylet_ip_address(\"\"),\n         driver_name(\"\"),\n         task_execution_callback(nullptr),\n+        free_actor_object_callback(nullptr),\n         check_signals(nullptr),\n         initialize_thread_callback(nullptr),\n         gc_collect(nullptr),\n@@ -146,6 +147,8 @@ struct CoreWorkerOptions {\n   std::string driver_name;\n   /// Application-language worker callback to execute tasks.\n   TaskExecutionCallback task_execution_callback;\n+  /// Callback to free GPU object from the in-actor object store.\n+  std::function<void(const ObjectID &)> free_actor_object_callback;\n   /// Application-language callback to check for signals that have been received\n   /// since calling into C++. This will be called periodically (at least every\n   /// 1s) during long-running operations. If the function returns anything but StatusOK,\ndiff --git a/src/ray/core_worker/reference_count.h b/src/ray/core_worker/reference_count.h\nindex 68352ed5b85e..69a216185766 100644\n--- a/src/ray/core_worker/reference_count.h\n+++ b/src/ray/core_worker/reference_count.h\n@@ -771,6 +771,8 @@ class ReferenceCounter : public ReferenceCounterInterface,\n     /// counting is enabled, then some raylet must be pinning the object value.\n     /// This is the address of that raylet.\n     std::optional<NodeID> pinned_at_raylet_id;\n+    /// TODO(kevin85421): Make tensor_transport a required field for all constructors.\n+    ///\n     /// The transport used for the object.\n     rpc::TensorTransport tensor_transport = rpc::TensorTransport::OBJECT_STORE;\n     /// Whether we own the object. If we own the object, then we are\ndiff --git a/src/ray/core_worker/task_manager.cc b/src/ray/core_worker/task_manager.cc\nindex 62f228d2ec4a..ee2288ce0ff8 100644\n--- a/src/ray/core_worker/task_manager.cc\n+++ b/src/ray/core_worker/task_manager.cc\n@@ -272,9 +272,33 @@ std::vector<rpc::ObjectReference> TaskManager::AddPendingTask(\n \n     return_ids.push_back(return_id);\n     rpc::ObjectReference ref;\n-    ref.set_object_id(spec.ReturnId(i).Binary());\n+    auto object_id = spec.ReturnId(i);\n+    ref.set_object_id(object_id.Binary());\n     ref.mutable_owner_address()->CopyFrom(caller_address);\n     ref.set_call_site(call_site);\n+\n+    // Register the callback to free the GPU object when it is out of scope.\n+    auto tensor_transport = reference_counter_.GetTensorTransport(object_id);\n+    if (tensor_transport.value_or(rpc::TensorTransport::OBJECT_STORE) !=\n+        rpc::TensorTransport::OBJECT_STORE) {\n+      reference_counter_.AddObjectOutOfScopeOrFreedCallback(\n+          object_id, [this](const ObjectID &object_id) {\n+            auto actor_id = ObjectID::ToActorID(object_id);\n+            auto rpc_client = get_actor_rpc_client_callback_(actor_id);\n+            auto request = rpc::FreeActorObjectRequest();\n+            request.set_object_id(object_id.Binary());\n+            rpc_client->FreeActorObject(\n+                request,\n+                [object_id, actor_id](Status status,\n+                                      const rpc::FreeActorObjectReply &reply) {\n+                  if (!status.ok()) {\n+                    RAY_LOG(ERROR).WithField(object_id).WithField(actor_id)\n+                        << \"Failed to free actor object: \" << status;\n+                  }\n+                });\n+          });\n+    }\n+\n     returned_refs.push_back(std::move(ref));\n   }\n \n@@ -1254,21 +1278,7 @@ void TaskManager::RemoveFinishedTaskReferences(\n     bool release_lineage,\n     const rpc::Address &borrower_addr,\n     const ReferenceCounter::ReferenceTableProto &borrowed_refs) {\n-  std::vector<ObjectID> plasma_dependencies;\n-  for (size_t i = 0; i < spec.NumArgs(); i++) {\n-    if (spec.ArgByRef(i)) {\n-      plasma_dependencies.push_back(spec.ArgObjectId(i));\n-    } else {\n-      const auto &inlined_refs = spec.ArgInlinedRefs(i);\n-      for (const auto &inlined_ref : inlined_refs) {\n-        plasma_dependencies.push_back(ObjectID::FromBinary(inlined_ref.object_id()));\n-      }\n-    }\n-  }\n-  if (spec.IsActorTask()) {\n-    const auto actor_creation_return_id = spec.ActorCreationDummyObjectId();\n-    plasma_dependencies.push_back(actor_creation_return_id);\n-  }\n+  std::vector<ObjectID> plasma_dependencies = ExtractPlasmaDependencies(spec);\n \n   std::vector<ObjectID> return_ids;\n   size_t num_returns = spec.NumReturns();\n@@ -1637,5 +1647,29 @@ ObjectID TaskManager::TaskGeneratorId(const TaskID &task_id) const {\n   return it->second.spec.ReturnId(0);\n }\n \n+std::vector<ObjectID> ExtractPlasmaDependencies(const TaskSpecification &spec) {\n+  std::vector<ObjectID> plasma_dependencies;\n+  for (size_t i = 0; i < spec.NumArgs(); i++) {\n+    if (spec.ArgByRef(i)) {\n+      plasma_dependencies.push_back(spec.ArgObjectId(i));\n+    } else if (spec.ArgTensorTransport(i) != rpc::TensorTransport::OBJECT_STORE) {\n+      // GPU objects are inlined but the actual data lives on the remote actor.\n+      // Therefore, we apply the reference counting protocol used for plasma objects\n+      // instead of decrementing the ref count upon inlining.\n+      plasma_dependencies.push_back(spec.ArgObjectId(i));\n+    } else {\n+      const auto &inlined_refs = spec.ArgInlinedRefs(i);\n+      for (const auto &inlined_ref : inlined_refs) {\n+        plasma_dependencies.push_back(ObjectID::FromBinary(inlined_ref.object_id()));\n+      }\n+    }\n+  }\n+  if (spec.IsActorTask()) {\n+    const auto actor_creation_return_id = spec.ActorCreationDummyObjectId();\n+    plasma_dependencies.push_back(actor_creation_return_id);\n+  }\n+  return plasma_dependencies;\n+}\n+\n }  // namespace core\n }  // namespace ray\ndiff --git a/src/ray/core_worker/task_manager.h b/src/ray/core_worker/task_manager.h\nindex 78d5e3ec2425..54503b3f53a3 100644\n--- a/src/ray/core_worker/task_manager.h\n+++ b/src/ray/core_worker/task_manager.h\n@@ -167,14 +167,17 @@ class ObjectRefStream {\n \n class TaskManager : public TaskManagerInterface {\n  public:\n-  TaskManager(CoreWorkerMemoryStore &in_memory_store,\n-              ReferenceCounter &reference_counter,\n-              PutInLocalPlasmaCallback put_in_local_plasma_callback,\n-              RetryTaskCallback retry_task_callback,\n-              std::function<bool(const TaskSpecification &spec)> queue_generator_resubmit,\n-              PushErrorCallback push_error_callback,\n-              int64_t max_lineage_bytes,\n-              worker::TaskEventBuffer &task_event_buffer)\n+  TaskManager(\n+      CoreWorkerMemoryStore &in_memory_store,\n+      ReferenceCounter &reference_counter,\n+      PutInLocalPlasmaCallback put_in_local_plasma_callback,\n+      RetryTaskCallback retry_task_callback,\n+      std::function<bool(const TaskSpecification &spec)> queue_generator_resubmit,\n+      PushErrorCallback push_error_callback,\n+      int64_t max_lineage_bytes,\n+      worker::TaskEventBuffer &task_event_buffer,\n+      std::function<std::shared_ptr<ray::rpc::CoreWorkerClientInterface>(const ActorID &)>\n+          client_factory)\n       : in_memory_store_(in_memory_store),\n         reference_counter_(reference_counter),\n         put_in_local_plasma_callback_(std::move(put_in_local_plasma_callback)),\n@@ -182,7 +185,8 @@ class TaskManager : public TaskManagerInterface {\n         queue_generator_resubmit_(std::move(queue_generator_resubmit)),\n         push_error_callback_(std::move(push_error_callback)),\n         max_lineage_bytes_(max_lineage_bytes),\n-        task_event_buffer_(task_event_buffer) {\n+        task_event_buffer_(task_event_buffer),\n+        get_actor_rpc_client_callback_(std::move(client_factory)) {\n     task_counter_.SetOnChangeCallback(\n         [this](const std::tuple<std::string, rpc::TaskStatus, bool> &key)\n             ABSL_EXCLUSIVE_LOCKS_REQUIRED(&mu_) {\n@@ -773,8 +777,21 @@ class TaskManager : public TaskManagerInterface {\n   /// error).\n   worker::TaskEventBuffer &task_event_buffer_;\n \n+  /// Callback to get the actor RPC client.\n+  std::function<std::shared_ptr<ray::rpc::CoreWorkerClientInterface>(\n+      const ActorID &actor_id)>\n+      get_actor_rpc_client_callback_;\n+\n   friend class TaskManagerTest;\n };\n \n+/// Extract plasma dependencies from a task specification.\n+/// This includes arguments passed by reference, inlined GPU objects,\n+/// inlined references, and actor creation dummy object IDs.\n+///\n+/// \\param[in] spec The task specification to extract dependencies from.\n+/// \\return Vector of ObjectIDs representing plasma dependencies.\n+std::vector<ObjectID> ExtractPlasmaDependencies(const TaskSpecification &spec);\n+\n }  // namespace core\n }  // namespace ray\ndiff --git a/src/ray/core_worker/transport/dependency_resolver.cc b/src/ray/core_worker/transport/dependency_resolver.cc\nindex 82cc757c78e8..c86d8889552b 100644\n--- a/src/ray/core_worker/transport/dependency_resolver.cc\n+++ b/src/ray/core_worker/transport/dependency_resolver.cc\n@@ -54,6 +54,12 @@ void InlineDependencies(\n             // the GPU object from the in-actor GPU object store using the object ID as\n             // the key.\n             mutable_arg->clear_object_ref();\n+            // We only push the object ID of the non-GPU object to the inlined dependency\n+            // IDs to avoid the reference count being updated immediately. GPU objects are\n+            // inlined, but the actual data lives on the remote actor. Therefore, if we\n+            // decrement the reference count upon inlining, we may cause the tensors on\n+            // the sender actor to be freed before transferring to the receiver actor.\n+            inlined_dependency_ids->push_back(id);\n           } else {\n             mutable_arg->set_tensor_transport(transport);\n           }\n@@ -71,7 +77,6 @@ void InlineDependencies(\n             mutable_arg->add_nested_inlined_refs()->CopyFrom(nested_ref);\n             contained_ids->push_back(ObjectID::FromBinary(nested_ref.object_id()));\n           }\n-          inlined_dependency_ids->push_back(id);\n         }\n         found++;\n       }\ndiff --git a/src/ray/protobuf/core_worker.proto b/src/ray/protobuf/core_worker.proto\nindex 581bc5a408f6..f8b1fd4968f6 100644\n--- a/src/ray/protobuf/core_worker.proto\n+++ b/src/ray/protobuf/core_worker.proto\n@@ -415,6 +415,12 @@ message NumPendingTasksReply {\n   int64 num_pending_tasks = 1;\n }\n \n+message FreeActorObjectRequest {\n+  bytes object_id = 1;\n+}\n+\n+message FreeActorObjectReply {}\n+\n message ReportGeneratorItemReturnsRequest {\n   // The intermediate return object that's dynamically\n   // generated from the executor side.\n@@ -572,6 +578,9 @@ service CoreWorkerService {\n   // API user.\n   rpc NumPendingTasks(NumPendingTasksRequest) returns (NumPendingTasksReply);\n \n+  // Free GPU object from in-actor GPU object store.\n+  rpc FreeActorObject(FreeActorObjectRequest) returns (FreeActorObjectReply);\n+\n   // Registers a mutable object reader for compiled graphs.\n   // Failure: TODO: Needs failure behavior.\n   rpc RegisterMutableObjectReader(RegisterMutableObjectReaderRequest)\ndiff --git a/src/ray/rpc/worker/core_worker_client.h b/src/ray/rpc/worker/core_worker_client.h\nindex 85b0eec5f589..29f3afe8fada 100644\n--- a/src/ray/rpc/worker/core_worker_client.h\n+++ b/src/ray/rpc/worker/core_worker_client.h\n@@ -189,6 +189,9 @@ class CoreWorkerClientInterface : public pubsub::SubscriberClientInterface {\n       const RayletNotifyGCSRestartRequest &request,\n       const ClientCallback<RayletNotifyGCSRestartReply> &callback) {}\n \n+  virtual void FreeActorObject(const FreeActorObjectRequest &request,\n+                               const ClientCallback<FreeActorObjectReply> &callback) {}\n+\n   virtual ~CoreWorkerClientInterface() = default;\n };\n \n@@ -342,6 +345,12 @@ class CoreWorkerClient : public std::enable_shared_from_this<CoreWorkerClient>,\n                          /*method_timeout_ms*/ -1,\n                          override)\n \n+  VOID_RPC_CLIENT_METHOD(CoreWorkerService,\n+                         FreeActorObject,\n+                         grpc_client_,\n+                         /*method_timeout_ms*/ -1,\n+                         override)\n+\n   void PushActorTask(std::unique_ptr<PushTaskRequest> request,\n                      bool skip_queue,\n                      ClientCallback<PushTaskReply> &&callback) override;\ndiff --git a/src/ray/rpc/worker/core_worker_server.h b/src/ray/rpc/worker/core_worker_server.h\nindex 2f37619de662..39a6a918414e 100644\n--- a/src/ray/rpc/worker/core_worker_server.h\n+++ b/src/ray/rpc/worker/core_worker_server.h\n@@ -59,7 +59,8 @@ namespace rpc {\n   RAY_CORE_WORKER_RPC_SERVICE_HANDLER(PlasmaObjectReady)           \\\n   RAY_CORE_WORKER_RPC_SERVICE_HANDLER(Exit)                        \\\n   RAY_CORE_WORKER_RPC_SERVICE_HANDLER(AssignObjectOwner)           \\\n-  RAY_CORE_WORKER_RPC_SERVICE_HANDLER(NumPendingTasks)\n+  RAY_CORE_WORKER_RPC_SERVICE_HANDLER(NumPendingTasks)             \\\n+  RAY_CORE_WORKER_RPC_SERVICE_HANDLER(FreeActorObject)\n \n #define RAY_CORE_WORKER_DECLARE_RPC_HANDLERS                           \\\n   DECLARE_VOID_RPC_SERVICE_HANDLER_METHOD(PushTask)                    \\\n@@ -85,7 +86,8 @@ namespace rpc {\n   DECLARE_VOID_RPC_SERVICE_HANDLER_METHOD(PlasmaObjectReady)           \\\n   DECLARE_VOID_RPC_SERVICE_HANDLER_METHOD(Exit)                        \\\n   DECLARE_VOID_RPC_SERVICE_HANDLER_METHOD(AssignObjectOwner)           \\\n-  DECLARE_VOID_RPC_SERVICE_HANDLER_METHOD(NumPendingTasks)\n+  DECLARE_VOID_RPC_SERVICE_HANDLER_METHOD(NumPendingTasks)             \\\n+  DECLARE_VOID_RPC_SERVICE_HANDLER_METHOD(FreeActorObject)\n \n /// Interface of the `CoreWorkerServiceHandler`, see `src/ray/protobuf/core_worker.proto`.\n class CoreWorkerServiceHandler : public DelayedServiceHandler {\n","test_patch":"diff --git a/python/ray/tests/test_gpu_objects_gloo.py b/python/ray/tests/test_gpu_objects_gloo.py\nindex b48ef625ae35..470f6c16cb9b 100644\n--- a/python/ray/tests/test_gpu_objects_gloo.py\n+++ b/python/ray/tests/test_gpu_objects_gloo.py\n@@ -5,6 +5,7 @@\n import ray\n from ray.experimental.collective import create_collective_group\n from ray._private.custom_types import TensorTransportEnum\n+from ray._common.test_utils import wait_for_condition\n \n \n @ray.remote\n@@ -24,10 +25,97 @@ def get_gpu_object(self, obj_id: str):\n         )\n         if gpu_object_store.has_gpu_object(obj_id):\n             gpu_object = gpu_object_store.get_gpu_object(obj_id)\n-            print(f\"gpu_object: {gpu_object}\")\n             return gpu_object\n         return None\n \n+    def get_num_gpu_objects(self):\n+        gpu_object_manager = ray._private.worker.global_worker.gpu_object_manager\n+        return len(gpu_object_manager.gpu_object_store.gpu_object_store)\n+\n+\n+@pytest.mark.parametrize(\"data_size_bytes\", [100])\n+def test_gc_gpu_object(ray_start_regular, data_size_bytes):\n+    \"\"\"\n+    For small data, GPU objects are inlined, but the actual data lives\n+    on the remote actor. Therefore, if we decrement the reference count\n+    upon inlining, we may cause the tensors on the sender actor to be\n+    freed before transferring to the receiver actor.\n+\n+    # TODO(kevin85421): Add a test for large CPU data that is not inlined\n+    # after https://github.com/ray-project/ray/issues/54281 is fixed.\n+    \"\"\"\n+    world_size = 2\n+    actors = [GPUTestActor.remote() for _ in range(world_size)]\n+    create_collective_group(actors, backend=\"torch_gloo\")\n+\n+    small_tensor = torch.randn((1,))\n+    cpu_data = b\"1\" * data_size_bytes\n+    data = [small_tensor, cpu_data]\n+    sender = actors[0]\n+    receiver = actors[1]\n+\n+    ref1 = sender.echo.remote(data)\n+    ref2 = receiver.double.remote(ref1)\n+    ref3 = receiver.double.remote(ref1)\n+\n+    result = ray.get(ref2)\n+    assert result[0] == pytest.approx(small_tensor * 2)\n+    assert result[1] == cpu_data * 2\n+    result = ray.get(ref3)\n+    assert result[0] == pytest.approx(small_tensor * 2)\n+    assert result[1] == cpu_data * 2\n+\n+    wait_for_condition(\n+        lambda: ray.get(receiver.get_num_gpu_objects.remote()) == 0,\n+        timeout=10,\n+        retry_interval_ms=100,\n+    )\n+\n+    del ref1\n+\n+    wait_for_condition(\n+        lambda: ray.get(sender.get_num_gpu_objects.remote()) == 0,\n+        timeout=10,\n+        retry_interval_ms=100,\n+    )\n+\n+\n+@pytest.mark.parametrize(\"data_size_bytes\", [100])\n+def test_gc_del_ref_before_recv_finish(ray_start_regular, data_size_bytes):\n+    \"\"\"\n+    This test deletes the ObjectRef of the GPU object before calling\n+    `ray.get` to ensure the receiver finishes receiving the GPU object.\n+    \"\"\"\n+    world_size = 2\n+    actors = [GPUTestActor.remote() for _ in range(world_size)]\n+    create_collective_group(actors, backend=\"torch_gloo\")\n+\n+    small_tensor = torch.randn((1,))\n+    cpu_data = b\"1\" * data_size_bytes\n+    data = [small_tensor, cpu_data]\n+    sender = actors[0]\n+    receiver = actors[1]\n+\n+    ref1 = sender.echo.remote(data)\n+    ref2 = receiver.double.remote(ref1)\n+\n+    del ref1\n+\n+    result = ray.get(ref2)\n+    assert result[0] == pytest.approx(small_tensor * 2)\n+    assert result[1] == cpu_data * 2\n+\n+    wait_for_condition(\n+        lambda: ray.get(receiver.get_num_gpu_objects.remote()) == 0,\n+        timeout=10,\n+        retry_interval_ms=100,\n+    )\n+    wait_for_condition(\n+        lambda: ray.get(sender.get_num_gpu_objects.remote()) == 0,\n+        timeout=10,\n+        retry_interval_ms=100,\n+    )\n+\n \n def test_p2p(ray_start_regular):\n     world_size = 2\n@@ -126,9 +214,10 @@ def test_trigger_out_of_band_tensor_transfer(ray_start_regular):\n \n     tensor = torch.tensor([1, 2, 3])\n     gpu_ref = src_actor.echo.remote(tensor)\n+    gpu_obj_id = gpu_ref.hex()\n \n     # Check src_actor has the GPU object\n-    ret_val_src = ray.get(src_actor.get_gpu_object.remote(gpu_ref.hex()))\n+    ret_val_src = ray.get(src_actor.get_gpu_object.remote(gpu_obj_id))\n     assert ret_val_src is not None\n     assert len(ret_val_src) == 1\n     assert torch.equal(ret_val_src[0], tensor)\n@@ -137,15 +226,11 @@ def test_trigger_out_of_band_tensor_transfer(ray_start_regular):\n     gpu_object_manager.add_gpu_object_ref(gpu_ref, src_actor, TensorTransportEnum.GLOO)\n \n     # Trigger out-of-band tensor transfer from src_actor to dst_actor.\n-    # The GPU object will be removed from src_actor's GPU object store\n-    # because the current GC implementation garbage collects GPU objects\n-    # whenever they are consumed once.\n     task_args = (gpu_ref,)\n     gpu_object_manager.trigger_out_of_band_tensor_transfer(dst_actor, task_args)\n-    assert ray.get(src_actor.get_gpu_object.remote(gpu_ref.hex())) is None\n \n     # Check dst_actor has the GPU object\n-    ret_val_dst = ray.get(dst_actor.get_gpu_object.remote(gpu_ref.hex()))\n+    ret_val_dst = ray.get(dst_actor.get_gpu_object.remote(gpu_obj_id))\n     assert ret_val_dst is not None\n     assert len(ret_val_dst) == 1\n     assert torch.equal(ret_val_dst[0], tensor)\ndiff --git a/src/ray/core_worker/test/dependency_resolver_test.cc b/src/ray/core_worker/test/dependency_resolver_test.cc\nindex da9f88f37932..1f9194c384d4 100644\n--- a/src/ray/core_worker/test/dependency_resolver_test.cc\n+++ b/src/ray/core_worker/test/dependency_resolver_test.cc\n@@ -537,7 +537,10 @@ TEST(LocalDependencyResolverTest, TestMixedTensorTransport) {\n   ASSERT_TRUE(task.GetMutableMessage().args(1).is_inlined());\n   ASSERT_FALSE(task.GetMutableMessage().args(1).has_object_ref());\n \n-  ASSERT_EQ(task_manager->num_inlined_dependencies, 2);\n+  // The first argument is inlined but will not be passed into\n+  // `OnTaskDependenciesInlined` because it is a GPU object reference.\n+  // Please see https://github.com/ray-project/ray/pull/53911 for more details.\n+  ASSERT_EQ(task_manager->num_inlined_dependencies, 1);\n   ASSERT_EQ(resolver.NumPendingTasks(), 0);\n }\n \ndiff --git a/src/ray/core_worker/test/task_manager_test.cc b/src/ray/core_worker/test/task_manager_test.cc\nindex 36ecdf07951e..8093eff9257d 100644\n--- a/src/ray/core_worker/test/task_manager_test.cc\n+++ b/src/ray/core_worker/test/task_manager_test.cc\n@@ -39,7 +39,8 @@ TaskSpecification CreateTaskHelper(uint64_t num_returns,\n                                    std::vector<ObjectID> dependencies,\n                                    bool dynamic_returns = false,\n                                    bool streaming_generator = false,\n-                                   int64_t generator_backpressure_num_objects = -1) {\n+                                   int64_t generator_backpressure_num_objects = -1,\n+                                   bool enable_tensor_transport = false) {\n   TaskSpecification task;\n   task.GetMutableMessage().set_task_id(TaskID::FromRandom(JobID::FromInt(1)).Binary());\n   task.GetMutableMessage().set_num_returns(num_returns);\n@@ -57,6 +58,14 @@ TaskSpecification CreateTaskHelper(uint64_t num_returns,\n         generator_backpressure_num_objects);\n   }\n \n+  auto tensor_transport = rpc::TensorTransport::OBJECT_STORE;\n+  if (enable_tensor_transport) {\n+    // Currently, only actors support transferring tensors out-of-band.\n+    task.GetMutableMessage().set_type(TaskType::ACTOR_TASK);\n+    tensor_transport = rpc::TensorTransport::NCCL;\n+  }\n+  task.GetMutableMessage().set_tensor_transport(tensor_transport);\n+\n   return task;\n }\n \n@@ -151,7 +160,11 @@ class TaskManagerTest : public ::testing::Test {\n                const std::string &error_message,\n                double timestamp) { return Status::OK(); },\n             max_lineage_bytes,\n-            *task_event_buffer_mock_.get()) {}\n+            *task_event_buffer_mock_.get(),\n+            [](const ActorID &actor_id)\n+                -> std::shared_ptr<ray::rpc::CoreWorkerClientInterface> {\n+              return nullptr;\n+            }) {}\n \n   virtual void TearDown() { AssertNoLeaks(); }\n \n@@ -2542,6 +2555,66 @@ TEST_F(TaskManagerLineageTest, RecoverIntermediateObjectInStreamingGenerator) {\n   CompletePendingStreamingTask(spec2, caller_address, 0);\n }\n \n+TEST_F(TaskManagerTest, TestGPUObjectTaskSuccess) {\n+  rpc::Address caller_address;\n+  auto spec = CreateTaskHelper(/*num_returns*/ 1,\n+                               {},\n+                               /*dynamic_returns=*/false,\n+                               /*streaming_generator=*/false,\n+                               /*generator_backpressure_num_objects*/ -1,\n+                               /*enable_tensor_transport=*/true);\n+\n+  // Pass a GPU ObjectRef as an argument.\n+  ObjectID gpu_obj_ref = ObjectID::FromRandom();\n+  auto *arg = spec.GetMutableMessage().add_args();\n+  arg->set_is_inlined(false);\n+  arg->set_tensor_transport(rpc::TensorTransport::NCCL);\n+  arg->mutable_object_ref()->set_object_id(gpu_obj_ref.Binary());\n+\n+  // `gpu_obj_ref` should have a local reference when the sender actor\n+  // generates the ObjectRef.\n+  reference_counter_->AddLocalReference(gpu_obj_ref, \"\");\n+\n+  // Call AddPendingTask to add the task to the task manager.\n+  auto object_refs = manager_.AddPendingTask(caller_address, spec, \"\");\n+  ASSERT_EQ(object_refs.size(), 1);\n+  ASSERT_EQ(manager_.NumSubmissibleTasks(), 1);\n+  ASSERT_EQ(manager_.NumPendingTasks(), 1);\n+  ASSERT_TRUE(manager_.IsTaskPending(spec.TaskId()));\n+\n+  // GPU object, the return object and the actor creation dummy object are in\n+  // scope.\n+  auto return_id = spec.ReturnId(0);\n+  ASSERT_EQ(reference_counter_->NumObjectIDsInScope(), 3);\n+  ASSERT_TRUE(reference_counter_->IsObjectPendingCreation(return_id));\n+\n+  manager_.MarkDependenciesResolved(spec.TaskId());\n+  ASSERT_TRUE(manager_.IsTaskPending(spec.TaskId()));\n+  ASSERT_FALSE(manager_.IsTaskWaitingForExecution(spec.TaskId()));\n+\n+  manager_.MarkTaskWaitingForExecution(\n+      spec.TaskId(), NodeID::FromRandom(), WorkerID::FromRandom());\n+  ASSERT_TRUE(manager_.IsTaskWaitingForExecution(spec.TaskId()));\n+\n+  rpc::PushTaskReply reply;\n+  auto return_object = reply.add_return_objects();\n+  return_object->set_object_id(return_id.Binary());\n+  auto data = GenerateRandomBuffer();\n+  return_object->set_data(data->Data(), data->Size());\n+  manager_.CompletePendingTask(spec.TaskId(), reply, rpc::Address(), false);\n+  ASSERT_FALSE(manager_.IsTaskPending(spec.TaskId()));\n+  // We assume that the GPU object ref is still in scope, so both the return object\n+  // and the GPU object ref should remain.\n+  ASSERT_EQ(reference_counter_->NumObjectIDsInScope(), 2);\n+  ASSERT_FALSE(reference_counter_->IsObjectPendingCreation(return_id));\n+\n+  // Call `RemoveLocalReference` to simulate that the GPU object ref is out of scope.\n+  // Then, the GPU object should be removed.\n+  std::vector<ObjectID> removed;\n+  reference_counter_->RemoveLocalReference(gpu_obj_ref, &removed);\n+  ASSERT_EQ(removed[0], gpu_obj_ref);\n+  ASSERT_EQ(reference_counter_->NumObjectIDsInScope(), 1);\n+}\n }  // namespace core\n }  // namespace ray\n \n","problem_statement":"[core][gpu-objects] Garbage collection for in-actor GPU objects\n### Description\n\nWe should garbage-collect the GPU objects in the in-actor store when their reference count reaches 0 for both the sender / receiver actors.\n\n### Use case\n\n_No response_\n[core][gpu-objects] Actor sends the same ObjectRef twice to another actor\n### Description\n\nIf an actor sends the same ObjectRef twice to another actor, we should not call NCCL send twice. Instead, we only call the NCCL send once and the receiver retrieve the tensor from its in-actor storage twice.\n\n### Use case\n\n_No response_\n","hints_text":"do this first\n\nI realized that the first copy could get modified by the receiving task, so it would be safer to actually just do the transfer twice and get two different copies.\n\n","all_hints_text":"do this first\n\nI realized that the first copy could get modified by the receiving task, so it would be safer to actually just do the transfer twice and get two different copies.\nThis is supported by #53911 natively.\n\n","commit_urls":["https://github.com/ray-project/ray/commit/4014897c67f76971d1ea344341d4fb52e6ffec8e","https://github.com/ray-project/ray/commit/b2075e34b26b1f95fc4e4bf66890674ab61344ad","https://github.com/ray-project/ray/commit/126015421a85dba7370d62e489c6d3ee51ad5e75","https://github.com/ray-project/ray/commit/7a0aeb279b39d9d705acad2c72cf30f3434aa6e0","https://github.com/ray-project/ray/commit/c19a7019f70101db26ec1d97252eb6e4932ff8e6","https://github.com/ray-project/ray/commit/d9b38dd584382ddb846eaded2dce036188baf934","https://github.com/ray-project/ray/commit/7db40750c76f6b6bcc206bc6f467f1238b5bb0ba","https://github.com/ray-project/ray/commit/3c686aece898eeea8279aad325179fadf577e98b","https://github.com/ray-project/ray/commit/cab77876ca14aee974c9b8bdd2223c58aafb7ab0","https://github.com/ray-project/ray/commit/4c99122b9f07c91a23f0719ac55054ca7293b027","https://github.com/ray-project/ray/commit/1b1d579167242fc43b9b6e11da408c716924d50f","https://github.com/ray-project/ray/commit/631b0c42661bfe9c659671a4f3b5fc278f6aa2ac","https://github.com/ray-project/ray/commit/5a65fb10e09ebd75026c8a1e2f71e1a2e054fbcd","https://github.com/ray-project/ray/commit/c371b0a0618d69d242010be51af550702a2d2d99","https://github.com/ray-project/ray/commit/31ab2b92a4bb50de2dada338dc12b4c5c8e61164","https://github.com/ray-project/ray/commit/b23df6dc8c3df5f26573898f413d939e8782e286","https://github.com/ray-project/ray/commit/116dda806e5adfe2eb22ddc8b935d94071c901b7","https://github.com/ray-project/ray/commit/9d43c6358c6560559c45b2b89c9c0b9ce3659fc0","https://github.com/ray-project/ray/commit/f40ae2ce1c35971c2f9cbfbbcd37af4d3ad6432a","https://github.com/ray-project/ray/commit/f203a1201fb9569ebabe8bb54e3cc74c42074b87","https://github.com/ray-project/ray/commit/42681e46cf34dbde1af63806f66d612257cb830c","https://github.com/ray-project/ray/commit/fbfd7c7ce087b3b196e880cc01946fc71c2ebd6a","https://github.com/ray-project/ray/commit/0e8e098b1f069a8617c81c84ef0dd3b4a6c0937b","https://github.com/ray-project/ray/commit/64aa8e8afd3764745b267bf3c2ab65eeb9cd5608","https://github.com/ray-project/ray/commit/24c807ca883e448b75edcf7d8f1be649d2dff460","https://github.com/ray-project/ray/commit/49099ac80719f12196073123359dabbcc442ecd2","https://github.com/ray-project/ray/commit/908599ac4df67dc16c8087c502a6269e66db82b2","https://github.com/ray-project/ray/commit/b6f3d6cf6b10029c657c1e6e8107b458ec733429","https://github.com/ray-project/ray/commit/96f41861d651a85ef60c3f611f449e0c01ba3a38","https://github.com/ray-project/ray/commit/7a46e2368099854cfcf80043005d9738ed87f390","https://github.com/ray-project/ray/commit/bfda04c357f617e6c46e5371180d31ab3c8e4c44","https://github.com/ray-project/ray/commit/b449bbe062d3c8c7a72dcc1b9577cbfe5f5a18bf","https://github.com/ray-project/ray/commit/98cbc12e482ed815d332003bd292cc0dca75fed6","https://github.com/ray-project/ray/commit/48d4eed990e72079111e66804ab81f8ac717c0af","https://github.com/ray-project/ray/commit/5dfade362021412b5c9ea2f61a535cd59fb9d163","https://github.com/ray-project/ray/commit/b06c11703b6611b19a5be3c974be3517e50a75e1","https://github.com/ray-project/ray/commit/0f271016cfcbf96904e6eaef1aea2754408ca9d8","https://github.com/ray-project/ray/commit/0f7f1f38aa4e465d5d13a424ee20bbb6077456e6","https://github.com/ray-project/ray/commit/db48e2d5753b6d40a187837718fc90f2a054a60f","https://github.com/ray-project/ray/commit/d1dbc869179de86e983ab6cee4a42257751df77a","https://github.com/ray-project/ray/commit/de5a15f1ac1e50aef99a51090a656333de9e7b94"],"created_at":"2025-06-18T08:38:42Z","classification":"Efficiency"}
{"repo":"ray-project/ray","pull_number":53283,"instance_id":"ray-project__ray-53283","issue_numbers":[53027],"base_commit":"5a720247fb59e2987d424796ef58ae031fea89d9","patch":"diff --git a/python/ray/autoscaler/_private/resource_demand_scheduler.py b/python/ray/autoscaler/_private/resource_demand_scheduler.py\nindex 1983b3896564..2d737c3933d8 100644\n--- a/python/ray/autoscaler/_private/resource_demand_scheduler.py\n+++ b/python/ray/autoscaler/_private/resource_demand_scheduler.py\n@@ -373,7 +373,7 @@ def _update_node_resources_from_runtime(\n             if runtime_resources:\n                 runtime_resources = copy.deepcopy(runtime_resources)\n                 resources = self.node_types[node_type].get(\"resources\", {})\n-                for key in [\"CPU\", \"GPU\", \"memory\", \"object_store_memory\"]:\n+                for key in [\"CPU\", \"GPU\", \"memory\"]:\n                     if key in runtime_resources:\n                         resources[key] = runtime_resources[key]\n                 self.node_types[node_type][\"resources\"] = resources\n","test_patch":"diff --git a/python/ray/tests/test_resource_demand_scheduler.py b/python/ray/tests/test_resource_demand_scheduler.py\nindex 26310e792567..69e4fc478ac0 100644\n--- a/python/ray/tests/test_resource_demand_scheduler.py\n+++ b/python/ray/tests/test_resource_demand_scheduler.py\n@@ -949,6 +949,78 @@ def test_request_resources_existing_usage():\n     assert not rem\n \n \n+def test_do_not_add_nodes_based_on_object_store_memory():\n+    provider = MockProvider()\n+    TYPES = {\n+        \"ray.worker.4090.standard\": {\n+            \"resources\": {\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24},\n+            \"max_workers\": 5,\n+        },\n+        \"ray.worker.4090.highmem\": {\n+            \"resources\": {\"CPU\": 16, \"GPU\": 1, \"memory\": 62277025792, \"gram\": 24},\n+            \"max_workers\": 5,\n+        },\n+    }\n+    provider.create_node(\n+        {},\n+        {\n+            TAG_RAY_USER_NODE_TYPE: \"ray.worker.4090.standard\",\n+            TAG_RAY_NODE_KIND: NODE_KIND_WORKER,\n+            TAG_RAY_NODE_STATUS: STATUS_UP_TO_DATE,\n+        },\n+        1,\n+    )\n+    scheduler = ResourceDemandScheduler(\n+        provider,\n+        TYPES,\n+        max_workers=100,\n+        head_node_type=\"empty_node\",\n+        upscaling_speed=1,\n+    )\n+\n+    ips = provider.non_terminated_node_ips({})\n+    assert len(ips) == 1\n+\n+    unused_resources_by_ip = {\n+        ips[0]: {\n+            \"CPU\": 0.0,\n+            \"GPU\": 0.0,\n+            \"memory\": 0.0,\n+            \"gram\": 0.0,\n+        }\n+    }\n+    max_resources_by_ip = {\n+        ips[0]: {\n+            \"CPU\": 16.0,\n+            \"GPU\": 1.0,\n+            \"memory\": 30107260928.0,\n+            \"gram\": 24.0,\n+            \"object_store_memory\": 4933059335.0,\n+        }\n+    }\n+    # At this point, there is one node of type \"ray.worker.4090.standard\" in the cluster,\n+    # but all its resources are used.\n+    # Now, we try to request a new resource_demand that matches \"ray.worker.4090.standard\".\n+    # The scheduler should add a new node of type \"ray.worker.4090.standard\".\n+    # This test ensures that the scheduler does not take \"object_store_memory\"\n+    # into account when deciding which node type to add. Previously, the scheduler\n+    # would consider \"object_store_memory\" from max_resources_by_ip, and as a result,\n+    # choose \"ray.worker.4090.highmem\" instead of \"ray.worker.4090.standard\".\n+    resource_demands = [{\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24}]\n+    to_launch, _ = scheduler.get_nodes_to_launch(\n+        nodes=provider.non_terminated_nodes({}),\n+        launching_nodes={},\n+        resource_demands=resource_demands,\n+        unused_resources_by_ip=unused_resources_by_ip,\n+        pending_placement_groups=[],\n+        max_resources_by_ip=max_resources_by_ip,\n+        ensure_min_cluster_size=[],\n+        node_availability_summary=NodeAvailabilitySummary(node_availabilities={}),\n+    )\n+    assert to_launch.get(\"ray.worker.4090.standard\") == 1, to_launch\n+    assert to_launch.get(\"ray.worker.4090.highmem\") is None, to_launch\n+\n+\n def test_backlog_queue_impact_on_binpacking_time():\n     new_types = copy.deepcopy(TYPES_A)\n     new_types[\"p2.8xlarge\"][\"max_workers\"] = 1000\n","problem_statement":"[Autoscaler] Exact resource match can skip expected node type during scheduling\n### What happened + What you expected to happen\n\n#### 1. What happened (the bug)\n```yaml\navailable_node_types:\n  ray.worker.4090.standard:\n    min_workers: 0\n    max_workers: 5\n    resources: {\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928 , \"gram\": 24 }\n    node_config: {}\n\n  ray.worker.4090.highmem:\n    min_workers: 0\n    max_workers: 5\n    resources: {\"CPU\": 16, \"GPU\": 1, \"memory\": 62277025792 , \"gram\": 24}\n    node_config: {}\n```\nWhen submitting a resource request that exactly matches the declared resource spec of a node type (e.g., ray.worker.4090.standard), the autoscaler does not select that node type as a launch candidate  even though:\n- There is no available node of that type with free resources (available resources = 0),\n- The node type config allows for more nodes (within max_worker_nodes),\n- The request matches the node types resource spec exactly.\n\nInstead, a different node type (e.g., ray.worker.4090.highmem) is selected and launched.\n\n#### 2. Expected behavior\n\nIf a node types declared resource spec matches a resource request exactly, and the current node is fully used (zero available), then the autoscaler should:\n- Still consider the same node type as a valid launch candidate,\n- Attempt to create a new instance of that type,\n\n#### 3. Useful information\nhttps://github.com/ray-project/ray/blob/e91c051e5813c0a9f69d075e9cdd0d8ca8abcbd4/python/ray/autoscaler/v2/scheduler.py#L430-L445\n\n### Versions / Dependencies\n\nRay-2.46.0\n\n### Reproduction script\n\n#### Workflow 1: One Replica - Expected Node Type Selected\n```py\nimport time\nimport ray\nfrom ray import serve\nfrom ray.autoscaler.sdk import request_resources\n\nray.init(address=\"auto\")\nserve.start()\n\n# Deploy tasks with specific resource requirements\n@serve.deployment(num_replicas=1, ray_actor_options={\"num_cpus\": 16, \"num_gpus\": 1, \"memory\": 30107260928 ,\"resources\": {\"gram\": 24}})\ndef CustomResourceTask(*args):\n    return \"ray.worker.4090.standard\"\n\nserve.run(CustomResourceTask.bind())\nprint(\"Requested additional resources...\")\n```\n```\n2025-05-15 02:16:18,261 INFO autoscaler.py:408 -- \n======== Autoscaler status: 2025-05-15 02:16:18.261802 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 ray.head.default\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nTotal Usage:\n 0.0/16.0 CPU\n 0B/9.09GiB memory\n 44B/4.55GiB object_store_memory\n\nTotal Constraints:\n (no request_resources() constraints)\nTotal Demands:\n {'CPU': 16.0, 'GPU': 1.0, 'gram': 24.0, 'memory': 30107260928.0}: 1+ pending tasks/actors\n2025-05-15 02:16:18,262 INFO custom_load_metrics.py:8 -- [CustomLoadMetrics] prune_active_ips called\n2025-05-15 02:16:18,262 INFO custom_load_metrics.py:25 -- [CustomLoadMetrics] ray_nodes_last_used_time_by_ip: {'172.28.0.10': 1747275378.2284896}\n2025-05-15 02:16:18,262 INFO custom_load_metrics.py:27 -- [CustomLoadMetrics] static_resources_by_ip: {'172.28.0.10': {'memory': 9765607835.0, 'CPU': 16.0, 'object_store_memory': 4882803916.0, 'node:__internal_head__': 1.0, 'node:172.28.0.10': 1.0}}\n2025-05-15 02:16:18,262 INFO custom_load_metrics.py:29 -- [CustomLoadMetrics] raylet_id_by_ip: {'172.28.0.10': b'$c\\xb7{\\xfb\\xe7\\x8a\\x1a\\xf6\\xef\\xd7\\xd7\\x18y~jH\\xa4\\xf1\\xf7U\\x97\\x1f\\xf4\\xbb\\xe2ef'}\n2025-05-15 02:16:18,262 INFO custom_load_metrics.py:31 -- [CustomLoadMetrics] dynamic_resources_by_ip: {'172.28.0.10': {'memory': 9765607835.0, 'CPU': 16.0, 'object_store_memory': 4882803872.0, 'node:__internal_head__': 0.999, 'node:172.28.0.10': 1.0}}\n2025-05-15 02:16:18,262 INFO custom_load_metrics.py:33 -- [CustomLoadMetrics] last_heartbeat_time_by_ip: {'172.28.0.10': 1747275378.2284896}\n2025-05-15 02:16:18,262 DEBUG resource_demand_scheduler.py:226 -- Cluster resources: [{'memory': 9765607835.0, 'CPU': 16.0, 'object_store_memory': 4882803872.0, 'node:__internal_head__': 0.999, 'node:172.28.0.10': 1.0}]\n2025-05-15 02:16:18,262 DEBUG resource_demand_scheduler.py:227 -- Node counts: defaultdict(<class 'int'>, {'ray.head.default': 1})\n2025-05-15 02:16:18,262 DEBUG resource_demand_scheduler.py:245 -- Placement group demands: []\n2025-05-15 02:16:18,262 DEBUG resource_demand_scheduler.py:306 -- Resource demands: [{'GPU': 1.0, 'memory': 30107260928.0, 'CPU': 16.0, 'gram': 24.0}]\n2025-05-15 02:16:18,262 DEBUG resource_demand_scheduler.py:307 -- Unfulfilled demands: [{'GPU': 1.0, 'memory': 30107260928.0, 'CPU': 16.0, 'gram': 24.0}]\n2025-05-15 02:16:18,262 DEBUG resource_demand_scheduler.py:316 -- Final unfulfilled: []\n2025-05-15 02:16:18,262 DEBUG resource_demand_scheduler.py:341 -- Node requests: {'ray.worker.4090.standard': 1}\n2025-05-15 02:16:18,262 INFO autoscaler.py:1399 -- StandardAutoscaler: Queue 1 new nodes for launch\n2025-05-15 02:16:18,262 INFO node_launcher.py:177 -- BaseNodeLauncher: Got 1 nodes to launch.\n2025-05-15 02:16:18,263 INFO node_launcher.py:177 -- BaseNodeLauncher: Launching 1 nodes, type ray.worker.4090.standard.\n\n2025-05-15 02:16:28,315 INFO autoscaler.py:408 -- \n======== Autoscaler status: 2025-05-15 02:16:28.314982 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 ray.head.default\n 1 ray.worker.4090.standard\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nTotal Usage:\n 16.0/32.0 CPU\n 1.0/1.0 GPU\n 24.0/24.0 gram\n 28.04GiB/37.13GiB memory\n 88B/9.14GiB object_store_memory\n\nTotal Constraints:\n (no request_resources() constraints)\nTotal Demands:\n (no resource demands)\n```\n- The autoscaler schedules a new node of the correct type (ray.worker.4090.standard).\n- After launch, all requested resources are allocated as expected.\n- No higher-tier node types are selected.\n- This demonstrates that the autoscaler behaves as expected when there are no existing nodes occupying resources.\n\n#### Workflow 2: Two Replicas - Unexpected High-Tier Node Chosen\n```py\nimport time\nimport ray\nfrom ray import serve\nfrom ray.autoscaler.sdk import request_resources\n\nray.init(address=\"auto\")\nserve.start()\n\n# Deploy tasks with specific resource requirements\n@serve.deployment(num_replicas=2, ray_actor_options={\"num_cpus\": 16, \"num_gpus\": 1, \"memory\": 30107260928 ,\"resources\": {\"gram\": 24}})\ndef CustomResourceTask(*args):\n    return \"ray.worker.4090.standard\"\n\nserve.run(CustomResourceTask.bind())\nprint(\"Requested additional resources...\")\n```\n```\n2025-05-15 02:19:09,422 INFO autoscaler.py:408 -- \n======== Autoscaler status: 2025-05-15 02:19:09.422432 ========\nNode status\n---------------------------------------------------------------\nActive:\n 1 ray.head.default\n 1 ray.worker.4090.standard\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nTotal Usage:\n 16.0/32.0 CPU\n 1.0/1.0 GPU\n 24.0/24.0 gram\n 28.04GiB/37.13GiB memory\n 88B/9.14GiB object_store_memory\n\nTotal Constraints:\n (no request_resources() constraints)\nTotal Demands:\n {'CPU': 16.0, 'GPU': 1.0, 'gram': 24.0, 'memory': 30107260928.0}: 1+ pending tasks/actors\n2025-05-15 02:19:09,423 INFO custom_load_metrics.py:8 -- [CustomLoadMetrics] prune_active_ips called\n2025-05-15 02:19:09,423 INFO custom_load_metrics.py:25 -- [CustomLoadMetrics] ray_nodes_last_used_time_by_ip: {'172.28.0.10': 1747275549.3913682, '172.28.0.16': 1747275549.3914287}\n2025-05-15 02:19:09,423 INFO custom_load_metrics.py:27 -- [CustomLoadMetrics] static_resources_by_ip: {'172.28.0.10': {'memory': 9765607835.0, 'CPU': 16.0, 'object_store_memory': 4882803916.0, 'node:__internal_head__': 1.0, 'node:172.28.0.10': 1.0}, '172.28.0.16': {'GPU': 1.0, 'node:172.28.0.16': 1.0, 'CPU': 16.0, 'object_store_memory': 4933059379.0, 'gram': 24.0, 'memory': 30107260928.0}}\n2025-05-15 02:19:09,423 INFO custom_load_metrics.py:29 -- [CustomLoadMetrics] raylet_id_by_ip: {'172.28.0.10': b'$c\\xb7{\\xfb\\xe7\\x8a\\x1a\\xf6\\xef\\xd7\\xd7\\x18y~jH\\xa4\\xf1\\xf7U\\x97\\x1f\\xf4\\xbb\\xe2ef', '172.28.0.16': b'oq\\x9fR\\x89E\\xa9v\\xaf=a\\xfe:9J\\xd4\\xe7SC96\\x05\\xcfNN\\xd4\\xe9\\xe4'}\n2025-05-15 02:19:09,424 INFO custom_load_metrics.py:31 -- [CustomLoadMetrics] dynamic_resources_by_ip: {'172.28.0.10': {'memory': 9765607835.0, 'CPU': 16.0, 'object_store_memory': 4882803872.0, 'node:__internal_head__': 0.999, 'node:172.28.0.10': 1.0}, '172.28.0.16': {'node:172.28.0.16': 1.0, 'object_store_memory': 4933059335.0, 'GPU': 0.0, 'CPU': 0.0, 'gram': 0.0, 'memory': 0.0}}\n2025-05-15 02:19:09,424 INFO custom_load_metrics.py:33 -- [CustomLoadMetrics] last_heartbeat_time_by_ip: {'172.28.0.10': 1747275549.3913682, '172.28.0.16': 1747275549.3914287}\n2025-05-15 02:19:09,424 DEBUG resource_demand_scheduler.py:226 -- Cluster resources: [{'memory': 9765607835.0, 'CPU': 16.0, 'object_store_memory': 4882803872.0, 'node:__internal_head__': 0.999, 'node:172.28.0.10': 1.0}, {'node:172.28.0.16': 1.0, 'object_store_memory': 4933059335.0, 'GPU': 0.0, 'CPU': 0.0, 'gram': 0.0, 'memory': 0.0}]\n2025-05-15 02:19:09,424 DEBUG resource_demand_scheduler.py:227 -- Node counts: defaultdict(<class 'int'>, {'ray.head.default': 1, 'ray.worker.4090.standard': 1})\n2025-05-15 02:19:09,424 DEBUG resource_demand_scheduler.py:245 -- Placement group demands: []\n2025-05-15 02:19:09,424 DEBUG resource_demand_scheduler.py:306 -- Resource demands: [{'memory': 30107260928.0, 'GPU': 1.0, 'CPU': 16.0, 'gram': 24.0}]\n2025-05-15 02:19:09,424 DEBUG resource_demand_scheduler.py:307 -- Unfulfilled demands: [{'memory': 30107260928.0, 'GPU': 1.0, 'CPU': 16.0, 'gram': 24.0}]\n2025-05-15 02:19:09,424 DEBUG resource_demand_scheduler.py:316 -- Final unfulfilled: []\n2025-05-15 02:19:09,424 DEBUG resource_demand_scheduler.py:341 -- Node requests: {'ray.worker.4090.highmem': 1}\n2025-05-15 02:19:09,424 INFO autoscaler.py:1399 -- StandardAutoscaler: Queue 1 new nodes for launch\n2025-05-15 02:19:09,425 INFO node_launcher.py:177 -- BaseNodeLauncher: Got 1 nodes to launch.\n2025-05-15 02:19:09,425 INFO node_launcher.py:177 -- BaseNodeLauncher: Launching 1 nodes, type ray.worker.4090.highmem.\n2025-05-15 02:19:09,425 INFO node_provider.py:250 -- post_process\n2025-05-15 02:19:09,425 INFO node_provider.py:176 -- submit_scale_request ScaleRequest(desired_num_workers=defaultdict(<class 'int'>, {'ray.worker.4090.standard': 1, 'ray.worker.4090.highmem': 1}), workers_to_delete=set())\n```\n- The first ray.worker.4090.standard node is already fully utilized by the first replica.\n- When the second replica is requested, autoscaler chooses to launch a ray.worker.4090.highmem node instead of launching another ray.worker.4090.standard.\n- This occurs even though the requested resource bundle still perfectly matches the ray.worker.4090.standard spec.\n- This behavior is unexpected and could indicate a bug or scoring issue in node selection logic.\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.\n","hints_text":"## reproduction\n```py\nfrom ray.autoscaler._private.resource_demand_scheduler import ResourceDemandScheduler\nfrom ray.autoscaler._private.node_provider_availability_tracker import NodeAvailabilitySummary\nimport logging\n\n# Set base logging level to DEBUG for visibility\nlogging.basicConfig(level=logging.DEBUG)\n\n# Enable DEBUG logs for the internal resource demand scheduler module\nlogging.getLogger(\"ray.autoscaler._private.resource_demand_scheduler\").setLevel(logging.DEBUG)\n\nclass FakeProvider:\n    def node_tags(self, node):\n        return node[\"Tags\"]\n\n    def internal_ip(self, node):\n        return node[\"IpAddress\"]\n\ndef run_test(test_name, demand, node_types, node_resources, node_type_counts):\n    print(f\"\\n== Running {test_name} ==\")\n\n    # Simulate active nodes and their resources depending on test type\n    from ray.autoscaler.tags import TAG_RAY_NODE_KIND, TAG_RAY_USER_NODE_TYPE, TAG_RAY_NODE_STATUS, NODE_KIND_WORKER, NODE_KIND_HEAD, STATUS_UP_TO_DATE\n\n    if test_name == \"No standard node yet -> should create standard\":\n        # Only head node exists; no worker node yet\n        non_terminated_nodes = [\n            {\n                \"NodeId\": \"node-2\",\n                \"InstanceType\": \"ray.head.default\",\n                \"Status\": \"running\",\n                \"Tags\": {\n                    TAG_RAY_USER_NODE_TYPE: \"ray.head.default\",\n                    TAG_RAY_NODE_KIND: NODE_KIND_HEAD,\n                    TAG_RAY_NODE_STATUS: STATUS_UP_TO_DATE\n                },\n                \"IpAddress\": \"172.28.0.10\"\n            }\n        ]\n        unused_resources_by_ip = {\n            \"172.28.0.10\": {\n                \"CPU\": 16.0,\n                \"memory\": 9765607835.0,\n                \"object_store_memory\": 4882803872.0,\n                \"node:172.28.0.10\": 1.0,\n                \"node:__internal_head__\": 0.999\n            }\n        }\n        max_resources_by_ip = {\n            \"172.28.0.10\": {\n                \"CPU\": 16.0,\n                \"memory\": 9765607835.0,\n                \"object_store_memory\": 4882803872.0,\n                \"node:172.28.0.10\": 1.0,\n                \"node:__internal_head__\": 0.999\n            }\n        }\n    else:\n        # One standard worker and one head node exist\n        non_terminated_nodes = [\n            {\n                \"NodeId\": \"node-1\",\n                \"InstanceType\": \"ray.worker.4090.standard\",\n                \"Status\": \"running\",\n                \"Tags\": {\n                    TAG_RAY_USER_NODE_TYPE: \"ray.worker.4090.standard\",\n                    TAG_RAY_NODE_KIND: NODE_KIND_WORKER,\n                    TAG_RAY_NODE_STATUS: STATUS_UP_TO_DATE\n                },\n                \"IpAddress\": \"172.28.0.16\"\n            },\n            {\n                \"NodeId\": \"node-2\",\n                \"InstanceType\": \"ray.head.default\",\n                \"Status\": \"running\",\n                \"Tags\": {\n                    TAG_RAY_USER_NODE_TYPE: \"ray.head.default\",\n                    TAG_RAY_NODE_KIND: NODE_KIND_HEAD,\n                    TAG_RAY_NODE_STATUS: STATUS_UP_TO_DATE\n                },\n                \"IpAddress\": \"172.28.0.10\"\n            }\n        ]\n\n        # Define available and maximum resources per IP address\n        unused_resources_by_ip = {\n            \"172.28.0.10\": {\n                \"CPU\": 16.0,\n                \"memory\": 9765607835.0,\n                \"object_store_memory\": 4882803872.0,\n                \"node:172.28.0.10\": 1.0,\n                \"node:__internal_head__\": 0.999\n            },\n            \"172.28.0.16\": {\n                \"CPU\": 8.0,\n                \"GPU\": 1.0,\n                \"memory\": 10000000000.0,\n                \"gram\": 12.0,\n                \"object_store_memory\": 4933059335.0,\n                \"node:172.28.0.16\": 1.0\n            }\n        }\n        max_resources_by_ip = {\n            \"172.28.0.10\": {\n                \"CPU\": 16.0,\n                \"memory\": 9765607835.0,\n                \"object_store_memory\": 4882803872.0,\n                \"node:172.28.0.10\": 1.0,\n                \"node:__internal_head__\": 0.999\n            },\n            \"172.28.0.16\": {\n                \"CPU\": 16.0,\n                \"GPU\": 1.0,\n                \"memory\": 30107260928.0,\n                \"gram\": 24.0,\n                \"object_store_memory\": 4933059335.0,\n                \"node:172.28.0.16\": 1.0\n            }\n        }\n    scheduler = ResourceDemandScheduler(\n        provider=FakeProvider(),\n        node_types=node_types,\n        max_workers=1000,\n        head_node_type=\"ray.head.default\",\n        upscaling_speed=1.0,\n    )\n\n    to_launch, _ = scheduler.get_nodes_to_launch(\n        nodes=non_terminated_nodes,\n        launching_nodes={},\n        resource_demands=demand,\n        unused_resources_by_ip=unused_resources_by_ip,\n        pending_placement_groups=[],\n        max_resources_by_ip=max_resources_by_ip,\n        ensure_min_cluster_size=[],\n        node_availability_summary=NodeAvailabilitySummary(node_availabilities={}),\n    )\n    print(\"Nodes to add:\", to_launch)\n\nif __name__ == \"__main__\":\n    common_node_types = {\n        \"ray.worker.4090.standard\": {\n            \"resources\": {\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24},\n            \"max_workers\": 5\n        },\n        \"ray.worker.4090.highmem\": {\n            \"resources\": {\"CPU\": 16, \"GPU\": 1, \"memory\": 62277025792, \"gram\": 24},\n            \"max_workers\": 5\n        },\n        \"ray.head.default\": {\n            \"resources\": {\"CPU\": 4, \"memory\": 8000000000},\n            \"max_workers\": 1\n        }\n    }\n\n    # Test: A standard node exists but is fully used; scheduler should select highmem node type.\n    run_test(\n        \"Standard node exists but fully used\",\n        demand=[{\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24}],\n        node_types=common_node_types,\n        node_resources=[{\"CPU\": 0.0, \"GPU\": 0.0, \"memory\": 0.0, \"gram\": 0.0, \"type\": \"ray.worker.4090.standard\"}],\n        node_type_counts={\"ray.worker.4090.standard\": 1}\n    )\n\n    # Test: Standard maxed out, scheduler should pick highmem node type.\n    run_test(\n        \"Standard maxed out, should pick highmem\",\n        demand=[{\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24}],\n        node_types={\n            \"ray.worker.4090.standard\": {\n                \"resources\": {\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24},\n                \"max_workers\": 1\n            },\n            \"ray.worker.4090.highmem\": {\n                \"resources\": {\"CPU\": 16, \"GPU\": 1, \"memory\": 62277025792, \"gram\": 24},\n                \"max_workers\": 5\n            },\n            \"ray.head.default\": {\n                \"resources\": {\"CPU\": 4, \"memory\": 8000000000},\n                \"max_workers\": 1\n            }\n        },\n        node_resources=[{\"CPU\": 0.0, \"GPU\": 0.0, \"memory\": 0.0, \"gram\": 0.0}],\n        node_type_counts={\"ray.worker.4090.standard\": 1}\n    )\n\n    # Test: No node type fits the demand; scheduler should not launch any nodes.\n    run_test(\n        \"No node type fits the demand\",\n        demand=[{\"CPU\": 32, \"GPU\": 2, \"memory\": 100000000000, \"gram\": 100}],\n        node_types=common_node_types,\n        node_resources=[],\n        node_type_counts={}\n    )\n\n    # Test: Standard node partially used, second task fits exactly; scheduler should reuse existing resources.\n    run_test(\n        \"Standard node partially used, second task fits exactly\",\n        demand=[{\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24}],\n        node_types=common_node_types,\n        node_resources=[\n            {\"CPU\": 8.0, \"GPU\": 1.0, \"memory\": 15000000000.0, \"gram\": 12.0, \"type\": \"ray.worker.4090.standard\"}\n        ],\n        node_type_counts={\"ray.worker.4090.standard\": 1}\n    )\n\n    # Test: Standard node available and has enough resources; scheduler should reuse standard node.\n    run_test(\n        \"Standard node available and enough resources -> use existing standard\",\n        demand=[{\"CPU\": 8, \"GPU\": 1, \"memory\": 10000000000, \"gram\": 12}],\n        node_types=common_node_types,\n        node_resources=[\n            {\"CPU\": 8.0, \"GPU\": 0.0, \"memory\": 20000000000.0, \"gram\": 12.0, \"type\": \"ray.worker.4090.standard\"}\n        ],\n        node_type_counts={\"ray.worker.4090.standard\": 1}\n    )\n\n    # Test: No standard node yet; scheduler should create a standard node.\n    run_test(\n        \"No standard node yet -> should create standard\",\n        demand=[{\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24}],\n        node_types=common_node_types,\n        node_resources=[],  # no running nodes\n        node_type_counts={}  # standard not present\n    )\n```\n## log\n```\n== Running Standard node exists but fully used ==\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 8.0, 'GPU': 1.0, 'memory': 10000000000.0, 'gram': 12.0, 'object_store_memory': 4933059335.0, 'node:172.28.0.16': 1.0}, {'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.worker.4090.standard': 1, 'ray.head.default': 1})\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: []\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:337 -- Node requests: {'ray.worker.4090.highmem': 1}\nNodes to add: {'ray.worker.4090.highmem': 1}\n\n== Running Standard maxed out, should pick highmem ==\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 8.0, 'GPU': 1.0, 'memory': 10000000000.0, 'gram': 12.0, 'object_store_memory': 4933059335.0, 'node:172.28.0.16': 1.0}, {'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.worker.4090.standard': 1, 'ray.head.default': 1})\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: []\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:337 -- Node requests: {'ray.worker.4090.highmem': 1}\nNodes to add: {'ray.worker.4090.highmem': 1}\n\n== Running No node type fits the demand ==\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 8.0, 'GPU': 1.0, 'memory': 10000000000.0, 'gram': 12.0, 'object_store_memory': 4933059335.0, 'node:172.28.0.16': 1.0}, {'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.worker.4090.standard': 1, 'ray.head.default': 1})\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 32, 'GPU': 2, 'memory': 100000000000, 'gram': 100}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: [{'CPU': 32, 'GPU': 2, 'memory': 100000000000, 'gram': 100}]\n2025-05-15 14:15:25,334 WARNING resource_demand_scheduler.py:782 -- The autoscaler could not find a node type to satisfy the request: [{'CPU': 32, 'GPU': 2, 'memory': 100000000000, 'gram': 100}]. Please specify a node type with the necessary resources.\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: [{'CPU': 32, 'GPU': 2, 'memory': 100000000000, 'gram': 100}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:337 -- Node requests: {}\nNodes to add: {}\n\n== Running Standard node partially used, second task fits exactly ==\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 8.0, 'GPU': 1.0, 'memory': 10000000000.0, 'gram': 12.0, 'object_store_memory': 4933059335.0, 'node:172.28.0.16': 1.0}, {'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.worker.4090.standard': 1, 'ray.head.default': 1})\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:337 -- Node requests: {'ray.worker.4090.highmem': 1}\nNodes to add: {'ray.worker.4090.highmem': 1}\n\n== Running Standard node available and enough resources -> use existing standard ==\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 8.0, 'GPU': 1.0, 'memory': 10000000000.0, 'gram': 12.0, 'object_store_memory': 4933059335.0, 'node:172.28.0.16': 1.0}, {'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.worker.4090.standard': 1, 'ray.head.default': 1})\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 8, 'GPU': 1, 'memory': 10000000000, 'gram': 12}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:337 -- Node requests: {}\nNodes to add: {}\n\n== Running No standard node yet -> should create standard ==\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.head.default': 1})\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:337 -- Node requests: {'ray.worker.4090.standard': 1}\nNodes to add: {'ray.worker.4090.standard': 1}\n```\n@rueian could you take a look? cc @kevin85421 \n### Additional Information\n\nAccording to the comment in the code:\n- https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/v2/scheduler.py#L437C1-L445C71\n```\nThe score is a tuple of 4 values:\n1. Whether this node is a GPU node and the current resource request has GPU requirements\n2. The number of resource types being scheduled\n3. The minimum utilization rate across all resource types\n4. The average utilization rate across all resource types\n```\n\nFrom the logs:\n```\nUtilization score for 'ray.worker.4090.standard': (True, 4, 0.0, 6021452193.8)\nUtilization score for 'ray.worker.4090.highmem': (True, 4, 1.0, 1759130358.4788647)\n```\n\nIt seems that ray.worker.4090.standard has a higher average utilization rate, but a lower minimum utilization rate.\nSince the scheduler prioritizes minimum utilization, it likely causes ray.worker.4090.highmem to be preferred in this case.\nI found that redefining _resource_based_utilization_scorer to sort only based on criteria (1), (2), and (4)specifically (gpu_ok, num_matching_resource_types, average_utilization)resolved the issue.\n\nHeres the custom scorer I used:\n\n```\nAUTOSCALER_UTILIZATION_SCORER=autoscaler.custom.custom_resource_demand_scheduler.custom_utilization_scorer\n```\n```python\ndef custom_utilization_scorer(self, node_resources, resources, node_type):\n    score = _resource_based_utilization_scorer(\n        node_resources, resources, node_availability_summary={}\n    )\n    if score is None:\n        return None\n    return score[0], score[1], score[3]\n```\nIt might be helpful to include some documentation or inline comments for the `AUTOSCALER_UTILIZATION_SCORER` option to make it easier for users to discover and customize the scorer logic.\n\n\nAlso, Im a bit confused about the value of score[2] (i.e., the minimum utilization rate across all resource types). For example, even when an actor has been scheduled on a node of type 'ray.worker.4090.standard', I see the following:\n```\nUtilization score for 'ray.worker.4090.standard': (True, 4, 0.0, 6021452193.8)\n```\n\nWhy is the minimum utilization reported as 0.0 in this case, even though the actor is already allocated to the node?\n> Why is the minimum utilization reported as 0.0 in this case, even though the actor is already allocated to the node?\n\nThat was because when it calculated the score for the 'ray.worker.4090.standard', it treated the \"object_store_memory\" resource as unused.\n\nThe behavior of the scorer looks weird to me for now. I am not sure why it considers \"utilization rate across all resource types\" instead of only \"utilization rate across **requested** resource types\". \n\nAnd the current [`util_by_resources.append(v * (util**3))`](https://github.com/ray-project/ray/blob/e91c051e5813c0a9f69d075e9cdd0d8ca8abcbd4/python/ray/autoscaler/v2/scheduler.py#L485) is also weird to me. It doesn't look like a utilization rate.\n@rueian I agree with your overall points. In my case, sorting the score based only on (1), (2), and (4)  that is, (gpu_ok, num_matching_resource_types, average_utilization)  was enough to resolve the issue and meet our project requirements.\n\nHowever, since the autoscaler is designed around predefined node types that are expected to match resource demands, selecting the most appropriate node type for the given demand is critical.\n\nIf the current scoring logic must be preserved, I think it would be better to provide an official configuration option (rather than relying solely on a custom scorer) to ensure that a node type that best fits the demand can be properly selected.\n\nAlso, including unused resources like object_store_memory when computing the minimum utilization rate can be misleading. It penalizes nodes unfairly even when the requested resources are already being utilized properly. This behavior hinders accurate scoring and effective scheduling.\nHere is why the scorer takes `object_store_memory` into consideration currently.\n\nhttps://github.com/ray-project/ray/blob/3fd6015d8f925d5dc7f61e234fa9f4cf1781c578/python/ray/autoscaler/_private/resource_demand_scheduler.py#L376-L379\n\nThe scheduler will try to fix these 4 resources `[\"CPU\", \"GPU\", \"memory\", \"object_store_memory\"]` after a node is started. And then those resources will be carried into the scorer. See https://github.com/ray-project/ray/pull/14567.\n\nHowever, I don't find any use cases that use \"object_store_memory\" for task/actor scheduling. There is even a document that says that users **cannot** use \"object_store_memory\" for scheduling: https://github.com/ray-project/ray/blob/c825dfea03fcce62c69a6a95cbe6e109925066f7/doc/source/ray-core/scheduling/resources.rst?plain=1#L91\n\nIf that statement is still true, I think we should drop the fix for \"object_store_memory\" after a node is started, as proposed in the comment https://github.com/ray-project/ray/pull/14567#issuecomment-794403566 and the deprecation https://github.com/ray-project/ray/pull/26252.\n\n","all_hints_text":"## reproduction\n```py\nfrom ray.autoscaler._private.resource_demand_scheduler import ResourceDemandScheduler\nfrom ray.autoscaler._private.node_provider_availability_tracker import NodeAvailabilitySummary\nimport logging\n\n# Set base logging level to DEBUG for visibility\nlogging.basicConfig(level=logging.DEBUG)\n\n# Enable DEBUG logs for the internal resource demand scheduler module\nlogging.getLogger(\"ray.autoscaler._private.resource_demand_scheduler\").setLevel(logging.DEBUG)\n\nclass FakeProvider:\n    def node_tags(self, node):\n        return node[\"Tags\"]\n\n    def internal_ip(self, node):\n        return node[\"IpAddress\"]\n\ndef run_test(test_name, demand, node_types, node_resources, node_type_counts):\n    print(f\"\\n== Running {test_name} ==\")\n\n    # Simulate active nodes and their resources depending on test type\n    from ray.autoscaler.tags import TAG_RAY_NODE_KIND, TAG_RAY_USER_NODE_TYPE, TAG_RAY_NODE_STATUS, NODE_KIND_WORKER, NODE_KIND_HEAD, STATUS_UP_TO_DATE\n\n    if test_name == \"No standard node yet -> should create standard\":\n        # Only head node exists; no worker node yet\n        non_terminated_nodes = [\n            {\n                \"NodeId\": \"node-2\",\n                \"InstanceType\": \"ray.head.default\",\n                \"Status\": \"running\",\n                \"Tags\": {\n                    TAG_RAY_USER_NODE_TYPE: \"ray.head.default\",\n                    TAG_RAY_NODE_KIND: NODE_KIND_HEAD,\n                    TAG_RAY_NODE_STATUS: STATUS_UP_TO_DATE\n                },\n                \"IpAddress\": \"172.28.0.10\"\n            }\n        ]\n        unused_resources_by_ip = {\n            \"172.28.0.10\": {\n                \"CPU\": 16.0,\n                \"memory\": 9765607835.0,\n                \"object_store_memory\": 4882803872.0,\n                \"node:172.28.0.10\": 1.0,\n                \"node:__internal_head__\": 0.999\n            }\n        }\n        max_resources_by_ip = {\n            \"172.28.0.10\": {\n                \"CPU\": 16.0,\n                \"memory\": 9765607835.0,\n                \"object_store_memory\": 4882803872.0,\n                \"node:172.28.0.10\": 1.0,\n                \"node:__internal_head__\": 0.999\n            }\n        }\n    else:\n        # One standard worker and one head node exist\n        non_terminated_nodes = [\n            {\n                \"NodeId\": \"node-1\",\n                \"InstanceType\": \"ray.worker.4090.standard\",\n                \"Status\": \"running\",\n                \"Tags\": {\n                    TAG_RAY_USER_NODE_TYPE: \"ray.worker.4090.standard\",\n                    TAG_RAY_NODE_KIND: NODE_KIND_WORKER,\n                    TAG_RAY_NODE_STATUS: STATUS_UP_TO_DATE\n                },\n                \"IpAddress\": \"172.28.0.16\"\n            },\n            {\n                \"NodeId\": \"node-2\",\n                \"InstanceType\": \"ray.head.default\",\n                \"Status\": \"running\",\n                \"Tags\": {\n                    TAG_RAY_USER_NODE_TYPE: \"ray.head.default\",\n                    TAG_RAY_NODE_KIND: NODE_KIND_HEAD,\n                    TAG_RAY_NODE_STATUS: STATUS_UP_TO_DATE\n                },\n                \"IpAddress\": \"172.28.0.10\"\n            }\n        ]\n\n        # Define available and maximum resources per IP address\n        unused_resources_by_ip = {\n            \"172.28.0.10\": {\n                \"CPU\": 16.0,\n                \"memory\": 9765607835.0,\n                \"object_store_memory\": 4882803872.0,\n                \"node:172.28.0.10\": 1.0,\n                \"node:__internal_head__\": 0.999\n            },\n            \"172.28.0.16\": {\n                \"CPU\": 8.0,\n                \"GPU\": 1.0,\n                \"memory\": 10000000000.0,\n                \"gram\": 12.0,\n                \"object_store_memory\": 4933059335.0,\n                \"node:172.28.0.16\": 1.0\n            }\n        }\n        max_resources_by_ip = {\n            \"172.28.0.10\": {\n                \"CPU\": 16.0,\n                \"memory\": 9765607835.0,\n                \"object_store_memory\": 4882803872.0,\n                \"node:172.28.0.10\": 1.0,\n                \"node:__internal_head__\": 0.999\n            },\n            \"172.28.0.16\": {\n                \"CPU\": 16.0,\n                \"GPU\": 1.0,\n                \"memory\": 30107260928.0,\n                \"gram\": 24.0,\n                \"object_store_memory\": 4933059335.0,\n                \"node:172.28.0.16\": 1.0\n            }\n        }\n    scheduler = ResourceDemandScheduler(\n        provider=FakeProvider(),\n        node_types=node_types,\n        max_workers=1000,\n        head_node_type=\"ray.head.default\",\n        upscaling_speed=1.0,\n    )\n\n    to_launch, _ = scheduler.get_nodes_to_launch(\n        nodes=non_terminated_nodes,\n        launching_nodes={},\n        resource_demands=demand,\n        unused_resources_by_ip=unused_resources_by_ip,\n        pending_placement_groups=[],\n        max_resources_by_ip=max_resources_by_ip,\n        ensure_min_cluster_size=[],\n        node_availability_summary=NodeAvailabilitySummary(node_availabilities={}),\n    )\n    print(\"Nodes to add:\", to_launch)\n\nif __name__ == \"__main__\":\n    common_node_types = {\n        \"ray.worker.4090.standard\": {\n            \"resources\": {\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24},\n            \"max_workers\": 5\n        },\n        \"ray.worker.4090.highmem\": {\n            \"resources\": {\"CPU\": 16, \"GPU\": 1, \"memory\": 62277025792, \"gram\": 24},\n            \"max_workers\": 5\n        },\n        \"ray.head.default\": {\n            \"resources\": {\"CPU\": 4, \"memory\": 8000000000},\n            \"max_workers\": 1\n        }\n    }\n\n    # Test: A standard node exists but is fully used; scheduler should select highmem node type.\n    run_test(\n        \"Standard node exists but fully used\",\n        demand=[{\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24}],\n        node_types=common_node_types,\n        node_resources=[{\"CPU\": 0.0, \"GPU\": 0.0, \"memory\": 0.0, \"gram\": 0.0, \"type\": \"ray.worker.4090.standard\"}],\n        node_type_counts={\"ray.worker.4090.standard\": 1}\n    )\n\n    # Test: Standard maxed out, scheduler should pick highmem node type.\n    run_test(\n        \"Standard maxed out, should pick highmem\",\n        demand=[{\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24}],\n        node_types={\n            \"ray.worker.4090.standard\": {\n                \"resources\": {\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24},\n                \"max_workers\": 1\n            },\n            \"ray.worker.4090.highmem\": {\n                \"resources\": {\"CPU\": 16, \"GPU\": 1, \"memory\": 62277025792, \"gram\": 24},\n                \"max_workers\": 5\n            },\n            \"ray.head.default\": {\n                \"resources\": {\"CPU\": 4, \"memory\": 8000000000},\n                \"max_workers\": 1\n            }\n        },\n        node_resources=[{\"CPU\": 0.0, \"GPU\": 0.0, \"memory\": 0.0, \"gram\": 0.0}],\n        node_type_counts={\"ray.worker.4090.standard\": 1}\n    )\n\n    # Test: No node type fits the demand; scheduler should not launch any nodes.\n    run_test(\n        \"No node type fits the demand\",\n        demand=[{\"CPU\": 32, \"GPU\": 2, \"memory\": 100000000000, \"gram\": 100}],\n        node_types=common_node_types,\n        node_resources=[],\n        node_type_counts={}\n    )\n\n    # Test: Standard node partially used, second task fits exactly; scheduler should reuse existing resources.\n    run_test(\n        \"Standard node partially used, second task fits exactly\",\n        demand=[{\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24}],\n        node_types=common_node_types,\n        node_resources=[\n            {\"CPU\": 8.0, \"GPU\": 1.0, \"memory\": 15000000000.0, \"gram\": 12.0, \"type\": \"ray.worker.4090.standard\"}\n        ],\n        node_type_counts={\"ray.worker.4090.standard\": 1}\n    )\n\n    # Test: Standard node available and has enough resources; scheduler should reuse standard node.\n    run_test(\n        \"Standard node available and enough resources -> use existing standard\",\n        demand=[{\"CPU\": 8, \"GPU\": 1, \"memory\": 10000000000, \"gram\": 12}],\n        node_types=common_node_types,\n        node_resources=[\n            {\"CPU\": 8.0, \"GPU\": 0.0, \"memory\": 20000000000.0, \"gram\": 12.0, \"type\": \"ray.worker.4090.standard\"}\n        ],\n        node_type_counts={\"ray.worker.4090.standard\": 1}\n    )\n\n    # Test: No standard node yet; scheduler should create a standard node.\n    run_test(\n        \"No standard node yet -> should create standard\",\n        demand=[{\"CPU\": 16, \"GPU\": 1, \"memory\": 30107260928, \"gram\": 24}],\n        node_types=common_node_types,\n        node_resources=[],  # no running nodes\n        node_type_counts={}  # standard not present\n    )\n```\n## log\n```\n== Running Standard node exists but fully used ==\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 8.0, 'GPU': 1.0, 'memory': 10000000000.0, 'gram': 12.0, 'object_store_memory': 4933059335.0, 'node:172.28.0.16': 1.0}, {'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.worker.4090.standard': 1, 'ray.head.default': 1})\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: []\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:337 -- Node requests: {'ray.worker.4090.highmem': 1}\nNodes to add: {'ray.worker.4090.highmem': 1}\n\n== Running Standard maxed out, should pick highmem ==\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 8.0, 'GPU': 1.0, 'memory': 10000000000.0, 'gram': 12.0, 'object_store_memory': 4933059335.0, 'node:172.28.0.16': 1.0}, {'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.worker.4090.standard': 1, 'ray.head.default': 1})\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: []\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:337 -- Node requests: {'ray.worker.4090.highmem': 1}\nNodes to add: {'ray.worker.4090.highmem': 1}\n\n== Running No node type fits the demand ==\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 8.0, 'GPU': 1.0, 'memory': 10000000000.0, 'gram': 12.0, 'object_store_memory': 4933059335.0, 'node:172.28.0.16': 1.0}, {'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.worker.4090.standard': 1, 'ray.head.default': 1})\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 32, 'GPU': 2, 'memory': 100000000000, 'gram': 100}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: [{'CPU': 32, 'GPU': 2, 'memory': 100000000000, 'gram': 100}]\n2025-05-15 14:15:25,334 WARNING resource_demand_scheduler.py:782 -- The autoscaler could not find a node type to satisfy the request: [{'CPU': 32, 'GPU': 2, 'memory': 100000000000, 'gram': 100}]. Please specify a node type with the necessary resources.\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: [{'CPU': 32, 'GPU': 2, 'memory': 100000000000, 'gram': 100}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:337 -- Node requests: {}\nNodes to add: {}\n\n== Running Standard node partially used, second task fits exactly ==\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 8.0, 'GPU': 1.0, 'memory': 10000000000.0, 'gram': 12.0, 'object_store_memory': 4933059335.0, 'node:172.28.0.16': 1.0}, {'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.worker.4090.standard': 1, 'ray.head.default': 1})\n2025-05-15 14:15:25,334 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:337 -- Node requests: {'ray.worker.4090.highmem': 1}\nNodes to add: {'ray.worker.4090.highmem': 1}\n\n== Running Standard node available and enough resources -> use existing standard ==\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 8.0, 'GPU': 1.0, 'memory': 10000000000.0, 'gram': 12.0, 'object_store_memory': 4933059335.0, 'node:172.28.0.16': 1.0}, {'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.worker.4090.standard': 1, 'ray.head.default': 1})\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 8, 'GPU': 1, 'memory': 10000000000, 'gram': 12}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:337 -- Node requests: {}\nNodes to add: {}\n\n== Running No standard node yet -> should create standard ==\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:222 -- Cluster resources: [{'CPU': 16.0, 'memory': 9765607835.0, 'object_store_memory': 4882803872.0, 'node:172.28.0.10': 1.0, 'node:__internal_head__': 0.999}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:223 -- Node counts: defaultdict(<class 'int'>, {'ray.head.default': 1})\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:241 -- Placement group demands: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:302 -- Resource demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:303 -- Unfulfilled demands: [{'CPU': 16, 'GPU': 1, 'memory': 30107260928, 'gram': 24}]\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:312 -- Final unfulfilled: []\n2025-05-15 14:15:25,335 DEBUG resource_demand_scheduler.py:337 -- Node requests: {'ray.worker.4090.standard': 1}\nNodes to add: {'ray.worker.4090.standard': 1}\n```\n@rueian could you take a look? cc @kevin85421 \n### Additional Information\n\nAccording to the comment in the code:\n- https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/v2/scheduler.py#L437C1-L445C71\n```\nThe score is a tuple of 4 values:\n1. Whether this node is a GPU node and the current resource request has GPU requirements\n2. The number of resource types being scheduled\n3. The minimum utilization rate across all resource types\n4. The average utilization rate across all resource types\n```\n\nFrom the logs:\n```\nUtilization score for 'ray.worker.4090.standard': (True, 4, 0.0, 6021452193.8)\nUtilization score for 'ray.worker.4090.highmem': (True, 4, 1.0, 1759130358.4788647)\n```\n\nIt seems that ray.worker.4090.standard has a higher average utilization rate, but a lower minimum utilization rate.\nSince the scheduler prioritizes minimum utilization, it likely causes ray.worker.4090.highmem to be preferred in this case.\nI found that redefining _resource_based_utilization_scorer to sort only based on criteria (1), (2), and (4)specifically (gpu_ok, num_matching_resource_types, average_utilization)resolved the issue.\n\nHeres the custom scorer I used:\n\n```\nAUTOSCALER_UTILIZATION_SCORER=autoscaler.custom.custom_resource_demand_scheduler.custom_utilization_scorer\n```\n```python\ndef custom_utilization_scorer(self, node_resources, resources, node_type):\n    score = _resource_based_utilization_scorer(\n        node_resources, resources, node_availability_summary={}\n    )\n    if score is None:\n        return None\n    return score[0], score[1], score[3]\n```\nIt might be helpful to include some documentation or inline comments for the `AUTOSCALER_UTILIZATION_SCORER` option to make it easier for users to discover and customize the scorer logic.\n\n\nAlso, Im a bit confused about the value of score[2] (i.e., the minimum utilization rate across all resource types). For example, even when an actor has been scheduled on a node of type 'ray.worker.4090.standard', I see the following:\n```\nUtilization score for 'ray.worker.4090.standard': (True, 4, 0.0, 6021452193.8)\n```\n\nWhy is the minimum utilization reported as 0.0 in this case, even though the actor is already allocated to the node?\n> Why is the minimum utilization reported as 0.0 in this case, even though the actor is already allocated to the node?\n\nThat was because when it calculated the score for the 'ray.worker.4090.standard', it treated the \"object_store_memory\" resource as unused.\n\nThe behavior of the scorer looks weird to me for now. I am not sure why it considers \"utilization rate across all resource types\" instead of only \"utilization rate across **requested** resource types\". \n\nAnd the current [`util_by_resources.append(v * (util**3))`](https://github.com/ray-project/ray/blob/e91c051e5813c0a9f69d075e9cdd0d8ca8abcbd4/python/ray/autoscaler/v2/scheduler.py#L485) is also weird to me. It doesn't look like a utilization rate.\n@rueian I agree with your overall points. In my case, sorting the score based only on (1), (2), and (4)  that is, (gpu_ok, num_matching_resource_types, average_utilization)  was enough to resolve the issue and meet our project requirements.\n\nHowever, since the autoscaler is designed around predefined node types that are expected to match resource demands, selecting the most appropriate node type for the given demand is critical.\n\nIf the current scoring logic must be preserved, I think it would be better to provide an official configuration option (rather than relying solely on a custom scorer) to ensure that a node type that best fits the demand can be properly selected.\n\nAlso, including unused resources like object_store_memory when computing the minimum utilization rate can be misleading. It penalizes nodes unfairly even when the requested resources are already being utilized properly. This behavior hinders accurate scoring and effective scheduling.\nHere is why the scorer takes `object_store_memory` into consideration currently.\n\nhttps://github.com/ray-project/ray/blob/3fd6015d8f925d5dc7f61e234fa9f4cf1781c578/python/ray/autoscaler/_private/resource_demand_scheduler.py#L376-L379\n\nThe scheduler will try to fix these 4 resources `[\"CPU\", \"GPU\", \"memory\", \"object_store_memory\"]` after a node is started. And then those resources will be carried into the scorer. See https://github.com/ray-project/ray/pull/14567.\n\nHowever, I don't find any use cases that use \"object_store_memory\" for task/actor scheduling. There is even a document that says that users **cannot** use \"object_store_memory\" for scheduling: https://github.com/ray-project/ray/blob/c825dfea03fcce62c69a6a95cbe6e109925066f7/doc/source/ray-core/scheduling/resources.rst?plain=1#L91\n\nIf that statement is still true, I think we should drop the fix for \"object_store_memory\" after a node is started, as proposed in the comment https://github.com/ray-project/ray/pull/14567#issuecomment-794403566 and the deprecation https://github.com/ray-project/ray/pull/26252.\n\n","commit_urls":["https://github.com/ray-project/ray/commit/48a3441a9a046b4cdf06d9107e21ccf29b2a03e4","https://github.com/ray-project/ray/commit/d21d308b0558a75610a8e29b963c9daa1c95ce02","https://github.com/ray-project/ray/commit/54ea9540f41cd0f10f0821eb61a33d88d4be4a40","https://github.com/ray-project/ray/commit/78cb7225009bec4ce014af22b121d123345597c3"],"created_at":"2025-05-23T21:40:09Z","classification":"Efficiency"}
{"repo":"ray-project/ray","pull_number":53096,"instance_id":"ray-project__ray-53096","issue_numbers":[53071],"base_commit":"d3fd0d255c00755b4eb2e6e2cd5a8f764e6898aa","patch":"diff --git a/ci/lint/pydoclint-baseline.txt b/ci/lint/pydoclint-baseline.txt\nindex 541ee8afe251..495710cdcbf3 100644\n--- a/ci/lint/pydoclint-baseline.txt\n+++ b/ci/lint/pydoclint-baseline.txt\n@@ -1828,7 +1828,6 @@ python/ray/serve/autoscaling_policy.py\n --------------------\n python/ray/serve/batching.py\n     DOC111: Method `_BatchQueue.__init__`: The option `--arg-type-hints-in-docstring` is `False` but there are type hints in the docstring arg list\n-    DOC103: Method `_BatchQueue.__init__`: Docstring arguments are different from function arguments. (Or could be other formatting issues: https://jsh9.github.io/pydoclint/violation_codes.html#notes-on-doc103 ). Arguments in the function signature but not in the docstring: [batch_wait_timeout_s: float]. Arguments in the docstring but not in the function signature: [timeout_s: ].\n     DOC101: Function `batch`: Docstring contains fewer arguments than in function signature.\n     DOC103: Function `batch`: Docstring arguments are different from function arguments. (Or could be other formatting issues: https://jsh9.github.io/pydoclint/violation_codes.html#notes-on-doc103 ). Arguments in the function signature but not in the docstring: [_func: Optional[Callable]].\n     DOC201: Function `batch` does not have a return section in docstring\ndiff --git a/doc/source/serve/tutorials/batch.md b/doc/source/serve/tutorials/batch.md\nindex e911d0c5af9a..f8c762eb0c8e 100644\n--- a/doc/source/serve/tutorials/batch.md\n+++ b/doc/source/serve/tutorials/batch.md\n@@ -175,4 +175,5 @@ results = [r.result() for r in responses]\n ## Performance Considerations\n \n - Increase `max_batch_size` if you have sufficient memory and want higher throughput - this may increase latency\n-- Increase `batch_wait_timeout_s` if throughput is more important than latency\n\\ No newline at end of file\n+- Increase `batch_wait_timeout_s` if throughput is more important than latency\n+- Increase `max_concurrent_batches` if you have an asynchronous function that you want to process multiple batches with concurrently\n\\ No newline at end of file\ndiff --git a/python/ray/serve/batching.py b/python/ray/serve/batching.py\nindex 1a4feeeb64cc..7979b1e48271 100644\n--- a/python/ray/serve/batching.py\n+++ b/python/ray/serve/batching.py\n@@ -18,6 +18,7 @@\n     Literal,\n     Optional,\n     Protocol,\n+    Set,\n     Tuple,\n     TypeVar,\n     overload,\n@@ -52,6 +53,29 @@ class _GeneratorResult:\n     next_future: asyncio.Future\n \n \n+@dataclass\n+class _RuntimeSummaryStatistics:\n+    start_times: List[float]\n+\n+    @property\n+    def min_start_time(self) -> Optional[float]:\n+        return min(self.start_times) if self.start_times else None\n+\n+    @property\n+    def mean_start_time(self) -> Optional[float]:\n+        return (\n+            sum(self.start_times) / len(self.start_times) if self.start_times else None\n+        )\n+\n+    @property\n+    def max_start_time(self) -> Optional[float]:\n+        return max(self.start_times) if self.start_times else None\n+\n+    @property\n+    def num_requests(self) -> int:\n+        return len(self.start_times)\n+\n+\n def _batch_args_kwargs(\n     list_of_flattened_args: List[List[Any]],\n ) -> Tuple[Tuple[Any], Dict[Any, Any]]:\n@@ -82,11 +106,12 @@ def __init__(\n         self,\n         max_batch_size: int,\n         batch_wait_timeout_s: float,\n+        max_concurrent_batches: int,\n         handle_batch_func: Optional[Callable] = None,\n     ) -> None:\n         \"\"\"Async queue that accepts individual items and returns batches.\n \n-        Respects max_batch_size and timeout_s; a batch will be returned when\n+        Respects max_batch_size and batch_wait_timeout_s; a batch will be returned when\n         max_batch_size elements are available or the timeout has passed since\n         the previous get.\n \n@@ -97,18 +122,22 @@ def __init__(\n \n         Arguments:\n             max_batch_size: max number of elements to return in a batch.\n-            timeout_s: time to wait before returning an incomplete\n+            batch_wait_timeout_s: time to wait before returning an incomplete\n                 batch.\n+            max_concurrent_batches: max number of batches to run concurrently.\n             handle_batch_func(Optional[Callable]): callback to run in the\n                 background to handle batches if provided.\n         \"\"\"\n         self.queue: asyncio.Queue[_SingleRequest] = asyncio.Queue()\n         self.max_batch_size = max_batch_size\n         self.batch_wait_timeout_s = batch_wait_timeout_s\n+        self.max_concurrent_batches = max_concurrent_batches\n+        self.semaphore = asyncio.Semaphore(max_concurrent_batches)\n         self.requests_available_event = asyncio.Event()\n+        self.tasks: Set[asyncio.Task] = set()\n \n         # Used for observability.\n-        self.curr_iteration_start_time = time.time()\n+        self.curr_iteration_start_times: Dict[asyncio.Task, float] = {}\n \n         self._handle_batch_task = None\n         self._loop = get_or_create_event_loop()\n@@ -126,12 +155,13 @@ def _warn_if_max_batch_size_exceeds_max_ongoing_requests(self):\n         max_ongoing_requests = (\n             serve.get_replica_context()._deployment_config.max_ongoing_requests\n         )\n-        if max_ongoing_requests < self.max_batch_size:\n+        if max_ongoing_requests < self.max_batch_size * self.max_concurrent_batches:\n             logger.warning(\n-                f\"`max_batch_size` ({self.max_batch_size}) is larger than \"\n-                f\"`max_ongoing_requests` ({max_ongoing_requests}). This means \"\n-                \"the replica will never receive a full batch. Please update \"\n-                \"`max_ongoing_requests` to be >= `max_batch_size`.\"\n+                f\"`max_batch_size` ({self.max_batch_size}) * `max_concurrent_batches` \"\n+                f\"({self.max_concurrent_batches}) is larger than `max_ongoing_requests` \"\n+                f\"({max_ongoing_requests}). This means the replica will never achieve \"\n+                \"the configured `max_batch_size` concurrently. Please update \"\n+                \"`max_ongoing_requests` to be >= `max_batch_size` * `max_concurrent_batches`.\"\n             )\n \n     def set_max_batch_size(self, new_max_batch_size: int) -> None:\n@@ -143,7 +173,7 @@ def put(self, request: Tuple[_SingleRequest, asyncio.Future]) -> None:\n         self.queue.put_nowait(request)\n         self.requests_available_event.set()\n \n-    async def wait_for_batch(self) -> List[Any]:\n+    async def wait_for_batch(self) -> List[_SingleRequest]:\n         \"\"\"Wait for batch respecting self.max_batch_size and self.timeout_s.\n \n         Returns a batch of up to self.max_batch_size items. Waits for up to\n@@ -276,55 +306,70 @@ async def _assign_func_results(\n \n     async def _process_batches(self, func: Callable) -> None:\n         \"\"\"Loops infinitely and processes queued request batches.\"\"\"\n-\n         while not self._loop.is_closed():\n+            batch = await self.wait_for_batch()\n+            promise = self._process_batch(func, batch)\n+            task = asyncio.create_task(promise)\n+            self.tasks.add(task)\n+            self.curr_iteration_start_times[task] = time.time()\n+            task.add_done_callback(self._handle_completed_task)\n+\n+    async def _process_batch(self, func: Callable, batch: List[_SingleRequest]) -> None:\n+        \"\"\"Processes queued request batch.\"\"\"\n+        # NOTE: this semaphore caps the number of concurrent batches specified by `max_concurrent_batches`\n+        async with self.semaphore:\n+            # Remove requests that have been cancelled from the batch. If\n+            # all requests have been cancelled, simply return and wait for\n+            # the next batch.\n+            batch = [req for req in batch if not req.future.cancelled()]\n+            if len(batch) == 0:\n+                return\n+\n+            futures = [item.future for item in batch]\n+\n+            # Most of the logic in the function should be wrapped in this try-\n+            # except block, so the futures' exceptions can be set if an exception\n+            # occurs. Otherwise, the futures' requests may hang indefinitely.\n             try:\n-                self.curr_iteration_start_time = time.time()\n-                await self._process_batch(func)\n-            except Exception:\n-                logger.exception(\n-                    \"_process_batches asyncio task ran into an unexpected exception.\"\n+                self_arg = batch[0].self_arg\n+                args, kwargs = _batch_args_kwargs(\n+                    [item.flattened_args for item in batch]\n                 )\n \n-    async def _process_batch(self, func: Callable) -> None:\n-        \"\"\"Processes queued request batch.\"\"\"\n-\n-        batch: List[_SingleRequest] = await self.wait_for_batch()\n-        # Remove requests that have been cancelled from the batch. If\n-        # all requests have been cancelled, simply return and wait for\n-        # the next batch.\n-        batch = [req for req in batch if not req.future.cancelled()]\n-        if len(batch) == 0:\n-            return\n-\n-        futures = [item.future for item in batch]\n-\n-        # Most of the logic in the function should be wrapped in this try-\n-        # except block, so the futures' exceptions can be set if an exception\n-        # occurs. Otherwise, the futures' requests may hang indefinitely.\n-        try:\n-            self_arg = batch[0].self_arg\n-            args, kwargs = _batch_args_kwargs([item.flattened_args for item in batch])\n+                # Method call.\n+                if self_arg is not None:\n+                    func_future_or_generator = func(self_arg, *args, **kwargs)\n+                # Normal function call.\n+                else:\n+                    func_future_or_generator = func(*args, **kwargs)\n+\n+                if isasyncgenfunction(func):\n+                    func_generator = func_future_or_generator\n+                    await self._consume_func_generator(\n+                        func_generator, futures, len(batch)\n+                    )\n+                else:\n+                    func_future = func_future_or_generator\n+                    await self._assign_func_results(func_future, futures, len(batch))\n+\n+            except Exception as e:\n+                logger.exception(\"_process_batch ran into an unexpected exception.\")\n+\n+                for future in futures:\n+                    _set_exception_if_not_done(future, e)\n \n-            # Method call.\n-            if self_arg is not None:\n-                func_future_or_generator = func(self_arg, *args, **kwargs)\n-            # Normal function call.\n-            else:\n-                func_future_or_generator = func(*args, **kwargs)\n+    def _handle_completed_task(self, task: asyncio.Task) -> None:\n+        self.tasks.remove(task)\n+        del self.curr_iteration_start_times[task]\n+        self._log_if_exception(task.exception())\n \n-            if isasyncgenfunction(func):\n-                func_generator = func_future_or_generator\n-                await self._consume_func_generator(func_generator, futures, len(batch))\n+    @staticmethod\n+    def _log_if_exception(exception_maybe: Optional[BaseException]) -> None:\n+        if exception_maybe is not None:\n+            if isinstance(exception_maybe, asyncio.CancelledError):\n+                logger.debug(\"Task was cancelled\")\n             else:\n-                func_future = func_future_or_generator\n-                await self._assign_func_results(func_future, futures, len(batch))\n-\n-        except Exception as e:\n-            logger.exception(\"_process_batch ran into an unexpected exception.\")\n-\n-            for future in futures:\n-                _set_exception_if_not_done(future, e)\n+                logger.exception(\"Task failed unexpectedly\")\n \n     def __del__(self):\n         if (\n@@ -351,11 +396,13 @@ def __init__(\n         self,\n         max_batch_size: int = 10,\n         batch_wait_timeout_s: float = 0.0,\n+        max_concurrent_batches: int = 1,\n         handle_batch_func: Optional[Callable] = None,\n     ):\n         self._queue: Optional[_BatchQueue] = None\n         self.max_batch_size = max_batch_size\n         self.batch_wait_timeout_s = batch_wait_timeout_s\n+        self.max_concurrent_batches = max_concurrent_batches\n         self.handle_batch_func = handle_batch_func\n \n     @property\n@@ -368,6 +415,7 @@ def queue(self) -> _BatchQueue:\n             self._queue = _BatchQueue(\n                 self.max_batch_size,\n                 self.batch_wait_timeout_s,\n+                self.max_concurrent_batches,\n                 self.handle_batch_func,\n             )\n         return self._queue\n@@ -392,16 +440,11 @@ def get_max_batch_size(self) -> int:\n     def get_batch_wait_timeout_s(self) -> float:\n         return self.batch_wait_timeout_s\n \n-    def _get_curr_iteration_start_time(self) -> Optional[float]:\n-        \"\"\"Gets current iteration's start time on default _BatchQueue implementation.\n-\n-        Returns None if the batch handler doesn't use a default _BatchQueue.\n-        \"\"\"\n-\n-        if hasattr(self.queue, \"curr_iteration_start_time\"):\n-            return self.queue.curr_iteration_start_time\n-        else:\n-            return None\n+    def _get_curr_iteration_start_times(self) -> _RuntimeSummaryStatistics:\n+        \"\"\"Gets summary statistics of current iteration's start times.\"\"\"\n+        return _RuntimeSummaryStatistics(\n+            list(self.queue.curr_iteration_start_times.values())\n+        )\n \n     async def _is_batching_task_alive(self) -> bool:\n         \"\"\"Gets whether default _BatchQueue's background task is alive.\n@@ -446,12 +489,19 @@ def _validate_max_batch_size(max_batch_size):\n def _validate_batch_wait_timeout_s(batch_wait_timeout_s):\n     if not isinstance(batch_wait_timeout_s, (float, int)):\n         raise TypeError(\n-            \"batch_wait_timeout_s must be a float >= 0, \" f\"got {batch_wait_timeout_s}\"\n+            f\"batch_wait_timeout_s must be a float >= 0, got {batch_wait_timeout_s}\"\n         )\n \n     if batch_wait_timeout_s < 0:\n         raise ValueError(\n-            \"batch_wait_timeout_s must be a float >= 0, \" f\"got {batch_wait_timeout_s}\"\n+            f\"batch_wait_timeout_s must be a float >= 0, got {batch_wait_timeout_s}\"\n+        )\n+\n+\n+def _validate_max_concurrent_batches(max_concurrent_batches: int) -> None:\n+    if not isinstance(max_concurrent_batches, int) or max_concurrent_batches < 1:\n+        raise TypeError(\n+            f\"max_concurrent_batches must be an integer >= 1, got {max_concurrent_batches}\"\n         )\n \n \n@@ -502,6 +552,7 @@ def batch(\n     /,\n     max_batch_size: int = 10,\n     batch_wait_timeout_s: float = 0.0,\n+    max_concurrent_batches: int = 1,\n ) -> \"_BatchDecorator\":\n     ...\n \n@@ -538,6 +589,7 @@ def batch(\n     /,\n     max_batch_size: int = 10,\n     batch_wait_timeout_s: float = 0.0,\n+    max_concurrent_batches: int = 1,\n ) -> Callable:\n     \"\"\"Converts a function to asynchronously handle batches.\n \n@@ -585,6 +637,10 @@ async def __call__(self, request: Request):\n             one call to the underlying function.\n         batch_wait_timeout_s: the maximum duration to wait for\n             `max_batch_size` elements before running the current batch.\n+        max_concurrent_batches: the maximum number of batches that can be\n+            executed concurrently. If the number of concurrent batches exceeds\n+            this limit, the batch handler will wait for a batch to complete\n+            before sending the next batch to the underlying function.\n     \"\"\"\n     # `_func` will be None in the case when the decorator is parametrized.\n     # See the comment at the end of this function for a detailed explanation.\n@@ -599,11 +655,13 @@ async def __call__(self, request: Request):\n \n     _validate_max_batch_size(max_batch_size)\n     _validate_batch_wait_timeout_s(batch_wait_timeout_s)\n+    _validate_max_concurrent_batches(max_concurrent_batches)\n \n     def _batch_decorator(_func):\n         lazy_batch_queue_wrapper = _LazyBatchQueueWrapper(\n             max_batch_size,\n             batch_wait_timeout_s,\n+            max_concurrent_batches,\n             _func,\n         )\n \n@@ -662,8 +720,8 @@ async def batch_wrapper(*args, **kwargs):\n         )\n \n         # Store debugging methods in the lazy_batch_queue wrapper\n-        wrapper._get_curr_iteration_start_time = (\n-            lazy_batch_queue_wrapper._get_curr_iteration_start_time\n+        wrapper._get_curr_iteration_start_times = (\n+            lazy_batch_queue_wrapper._get_curr_iteration_start_times\n         )\n         wrapper._is_batching_task_alive = (\n             lazy_batch_queue_wrapper._is_batching_task_alive\n","test_patch":"diff --git a/python/ray/serve/tests/test_batching.py b/python/ray/serve/tests/test_batching.py\nindex 96343cf37ba1..53ea67c8cbf4 100644\n--- a/python/ray/serve/tests/test_batching.py\n+++ b/python/ray/serve/tests/test_batching.py\n@@ -1,7 +1,7 @@\n import asyncio\n from concurrent.futures.thread import ThreadPoolExecutor\n from functools import partial\n-from typing import List, Optional\n+from typing import Dict, List, Optional\n \n import httpx\n import pytest\n@@ -34,6 +34,58 @@ async def __call__(self, request):\n     assert max([r.result() for r in result_list]) < 20\n \n \n+def test_concurrent_batching(serve_instance):\n+    BATCHES_IN_FLIGHT = 2\n+    MAX_BATCH_SIZE = 5\n+    BATCH_WAIT_TIMEOUT_S = 1\n+    MAX_REQUESTS_IN_FLIGHT = BATCHES_IN_FLIGHT * MAX_BATCH_SIZE\n+\n+    @serve.deployment(max_ongoing_requests=MAX_REQUESTS_IN_FLIGHT * 2)\n+    class BatchingExample:\n+        def __init__(self):\n+            self.n_batches_in_flight = 0\n+            self.n_requests_in_flight = 0\n+\n+        @serve.batch(\n+            max_batch_size=MAX_BATCH_SIZE,\n+            batch_wait_timeout_s=BATCH_WAIT_TIMEOUT_S,\n+            max_concurrent_batches=BATCHES_IN_FLIGHT,\n+        )\n+        async def handle_batch(self, requests):\n+            self.n_batches_in_flight += 1\n+            self.n_requests_in_flight += len(requests)\n+            await asyncio.sleep(0.5)\n+            out = [\n+                (req_idx, self.n_batches_in_flight, self.n_requests_in_flight)\n+                for req_idx in requests\n+            ]\n+            await asyncio.sleep(0.5)\n+            self.n_requests_in_flight -= len(requests)\n+            self.n_batches_in_flight -= 1\n+            return out\n+\n+        async def __call__(self, request):\n+            return await self.handle_batch(request)\n+\n+    handle = serve.run(BatchingExample.bind())\n+\n+    idxs = set(range(20))\n+    result_futures = [handle.remote(i) for i in idxs]\n+    result_list = [future.result() for future in result_futures]\n+\n+    out_idxs = set()\n+    for idx, batches_in_flight, requests_in_flight in result_list:\n+        out_idxs.add(idx)\n+        assert (\n+            batches_in_flight == BATCHES_IN_FLIGHT\n+        ), f\"Should have been {BATCHES_IN_FLIGHT} batches in flight at all times, got {batches_in_flight}\"\n+        assert (\n+            requests_in_flight == MAX_REQUESTS_IN_FLIGHT\n+        ), f\"Should have been {MAX_REQUESTS_IN_FLIGHT} requests in flight at all times, got {requests_in_flight}\"\n+\n+    assert idxs == out_idxs, \"All requests should be processed\"\n+\n+\n def test_batching_exception(serve_instance):\n     @serve.deployment\n     class NoListReturned:\n@@ -152,11 +204,12 @@ async def __call__(self, request):\n     assert resp.text == \"0123456789\"\n \n \n-def test_observability_helpers():\n+@pytest.mark.asyncio\n+async def test_observability_helpers():\n     \"\"\"Checks observability helper methods that are used for batching.\n \n     Tests three observability helper methods:\n-        * _get_curr_iteration_start_time: gets the current iteration's start\n+        * _get_curr_iteration_start_times: gets the current iteration's start\n             time.\n         * _is_batching_task_alive: returns whether the batch-handler task is\n             alive.\n@@ -167,13 +220,14 @@ def test_observability_helpers():\n     class Batcher:\n         @serve.batch(max_batch_size=3)\n         async def handle_batch(self, requests):\n+            await asyncio.sleep(1)\n             return [0] * len(requests)\n \n         async def __call__(self, request):\n             return await self.handle_batch(request)\n \n-        async def _get_curr_iteration_start_time(self) -> Optional[float]:\n-            return self.handle_batch._get_curr_iteration_start_time()\n+        async def _get_curr_iteration_start_times(self) -> Dict[asyncio.Task, float]:\n+            return self.handle_batch._get_curr_iteration_start_times()\n \n         async def _is_batching_task_alive(self) -> bool:\n             return await self.handle_batch._is_batching_task_alive()\n@@ -184,23 +238,27 @@ async def _get_handling_task_stack(self) -> Optional[str]:\n     serve.run(target=Batcher.bind(), name=\"app_name\")\n     handle = serve.get_deployment_handle(deployment_name=\"batcher\", app_name=\"app_name\")\n \n-    assert handle._is_batching_task_alive.remote().result()\n-\n-    httpx.get(\"http://localhost:8000/\")\n-\n-    assert len(handle._get_handling_task_stack.remote().result()) is not None\n-    assert handle._is_batching_task_alive.remote().result()\n+    assert await handle._is_batching_task_alive.remote()\n \n-    curr_iteration_start_time = handle._get_curr_iteration_start_time.remote().result()\n+    async with httpx.AsyncClient() as client:\n+        task = asyncio.create_task(client.get(\"http://localhost:8000/\"))\n+        await asyncio.sleep(0.1)  # yield control to the above task\n+        prev_iter_times = await handle._get_curr_iteration_start_times.remote()\n+        await task\n \n-    for _ in range(5):\n-        httpx.get(\"http://localhost:8000/\")\n+    assert len(await handle._get_handling_task_stack.remote()) is not None\n+    assert await handle._is_batching_task_alive.remote()\n \n-    new_iteration_start_time = handle._get_curr_iteration_start_time.remote().result()\n+    async with httpx.AsyncClient() as client:\n+        futures = [client.get(\"http://localhost:8000/\") for _ in range(5)]\n+        tasks = [asyncio.create_task(future) for future in futures]\n+        await asyncio.sleep(0.1)  # yield control to the above tasks\n+        new_iter_times = await handle._get_curr_iteration_start_times.remote()\n+        await asyncio.gather(*tasks)\n \n-    assert new_iteration_start_time > curr_iteration_start_time\n-    assert len(handle._get_handling_task_stack.remote().result()) is not None\n-    assert handle._is_batching_task_alive.remote().result()\n+    assert new_iter_times.min_start_time > prev_iter_times.max_start_time\n+    assert len(await handle._get_handling_task_stack.remote()) is not None\n+    assert await handle._is_batching_task_alive.remote()\n \n \n if __name__ == \"__main__\":\ndiff --git a/python/ray/serve/tests/unit/test_batching.py b/python/ray/serve/tests/unit/test_batching.py\nindex 923eb98eea42..fa83a018c85a 100644\n--- a/python/ray/serve/tests/unit/test_batching.py\n+++ b/python/ray/serve/tests/unit/test_batching.py\n@@ -818,16 +818,21 @@ def test_warn_if_max_batch_size_exceeds_max_ongoing_requests():\n     over_bound = bound + 1\n     under_bound = bound - 1\n     over_bound_warning_message = (\n-        f\"`max_batch_size` ({over_bound}) is larger than \"\n-        f\"`max_ongoing_requests` ({bound}). This means \"\n-        \"the replica will never receive a full batch. Please update \"\n-        \"`max_ongoing_requests` to be >= `max_batch_size`.\\n\"\n+        f\"`max_batch_size` ({over_bound}) * `max_concurrent_batches` \"\n+        f\"({1}) is larger than `max_ongoing_requests` \"\n+        f\"({bound}). This means the replica will never achieve \"\n+        \"the configured `max_batch_size` concurrently. Please update \"\n+        \"`max_ongoing_requests` to be >= `max_batch_size` * `max_concurrent_batches`.\\n\"\n     )\n \n     # Start queue above the bound will log warning. Start at under or at the bound will\n     # not log warning\n     for max_batch_size in [over_bound, under_bound, bound]:\n-        queue = _BatchQueue(max_batch_size=max_batch_size, batch_wait_timeout_s=1000)\n+        queue = _BatchQueue(\n+            max_batch_size=max_batch_size,\n+            batch_wait_timeout_s=1000,\n+            max_concurrent_batches=1,\n+        )\n         if max_batch_size > bound:\n             assert over_bound_warning_message in stream.messages\n         else:\n","problem_statement":"[Serve] concurrency in ray.serve.batch\n### Description\n\nThe [current implementation for ray.serve.batch](https://github.com/ray-project/ray/blob/58c081a72aecc33ae31797320ab3a4e17ef02b7f/python/ray/serve/batching.py#L277) executes the batches synchronously. This throttles throughput for asynchronous methods wrapped in `ray.serve.batch`.\n\n\n### Use case\n\nThis could significantly improve usability when doing I/O calls to an endpoint that expects batching. It also would increase throughput for router-style composed actor where the sub-actors by avoiding the bubble that arises from waiting for the slowest sub-actor.\n","hints_text":"repro script\n```python\nfrom typing import List\n\nimport numpy as np\nimport asyncio\nimport time\nfrom ray import serve\nfrom ray.serve.handle import DeploymentHandle\n\n\n@serve.deployment(max_ongoing_requests=(8*3))\nclass Model:\n    @serve.batch(max_batch_size=8, batch_wait_timeout_s=100)\n    async def __call__(self, multiple_samples: List[int]) -> List[int]:\n        print(f\"Processing batch of size {len(multiple_samples)} at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n        await asyncio.sleep(1)\n        return np.array(multiple_samples) * 2\n\n\nhandle: DeploymentHandle = serve.run(Model.bind())\nresponses = [handle.remote(i) for i in range(8*3)]\n\nfor response in responses:\n    print(await response)\n\n```\n\n```bash\nINFO 2025-05-16 17:21:45,965 serve 92754 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n(ServeController pid=93299) INFO 2025-05-16 17:21:46,060 controller 93299 -- Deploying new version of Deployment(name='Model', app='default') (initial target replicas: 1).\n(ServeController pid=93299) INFO 2025-05-16 17:21:46,164 controller 93299 -- Stopping 1 replicas of Deployment(name='Model', app='default') with outdated versions.\n(ServeController pid=93299) INFO 2025-05-16 17:21:46,165 controller 93299 -- Adding 1 replica to Deployment(name='Model', app='default').\n(ServeController pid=93299) INFO 2025-05-16 17:21:48,265 controller 93299 -- Replica(id='k0a8blf6', deployment='Model', app='default') is stopped.\nINFO 2025-05-16 17:21:48,979 serve 92754 -- Application 'default' is ready at http://127.0.0.1:8000/.\n(ServeReplica:default:Model pid=95889) Processing batch of size 8 at 2025-05-16 17:21:48\n0\n2\n4\n6\n8\n10\n12\n14\n(ServeReplica:default:Model pid=95889) Processing batch of size 8 at 2025-05-16 17:21:50\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,004 default_Model t1cwzo6p dadef1de-b618-4ea2-9b16-b47717e1922a -- CALL __call__ OK 1017.5ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,004 default_Model t1cwzo6p 4887adc4-876d-4d45-a005-b25e32075702 -- CALL __call__ OK 1017.3ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,005 default_Model t1cwzo6p a1afaf9b-3503-4bb0-86c6-b531df778493 -- CALL __call__ OK 1017.7ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,005 default_Model t1cwzo6p 0fb99406-2184-49ed-9497-04318e35a4ed -- CALL __call__ OK 1016.9ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,006 default_Model t1cwzo6p 2bea281a-016b-4b00-a28c-b01e1a533638 -- CALL __call__ OK 1016.2ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,006 default_Model t1cwzo6p e336f9b1-8868-4831-a529-704767a9f3b3 -- CALL __call__ OK 1015.4ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,007 default_Model t1cwzo6p c51f7f6e-0df9-4ebc-ba37-7a57ff5d12e4 -- CALL __call__ OK 1016.0ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,007 default_Model t1cwzo6p 16b3f616-0e86-430b-aa04-662fa6e59760 -- CALL __call__ OK 1014.6ms\n16\n18\n20\n22\n24\n26\n28\n30\n(ServeReplica:default:Model pid=95889) Processing batch of size 8 at 2025-05-16 17:21:51\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,008 default_Model t1cwzo6p 4595fe29-b119-4988-a252-766469e0f2c6 -- CALL __call__ OK 2013.5ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,009 default_Model t1cwzo6p c5fdcc23-fdf1-405c-85ee-8ea78a2d6f3f -- CALL __call__ OK 2013.6ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,009 default_Model t1cwzo6p ca5bd40e-1a88-4580-8559-079285da167e -- CALL __call__ OK 2013.0ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,010 default_Model t1cwzo6p 9e26e8db-03dc-4327-8faf-4ca2740bbbe2 -- CALL __call__ OK 2012.8ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,011 default_Model t1cwzo6p 52e311ef-034d-4370-818c-e4d278c47370 -- CALL __call__ OK 2012.5ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,012 default_Model t1cwzo6p 8dc25957-2c1d-4f2d-8a97-5ec49d216da3 -- CALL __call__ OK 2012.9ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,012 default_Model t1cwzo6p 209f0090-b275-435e-9dd1-3c226f4b9783 -- CALL __call__ OK 2012.7ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,012 default_Model t1cwzo6p 7c2d428d-6cba-4a0e-8a67-0f3bd3e908e8 -- CALL __call__ OK 2011.4ms\n32\n34\n36\n38\n40\n42\n44\n46\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,014 default_Model t1cwzo6p 34a8ea8d-706b-4c55-b150-5d8588c1361c -- CALL __call__ OK 3012.2ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,015 default_Model t1cwzo6p b3412fb8-bb8f-4483-8f4b-5a48607cd150 -- CALL __call__ OK 3011.3ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,016 default_Model t1cwzo6p 8ea21bf3-e26b-431e-9065-28ed13f4488d -- CALL __call__ OK 3011.7ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,017 default_Model t1cwzo6p 2826b06b-7ecf-4686-83f1-31052e46fb5c -- CALL __call__ OK 3011.6ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,017 default_Model t1cwzo6p 374bab4b-6316-422f-97a2-15df99be1aa9 -- CALL __call__ OK 3011.2ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,017 default_Model t1cwzo6p 7edd45c3-cdf8-4435-ade0-e9c6d9b87999 -- CALL __call__ OK 3010.6ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,018 default_Model t1cwzo6p 0343ae29-24fe-40f8-9c16-75850a154086 -- CALL __call__ OK 3010.6ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,018 default_Model t1cwzo6p 92689615-e3bb-411a-9d08-63e1720f627e -- CALL __call__ OK 3010.2ms\n``` \nHere is an idea: \nhttps://github.com/ray-project/ray/pull/53096\n not tested or confirmed to work \n\n","all_hints_text":"repro script\n```python\nfrom typing import List\n\nimport numpy as np\nimport asyncio\nimport time\nfrom ray import serve\nfrom ray.serve.handle import DeploymentHandle\n\n\n@serve.deployment(max_ongoing_requests=(8*3))\nclass Model:\n    @serve.batch(max_batch_size=8, batch_wait_timeout_s=100)\n    async def __call__(self, multiple_samples: List[int]) -> List[int]:\n        print(f\"Processing batch of size {len(multiple_samples)} at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n        await asyncio.sleep(1)\n        return np.array(multiple_samples) * 2\n\n\nhandle: DeploymentHandle = serve.run(Model.bind())\nresponses = [handle.remote(i) for i in range(8*3)]\n\nfor response in responses:\n    print(await response)\n\n```\n\n```bash\nINFO 2025-05-16 17:21:45,965 serve 92754 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n(ServeController pid=93299) INFO 2025-05-16 17:21:46,060 controller 93299 -- Deploying new version of Deployment(name='Model', app='default') (initial target replicas: 1).\n(ServeController pid=93299) INFO 2025-05-16 17:21:46,164 controller 93299 -- Stopping 1 replicas of Deployment(name='Model', app='default') with outdated versions.\n(ServeController pid=93299) INFO 2025-05-16 17:21:46,165 controller 93299 -- Adding 1 replica to Deployment(name='Model', app='default').\n(ServeController pid=93299) INFO 2025-05-16 17:21:48,265 controller 93299 -- Replica(id='k0a8blf6', deployment='Model', app='default') is stopped.\nINFO 2025-05-16 17:21:48,979 serve 92754 -- Application 'default' is ready at http://127.0.0.1:8000/.\n(ServeReplica:default:Model pid=95889) Processing batch of size 8 at 2025-05-16 17:21:48\n0\n2\n4\n6\n8\n10\n12\n14\n(ServeReplica:default:Model pid=95889) Processing batch of size 8 at 2025-05-16 17:21:50\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,004 default_Model t1cwzo6p dadef1de-b618-4ea2-9b16-b47717e1922a -- CALL __call__ OK 1017.5ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,004 default_Model t1cwzo6p 4887adc4-876d-4d45-a005-b25e32075702 -- CALL __call__ OK 1017.3ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,005 default_Model t1cwzo6p a1afaf9b-3503-4bb0-86c6-b531df778493 -- CALL __call__ OK 1017.7ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,005 default_Model t1cwzo6p 0fb99406-2184-49ed-9497-04318e35a4ed -- CALL __call__ OK 1016.9ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,006 default_Model t1cwzo6p 2bea281a-016b-4b00-a28c-b01e1a533638 -- CALL __call__ OK 1016.2ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,006 default_Model t1cwzo6p e336f9b1-8868-4831-a529-704767a9f3b3 -- CALL __call__ OK 1015.4ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,007 default_Model t1cwzo6p c51f7f6e-0df9-4ebc-ba37-7a57ff5d12e4 -- CALL __call__ OK 1016.0ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:50,007 default_Model t1cwzo6p 16b3f616-0e86-430b-aa04-662fa6e59760 -- CALL __call__ OK 1014.6ms\n16\n18\n20\n22\n24\n26\n28\n30\n(ServeReplica:default:Model pid=95889) Processing batch of size 8 at 2025-05-16 17:21:51\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,008 default_Model t1cwzo6p 4595fe29-b119-4988-a252-766469e0f2c6 -- CALL __call__ OK 2013.5ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,009 default_Model t1cwzo6p c5fdcc23-fdf1-405c-85ee-8ea78a2d6f3f -- CALL __call__ OK 2013.6ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,009 default_Model t1cwzo6p ca5bd40e-1a88-4580-8559-079285da167e -- CALL __call__ OK 2013.0ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,010 default_Model t1cwzo6p 9e26e8db-03dc-4327-8faf-4ca2740bbbe2 -- CALL __call__ OK 2012.8ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,011 default_Model t1cwzo6p 52e311ef-034d-4370-818c-e4d278c47370 -- CALL __call__ OK 2012.5ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,012 default_Model t1cwzo6p 8dc25957-2c1d-4f2d-8a97-5ec49d216da3 -- CALL __call__ OK 2012.9ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,012 default_Model t1cwzo6p 209f0090-b275-435e-9dd1-3c226f4b9783 -- CALL __call__ OK 2012.7ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:51,012 default_Model t1cwzo6p 7c2d428d-6cba-4a0e-8a67-0f3bd3e908e8 -- CALL __call__ OK 2011.4ms\n32\n34\n36\n38\n40\n42\n44\n46\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,014 default_Model t1cwzo6p 34a8ea8d-706b-4c55-b150-5d8588c1361c -- CALL __call__ OK 3012.2ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,015 default_Model t1cwzo6p b3412fb8-bb8f-4483-8f4b-5a48607cd150 -- CALL __call__ OK 3011.3ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,016 default_Model t1cwzo6p 8ea21bf3-e26b-431e-9065-28ed13f4488d -- CALL __call__ OK 3011.7ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,017 default_Model t1cwzo6p 2826b06b-7ecf-4686-83f1-31052e46fb5c -- CALL __call__ OK 3011.6ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,017 default_Model t1cwzo6p 374bab4b-6316-422f-97a2-15df99be1aa9 -- CALL __call__ OK 3011.2ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,017 default_Model t1cwzo6p 7edd45c3-cdf8-4435-ade0-e9c6d9b87999 -- CALL __call__ OK 3010.6ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,018 default_Model t1cwzo6p 0343ae29-24fe-40f8-9c16-75850a154086 -- CALL __call__ OK 3010.6ms\n(ServeReplica:default:Model pid=95889) INFO 2025-05-16 17:21:52,018 default_Model t1cwzo6p 92689615-e3bb-411a-9d08-63e1720f627e -- CALL __call__ OK 3010.2ms\n``` \nHere is an idea: \nhttps://github.com/ray-project/ray/pull/53096\n not tested or confirmed to work \n\n","commit_urls":["https://github.com/ray-project/ray/commit/5cf626ac7740c903085c22762fceddb5eb0e7807","https://github.com/ray-project/ray/commit/fb125527214f0669a94e5d48b924128ed2460114","https://github.com/ray-project/ray/commit/f0d79ae33ba6bc22480b36826393d82c1375568e","https://github.com/ray-project/ray/commit/19b5e94c72390f9425243ffd2b4adcfa297ba535","https://github.com/ray-project/ray/commit/ae4cab37bd0747b41b4cf82f9a695ea4512c632b","https://github.com/ray-project/ray/commit/2e25302a1615b81eaa3ee85587b0076d34fc3941","https://github.com/ray-project/ray/commit/d626d870ef4a98ca2bb4e9d6f2aa56a56ee446c9","https://github.com/ray-project/ray/commit/0c4bdecdf300c9164c4614869eb7b11738c1f980","https://github.com/ray-project/ray/commit/37d5fc5f5725f8f5fdab4f0bb423709e3400a5b6","https://github.com/ray-project/ray/commit/9c74ec20bf5fa643eb067f358a611440040e1596","https://github.com/ray-project/ray/commit/0ae35908af11ddf4a48ed8a8f6a114cd499664e5","https://github.com/ray-project/ray/commit/6d9cadc1dd787c724630b80dcf5bef03e8ca543e","https://github.com/ray-project/ray/commit/48f427e20087c9e59fac1ab830e71f4f4c26c6ac","https://github.com/ray-project/ray/commit/05d80eb39d61a0e700f8e4fbe73e792801d23df8","https://github.com/ray-project/ray/commit/57d6cb85cff413cf88faad144739ff9b0c7029a9","https://github.com/ray-project/ray/commit/8bcbb9559e4291babf644ff67e95873332c20d54","https://github.com/ray-project/ray/commit/0876c2c271aef3b0a6a1e3e7934b9ceba460da8f","https://github.com/ray-project/ray/commit/24fca0b516a1277c8252e5e61def04a8602269b0","https://github.com/ray-project/ray/commit/ff987df146e046e14b02cf94df35d21b9166b9ec","https://github.com/ray-project/ray/commit/79d910705d8801cefadd904794e7c231373070e3","https://github.com/ray-project/ray/commit/5a0411a7119fdd7b257799dbfd6d64adde53b8a4"],"created_at":"2025-05-16T17:42:25Z","classification":"Efficiency"}
{"repo":"ray-project/ray","pull_number":53692,"instance_id":"ray-project__ray-53692","issue_numbers":[53623],"base_commit":"385e000b8d10de6d1ccc0111621cb1bce6e74f6a","patch":"diff --git a/src/mock/ray/core_worker/reference_count.h b/src/mock/ray/core_worker/reference_count.h\nindex f07ba8acc8c4..9efc65afc25d 100644\n--- a/src/mock/ray/core_worker/reference_count.h\n+++ b/src/mock/ray/core_worker/reference_count.h\n@@ -31,7 +31,7 @@ class MockReferenceCounter : public ReferenceCounterInterface {\n                     const rpc::Address &owner_address,\n                     bool foreign_owner_already_monitoring));\n \n-  MOCK_METHOD8(AddOwnedObject,\n+  MOCK_METHOD9(AddOwnedObject,\n                void(const ObjectID &object_id,\n                     const std::vector<ObjectID> &contained_ids,\n                     const rpc::Address &owner_address,\n@@ -39,7 +39,8 @@ class MockReferenceCounter : public ReferenceCounterInterface {\n                     const int64_t object_size,\n                     bool is_reconstructable,\n                     bool add_local_ref,\n-                    const std::optional<NodeID> &pinned_at_raylet_id));\n+                    const std::optional<NodeID> &pinned_at_raylet_id,\n+                    rpc::TensorTransport tensor_transport));\n \n   MOCK_METHOD2(AddObjectOutOfScopeOrFreedCallback,\n                bool(const ObjectID &object_id,\ndiff --git a/src/ray/core_worker/core_worker.cc b/src/ray/core_worker/core_worker.cc\nindex 3de070ca4390..49e959455120 100644\n--- a/src/ray/core_worker/core_worker.cc\n+++ b/src/ray/core_worker/core_worker.cc\n@@ -785,13 +785,18 @@ CoreWorker::CoreWorker(CoreWorkerOptions options, const WorkerID &worker_id)\n \n   actor_creator_ = std::make_shared<DefaultActorCreator>(gcs_client_);\n \n-  actor_task_submitter_ = std::make_unique<ActorTaskSubmitter>(*core_worker_client_pool_,\n-                                                               *memory_store_,\n-                                                               *task_manager_,\n-                                                               *actor_creator_,\n-                                                               on_excess_queueing,\n-                                                               io_service_,\n-                                                               reference_counter_);\n+  actor_task_submitter_ = std::make_unique<ActorTaskSubmitter>(\n+      *core_worker_client_pool_,\n+      *memory_store_,\n+      *task_manager_,\n+      *actor_creator_,\n+      /*tensor_transport_getter=*/\n+      [this](const ObjectID &object_id) {\n+        return reference_counter_->GetTensorTransport(object_id);\n+      },\n+      on_excess_queueing,\n+      io_service_,\n+      reference_counter_);\n \n   auto node_addr_factory = [this](const NodeID &node_id) {\n     std::optional<rpc::Address> addr;\n@@ -825,6 +830,13 @@ CoreWorker::CoreWorker(CoreWorkerOptions options, const WorkerID &worker_id)\n       actor_creator_,\n       worker_context_.GetCurrentJobID(),\n       lease_request_rate_limiter_,\n+      /*tensor_transport_getter=*/\n+      [](const ObjectID &object_id) {\n+        // Currently, out-of-band tensor transport (i.e., GPU objects) is only\n+        // supported for actor tasks. Therefore, normal tasks should always use\n+        // OBJECT_STORE.\n+        return rpc::TensorTransport::OBJECT_STORE;\n+      },\n       boost::asio::steady_timer(io_service_));\n   auto report_locality_data_callback = [this](\n                                            const ObjectID &object_id,\ndiff --git a/src/ray/core_worker/reference_count.cc b/src/ray/core_worker/reference_count.cc\nindex 491e90d76523..ccd8b2366b31 100644\n--- a/src/ray/core_worker/reference_count.cc\n+++ b/src/ray/core_worker/reference_count.cc\n@@ -193,7 +193,8 @@ void ReferenceCounter::AddOwnedObject(const ObjectID &object_id,\n                                       const int64_t object_size,\n                                       bool is_reconstructable,\n                                       bool add_local_ref,\n-                                      const std::optional<NodeID> &pinned_at_raylet_id) {\n+                                      const std::optional<NodeID> &pinned_at_raylet_id,\n+                                      rpc::TensorTransport tensor_transport) {\n   absl::MutexLock lock(&mutex_);\n   RAY_CHECK(AddOwnedObjectInternal(object_id,\n                                    inner_ids,\n@@ -202,7 +203,8 @@ void ReferenceCounter::AddOwnedObject(const ObjectID &object_id,\n                                    object_size,\n                                    is_reconstructable,\n                                    add_local_ref,\n-                                   pinned_at_raylet_id))\n+                                   pinned_at_raylet_id,\n+                                   tensor_transport))\n       << \"Tried to create an owned object that already exists: \" << object_id;\n }\n \n@@ -315,7 +317,8 @@ bool ReferenceCounter::AddOwnedObjectInternal(\n     const int64_t object_size,\n     bool is_reconstructable,\n     bool add_local_ref,\n-    const std::optional<NodeID> &pinned_at_raylet_id) {\n+    const std::optional<NodeID> &pinned_at_raylet_id,\n+    rpc::TensorTransport tensor_transport) {\n   if (object_id_refs_.count(object_id) != 0) {\n     return false;\n   }\n@@ -336,7 +339,8 @@ bool ReferenceCounter::AddOwnedObjectInternal(\n                                    call_site,\n                                    object_size,\n                                    is_reconstructable,\n-                                   pinned_at_raylet_id))\n+                                   pinned_at_raylet_id,\n+                                   tensor_transport))\n                 .first;\n   if (!inner_ids.empty()) {\n     // Mark that this object ID contains other inner IDs. Then, we will not GC\n@@ -1704,5 +1708,15 @@ void ReferenceCounter::Reference::ToProto(rpc::ObjectReferenceCount *ref,\n   }\n }\n \n+std::optional<rpc::TensorTransport> ReferenceCounter::GetTensorTransport(\n+    const ObjectID &object_id) const {\n+  absl::MutexLock lock(&mutex_);\n+  auto it = object_id_refs_.find(object_id);\n+  if (it == object_id_refs_.end()) {\n+    return absl::nullopt;\n+  }\n+  return it->second.tensor_transport;\n+}\n+\n }  // namespace core\n }  // namespace ray\ndiff --git a/src/ray/core_worker/reference_count.h b/src/ray/core_worker/reference_count.h\nindex d59222b5bafb..fc234ba1fe8e 100644\n--- a/src/ray/core_worker/reference_count.h\n+++ b/src/ray/core_worker/reference_count.h\n@@ -56,7 +56,8 @@ class ReferenceCounterInterface {\n       const int64_t object_size,\n       bool is_reconstructable,\n       bool add_local_ref,\n-      const std::optional<NodeID> &pinned_at_raylet_id = std::optional<NodeID>()) = 0;\n+      const std::optional<NodeID> &pinned_at_raylet_id = std::optional<NodeID>(),\n+      rpc::TensorTransport tensor_transport = rpc::TensorTransport::OBJECT_STORE) = 0;\n   virtual bool AddObjectOutOfScopeOrFreedCallback(\n       const ObjectID &object_id,\n       const std::function<void(const ObjectID &)> callback) = 0;\n@@ -189,15 +190,18 @@ class ReferenceCounter : public ReferenceCounterInterface,\n   /// corresponding ObjectRef has been returned to the language frontend.\n   /// \\param[in] pinned_at_raylet_id The primary location for the object, if it\n   /// is already known. This is only used for ray.put calls.\n-  void AddOwnedObject(const ObjectID &object_id,\n-                      const std::vector<ObjectID> &contained_ids,\n-                      const rpc::Address &owner_address,\n-                      const std::string &call_site,\n-                      const int64_t object_size,\n-                      bool is_reconstructable,\n-                      bool add_local_ref,\n-                      const std::optional<NodeID> &pinned_at_raylet_id =\n-                          std::optional<NodeID>()) override ABSL_LOCKS_EXCLUDED(mutex_);\n+  /// \\param[in] tensor_transport The transport used for the object.\n+  void AddOwnedObject(\n+      const ObjectID &object_id,\n+      const std::vector<ObjectID> &contained_ids,\n+      const rpc::Address &owner_address,\n+      const std::string &call_site,\n+      const int64_t object_size,\n+      bool is_reconstructable,\n+      bool add_local_ref,\n+      const std::optional<NodeID> &pinned_at_raylet_id = std::optional<NodeID>(),\n+      rpc::TensorTransport tensor_transport = rpc::TensorTransport::OBJECT_STORE) override\n+      ABSL_LOCKS_EXCLUDED(mutex_);\n \n   /// Add an owned object that was dynamically created. These are objects that\n   /// were created by a task that we called, but that we own.\n@@ -588,6 +592,9 @@ class ReferenceCounter : public ReferenceCounterInterface,\n   /// Release all local references which registered on this local.\n   void ReleaseAllLocalReferences();\n \n+  /// Get the tensor transport for the given object.\n+  std::optional<rpc::TensorTransport> GetTensorTransport(const ObjectID &object_id) const;\n+\n  private:\n   /// Contains information related to nested object refs only.\n   struct NestedReferenceCount {\n@@ -645,11 +652,13 @@ class ReferenceCounter : public ReferenceCounterInterface,\n               std::string call_site,\n               int64_t object_size,\n               bool is_reconstructable,\n-              std::optional<NodeID> pinned_at_raylet_id)\n+              std::optional<NodeID> pinned_at_raylet_id,\n+              rpc::TensorTransport tensor_transport)\n         : call_site(std::move(call_site)),\n           object_size(object_size),\n           owner_address(std::move(owner_address)),\n           pinned_at_raylet_id(std::move(pinned_at_raylet_id)),\n+          tensor_transport(tensor_transport),\n           owned_by_us(true),\n           is_reconstructable(is_reconstructable),\n           pending_creation(!pinned_at_raylet_id.has_value()) {}\n@@ -762,6 +771,8 @@ class ReferenceCounter : public ReferenceCounterInterface,\n     /// counting is enabled, then some raylet must be pinning the object value.\n     /// This is the address of that raylet.\n     std::optional<NodeID> pinned_at_raylet_id;\n+    /// The transport used for the object.\n+    rpc::TensorTransport tensor_transport;\n     /// Whether we own the object. If we own the object, then we are\n     /// responsible for tracking the state of the task that creates the object\n     /// (see task_manager.h).\n@@ -839,14 +850,16 @@ class ReferenceCounter : public ReferenceCounterInterface,\n   using ReferenceTable = absl::flat_hash_map<ObjectID, Reference>;\n   using ReferenceProtoTable = absl::flat_hash_map<ObjectID, rpc::ObjectReferenceCount>;\n \n-  bool AddOwnedObjectInternal(const ObjectID &object_id,\n-                              const std::vector<ObjectID> &contained_ids,\n-                              const rpc::Address &owner_address,\n-                              const std::string &call_site,\n-                              const int64_t object_size,\n-                              bool is_reconstructable,\n-                              bool add_local_ref,\n-                              const std::optional<NodeID> &pinned_at_raylet_id)\n+  bool AddOwnedObjectInternal(\n+      const ObjectID &object_id,\n+      const std::vector<ObjectID> &contained_ids,\n+      const rpc::Address &owner_address,\n+      const std::string &call_site,\n+      const int64_t object_size,\n+      bool is_reconstructable,\n+      bool add_local_ref,\n+      const std::optional<NodeID> &pinned_at_raylet_id,\n+      rpc::TensorTransport tensor_transport = rpc::TensorTransport::OBJECT_STORE)\n       ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_);\n \n   void SetNestedRefInUseRecursive(ReferenceTable::iterator inner_ref_it)\ndiff --git a/src/ray/core_worker/task_manager.cc b/src/ray/core_worker/task_manager.cc\nindex 9da087239c49..1cb5c3480e1c 100644\n--- a/src/ray/core_worker/task_manager.cc\n+++ b/src/ray/core_worker/task_manager.cc\n@@ -265,7 +265,9 @@ std::vector<rpc::ObjectReference> TaskManager::AddPendingTask(\n                                         call_site,\n                                         -1,\n                                         is_reconstructable,\n-                                        /*add_local_ref=*/true);\n+                                        /*add_local_ref=*/true,\n+                                        /*pinned_at_raylet_id=*/std::optional<NodeID>(),\n+                                        /*tensor_transport=*/spec.TensorTransport());\n     }\n \n     return_ids.push_back(return_id);\ndiff --git a/src/ray/core_worker/transport/actor_task_submitter.h b/src/ray/core_worker/transport/actor_task_submitter.h\nindex 46ddb2e06668..3d0a1fa463a8 100644\n--- a/src/ray/core_worker/transport/actor_task_submitter.h\n+++ b/src/ray/core_worker/transport/actor_task_submitter.h\n@@ -78,12 +78,13 @@ class ActorTaskSubmitter : public ActorTaskSubmitterInterface {\n                      CoreWorkerMemoryStore &store,\n                      TaskFinisherInterface &task_finisher,\n                      ActorCreatorInterface &actor_creator,\n+                     const TensorTransportGetter &tensor_transport_getter,\n                      std::function<void(const ActorID &, int64_t)> warn_excess_queueing,\n                      instrumented_io_context &io_service,\n                      std::shared_ptr<ReferenceCounterInterface> reference_counter)\n       : core_worker_client_pool_(core_worker_client_pool),\n         actor_creator_(actor_creator),\n-        resolver_(store, task_finisher, actor_creator),\n+        resolver_(store, task_finisher, actor_creator, tensor_transport_getter),\n         task_finisher_(task_finisher),\n         warn_excess_queueing_(warn_excess_queueing),\n         io_service_(io_service),\ndiff --git a/src/ray/core_worker/transport/dependency_resolver.cc b/src/ray/core_worker/transport/dependency_resolver.cc\nindex a9d2856e78e9..5d97825e2586 100644\n--- a/src/ray/core_worker/transport/dependency_resolver.cc\n+++ b/src/ray/core_worker/transport/dependency_resolver.cc\n@@ -27,7 +27,8 @@ void InlineDependencies(\n     const absl::flat_hash_map<ObjectID, std::shared_ptr<RayObject>> &dependencies,\n     TaskSpecification &task,\n     std::vector<ObjectID> *inlined_dependency_ids,\n-    std::vector<ObjectID> *contained_ids) {\n+    std::vector<ObjectID> *contained_ids,\n+    const TensorTransportGetter &tensor_transport_getter) {\n   auto &msg = task.GetMutableMessage();\n   size_t found = 0;\n   for (size_t i = 0; i < task.NumArgs(); i++) {\n@@ -39,7 +40,19 @@ void InlineDependencies(\n         auto *mutable_arg = msg.mutable_args(i);\n         if (!it->second->IsInPlasmaError()) {\n           // The object has not been promoted to plasma. Inline the object by\n-          // clearing the reference and replacing it with the raw value.\n+          // replacing it with the raw value.\n+          if (tensor_transport_getter(id) == rpc::TensorTransport::OBJECT_STORE) {\n+            // Clear the object reference if the object is transferred via the object\n+            // store. If we don't clear the object reference, tasks with a large number of\n+            // arguments will experience performance degradation due to higher\n+            // serialization overhead.\n+            //\n+            // However, if the tensor transport is not OBJECT_STORE (e.g., NCCL),\n+            // we must keep the object reference so that the receiver can retrieve\n+            // the GPU object from the in-actor GPU object store using the object ID as\n+            // the key.\n+            mutable_arg->clear_object_ref();\n+          }\n           mutable_arg->set_is_inlined(true);\n           if (it->second->HasData()) {\n             const auto &data = it->second->GetData();\n@@ -128,7 +141,8 @@ void LocalDependencyResolver::ResolveDependencies(\n               InlineDependencies(state->local_dependencies,\n                                  state->task,\n                                  &inlined_dependency_ids,\n-                                 &contained_ids);\n+                                 &contained_ids,\n+                                 tensor_transport_getter_);\n               if (state->actor_dependencies_remaining == 0) {\n                 resolved_task_state = std::move(state);\n                 pending_tasks_.erase(it);\ndiff --git a/src/ray/core_worker/transport/dependency_resolver.h b/src/ray/core_worker/transport/dependency_resolver.h\nindex 06fbe6332a31..6eab2c30ba5a 100644\n--- a/src/ray/core_worker/transport/dependency_resolver.h\n+++ b/src/ray/core_worker/transport/dependency_resolver.h\n@@ -28,15 +28,20 @@\n namespace ray {\n namespace core {\n \n+using TensorTransportGetter =\n+    std::function<std::optional<rpc::TensorTransport>(const ObjectID &object_id)>;\n+\n // This class is thread-safe.\n class LocalDependencyResolver {\n  public:\n   LocalDependencyResolver(CoreWorkerMemoryStore &store,\n                           TaskFinisherInterface &task_finisher,\n-                          ActorCreatorInterface &actor_creator)\n+                          ActorCreatorInterface &actor_creator,\n+                          const TensorTransportGetter &tensor_transport_getter)\n       : in_memory_store_(store),\n         task_finisher_(task_finisher),\n-        actor_creator_(actor_creator) {}\n+        actor_creator_(actor_creator),\n+        tensor_transport_getter_(tensor_transport_getter) {}\n \n   /// Resolve all local and remote dependencies for the task, calling the specified\n   /// callback when done. Direct call ids in the task specification will be resolved\n@@ -104,6 +109,14 @@ class LocalDependencyResolver {\n \n   ActorCreatorInterface &actor_creator_;\n \n+  /// Used to get the tensor transport for an object.\n+  /// ObjectRefs with a tensor transport other than OBJECT_STORE will be only\n+  /// partially inlined. The rest of the data will be transferred via a\n+  /// different communication backend directly between actors. Thus, for these\n+  /// objects, we will not clear the ObjectRef metadata, even if the task\n+  /// executor has inlined the object value.\n+  const TensorTransportGetter tensor_transport_getter_;\n+\n   absl::flat_hash_map<TaskID, std::unique_ptr<TaskState>> pending_tasks_\n       ABSL_GUARDED_BY(mu_);\n \ndiff --git a/src/ray/core_worker/transport/normal_task_submitter.h b/src/ray/core_worker/transport/normal_task_submitter.h\nindex de50622fb3a9..8d7cdaea30a3 100644\n--- a/src/ray/core_worker/transport/normal_task_submitter.h\n+++ b/src/ray/core_worker/transport/normal_task_submitter.h\n@@ -92,12 +92,13 @@ class NormalTaskSubmitter {\n       std::shared_ptr<ActorCreatorInterface> actor_creator,\n       const JobID &job_id,\n       std::shared_ptr<LeaseRequestRateLimiter> lease_request_rate_limiter,\n+      const TensorTransportGetter &tensor_transport_getter,\n       std::optional<boost::asio::steady_timer> cancel_timer = absl::nullopt)\n       : rpc_address_(std::move(rpc_address)),\n         local_lease_client_(lease_client),\n         lease_client_factory_(lease_client_factory),\n         lease_policy_(std::move(lease_policy)),\n-        resolver_(*store, task_finisher, *actor_creator),\n+        resolver_(*store, task_finisher, *actor_creator, tensor_transport_getter),\n         task_finisher_(task_finisher),\n         lease_timeout_ms_(lease_timeout_ms),\n         local_raylet_id_(local_raylet_id),\n","test_patch":"diff --git a/src/ray/core_worker/test/actor_task_submitter_test.cc b/src/ray/core_worker/test/actor_task_submitter_test.cc\nindex 0ef15c37e5a1..40e74c7803c4 100644\n--- a/src/ray/core_worker/test/actor_task_submitter_test.cc\n+++ b/src/ray/core_worker/test/actor_task_submitter_test.cc\n@@ -100,6 +100,7 @@ class ActorTaskSubmitterTest : public ::testing::TestWithParam<bool> {\n             *store_,\n             *task_finisher_,\n             actor_creator_,\n+            [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; },\n             [this](const ActorID &actor_id, int64_t num_queued) {\n               last_queue_warning_ = num_queued;\n             },\ndiff --git a/src/ray/core_worker/test/dependency_resolver_test.cc b/src/ray/core_worker/test/dependency_resolver_test.cc\nindex 9d4910e74c6d..c20e78d48007 100644\n--- a/src/ray/core_worker/test/dependency_resolver_test.cc\n+++ b/src/ray/core_worker/test/dependency_resolver_test.cc\n@@ -183,7 +183,10 @@ TEST(LocalDependencyResolverTest, TestNoDependencies) {\n   auto store = DefaultCoreWorkerMemoryStoreWithThread::Create();\n   auto task_finisher = std::make_shared<MockTaskFinisher>();\n   MockActorCreator actor_creator;\n-  LocalDependencyResolver resolver(*store, *task_finisher, actor_creator);\n+  LocalDependencyResolver resolver(\n+      *store, *task_finisher, actor_creator, [](const ObjectID &object_id) {\n+        return rpc::TensorTransport::OBJECT_STORE;\n+      });\n   TaskSpecification task;\n   bool ok = false;\n   resolver.ResolveDependencies(task, [&ok](Status) { ok = true; });\n@@ -196,7 +199,10 @@ TEST(LocalDependencyResolverTest, TestActorAndObjectDependencies1) {\n   auto store = DefaultCoreWorkerMemoryStoreWithThread::Create();\n   auto task_finisher = std::make_shared<MockTaskFinisher>();\n   MockActorCreator actor_creator;\n-  LocalDependencyResolver resolver(*store, *task_finisher, actor_creator);\n+  LocalDependencyResolver resolver(\n+      *store, *task_finisher, actor_creator, [](const ObjectID &object_id) {\n+        return rpc::TensorTransport::OBJECT_STORE;\n+      });\n   TaskSpecification task;\n   ObjectID obj = ObjectID::FromRandom();\n   task.GetMutableMessage().add_args()->mutable_object_ref()->set_object_id(obj.Binary());\n@@ -238,7 +244,10 @@ TEST(LocalDependencyResolverTest, TestActorAndObjectDependencies2) {\n   auto store = DefaultCoreWorkerMemoryStoreWithThread::Create();\n   auto task_finisher = std::make_shared<MockTaskFinisher>();\n   MockActorCreator actor_creator;\n-  LocalDependencyResolver resolver(*store, *task_finisher, actor_creator);\n+  LocalDependencyResolver resolver(\n+      *store, *task_finisher, actor_creator, [](const ObjectID &object_id) {\n+        return rpc::TensorTransport::OBJECT_STORE;\n+      });\n   TaskSpecification task;\n   ObjectID obj = ObjectID::FromRandom();\n   task.GetMutableMessage().add_args()->mutable_object_ref()->set_object_id(obj.Binary());\n@@ -279,7 +288,10 @@ TEST(LocalDependencyResolverTest, TestHandlePlasmaPromotion) {\n   auto store = DefaultCoreWorkerMemoryStoreWithThread::Create();\n   auto task_finisher = std::make_shared<MockTaskFinisher>();\n   MockActorCreator actor_creator;\n-  LocalDependencyResolver resolver(*store, *task_finisher, actor_creator);\n+  LocalDependencyResolver resolver(\n+      *store, *task_finisher, actor_creator, [](const ObjectID &object_id) {\n+        return rpc::TensorTransport::OBJECT_STORE;\n+      });\n   ObjectID obj1 = ObjectID::FromRandom();\n   std::string meta = std::to_string(static_cast<int>(rpc::ErrorType::OBJECT_IN_PLASMA));\n   auto metadata = const_cast<uint8_t *>(reinterpret_cast<const uint8_t *>(meta.data()));\n@@ -306,7 +318,10 @@ TEST(LocalDependencyResolverTest, TestInlineLocalDependencies) {\n   auto store = DefaultCoreWorkerMemoryStoreWithThread::Create();\n   auto task_finisher = std::make_shared<MockTaskFinisher>();\n   MockActorCreator actor_creator;\n-  LocalDependencyResolver resolver(*store, *task_finisher, actor_creator);\n+  LocalDependencyResolver resolver(\n+      *store, *task_finisher, actor_creator, [](const ObjectID &object_id) {\n+        return rpc::TensorTransport::OBJECT_STORE;\n+      });\n   ObjectID obj1 = ObjectID::FromRandom();\n   ObjectID obj2 = ObjectID::FromRandom();\n   auto data = GenerateRandomObject();\n@@ -337,7 +352,10 @@ TEST(LocalDependencyResolverTest, TestInlinePendingDependencies) {\n   auto store = DefaultCoreWorkerMemoryStoreWithThread::Create();\n   auto task_finisher = std::make_shared<MockTaskFinisher>();\n   MockActorCreator actor_creator;\n-  LocalDependencyResolver resolver(*store, *task_finisher, actor_creator);\n+  LocalDependencyResolver resolver(\n+      *store, *task_finisher, actor_creator, [](const ObjectID &object_id) {\n+        return rpc::TensorTransport::OBJECT_STORE;\n+      });\n   ObjectID obj1 = ObjectID::FromRandom();\n   ObjectID obj2 = ObjectID::FromRandom();\n   auto data = GenerateRandomObject();\n@@ -372,7 +390,10 @@ TEST(LocalDependencyResolverTest, TestInlinedObjectIds) {\n   auto store = DefaultCoreWorkerMemoryStoreWithThread::Create();\n   auto task_finisher = std::make_shared<MockTaskFinisher>();\n   MockActorCreator actor_creator;\n-  LocalDependencyResolver resolver(*store, *task_finisher, actor_creator);\n+  LocalDependencyResolver resolver(\n+      *store, *task_finisher, actor_creator, [](const ObjectID &object_id) {\n+        return rpc::TensorTransport::OBJECT_STORE;\n+      });\n   ObjectID obj1 = ObjectID::FromRandom();\n   ObjectID obj2 = ObjectID::FromRandom();\n   ObjectID obj3 = ObjectID::FromRandom();\n@@ -409,7 +430,10 @@ TEST(LocalDependencyResolverTest, TestCancelDependencyResolution) {\n   auto store = std::make_shared<CoreWorkerMemoryStore>(io_context.GetIoService());\n   auto task_finisher = std::make_shared<MockTaskFinisher>();\n   MockActorCreator actor_creator;\n-  LocalDependencyResolver resolver(*store, *task_finisher, actor_creator);\n+  LocalDependencyResolver resolver(\n+      *store, *task_finisher, actor_creator, [](const ObjectID &object_id) {\n+        return rpc::TensorTransport::OBJECT_STORE;\n+      });\n   ObjectID obj1 = ObjectID::FromRandom();\n   ObjectID obj2 = ObjectID::FromRandom();\n   auto data = GenerateRandomObject();\n@@ -441,7 +465,10 @@ TEST(LocalDependencyResolverTest, TestDependenciesAlreadyLocal) {\n   auto store = DefaultCoreWorkerMemoryStoreWithThread::Create();\n   auto task_finisher = std::make_shared<MockTaskFinisher>();\n   MockActorCreator actor_creator;\n-  LocalDependencyResolver resolver(*store, *task_finisher, actor_creator);\n+  LocalDependencyResolver resolver(\n+      *store, *task_finisher, actor_creator, [](const ObjectID &object_id) {\n+        return rpc::TensorTransport::OBJECT_STORE;\n+      });\n \n   ObjectID obj = ObjectID::FromRandom();\n   auto data = GenerateRandomObject();\n@@ -461,6 +488,56 @@ TEST(LocalDependencyResolverTest, TestDependenciesAlreadyLocal) {\n   ASSERT_EQ(resolver.NumPendingTasks(), 0);\n }\n \n+TEST(LocalDependencyResolverTest, TestMixedTensorTransport) {\n+  // There are two arguments of the task, and the first argument is a GPU object\n+  // with tensor transport NCCL, and the second argument is a normal object with\n+  // tensor transport OBJECT_STORE.\n+  //\n+  // Both objects are small enough to be inlined. The first argument should be inlined\n+  // and the `object_ref` field should not be cleared so that this actor can use the\n+  // object ID as a key to retrieve the tensor from the GPU store. The second argument\n+  // should be inlined and the `object_ref` field should be cleared. If it is not cleared,\n+  // there will be performance regression in some edge cases.\n+  auto store = DefaultCoreWorkerMemoryStoreWithThread::Create();\n+  auto task_finisher = std::make_shared<MockTaskFinisher>();\n+  MockActorCreator actor_creator;\n+\n+  // `obj1` is a GPU object, and `obj2` is a normal object.\n+  ObjectID obj1 = ObjectID::FromRandom();\n+  ObjectID obj2 = ObjectID::FromRandom();\n+\n+  LocalDependencyResolver resolver(\n+      *store, *task_finisher, actor_creator, [&](const ObjectID &object_id) {\n+        if (object_id == obj1) {\n+          return rpc::TensorTransport::NCCL;\n+        }\n+        return rpc::TensorTransport::OBJECT_STORE;\n+      });\n+\n+  auto data = GenerateRandomObject();\n+  ASSERT_TRUE(store->Put(*data, obj1));\n+  ASSERT_TRUE(store->Put(*data, obj2));\n+\n+  TaskSpecification task;\n+  task.GetMutableMessage().add_args()->mutable_object_ref()->set_object_id(obj1.Binary());\n+  task.GetMutableMessage().add_args()->mutable_object_ref()->set_object_id(obj2.Binary());\n+\n+  std::promise<bool> dependencies_resolved;\n+  resolver.ResolveDependencies(task,\n+                               [&](Status) { dependencies_resolved.set_value(true); });\n+  ASSERT_TRUE(dependencies_resolved.get_future().get());\n+\n+  // First arg (NCCL) should not be cleared\n+  ASSERT_TRUE(task.GetMutableMessage().args(0).is_inlined());\n+  ASSERT_TRUE(task.GetMutableMessage().args(0).has_object_ref());\n+  // Second arg (OBJECT_STORE) should be cleared\n+  ASSERT_TRUE(task.GetMutableMessage().args(1).is_inlined());\n+  ASSERT_FALSE(task.GetMutableMessage().args(1).has_object_ref());\n+\n+  ASSERT_EQ(task_finisher->num_inlined_dependencies, 2);\n+  ASSERT_EQ(resolver.NumPendingTasks(), 0);\n+}\n+\n }  // namespace core\n }  // namespace ray\n \ndiff --git a/src/ray/core_worker/test/direct_actor_transport_mock_test.cc b/src/ray/core_worker/test/direct_actor_transport_mock_test.cc\nindex 1f08b50bba17..305817bae202 100644\n--- a/src/ray/core_worker/test/direct_actor_transport_mock_test.cc\n+++ b/src/ray/core_worker/test/direct_actor_transport_mock_test.cc\n@@ -44,13 +44,15 @@ class DirectTaskTransportTest : public ::testing::Test {\n         [&](const rpc::Address &) { return nullptr; });\n     memory_store = DefaultCoreWorkerMemoryStoreWithThread::Create();\n     reference_counter = std::make_shared<MockReferenceCounter>();\n-    actor_task_submitter = std::make_unique<ActorTaskSubmitter>(*client_pool,\n-                                                                *memory_store,\n-                                                                *task_finisher,\n-                                                                *actor_creator,\n-                                                                nullptr,\n-                                                                io_context,\n-                                                                reference_counter);\n+    actor_task_submitter = std::make_unique<ActorTaskSubmitter>(\n+        *client_pool,\n+        *memory_store,\n+        *task_finisher,\n+        *actor_creator,\n+        [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; },\n+        nullptr,\n+        io_context,\n+        reference_counter);\n   }\n \n   TaskSpecification GetActorTaskSpec(const ActorID &actor_id) {\ndiff --git a/src/ray/core_worker/test/normal_task_submitter_test.cc b/src/ray/core_worker/test/normal_task_submitter_test.cc\nindex 92f56add15e1..484276dda855 100644\n--- a/src/ray/core_worker/test/normal_task_submitter_test.cc\n+++ b/src/ray/core_worker/test/normal_task_submitter_test.cc\n@@ -465,19 +465,21 @@ TEST(NormalTaskSubmitterTest, TestLocalityAwareSubmitOneTask) {\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n   auto *lease_policy_ptr = lease_policy.get();\n   lease_policy->is_locality_aware = true;\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   TaskSpecification task = BuildEmptyTaskSpec();\n \n@@ -518,19 +520,21 @@ TEST(NormalTaskSubmitterTest, TestSubmitOneTask) {\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n   auto *lease_policy_ptr = lease_policy.get();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   TaskSpecification task = BuildEmptyTaskSpec();\n \n@@ -570,19 +574,21 @@ TEST(NormalTaskSubmitterTest, TestRetryTaskApplicationLevelError) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task = BuildEmptyTaskSpec();\n   task.GetMutableMessage().set_retry_exceptions(true);\n \n@@ -627,19 +633,21 @@ TEST(NormalTaskSubmitterTest, TestHandleTaskFailure) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task = BuildEmptyTaskSpec();\n \n   ASSERT_TRUE(submitter.SubmitTask(task).ok());\n@@ -671,19 +679,21 @@ TEST(NormalTaskSubmitterTest, TestHandleUnschedulableTask) {\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n   auto *lease_policy_ptr = lease_policy.get();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kTwoRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kTwoRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   TaskSpecification task1 = BuildEmptyTaskSpec();\n   TaskSpecification task2 = BuildEmptyTaskSpec();\n@@ -742,19 +752,21 @@ TEST(NormalTaskSubmitterTest, TestHandleRuntimeEnvSetupFailed) {\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n   auto *lease_policy_ptr = lease_policy.get();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kTwoRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kTwoRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   TaskSpecification task1 = BuildEmptyTaskSpec();\n   TaskSpecification task2 = BuildEmptyTaskSpec();\n@@ -812,19 +824,21 @@ TEST(NormalTaskSubmitterTest, TestWorkerHandleLocalRayletDied) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kTwoRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kTwoRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   TaskSpecification task1 = BuildEmptyTaskSpec();\n   ASSERT_TRUE(submitter.SubmitTask(task1).ok());\n@@ -842,19 +856,21 @@ TEST(NormalTaskSubmitterTest, TestDriverHandleLocalRayletDied) {\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n   auto *lease_policy_ptr = lease_policy.get();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::DRIVER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kTwoRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::DRIVER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kTwoRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   TaskSpecification task1 = BuildEmptyTaskSpec();\n   TaskSpecification task2 = BuildEmptyTaskSpec();\n@@ -900,19 +916,21 @@ TEST(NormalTaskSubmitterTest, TestConcurrentWorkerLeases) {\n \n   int64_t concurrency = 10;\n   auto rateLimiter = std::make_shared<StaticLeaseRequestRateLimiter>(concurrency);\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                rateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      rateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   std::vector<TaskSpecification> tasks;\n   for (int i = 0; i < 2 * concurrency; i++) {\n@@ -979,19 +997,21 @@ TEST(NormalTaskSubmitterTest, TestConcurrentWorkerLeasesDynamic) {\n \n   int64_t concurrency = 10;\n   auto rateLimiter = std::make_shared<DynamicRateLimiter>(1);\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                rateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      rateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   std::vector<TaskSpecification> tasks;\n   for (int i = 0; i < 2 * concurrency; i++) {\n@@ -1087,19 +1107,21 @@ TEST(NormalTaskSubmitterTest, TestConcurrentWorkerLeasesDynamicWithSpillback) {\n \n   int64_t concurrency = 10;\n   auto rateLimiter = std::make_shared<DynamicRateLimiter>(1);\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                lease_client_factory,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                rateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      lease_client_factory,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      rateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   std::vector<TaskSpecification> tasks;\n   for (int i = 0; i < 2 * concurrency; i++) {\n@@ -1192,19 +1214,21 @@ TEST(NormalTaskSubmitterTest, TestSubmitMultipleTasks) {\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n   auto *lease_policy_ptr = lease_policy.get();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   TaskSpecification task1 = BuildEmptyTaskSpec();\n   TaskSpecification task2 = BuildEmptyTaskSpec();\n@@ -1265,19 +1289,21 @@ TEST(NormalTaskSubmitterTest, TestReuseWorkerLease) {\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n   auto *lease_policy_ptr = lease_policy.get();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   TaskSpecification task1 = BuildEmptyTaskSpec();\n   TaskSpecification task2 = BuildEmptyTaskSpec();\n@@ -1338,19 +1364,21 @@ TEST(NormalTaskSubmitterTest, TestRetryLeaseCancellation) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task1 = BuildEmptyTaskSpec();\n   TaskSpecification task2 = BuildEmptyTaskSpec();\n   TaskSpecification task3 = BuildEmptyTaskSpec();\n@@ -1406,19 +1434,21 @@ TEST(NormalTaskSubmitterTest, TestConcurrentCancellationAndSubmission) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task1 = BuildEmptyTaskSpec();\n   TaskSpecification task2 = BuildEmptyTaskSpec();\n   TaskSpecification task3 = BuildEmptyTaskSpec();\n@@ -1471,19 +1501,21 @@ TEST(NormalTaskSubmitterTest, TestWorkerNotReusedOnError) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task1 = BuildEmptyTaskSpec();\n   TaskSpecification task2 = BuildEmptyTaskSpec();\n \n@@ -1527,19 +1559,21 @@ TEST(NormalTaskSubmitterTest, TestWorkerNotReturnedOnExit) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task1 = BuildEmptyTaskSpec();\n \n   ASSERT_TRUE(submitter.SubmitTask(task1).ok());\n@@ -1584,19 +1618,21 @@ TEST(NormalTaskSubmitterTest, TestSpillback) {\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n   auto *lease_policy_ptr = lease_policy.get();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                lease_client_factory,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      lease_client_factory,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task = BuildEmptyTaskSpec();\n \n   ASSERT_TRUE(submitter.SubmitTask(task).ok());\n@@ -1659,19 +1695,21 @@ TEST(NormalTaskSubmitterTest, TestSpillbackRoundTrip) {\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>(local_raylet_id);\n   auto *lease_policy_ptr = lease_policy.get();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                lease_client_factory,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                local_raylet_id,\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      lease_client_factory,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      local_raylet_id,\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task = BuildEmptyTaskSpec();\n \n   ASSERT_TRUE(submitter.SubmitTask(task).ok());\n@@ -1737,19 +1775,21 @@ void TestSchedulingKey(const std::shared_ptr<CoreWorkerMemoryStore> store,\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   ASSERT_TRUE(submitter.SubmitTask(same1).ok());\n   ASSERT_TRUE(submitter.SubmitTask(same2).ok());\n@@ -1895,19 +1935,21 @@ TEST(NormalTaskSubmitterTest, TestBacklogReport) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n \n   TaskSpecification task1 = BuildEmptyTaskSpec();\n \n@@ -1972,19 +2014,21 @@ TEST(NormalTaskSubmitterTest, TestWorkerLeaseTimeout) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                /*lease_timeout_ms=*/5,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      /*lease_timeout_ms=*/5,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task1 = BuildEmptyTaskSpec();\n   TaskSpecification task2 = BuildEmptyTaskSpec();\n   TaskSpecification task3 = BuildEmptyTaskSpec();\n@@ -2039,19 +2083,21 @@ TEST(NormalTaskSubmitterTest, TestKillExecutingTask) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task = BuildEmptyTaskSpec();\n \n   ASSERT_TRUE(submitter.SubmitTask(task).ok());\n@@ -2101,19 +2147,21 @@ TEST(NormalTaskSubmitterTest, TestKillPendingTask) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task = BuildEmptyTaskSpec();\n \n   ASSERT_TRUE(submitter.SubmitTask(task).ok());\n@@ -2146,19 +2194,21 @@ TEST(NormalTaskSubmitterTest, TestKillResolvingTask) {\n   auto task_finisher = std::make_unique<MockTaskFinisher>();\n   auto actor_creator = std::make_shared<MockActorCreator>();\n   auto lease_policy = std::make_unique<MockLeasePolicy>();\n-  NormalTaskSubmitter submitter(address,\n-                                raylet_client,\n-                                client_pool,\n-                                nullptr,\n-                                std::move(lease_policy),\n-                                store,\n-                                *task_finisher,\n-                                NodeID::Nil(),\n-                                WorkerType::WORKER,\n-                                kLongTimeout,\n-                                actor_creator,\n-                                JobID::Nil(),\n-                                kOneRateLimiter);\n+  NormalTaskSubmitter submitter(\n+      address,\n+      raylet_client,\n+      client_pool,\n+      nullptr,\n+      std::move(lease_policy),\n+      store,\n+      *task_finisher,\n+      NodeID::Nil(),\n+      WorkerType::WORKER,\n+      kLongTimeout,\n+      actor_creator,\n+      JobID::Nil(),\n+      kOneRateLimiter,\n+      [](const ObjectID &object_id) { return rpc::TensorTransport::OBJECT_STORE; });\n   TaskSpecification task = BuildEmptyTaskSpec();\n   ObjectID obj1 = ObjectID::FromRandom();\n   task.GetMutableMessage().add_args()->mutable_object_ref()->set_object_id(obj1.Binary());\n","problem_statement":"[core][gpu-objects] Performance regression caused by transferring object references for small objects\n### What happened + What you expected to happen\n\nThe GPU objects POC PR (https://github.com/ray-project/ray/pull/52938) causes a performance degradation because it passes ObjectRef for small objects.\n\n* Before GPU objects POC PR, `mutable_arg->clear_object_ref();` will be called. After GPU objects POC PR, the whole object ref will be transferred which slow down a very niche [test stage2 in test_many_tasks.py](https://github.com/ray-project/ray/blob/0e53e1ddc84662adbffbddcb7e3ff5d845747838/release/nightly_tests/stress_tests/test_many_tasks.py#L77). (See https://github.com/ray-project/ray/pull/53575 for more details)\n  * Before GPU objects POC PR: stage_2_time 168.2283821105957 seconds\n  * After GPU objects POC PR: stage_2_time 287.988343000412 seconds\n\n* Clear owner_address (https://github.com/ray-project/ray/pull/53590) to reduce the size of data transfer because we only need object id. \n  * `stage_2_time` = 198.5783519744873\n \nSolution is to clear object ref if the object ref is not a GPU object ref.\n\n### Versions / Dependencies\n\nnightly\n\n### Reproduction script\n\nTrigger release tests with `name:stress_test_many_tasks.aws`\n\n### Issue Severity\n\nNone\n","hints_text":"\n\n","all_hints_text":"\n\n","commit_urls":["https://github.com/ray-project/ray/commit/bb719e30bc0bb4b2e7713068d64b6807bd3304ae","https://github.com/ray-project/ray/commit/330f818770373821b646dfa7a3aec5e1c0290cbd","https://github.com/ray-project/ray/commit/f32148b2d59f83701a230477d4d0f6716bbf1b3e","https://github.com/ray-project/ray/commit/f2f599cdad1bf79d09866221ba65734d57b38f7d","https://github.com/ray-project/ray/commit/50bcd9d3d4c7a9d79407b963a161efa0e7e7a917","https://github.com/ray-project/ray/commit/72200ee5876ad9ea7ef7b3409138e1200e2aadbd","https://github.com/ray-project/ray/commit/dcbfabcafc141a94100603a3f1d868c2c2ec3850"],"created_at":"2025-06-10T07:03:52Z","classification":"Efficiency"}
{"repo":"ray-project/ray","pull_number":54469,"instance_id":"ray-project__ray-54469","issue_numbers":[54448],"base_commit":"05d78b8c75ebebf96cf589aea12eb336de329d11","patch":"diff --git a/python/ray/data/_internal/planner/plan_udf_map_op.py b/python/ray/data/_internal/planner/plan_udf_map_op.py\nindex ff30f43bf76c..f6ccf861bd0e 100644\n--- a/python/ray/data/_internal/planner/plan_udf_map_op.py\n+++ b/python/ray/data/_internal/planner/plan_udf_map_op.py\n@@ -149,6 +149,7 @@ def plan_streaming_repartition_op(\n     transform_fn = BuildOutputBlocksMapTransformFn.for_blocks()\n     transform_fn.set_target_num_rows_per_block(op.target_num_rows_per_block)\n     map_transformer = MapTransformer([transform_fn])\n+    # Disable fusion for streaming repartition with the downstream op.\n     return MapOperator.create(\n         map_transformer,\n         input_physical_dag,\n@@ -157,6 +158,7 @@ def plan_streaming_repartition_op(\n         compute_strategy=compute,\n         ray_remote_args=op._ray_remote_args,\n         ray_remote_args_fn=op._ray_remote_args_fn,\n+        supports_fusion=False,\n     )\n \n \n","test_patch":"diff --git a/python/ray/data/tests/test_repartition_e2e.py b/python/ray/data/tests/test_repartition_e2e.py\nindex 5030060c7e18..481022738a01 100644\n--- a/python/ray/data/tests/test_repartition_e2e.py\n+++ b/python/ray/data/tests/test_repartition_e2e.py\n@@ -2,6 +2,8 @@\n import pytest\n \n import ray\n+from ray.data._internal.logical.optimizers import PhysicalOptimizer\n+from ray.data._internal.planner import create_planner\n from ray.data.block import BlockAccessor\n from ray.data.context import DataContext, ShuffleStrategy\n from ray.data.tests.conftest import *  # noqa\n@@ -222,6 +224,88 @@ def test_repartition_empty_datasets(ray_start_regular_shared_2_cpus, shuffle):\n         assert metadata.size_bytes == 0\n \n \n+def test_streaming_repartition_write_no_operator_fusion(\n+    ray_start_regular_shared_2_cpus, tmp_path, disable_fallback_to_object_extension\n+):\n+    \"\"\"Test that write with streaming repartition produces exact partitions\n+    without operator fusion.\n+    This test verifies:\n+    1. StreamingRepartition and Write operators are not fused\n+    2. Exact partition structure is maintained\n+    3. Skewed data is properly distributed across partitions\n+    \"\"\"\n+\n+    # Configure shuffle strategy\n+    ctx = DataContext.get_current()\n+    ctx._shuffle_strategy = ShuffleStrategy.HASH_SHUFFLE\n+\n+    num_rows = 100\n+    partition_col = \"skewed_key\"\n+\n+    # Create sample data with skewed partitioning\n+    # 1 occurs for every 5th row (20 rows), 0 for others (80 rows)\n+    table = [{\"id\": n, partition_col: 1 if n % 5 == 0 else 0} for n in range(num_rows)]\n+    ds = ray.data.from_items(table)\n+\n+    # Repartition by key to simulate shuffle\n+    ds = ds.repartition(num_blocks=2, keys=[partition_col])\n+\n+    # Further rebalance to meet target row size\n+    ds = ds.repartition(target_num_rows_per_block=20)\n+\n+    # Verify non-fusion of map_batches with repartition\n+    ds = ds.map_batches(lambda x: x)\n+    planner = create_planner()\n+    physical_plan = planner.plan(ds._logical_plan)\n+    physical_plan = PhysicalOptimizer().optimize(physical_plan)\n+    physical_op = physical_plan.dag\n+    assert physical_op.name == \"MapBatches(<lambda>)\"\n+    assert len(physical_op.input_dependencies) == 1\n+\n+    # Verify that StreamingRepartition physical operator has supports_fusion=False\n+    up_physical_op = physical_op.input_dependencies[0]\n+    assert up_physical_op.name == \"StreamingRepartition\"\n+    assert not getattr(\n+        up_physical_op, \"_supports_fusion\", True\n+    ), \"StreamingRepartition should have supports_fusion=False\"\n+\n+    # Write output to local Parquet files partitioned by key\n+    ds.write_parquet(path=tmp_path, partition_cols=[partition_col])\n+\n+    # Verify exact number of files created based on target_num_rows_per_block=20\n+    # 80 rows with key=0 should create 4 files (80/20=4)\n+    # 20 rows with key=1 should create 1 file (20/20=1)\n+    # Total should be 5 files\n+    # Note: Partition column values are returned as strings when reading partitioned Parquet\n+    partition_0_files = list((tmp_path / f\"{partition_col}=0\").glob(\"*.parquet\"))\n+    partition_1_files = list((tmp_path / f\"{partition_col}=1\").glob(\"*.parquet\"))\n+\n+    assert (\n+        len(partition_0_files) == 4\n+    ), f\"Expected 4 files in partition 0, got {len(partition_0_files)}\"\n+    assert (\n+        len(partition_1_files) == 1\n+    ), f\"Expected 1 file in partition 1, got {len(partition_1_files)}\"\n+\n+    total_files = len(partition_0_files) + len(partition_1_files)\n+    assert (\n+        total_files == 5\n+    ), f\"Expected exactly 5 parquet files total, got {total_files}\"\n+\n+    # Verify data can be read back correctly with expected row count\n+    ds_read_back = ray.data.read_parquet(str(tmp_path))\n+    assert (\n+        ds_read_back.count() == num_rows\n+    ), f\"Expected {num_rows} total rows when reading back\"\n+\n+    # Verify per-partition row counts\n+    partition_0_ds = ray.data.read_parquet(str(tmp_path / f\"{partition_col}=0\"))\n+    partition_1_ds = ray.data.read_parquet(str(tmp_path / f\"{partition_col}=1\"))\n+\n+    assert partition_0_ds.count() == 80, \"Expected 80 rows in partition 0\"\n+    assert partition_1_ds.count() == 20, \"Expected 20 rows in partition 1\"\n+\n+\n if __name__ == \"__main__\":\n     import sys\n \n","problem_statement":"[data][bug] repartition(target_num_rows_per_block) should not be fused with downstream op\n\n","hints_text":"\n\n","all_hints_text":"\n\n","commit_urls":["https://github.com/ray-project/ray/commit/017235bfb003eaedd5813661ae1b48c731da3f60","https://github.com/ray-project/ray/commit/d3eaa9158f00b0086c092860a3078cb0fbc93ce4"],"created_at":"2025-07-09T16:56:57Z","classification":"Efficiency"}
{"repo": "agno-agi/agno", "pull_number": 3877, "instance_id": "agno-agi__agno-3877", "issue_numbers": [3753], "base_commit": "ea09870c9c5a69302e8293e9a339821ce63dd4e0", "patch": "diff --git a/cookbook/evals/performance/team_response_with_memory_and_reasoning.py b/cookbook/evals/performance/team_response_with_memory_and_reasoning.py\nindex 7ac2b09ca4..30509f96e0 100644\n--- a/cookbook/evals/performance/team_response_with_memory_and_reasoning.py\n+++ b/cookbook/evals/performance/team_response_with_memory_and_reasoning.py\n@@ -6,10 +6,11 @@\n from agno.eval.performance import PerformanceEval\n from agno.memory.v2.db.postgres import PostgresMemoryDb\n from agno.memory.v2.memory import Memory\n-from agno.models.openai import OpenAIChat\n+from agno.models.openai import OpenAIResponses\n from agno.storage.postgres import PostgresStorage\n from agno.team.team import Team\n from agno.tools.reasoning import ReasoningTools\n+from agno.utils.pprint import apprint_run_response\n \n users = [\n     \"abel@example.com\",\n@@ -44,9 +45,9 @@\n team_storage = PostgresStorage(\n     table_name=\"team_sessions\", db_url=db_url, auto_upgrade_schema=True\n )\n+agent_memory = Memory(db=PostgresMemoryDb(table_name=\"agent_memory\", db_url=db_url))\n \n team_memory = Memory(db=PostgresMemoryDb(table_name=\"team_memory\", db_url=db_url))\n-agent_memory = Memory(db=PostgresMemoryDb(table_name=\"agent_memory\", db_url=db_url))\n \n \n def get_weather(city: str) -> str:\n@@ -65,11 +66,11 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 75\u00b0F, Low 58\u00b0F with afternoon thunderstorms\",\n                 \"tomorrow\": \"High 78\u00b0F, Low 62\u00b0F, mostly sunny\",\n-                \"weekend\": \"High 82\u00b0F, Low 65\u00b0F, clear skies\"\n+                \"weekend\": \"High 82\u00b0F, Low 65\u00b0F, clear skies\",\n             },\n             \"air_quality\": \"Good (AQI: 45)\",\n             \"pollen_count\": \"Moderate\",\n-            \"marine_conditions\": \"Waves 2-3 feet, water temperature 68\u00b0F\"\n+            \"marine_conditions\": \"Waves 2-3 feet, water temperature 68\u00b0F\",\n         },\n         \"Los Angeles\": {\n             \"current\": \"Sunny and clear\",\n@@ -84,11 +85,11 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 88\u00b0F, Low 65\u00b0F, sunny throughout\",\n                 \"tomorrow\": \"High 86\u00b0F, Low 63\u00b0F, morning fog then sunny\",\n-                \"weekend\": \"High 90\u00b0F, Low 68\u00b0F, clear and warm\"\n+                \"weekend\": \"High 90\u00b0F, Low 68\u00b0F, clear and warm\",\n             },\n             \"air_quality\": \"Moderate (AQI: 78)\",\n             \"pollen_count\": \"High\",\n-            \"marine_conditions\": \"Waves 3-4 feet, water temperature 72\u00b0F\"\n+            \"marine_conditions\": \"Waves 3-4 feet, water temperature 72\u00b0F\",\n         },\n         \"Chicago\": {\n             \"current\": \"Overcast with light drizzle\",\n@@ -103,11 +104,11 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 62\u00b0F, Low 48\u00b0F, rain likely\",\n                 \"tomorrow\": \"High 65\u00b0F, Low 52\u00b0F, partly cloudy\",\n-                \"weekend\": \"High 70\u00b0F, Low 55\u00b0F, sunny intervals\"\n+                \"weekend\": \"High 70\u00b0F, Low 55\u00b0F, sunny intervals\",\n             },\n             \"air_quality\": \"Good (AQI: 52)\",\n             \"pollen_count\": \"Low\",\n-            \"marine_conditions\": \"Waves 4-6 feet, water temperature 55\u00b0F\"\n+            \"marine_conditions\": \"Waves 4-6 feet, water temperature 55\u00b0F\",\n         },\n         \"Houston\": {\n             \"current\": \"Hot and humid with scattered clouds\",\n@@ -122,11 +123,11 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 94\u00b0F, Low 76\u00b0F, chance of afternoon storms\",\n                 \"tomorrow\": \"High 96\u00b0F, Low 78\u00b0F, hot and humid\",\n-                \"weekend\": \"High 98\u00b0F, Low 80\u00b0F, isolated thunderstorms\"\n+                \"weekend\": \"High 98\u00b0F, Low 80\u00b0F, isolated thunderstorms\",\n             },\n             \"air_quality\": \"Moderate (AQI: 85)\",\n             \"pollen_count\": \"Very High\",\n-            \"marine_conditions\": \"Waves 1-2 feet, water temperature 82\u00b0F\"\n+            \"marine_conditions\": \"Waves 1-2 feet, water temperature 82\u00b0F\",\n         },\n         \"Miami\": {\n             \"current\": \"Partly cloudy with high humidity\",\n@@ -141,11 +142,11 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 90\u00b0F, Low 78\u00b0F, afternoon showers likely\",\n                 \"tomorrow\": \"High 89\u00b0F, Low 77\u00b0F, partly sunny\",\n-                \"weekend\": \"High 92\u00b0F, Low 79\u00b0F, scattered thunderstorms\"\n+                \"weekend\": \"High 92\u00b0F, Low 79\u00b0F, scattered thunderstorms\",\n             },\n             \"air_quality\": \"Good (AQI: 48)\",\n             \"pollen_count\": \"Moderate\",\n-            \"marine_conditions\": \"Waves 2-3 feet, water temperature 85\u00b0F\"\n+            \"marine_conditions\": \"Waves 2-3 feet, water temperature 85\u00b0F\",\n         },\n         \"San Francisco\": {\n             \"current\": \"Foggy and cool\",\n@@ -160,11 +161,11 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 65\u00b0F, Low 55\u00b0F, fog clearing by afternoon\",\n                 \"tomorrow\": \"High 68\u00b0F, Low 58\u00b0F, partly cloudy\",\n-                \"weekend\": \"High 72\u00b0F, Low 60\u00b0F, sunny and mild\"\n+                \"weekend\": \"High 72\u00b0F, Low 60\u00b0F, sunny and mild\",\n             },\n             \"air_quality\": \"Good (AQI: 42)\",\n             \"pollen_count\": \"Low\",\n-            \"marine_conditions\": \"Waves 5-7 feet, water temperature 58\u00b0F\"\n+            \"marine_conditions\": \"Waves 5-7 feet, water temperature 58\u00b0F\",\n         },\n         \"Seattle\": {\n             \"current\": \"Light rain with overcast skies\",\n@@ -179,11 +180,11 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 58\u00b0F, Low 48\u00b0F, rain throughout the day\",\n                 \"tomorrow\": \"High 62\u00b0F, Low 50\u00b0F, showers likely\",\n-                \"weekend\": \"High 65\u00b0F, Low 52\u00b0F, partly cloudy\"\n+                \"weekend\": \"High 65\u00b0F, Low 52\u00b0F, partly cloudy\",\n             },\n             \"air_quality\": \"Good (AQI: 38)\",\n             \"pollen_count\": \"Low\",\n-            \"marine_conditions\": \"Waves 3-5 feet, water temperature 52\u00b0F\"\n+            \"marine_conditions\": \"Waves 3-5 feet, water temperature 52\u00b0F\",\n         },\n         \"Boston\": {\n             \"current\": \"Clear and crisp\",\n@@ -198,11 +199,11 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 72\u00b0F, Low 58\u00b0F, sunny and pleasant\",\n                 \"tomorrow\": \"High 75\u00b0F, Low 62\u00b0F, mostly sunny\",\n-                \"weekend\": \"High 78\u00b0F, Low 65\u00b0F, clear skies\"\n+                \"weekend\": \"High 78\u00b0F, Low 65\u00b0F, clear skies\",\n             },\n             \"air_quality\": \"Good (AQI: 55)\",\n             \"pollen_count\": \"Moderate\",\n-            \"marine_conditions\": \"Waves 2-4 feet, water temperature 62\u00b0F\"\n+            \"marine_conditions\": \"Waves 2-4 feet, water temperature 62\u00b0F\",\n         },\n         \"Washington D.C.\": {\n             \"current\": \"Partly sunny with mild temperatures\",\n@@ -217,11 +218,11 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 78\u00b0F, Low 62\u00b0F, partly cloudy\",\n                 \"tomorrow\": \"High 80\u00b0F, Low 65\u00b0F, sunny intervals\",\n-                \"weekend\": \"High 82\u00b0F, Low 68\u00b0F, clear and warm\"\n+                \"weekend\": \"High 82\u00b0F, Low 68\u00b0F, clear and warm\",\n             },\n             \"air_quality\": \"Moderate (AQI: 72)\",\n             \"pollen_count\": \"High\",\n-            \"marine_conditions\": \"Waves 1-2 feet, water temperature 70\u00b0F\"\n+            \"marine_conditions\": \"Waves 1-2 feet, water temperature 70\u00b0F\",\n         },\n         \"Atlanta\": {\n             \"current\": \"Warm and humid with scattered clouds\",\n@@ -236,11 +237,11 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 85\u00b0F, Low 68\u00b0F, chance of afternoon storms\",\n                 \"tomorrow\": \"High 87\u00b0F, Low 70\u00b0F, hot and humid\",\n-                \"weekend\": \"High 90\u00b0F, Low 72\u00b0F, isolated thunderstorms\"\n+                \"weekend\": \"High 90\u00b0F, Low 72\u00b0F, isolated thunderstorms\",\n             },\n             \"air_quality\": \"Moderate (AQI: 68)\",\n             \"pollen_count\": \"Very High\",\n-            \"marine_conditions\": \"Waves 1-2 feet, water temperature 75\u00b0F\"\n+            \"marine_conditions\": \"Waves 1-2 feet, water temperature 75\u00b0F\",\n         },\n         \"Denver\": {\n             \"current\": \"Sunny and dry\",\n@@ -255,11 +256,11 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 82\u00b0F, Low 55\u00b0F, sunny and clear\",\n                 \"tomorrow\": \"High 85\u00b0F, Low 58\u00b0F, mostly sunny\",\n-                \"weekend\": \"High 88\u00b0F, Low 62\u00b0F, clear skies\"\n+                \"weekend\": \"High 88\u00b0F, Low 62\u00b0F, clear skies\",\n             },\n             \"air_quality\": \"Good (AQI: 45)\",\n             \"pollen_count\": \"Moderate\",\n-            \"marine_conditions\": \"N/A - Landlocked location\"\n+            \"marine_conditions\": \"N/A - Landlocked location\",\n         },\n         \"Las Vegas\": {\n             \"current\": \"Hot and dry with clear skies\",\n@@ -274,52 +275,52 @@ def get_weather(city: str) -> str:\n             \"forecast\": {\n                 \"today\": \"High 98\u00b0F, Low 75\u00b0F, sunny and hot\",\n                 \"tomorrow\": \"High 100\u00b0F, Low 78\u00b0F, clear and very hot\",\n-                \"weekend\": \"High 102\u00b0F, Low 80\u00b0F, extreme heat\"\n+                \"weekend\": \"High 102\u00b0F, Low 80\u00b0F, extreme heat\",\n             },\n             \"air_quality\": \"Moderate (AQI: 82)\",\n             \"pollen_count\": \"Low\",\n-            \"marine_conditions\": \"N/A - Desert location\"\n-        }\n+            \"marine_conditions\": \"N/A - Desert location\",\n+        },\n     }\n-    \n+\n     if city not in weather_conditions:\n         return f\"Weather data for {city} is not available in our database.\"\n-    \n+\n     weather = weather_conditions[city]\n-    \n+\n     return f\"\"\"\n # Comprehensive Weather Report for {city}\n \n ## Current Conditions\n-- **Temperature**: {weather['temperature']}\n-- **Conditions**: {weather['current']}\n-- **Humidity**: {weather['humidity']}\n-- **Wind**: {weather['wind']}\n-- **Visibility**: {weather['visibility']}\n-- **Pressure**: {weather['pressure']}\n-- **UV Index**: {weather['uv_index']}\n+- **Temperature**: {weather[\"temperature\"]}\n+- **Conditions**: {weather[\"current\"]}\n+- **Humidity**: {weather[\"humidity\"]}\n+- **Wind**: {weather[\"wind\"]}\n+- **Visibility**: {weather[\"visibility\"]}\n+- **Pressure**: {weather[\"pressure\"]}\n+- **UV Index**: {weather[\"uv_index\"]}\n \n ## Daily Schedule\n-- **Sunrise**: {weather['sunrise']}\n-- **Sunset**: {weather['sunset']}\n+- **Sunrise**: {weather[\"sunrise\"]}\n+- **Sunset**: {weather[\"sunset\"]}\n \n ## Extended Forecast\n-- **Today**: {weather['forecast']['today']}\n-- **Tomorrow**: {weather['forecast']['tomorrow']}\n-- **Weekend**: {weather['forecast']['weekend']}\n+- **Today**: {weather[\"forecast\"][\"today\"]}\n+- **Tomorrow**: {weather[\"forecast\"][\"tomorrow\"]}\n+- **Weekend**: {weather[\"forecast\"][\"weekend\"]}\n \n ## Environmental Conditions\n-- **Air Quality**: {weather['air_quality']}\n-- **Pollen Count**: {weather['pollen_count']}\n-- **Marine Conditions**: {weather['marine_conditions']}\n+- **Air Quality**: {weather[\"air_quality\"]}\n+- **Pollen Count**: {weather[\"pollen_count\"]}\n+- **Marine Conditions**: {weather[\"marine_conditions\"]}\n \n ## Weather Advisory\n-Based on current conditions, visitors to {city} should be prepared for {weather['current'].lower()}. The UV index of {weather['uv_index']} indicates {'sun protection is essential' if 'High' in weather['uv_index'] or 'Very High' in weather['uv_index'] or 'Extreme' in weather['uv_index'] else 'moderate sun protection recommended'}. {'High humidity may make temperatures feel warmer than actual readings.' if int(weather['humidity'].replace('%', '')) > 70 else 'Comfortable humidity levels are expected.'}\n+Based on current conditions, visitors to {city} should be prepared for {weather[\"current\"].lower()}. The UV index of {weather[\"uv_index\"]} indicates {\"sun protection is essential\" if \"High\" in weather[\"uv_index\"] or \"Very High\" in weather[\"uv_index\"] or \"Extreme\" in weather[\"uv_index\"] else \"moderate sun protection recommended\"}. {\"High humidity may make temperatures feel warmer than actual readings.\" if int(weather[\"humidity\"].replace(\"%\", \"\")) > 70 else \"Comfortable humidity levels are expected.\"}\n \n ## Travel Recommendations\n-- **Best Time for Outdoor Activities**: {'Early morning or late afternoon to avoid peak heat' if int(weather['temperature'].split('\u00b0')[0]) > 85 else 'Any time during daylight hours'}\n-- **Clothing Suggestions**: {'Light, breathable clothing recommended' if int(weather['temperature'].split('\u00b0')[0]) > 80 else 'Comfortable clothing suitable for current temperatures'}\n-- **Hydration**: {'Stay well-hydrated due to high temperatures' if int(weather['temperature'].split('\u00b0')[0]) > 85 else 'Normal hydration levels recommended'}\n+- **Best Time for Outdoor Activities**: {\"Early morning or late afternoon to avoid peak heat\" if int(weather[\"temperature\"].split(\"\u00b0\")[0]) > 85 else \"Any time during daylight hours\"}\n+- **Clothing Suggestions**: {\"Light, breathable clothing recommended\" if int(weather[\"temperature\"].split(\"\u00b0\")[0]) > 80 else \"Comfortable clothing suitable for current temperatures\"}\n+- **Hydration**: {\"Stay well-hydrated due to high temperatures\" if int(weather[\"temperature\"].split(\"\u00b0\")[0]) > 85 else \"Normal hydration levels recommended\"}\n \n This comprehensive weather report provides all the essential information needed for planning activities and ensuring comfort during your visit to {city}.\n \"\"\"\n@@ -337,7 +338,7 @@ def get_activities(city: str) -> str:\n                 \"Prospect Park nature trails\",\n                 \"Governors Island weekend visits\",\n                 \"Riverside Park cycling paths\",\n-                \"Bryant Park seasonal activities\"\n+                \"Bryant Park seasonal activities\",\n             ],\n             \"cultural\": [\n                 \"Metropolitan Museum of Art comprehensive tours\",\n@@ -347,7 +348,7 @@ def get_activities(city: str) -> str:\n                 \"Lincoln Center performing arts\",\n                 \"Guggenheim Museum architecture and art\",\n                 \"Whitney Museum of American Art\",\n-                \"Brooklyn Museum cultural exhibits\"\n+                \"Brooklyn Museum cultural exhibits\",\n             ],\n             \"entertainment\": [\n                 \"Times Square nightlife and entertainment\",\n@@ -357,7 +358,7 @@ def get_activities(city: str) -> str:\n                 \"Madison Square Garden events\",\n                 \"Radio City Music Hall shows\",\n                 \"Carnegie Hall classical concerts\",\n-                \"Comedy Cellar stand-up comedy\"\n+                \"Comedy Cellar stand-up comedy\",\n             ],\n             \"shopping\": [\n                 \"Fifth Avenue luxury shopping district\",\n@@ -367,7 +368,7 @@ def get_activities(city: str) -> str:\n                 \"Union Square Greenmarket farmers market\",\n                 \"Century 21 discount designer shopping\",\n                 \"Bergdorf Goodman luxury department store\",\n-                \"ABC Carpet & Home home decor\"\n+                \"ABC Carpet & Home home decor\",\n             ],\n             \"dining\": [\n                 \"Katz's Delicatessen pastrami sandwiches\",\n@@ -377,8 +378,8 @@ def get_activities(city: str) -> str:\n                 \"Gramercy Tavern farm-to-table dining\",\n                 \"Le Bernardin seafood excellence\",\n                 \"Momofuku Noodle Bar Asian fusion\",\n-                \"Magnolia Bakery cupcakes and desserts\"\n-            ]\n+                \"Magnolia Bakery cupcakes and desserts\",\n+            ],\n         },\n         \"Los Angeles\": {\n             \"outdoor\": [\n@@ -389,7 +390,7 @@ def get_activities(city: str) -> str:\n                 \"Malibu beach surfing and swimming\",\n                 \"Echo Park Lake paddle boating\",\n                 \"Griffith Park horseback riding\",\n-                \"Topanga State Park wilderness trails\"\n+                \"Topanga State Park wilderness trails\",\n             ],\n             \"cultural\": [\n                 \"Getty Center art museum and gardens\",\n@@ -399,7 +400,7 @@ def get_activities(city: str) -> str:\n                 \"Warner Bros. Studio Tour\",\n                 \"Natural History Museum dinosaur exhibits\",\n                 \"California Science Center space shuttle\",\n-                \"The Broad contemporary art museum\"\n+                \"The Broad contemporary art museum\",\n             ],\n             \"entertainment\": [\n                 \"Disneyland Resort theme park adventure\",\n@@ -409,7 +410,7 @@ def get_activities(city: str) -> str:\n                 \"Comedy Store stand-up comedy\",\n                 \"Roxy Theatre live music venue\",\n                 \"Greek Theatre outdoor amphitheater\",\n-                \"TCL Chinese Theatre movie premieres\"\n+                \"TCL Chinese Theatre movie premieres\",\n             ],\n             \"shopping\": [\n                 \"Rodeo Drive luxury shopping experience\",\n@@ -419,7 +420,7 @@ def get_activities(city: str) -> str:\n                 \"Abbot Kinney Boulevard unique shops\",\n                 \"Third Street Promenade Santa Monica\",\n                 \"Glendale Galleria shopping complex\",\n-                \"Fashion District wholesale shopping\"\n+                \"Fashion District wholesale shopping\",\n             ],\n             \"dining\": [\n                 \"In-N-Out Burger classic California burgers\",\n@@ -429,8 +430,8 @@ def get_activities(city: str) -> str:\n                 \"Nobu Los Angeles celebrity sushi spot\",\n                 \"Gjelina Venice Beach farm-to-table\",\n                 \"Animal Restaurant innovative cuisine\",\n-                \"Bottega Louie Italian pastries and dining\"\n-            ]\n+                \"Bottega Louie Italian pastries and dining\",\n+            ],\n         },\n         \"Chicago\": {\n             \"outdoor\": [\n@@ -441,7 +442,7 @@ def get_activities(city: str) -> str:\n                 \"Lake Michigan beach activities\",\n                 \"Chicago Riverwalk scenic strolls\",\n                 \"Maggie Daley Park family activities\",\n-                \"606 elevated trail cycling\"\n+                \"606 elevated trail cycling\",\n             ],\n             \"cultural\": [\n                 \"Art Institute of Chicago world-class art\",\n@@ -451,7 +452,7 @@ def get_activities(city: str) -> str:\n                 \"Museum of Science and Industry hands-on exhibits\",\n                 \"Chicago History Museum local heritage\",\n                 \"National Museum of Mexican Art\",\n-                \"DuSable Museum of African American History\"\n+                \"DuSable Museum of African American History\",\n             ],\n             \"entertainment\": [\n                 \"Willis Tower Skydeck observation deck\",\n@@ -461,7 +462,7 @@ def get_activities(city: str) -> str:\n                 \"Chicago Theatre historic venue\",\n                 \"Arie Crown Theater performances\",\n                 \"House of Blues live music\",\n-                \"Blue Man Group theatrical experience\"\n+                \"Blue Man Group theatrical experience\",\n             ],\n             \"shopping\": [\n                 \"Magnificent Mile luxury shopping district\",\n@@ -471,7 +472,7 @@ def get_activities(city: str) -> str:\n                 \"Michigan Avenue shopping experience\",\n                 \"Wicker Park trendy shops\",\n                 \"Andersonville unique stores\",\n-                \"Lincoln Square German heritage shopping\"\n+                \"Lincoln Square German heritage shopping\",\n             ],\n             \"dining\": [\n                 \"Giordano's deep dish pizza\",\n@@ -481,8 +482,8 @@ def get_activities(city: str) -> str:\n                 \"Billy Goat Tavern historic bar\",\n                 \"Girl & the Goat innovative cuisine\",\n                 \"Alinea molecular gastronomy\",\n-                \"Au Cheval gourmet burgers\"\n-            ]\n+                \"Au Cheval gourmet burgers\",\n+            ],\n         },\n         \"Houston\": {\n             \"outdoor\": [\n@@ -493,7 +494,7 @@ def get_activities(city: str) -> str:\n                 \"Houston Arboretum nature education\",\n                 \"Rice University campus walking tours\",\n                 \"Sam Houston Park historic buildings\",\n-                \"Eleanor Tinsley Park bayou views\"\n+                \"Eleanor Tinsley Park bayou views\",\n             ],\n             \"cultural\": [\n                 \"Museum of Fine Arts Houston\",\n@@ -503,7 +504,7 @@ def get_activities(city: str) -> str:\n                 \"Holocaust Museum Houston\",\n                 \"Buffalo Soldiers National Museum\",\n                 \"Asia Society Texas Center\",\n-                \"Houston Center for Photography\"\n+                \"Houston Center for Photography\",\n             ],\n             \"entertainment\": [\n                 \"Space Center Houston NASA exhibits\",\n@@ -513,7 +514,7 @@ def get_activities(city: str) -> str:\n                 \"Minute Maid Park Astros baseball\",\n                 \"NRG Stadium Texans football\",\n                 \"House of Blues Houston live music\",\n-                \"Jones Hall performing arts\"\n+                \"Jones Hall performing arts\",\n             ],\n             \"shopping\": [\n                 \"Galleria Mall luxury shopping complex\",\n@@ -523,7 +524,7 @@ def get_activities(city: str) -> str:\n                 \"Katy Mills outlet shopping\",\n                 \"Houston Premium Outlets\",\n                 \"Baybrook Mall suburban shopping\",\n-                \"Willowbrook Mall northwest shopping\"\n+                \"Willowbrook Mall northwest shopping\",\n             ],\n             \"dining\": [\n                 \"Pappas Bros. Steakhouse premium steaks\",\n@@ -533,8 +534,8 @@ def get_activities(city: str) -> str:\n                 \"Hugo's upscale Mexican cuisine\",\n                 \"Uchi Houston sushi excellence\",\n                 \"Underbelly Houston Southern cuisine\",\n-                \"Truth BBQ award-winning barbecue\"\n-            ]\n+                \"Truth BBQ award-winning barbecue\",\n+            ],\n         },\n         \"Miami\": {\n             \"outdoor\": [\n@@ -545,7 +546,7 @@ def get_activities(city: str) -> str:\n                 \"Fairchild Tropical Botanic Garden\",\n                 \"Matheson Hammock Park natural areas\",\n                 \"Bill Baggs Cape Florida State Park\",\n-                \"Oleta River State Park kayaking\"\n+                \"Oleta River State Park kayaking\",\n             ],\n             \"cultural\": [\n                 \"P\u00e9rez Art Museum Miami contemporary art\",\n@@ -555,7 +556,7 @@ def get_activities(city: str) -> str:\n                 \"Jewish Museum of Florida\",\n                 \"Coral Gables Museum architecture\",\n                 \"Lowe Art Museum University of Miami\",\n-                \"Bass Museum of Art contemporary\"\n+                \"Bass Museum of Art contemporary\",\n             ],\n             \"entertainment\": [\n                 \"Wynwood Walls street art district\",\n@@ -565,7 +566,7 @@ def get_activities(city: str) -> str:\n                 \"Hard Rock Stadium Dolphins football\",\n                 \"Marlins Park baseball games\",\n                 \"Fillmore Miami Beach live music\",\n-                \"Adrienne Arsht Center performing arts\"\n+                \"Adrienne Arsht Center performing arts\",\n             ],\n             \"shopping\": [\n                 \"Lincoln Road Mall outdoor shopping\",\n@@ -575,7 +576,7 @@ def get_activities(city: str) -> str:\n                 \"Dolphin Mall outlet shopping\",\n                 \"Sawgrass Mills outlet complex\",\n                 \"Merrick Park Coral Gables shopping\",\n-                \"CocoWalk Coconut Grove retail\"\n+                \"CocoWalk Coconut Grove retail\",\n             ],\n             \"dining\": [\n                 \"Joe's Stone Crab Miami Beach institution\",\n@@ -585,8 +586,8 @@ def get_activities(city: str) -> str:\n                 \"Zuma Miami Japanese izakaya\",\n                 \"Nobu Miami Beach celebrity dining\",\n                 \"Prime 112 steakhouse excellence\",\n-                \"La Sandwicherie French sandwiches\"\n-            ]\n+                \"La Sandwicherie French sandwiches\",\n+            ],\n         },\n         \"San Francisco\": {\n             \"outdoor\": [\n@@ -597,7 +598,7 @@ def get_activities(city: str) -> str:\n                 \"Lands End coastal hiking trails\",\n                 \"Twin Peaks panoramic city views\",\n                 \"Crissy Field beach and recreation\",\n-                \"Angel Island State Park hiking\"\n+                \"Angel Island State Park hiking\",\n             ],\n             \"cultural\": [\n                 \"de Young Museum fine arts\",\n@@ -607,7 +608,7 @@ def get_activities(city: str) -> str:\n                 \"Asian Art Museum comprehensive collection\",\n                 \"Legion of Honor European art\",\n                 \"Contemporary Jewish Museum\",\n-                \"Walt Disney Family Museum\"\n+                \"Walt Disney Family Museum\",\n             ],\n             \"entertainment\": [\n                 \"Pier 39 sea lions and attractions\",\n@@ -617,7 +618,7 @@ def get_activities(city: str) -> str:\n                 \"Fillmore Auditorium live music\",\n                 \"Warfield Theatre historic venue\",\n                 \"Great American Music Hall\",\n-                \"SFJAZZ Center jazz performances\"\n+                \"SFJAZZ Center jazz performances\",\n             ],\n             \"shopping\": [\n                 \"Union Square luxury shopping district\",\n@@ -627,7 +628,7 @@ def get_activities(city: str) -> str:\n                 \"Chestnut Street boutique shopping\",\n                 \"Fillmore Street upscale retail\",\n                 \"Valencia Street Mission District\",\n-                \"Grant Avenue Chinatown shopping\"\n+                \"Grant Avenue Chinatown shopping\",\n             ],\n             \"dining\": [\n                 \"Tartine Bakery artisanal breads\",\n@@ -637,8 +638,8 @@ def get_activities(city: str) -> str:\n                 \"Gary Danko fine dining experience\",\n                 \"State Bird Provisions innovative\",\n                 \"Tadich Grill historic seafood\",\n-                \"Boudin Bakery sourdough bread\"\n-            ]\n+                \"Boudin Bakery sourdough bread\",\n+            ],\n         },\n         \"Seattle\": {\n             \"outdoor\": [\n@@ -649,7 +650,7 @@ def get_activities(city: str) -> str:\n                 \"Green Lake Park walking and cycling\",\n                 \"Kerry Park panoramic city views\",\n                 \"Alki Beach West Seattle activities\",\n-                \"Washington Park Arboretum gardens\"\n+                \"Washington Park Arboretum gardens\",\n             ],\n             \"cultural\": [\n                 \"Seattle Art Museum comprehensive collection\",\n@@ -659,7 +660,7 @@ def get_activities(city: str) -> str:\n                 \"Wing Luke Museum Asian American history\",\n                 \"Museum of Flight aviation history\",\n                 \"Frye Art Museum free admission\",\n-                \"Nordic Heritage Museum Scandinavian\"\n+                \"Nordic Heritage Museum Scandinavian\",\n             ],\n             \"entertainment\": [\n                 \"CenturyLink Field Seahawks football\",\n@@ -669,7 +670,7 @@ def get_activities(city: str) -> str:\n                 \"Showbox at the Market live music\",\n                 \"Neptune Theatre University District\",\n                 \"Moore Theatre downtown venue\",\n-                \"Crocodile Caf\u00e9 intimate music venue\"\n+                \"Crocodile Caf\u00e9 intimate music venue\",\n             ],\n             \"shopping\": [\n                 \"Pike Place Market local crafts and food\",\n@@ -679,7 +680,7 @@ def get_activities(city: str) -> str:\n                 \"Northgate Mall north Seattle\",\n                 \"Southcenter Mall south Seattle\",\n                 \"Alderwood Mall north suburbs\",\n-                \"Redmond Town Center eastside retail\"\n+                \"Redmond Town Center eastside retail\",\n             ],\n             \"dining\": [\n                 \"Pike Place Chowder award-winning chowder\",\n@@ -689,8 +690,8 @@ def get_activities(city: str) -> str:\n                 \"The Walrus and the Carpenter oysters\",\n                 \"Paseo Caribbean sandwiches\",\n                 \"Molly Moon's Homemade Ice Cream\",\n-                \"Top Pot Doughnuts hand-forged doughnuts\"\n-            ]\n+                \"Top Pot Doughnuts hand-forged doughnuts\",\n+            ],\n         },\n         \"Boston\": {\n             \"outdoor\": [\n@@ -701,7 +702,7 @@ def get_activities(city: str) -> str:\n                 \"Emerald Necklace park system\",\n                 \"Castle Island South Boston waterfront\",\n                 \"Arnold Arboretum Harvard University\",\n-                \"Jamaica Pond walking and boating\"\n+                \"Jamaica Pond walking and boating\",\n             ],\n             \"cultural\": [\n                 \"Museum of Fine Arts Boston\",\n@@ -711,7 +712,7 @@ def get_activities(city: str) -> str:\n                 \"Museum of Science interactive exhibits\",\n                 \"New England Aquarium marine life\",\n                 \"Institute of Contemporary Art\",\n-                \"Boston Children's Museum family\"\n+                \"Boston Children's Museum family\",\n             ],\n             \"entertainment\": [\n                 \"Fenway Park Red Sox baseball\",\n@@ -721,7 +722,7 @@ def get_activities(city: str) -> str:\n                 \"House of Blues Boston live music\",\n                 \"Paradise Rock Club intimate venue\",\n                 \"Orpheum Theatre historic venue\",\n-                \"Wang Theatre performing arts\"\n+                \"Wang Theatre performing arts\",\n             ],\n             \"shopping\": [\n                 \"Faneuil Hall Marketplace historic shopping\",\n@@ -731,7 +732,7 @@ def get_activities(city: str) -> str:\n                 \"Assembly Row outlet shopping\",\n                 \"Natick Mall suburban shopping\",\n                 \"Burlington Mall north suburbs\",\n-                \"South Shore Plaza south suburbs\"\n+                \"South Shore Plaza south suburbs\",\n             ],\n             \"dining\": [\n                 \"Legal Sea Foods fresh seafood\",\n@@ -741,8 +742,8 @@ def get_activities(city: str) -> str:\n                 \"Giacomo's Ristorante Italian cuisine\",\n                 \"Flour Bakery + Caf\u00e9 artisanal pastries\",\n                 \"Santarpio's Pizza East Boston\",\n-                \"Kelly's Roast Beef North Shore\"\n-            ]\n+                \"Kelly's Roast Beef North Shore\",\n+            ],\n         },\n         \"Washington D.C.\": {\n             \"outdoor\": [\n@@ -753,7 +754,7 @@ def get_activities(city: str) -> str:\n                 \"East Potomac Park golf and recreation\",\n                 \"Kenilworth Aquatic Gardens\",\n                 \"C&O Canal National Historical Park\",\n-                \"Great Falls Park Virginia side\"\n+                \"Great Falls Park Virginia side\",\n             ],\n             \"cultural\": [\n                 \"Smithsonian Institution museums\",\n@@ -763,7 +764,7 @@ def get_activities(city: str) -> str:\n                 \"Library of Congress largest library\",\n                 \"National Archives historical documents\",\n                 \"International Spy Museum\",\n-                \"Newseum journalism museum\"\n+                \"Newseum journalism museum\",\n             ],\n             \"entertainment\": [\n                 \"Capitol Building guided tours\",\n@@ -773,7 +774,7 @@ def get_activities(city: str) -> str:\n                 \"National Theatre historic venue\",\n                 \"9:30 Club live music venue\",\n                 \"The Anthem waterfront venue\",\n-                \"Wolf Trap performing arts center\"\n+                \"Wolf Trap performing arts center\",\n             ],\n             \"shopping\": [\n                 \"Georgetown historic shopping district\",\n@@ -783,7 +784,7 @@ def get_activities(city: str) -> str:\n                 \"Potomac Mills outlet shopping\",\n                 \"National Harbor waterfront retail\",\n                 \"CityCenterDC luxury shopping\",\n-                \"Eastern Market Capitol Hill\"\n+                \"Eastern Market Capitol Hill\",\n             ],\n             \"dining\": [\n                 \"Ben's Chili Bowl Washington institution\",\n@@ -793,8 +794,8 @@ def get_activities(city: str) -> str:\n                 \"Le Diplomate French bistro\",\n                 \"Rose's Luxury innovative American\",\n                 \"Komi Mediterranean fine dining\",\n-                \"Toki Underground ramen noodles\"\n-            ]\n+                \"Toki Underground ramen noodles\",\n+            ],\n         },\n         \"Atlanta\": {\n             \"outdoor\": [\n@@ -805,7 +806,7 @@ def get_activities(city: str) -> str:\n                 \"Atlanta Botanical Garden\",\n                 \"Grant Park Zoo Atlanta\",\n                 \"Centennial Olympic Park\",\n-                \"Chastain Park amphitheater and trails\"\n+                \"Chastain Park amphitheater and trails\",\n             ],\n             \"cultural\": [\n                 \"High Museum of Art\",\n@@ -815,7 +816,7 @@ def get_activities(city: str) -> str:\n                 \"Center for Civil and Human Rights\",\n                 \"Atlanta Contemporary Art Center\",\n                 \"Michael C. Carlos Museum Emory\",\n-                \"Spelman College Museum of Fine Art\"\n+                \"Spelman College Museum of Fine Art\",\n             ],\n             \"entertainment\": [\n                 \"World of Coca-Cola museum\",\n@@ -825,7 +826,7 @@ def get_activities(city: str) -> str:\n                 \"State Farm Arena Hawks basketball\",\n                 \"Fox Theatre historic venue\",\n                 \"Tabernacle live music venue\",\n-                \"Variety Playhouse intimate concerts\"\n+                \"Variety Playhouse intimate concerts\",\n             ],\n             \"shopping\": [\n                 \"Lenox Square luxury shopping\",\n@@ -835,7 +836,7 @@ def get_activities(city: str) -> str:\n                 \"Krog Street Market food and retail\",\n                 \"Buckhead Village boutique shopping\",\n                 \"Virginia-Highland unique stores\",\n-                \"Little Five Points alternative shopping\"\n+                \"Little Five Points alternative shopping\",\n             ],\n             \"dining\": [\n                 \"The Varsity classic drive-in\",\n@@ -845,8 +846,8 @@ def get_activities(city: str) -> str:\n                 \"Miller Union farm-to-table\",\n                 \"Staplehouse innovative American\",\n                 \"Gunshow creative Southern cuisine\",\n-                \"Atlanta Fish Market fresh seafood\"\n-            ]\n+                \"Atlanta Fish Market fresh seafood\",\n+            ],\n         },\n         \"Denver\": {\n             \"outdoor\": [\n@@ -857,7 +858,7 @@ def get_activities(city: str) -> str:\n                 \"Washington Park recreation\",\n                 \"Cherry Creek State Park\",\n                 \"Mount Evans Scenic Byway\",\n-                \"Garden of the Gods Colorado Springs\"\n+                \"Garden of the Gods Colorado Springs\",\n             ],\n             \"cultural\": [\n                 \"Denver Art Museum\",\n@@ -867,7 +868,7 @@ def get_activities(city: str) -> str:\n                 \"History Colorado Center\",\n                 \"Black American West Museum\",\n                 \"Mizel Museum Jewish culture\",\n-                \"Kirkland Museum of Fine & Decorative Art\"\n+                \"Kirkland Museum of Fine & Decorative Art\",\n             ],\n             \"entertainment\": [\n                 \"Coors Field Rockies baseball\",\n@@ -877,7 +878,7 @@ def get_activities(city: str) -> str:\n                 \"Ogden Theatre live music\",\n                 \"Bluebird Theatre intimate venue\",\n                 \"Fillmore Auditorium historic venue\",\n-                \"Paramount Theatre performing arts\"\n+                \"Paramount Theatre performing arts\",\n             ],\n             \"shopping\": [\n                 \"Cherry Creek Shopping Center\",\n@@ -887,7 +888,7 @@ def get_activities(city: str) -> str:\n                 \"Flatiron Crossing Broomfield\",\n                 \"Aspen Grove Littleton\",\n                 \"Belmar Lakewood shopping\",\n-                \"Southlands Aurora retail\"\n+                \"Southlands Aurora retail\",\n             ],\n             \"dining\": [\n                 \"Casa Bonita Mexican restaurant\",\n@@ -897,8 +898,8 @@ def get_activities(city: str) -> str:\n                 \"Root Down farm-to-table\",\n                 \"Fruition Restaurant fine dining\",\n                 \"Acorn at The Source market hall\",\n-                \"Work & Class contemporary American\"\n-            ]\n+                \"Work & Class contemporary American\",\n+            ],\n         },\n         \"Las Vegas\": {\n             \"outdoor\": [\n@@ -909,7 +910,7 @@ def get_activities(city: str) -> str:\n                 \"Springs Preserve desert gardens\",\n                 \"Floyd Lamb Park at Tule Springs\",\n                 \"Clark County Wetlands Park\",\n-                \"Sloan Canyon National Conservation Area\"\n+                \"Sloan Canyon National Conservation Area\",\n             ],\n             \"cultural\": [\n                 \"The Mob Museum organized crime history\",\n@@ -919,7 +920,7 @@ def get_activities(city: str) -> str:\n                 \"Nevada State Museum\",\n                 \"Old Las Vegas Mormon Fort\",\n                 \"Atomic Testing Museum\",\n-                \"Las Vegas Art Museum\"\n+                \"Las Vegas Art Museum\",\n             ],\n             \"entertainment\": [\n                 \"The Strip casino and resort hopping\",\n@@ -929,7 +930,7 @@ def get_activities(city: str) -> str:\n                 \"High Roller observation wheel\",\n                 \"Stratosphere Tower thrill rides\",\n                 \"Downtown Container Park\",\n-                \"Area 15 immersive experiences\"\n+                \"Area 15 immersive experiences\",\n             ],\n             \"shopping\": [\n                 \"Fashion Show Mall\",\n@@ -939,7 +940,7 @@ def get_activities(city: str) -> str:\n                 \"Town Square Las Vegas\",\n                 \"Las Vegas Premium Outlets North\",\n                 \"Las Vegas Premium Outlets South\",\n-                \"Meadows Mall local shopping\"\n+                \"Meadows Mall local shopping\",\n             ],\n             \"dining\": [\n                 \"In-N-Out Burger California burgers\",\n@@ -949,33 +950,33 @@ def get_activities(city: str) -> str:\n                 \"Gordon Ramsay Hell's Kitchen\",\n                 \"Jo\u00ebl Robuchon fine dining\",\n                 \"Raku Japanese izakaya\",\n-                \"Echo & Rig Butcher and Steakhouse\"\n-            ]\n-        }\n+                \"Echo & Rig Butcher and Steakhouse\",\n+            ],\n+        },\n     }\n-    \n+\n     if city not in city_activities:\n         return f\"Activity information for {city} is not available in our database.\"\n-    \n+\n     activities = city_activities[city]\n-    \n+\n     return f\"\"\"\n # Comprehensive Activity Guide for {city}\n \n ## Outdoor Adventures & Recreation\n-{chr(10).join([f\"- {activity}\" for activity in activities['outdoor']])}\n+{chr(10).join([f\"- {activity}\" for activity in activities[\"outdoor\"]])}\n \n ## Cultural Experiences & Museums\n-{chr(10).join([f\"- {activity}\" for activity in activities['cultural']])}\n+{chr(10).join([f\"- {activity}\" for activity in activities[\"cultural\"]])}\n \n ## Entertainment & Nightlife\n-{chr(10).join([f\"- {activity}\" for activity in activities['entertainment']])}\n+{chr(10).join([f\"- {activity}\" for activity in activities[\"entertainment\"]])}\n \n ## Shopping Destinations\n-{chr(10).join([f\"- {activity}\" for activity in activities['shopping']])}\n+{chr(10).join([f\"- {activity}\" for activity in activities[\"shopping\"]])}\n \n ## Dining & Culinary Experiences\n-{chr(10).join([f\"- {activity}\" for activity in activities['dining']])}\n+{chr(10).join([f\"- {activity}\" for activity in activities[\"dining\"]])}\n \n ## Activity Recommendations by Interest\n \n@@ -1003,7 +1004,7 @@ def get_activities(city: str) -> str:\n \n weather_agent = Agent(\n     agent_id=\"weather_agent\",\n-    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    model=OpenAIResponses(id=\"gpt-4o\"),\n     description=\"You are a helpful assistant that can answer questions about the weather.\",\n     instructions=\"Be concise, reply with one sentence.\",\n     tools=[ReasoningTools(add_instructions=True), get_weather],\n@@ -1018,7 +1019,7 @@ def get_activities(city: str) -> str:\n \n activities_agent = Agent(\n     agent_id=\"activities_agent\",\n-    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    model=OpenAIResponses(id=\"gpt-4o\"),\n     description=\"You are a helpful assistant that can answer questions about activities in a city.\",\n     instructions=\"Be concise, reply with one sentence.\",\n     tools=[ReasoningTools(add_instructions=True), get_activities],\n@@ -1032,7 +1033,7 @@ def get_activities(city: str) -> str:\n )\n \n team = Team(\n-    model=OpenAIChat(id=\"gpt-4o-mini\"),\n+    model=OpenAIResponses(id=\"gpt-4o\"),\n     members=[weather_agent, activities_agent],\n     tools=[ReasoningTools(add_instructions=True)],\n     instructions=\"Be concise, reply with one sentence.\",\n@@ -1047,54 +1048,69 @@ def get_activities(city: str) -> str:\n     num_history_runs=1,\n     stream=True,\n     stream_intermediate_steps=True,\n+    cache_session=False,\n )\n \n \n-async def run_team():\n-    async def run_team_for_user(user: str):\n-        random_city = random.choice(cities)\n-        response_iterator = await team.arun(\n-            message=f\"I love {random_city}!\",\n-            user_id=user,\n-            session_id=f\"session_{user}\",\n-        )\n-        async for response in response_iterator:\n+async def run_team_for_user(user: str, print_responses: bool = False):\n+    # Make four requests to the team, to build up history\n+    random_city = random.choice(cities)\n+    session_id = f\"session_{user}_{uuid.uuid4()}\"\n+    response_iterator = await team.arun(\n+        message=f\"I love {random_city}!\",\n+        user_id=user,\n+        session_id=session_id,\n+    )\n+    if print_responses:\n+        await apprint_run_response(response_iterator)\n+    else:\n+        async for _ in response_iterator:\n             pass\n-        \n-        response_iterator = await team.arun(\n-            message=f\"Create a report on the activities and weather in {random_city}.\",\n-            user_id=user,\n-            session_id=f\"session_{user}\",\n-        )\n-        async for response in response_iterator:\n+\n+    response_iterator = await team.arun(\n+        message=f\"Create a report on the activities and weather in {random_city}.\",\n+        user_id=user,\n+        session_id=session_id,\n+    )\n+    if print_responses:\n+        await apprint_run_response(response_iterator)\n+    else:\n+        async for _ in response_iterator:\n             pass\n-        \n-        response_iterator = await team.arun(\n-            message=f\"What else can you tell me about {random_city}?\",\n-            user_id=user,\n-            session_id=f\"session_{user}\",\n-        )\n-        async for response in response_iterator:\n+\n+    response_iterator = await team.arun(\n+        message=f\"What else can you tell me about {random_city}?\",\n+        user_id=user,\n+        session_id=session_id,\n+    )\n+    if print_responses:\n+        await apprint_run_response(response_iterator)\n+    else:\n+        async for _ in response_iterator:\n             pass\n-        \n-        response_iterator = await team.arun(\n-            message=f\"What other cities are similar to {random_city}?\",\n-            user_id=user,\n-            session_id=f\"session_{user}\",\n-        )\n-        async for response in response_iterator:\n+\n+    response_iterator = await team.arun(\n+        message=f\"What other cities are similar to {random_city}?\",\n+        user_id=user,\n+        session_id=session_id,\n+    )\n+    if print_responses:\n+        await apprint_run_response(response_iterator)\n+    else:\n+        async for _ in response_iterator:\n             pass\n-        \n+\n+\n+async def run_team():\n     tasks = []\n \n     # Run all 5 users concurrently\n-    # for user in users:\n-    #     tasks.append(run_team_for_user(user))\n+    for user in users:\n+        tasks.append(run_team_for_user(user))\n \n-    # await asyncio.gather(*tasks)\n-    await run_team_for_user(\"user_1\")\n+    await asyncio.gather(*tasks)\n \n-    print(\"Team memory runs:\", len(team.memory.runs))\n+    print(\"Team memory runs:\", sum(len(runs) for runs in team.memory.runs.values()))\n     print(\"Team memory memories:\", len(team.memory.memories))\n \n     return \"Successfully ran team\"\n@@ -1103,7 +1119,7 @@ async def run_team_for_user(user: str):\n team_response_with_memory_impact = PerformanceEval(\n     name=\"Team Memory Impact\",\n     func=run_team,\n-    num_iterations=1,\n+    num_iterations=5,\n     warmup_runs=0,\n     measure_runtime=False,\n     debug_mode=True,\ndiff --git a/cookbook/evals/performance/team_response_with_memory_multi_user.py b/cookbook/evals/performance/team_response_with_memory_multi_user.py\nindex 4d95e5801d..cef80dcca1 100644\n--- a/cookbook/evals/performance/team_response_with_memory_multi_user.py\n+++ b/cookbook/evals/performance/team_response_with_memory_multi_user.py\n@@ -121,7 +121,7 @@ async def run_team_for_user(user: str):\n \n     await asyncio.gather(*tasks)\n \n-    print(\"Team memory runs:\", len(team.memory.runs))\n+    print(\"Team memory runs:\", sum(len(runs) for runs in team.memory.runs.values()))\n     print(\"Team memory memories:\", len(team.memory.memories))\n \n     return \"Successfully ran team\"\ndiff --git a/libs/agno/agno/agent/agent.py b/libs/agno/agno/agent/agent.py\nindex 59813438b0..9af67024d5 100644\n--- a/libs/agno/agno/agent/agent.py\n+++ b/libs/agno/agno/agent/agent.py\n@@ -119,6 +119,8 @@ class Agent:\n     session_state: Optional[Dict[str, Any]] = None\n     search_previous_sessions_history: Optional[bool] = False\n     num_history_sessions: Optional[int] = None\n+    # If True, cache the session in memory\n+    cache_session: bool = True\n \n     # --- Agent Context ---\n     # Context available for tools and prompt functions\n@@ -351,6 +353,7 @@ def __init__(\n         session_state: Optional[Dict[str, Any]] = None,\n         search_previous_sessions_history: Optional[bool] = False,\n         num_history_sessions: Optional[int] = None,\n+        cache_session: bool = True,\n         context: Optional[Dict[str, Any]] = None,\n         add_context: bool = False,\n         resolve_context: bool = True,\n@@ -441,6 +444,8 @@ def __init__(\n         self.search_previous_sessions_history = search_previous_sessions_history\n         self.num_history_sessions = num_history_sessions\n \n+        self.cache_session = cache_session\n+\n         self.context = context\n         self.add_context = add_context\n         self.resolve_context = resolve_context\n@@ -754,9 +759,6 @@ def _initialize_session(\n \n         self._initialize_session_state(user_id=user_id, session_id=session_id)\n \n-        # Read existing session from storage\n-        self.read_from_storage(session_id=session_id)\n-\n         return session_id, user_id\n \n     def _run(\n@@ -996,15 +998,17 @@ def run(\n         **kwargs: Any,\n     ) -> Union[RunResponse, Iterator[RunResponseEvent]]:\n         \"\"\"Run the Agent and return the response.\"\"\"\n-\n         session_id, user_id = self._initialize_session(\n             session_id=session_id, user_id=user_id, session_state=session_state\n         )\n-\n-        log_debug(f\"Session ID: {session_id}\", center=True)\n-\n+        \n         # Initialize the Agent\n         self.initialize_agent()\n+        \n+        # Read existing session from storage\n+        self.read_from_storage(session_id=session_id)\n+\n+        log_debug(f\"Session ID: {session_id}\", center=True)\n \n         # Initialize Knowledge Filters\n         effective_filters = knowledge_filters\n@@ -1373,14 +1377,18 @@ async def arun(\n     ) -> Any:\n         \"\"\"Async Run the Agent and return the response.\"\"\"\n \n+\n         session_id, user_id = self._initialize_session(\n             session_id=session_id, user_id=user_id, session_state=session_state\n         )\n \n         log_debug(f\"Session ID: {session_id}\", center=True)\n-\n+        \n         # Initialize the Agent\n         self.initialize_agent()\n+        \n+        # Read existing session from storage\n+        self.read_from_storage(session_id=session_id)\n \n         effective_filters = knowledge_filters\n         # When filters are passed manually\n@@ -1606,6 +1614,9 @@ def continue_run(\n             retries: The number of retries to continue the run for.\n             knowledge_filters: The knowledge filters to use for the run.\n         \"\"\"\n+        # Initialize the Agent\n+        self.initialize_agent()\n+\n         if session_id is not None:\n             self.reset_run_state()\n             # Reset session state if a session_id is provided. Session name and session state will be loaded from storage.\n@@ -1614,9 +1625,6 @@ def continue_run(\n             if self.session_id is not None and session_id != self.session_id:\n                 self.session_state = None\n \n-        # Initialize the Agent\n-        self.initialize_agent()\n-\n         # Initialize Session\n         # Use the default user_id and session_id when necessary\n         user_id = user_id if user_id is not None else self.user_id\n@@ -1996,6 +2004,9 @@ async def acontinue_run(\n             retries: The number of retries to continue the run for.\n             knowledge_filters: The knowledge filters to use for the run.\n         \"\"\"\n+        # Initialize the Agent\n+        self.initialize_agent()\n+\n         if session_id is not None:\n             self.reset_run_state()\n             # Reset session state if a session_id is provided. Session name and session state will be loaded from storage.\n@@ -2004,9 +2015,6 @@ async def acontinue_run(\n             if self.session_id is not None and session_id != self.session_id:\n                 self.session_state = None\n \n-        # Initialize the Agent\n-        self.initialize_agent()\n-\n         # Initialize Session\n         # Use the default user_id and session_id when necessary\n         user_id = user_id if user_id is not None else self.user_id\n@@ -4056,6 +4064,7 @@ def load_agent_session(self, session: AgentSession):\n                 # Convert dict to Memory\n             elif isinstance(self.memory, dict):\n                 memory_dict = self.memory\n+\n                 memory_dict.pop(\"runs\")\n                 self.memory = Memory(**memory_dict)\n             else:\n@@ -4111,6 +4120,7 @@ def load_agent_session(self, session: AgentSession):\n                         self.memory.runs[session.session_id] = []\n                         for run in session.memory[\"runs\"]:\n                             run_session_id = run[\"session_id\"]\n+\n                             if \"team_id\" in run:\n                                 self.memory.runs[run_session_id].append(TeamRunResponse.from_dict(run))\n                             else:\n@@ -4224,6 +4234,11 @@ def write_to_storage(\n                 AgentSession,\n                 self.storage.upsert(session=self.get_agent_session(session_id=session_id, user_id=user_id)),\n             )\n+\n+        if not self.cache_session:\n+            if self.memory is not None and self.memory.runs is not None and session_id in self.memory.runs:\n+                self.memory.runs.pop(session_id)  # type: ignore\n+\n         return self.agent_session\n \n     def add_introduction(self, introduction: str) -> None:\ndiff --git a/libs/agno/agno/team/team.py b/libs/agno/agno/team/team.py\nindex 414b7bb8a5..146581fc63 100644\n--- a/libs/agno/agno/team/team.py\n+++ b/libs/agno/agno/team/team.py\n@@ -130,6 +130,8 @@ class Team:\n     session_name: Optional[str] = None\n     # Session state (stored in the database to persist across runs)\n     session_state: Optional[Dict[str, Any]] = None\n+    # If True, cache the session in memory\n+    cache_session: bool = True\n \n     # Team session state (shared between team leaders and team members)\n     team_session_state: Optional[Dict[str, Any]] = None\n@@ -311,6 +313,7 @@ def __init__(\n         team_session_state: Optional[Dict[str, Any]] = None,\n         workflow_session_state: Optional[Dict[str, Any]] = None,\n         add_state_in_messages: bool = False,\n+        cache_session: bool = True,\n         description: Optional[str] = None,\n         instructions: Optional[Union[str, List[str], Callable]] = None,\n         expected_output: Optional[str] = None,\n@@ -390,6 +393,8 @@ def __init__(\n         self.workflow_session_state = workflow_session_state\n         self.add_state_in_messages = add_state_in_messages\n \n+        self.cache_session = cache_session\n+\n         self.description = description\n         self.instructions = instructions\n         self.expected_output = expected_output\n@@ -729,9 +734,6 @@ def _initialize_session(\n \n         self._initialize_session_state(user_id=user_id, session_id=session_id)\n \n-        # Read existing session from storage\n-        self.read_from_storage(session_id=session_id)\n-\n         return session_id, user_id\n \n     @overload\n@@ -798,6 +800,9 @@ def run(\n \n         # Initialize Team\n         self.initialize_team(session_id=session_id)\n+        \n+        # Read existing session from storage\n+        self.read_from_storage(session_id=session_id)\n \n         # Initialize Knowledge Filters\n         effective_filters = knowledge_filters\n@@ -1195,8 +1200,12 @@ async def arun(\n             session_id=session_id, user_id=user_id, session_state=session_state\n         )\n         log_debug(f\"Session ID: {session_id}\", center=True)\n-\n+        \n+        # Initialize Team\n         self.initialize_team(session_id=session_id)\n+        \n+        # Read existing session from storage\n+        self.read_from_storage(session_id=session_id)\n \n         effective_filters = knowledge_filters\n \n@@ -1629,7 +1638,7 @@ def _add_run_to_memory(\n             self.memory.add_team_run(team_run)  # type: ignore\n \n         elif isinstance(self.memory, Memory):\n-            # Add AgentRun to memory\n+            # Add run to memory\n             self.memory.add_run(session_id=session_id, run=run_response)\n \n     def _update_memory(\n@@ -6847,6 +6856,11 @@ def write_to_storage(self, session_id: str, user_id: Optional[str] = None) -> Op\n             self.team_session = cast(\n                 TeamSession, self.storage.upsert(session=self._get_team_session(session_id=session_id, user_id=user_id))\n             )\n+\n+        # Remove session from memory\n+        if not self.cache_session:\n+            if self.memory is not None and self.memory.runs is not None and session_id in self.memory.runs:\n+                self.memory.runs.pop(session_id)  # type: ignore\n         return self.team_session\n \n     def rename_session(self, session_name: str, session_id: Optional[str] = None) -> None:\n@@ -7024,6 +7038,7 @@ def load_team_session(self, session: TeamSession):\n                         self.memory.runs[session.session_id] = []\n                         for run in session.memory[\"runs\"]:\n                             run_session_id = run[\"session_id\"]\n+\n                             if \"team_id\" in run:\n                                 self.memory.runs[run_session_id].append(TeamRunResponse.from_dict(run))\n                             else:\n@@ -7738,6 +7753,7 @@ def _get_team_session(self, session_id: str, user_id: Optional[str] = None) -> T\n                     run_responses = self.memory.runs.get(session_id)\n                     if run_responses is not None:\n                         memory_dict[\"runs\"] = [rr.to_dict() for rr in run_responses]\n+\n         return TeamSession(\n             session_id=session_id,\n             team_id=self.team_id,\ndiff --git a/libs/agno/agno/utils/pprint.py b/libs/agno/agno/utils/pprint.py\nindex 8f52f1667b..3e7e97d71a 100644\n--- a/libs/agno/agno/utils/pprint.py\n+++ b/libs/agno/agno/utils/pprint.py\n@@ -163,6 +163,8 @@ async def apprint_run_response(\n                         except Exception as e:\n                             logger.warning(f\"Failed to convert response to Markdown: {e}\")\n                     else:\n+                        if isinstance(streaming_response_content, JSON):\n+                            streaming_response_content = streaming_response_content.text + \"\\n\"  # type: ignore\n                         streaming_response_content += resp.content  # type: ignore\n \n                 formatted_response = Markdown(streaming_response_content) if markdown else streaming_response_content  # type: ignore\n", "test_patch": "diff --git a/libs/agno/tests/integration/agent/test_agent_with_storage_and_memory.py b/libs/agno/tests/integration/agent/test_agent_with_storage_and_memory.py\nindex 58a5d3a693..7f014813e3 100644\n--- a/libs/agno/tests/integration/agent/test_agent_with_storage_and_memory.py\n+++ b/libs/agno/tests/integration/agent/test_agent_with_storage_and_memory.py\n@@ -43,6 +43,40 @@ def test_agent_runs_in_memory(chat_agent):\n     assert len(chat_agent.agent_session.memory[\"runs\"]) == 1\n \n \n+def test_agent_runs_in_memory_default_memory(agent_storage):\n+    # No memory is set on the agent\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        storage=agent_storage,\n+        add_history_to_messages=True,\n+        num_history_runs=3,\n+    )\n+    session_id = \"test_session\"\n+    agent.run(\"What is the capital of France?\", session_id=session_id)\n+    assert len(agent.memory.runs[session_id]) == 1\n+    assert len(agent.agent_session.memory[\"runs\"]) == 1\n+\n+    agent.run(\"What is the capital of Germany?\", session_id=session_id)\n+    assert len(agent.memory.runs[session_id]) == 2\n+    assert len(agent.agent_session.memory[\"runs\"]) == 2\n+\n+    agent.run(\"What have I asked so far?\", session_id=session_id)\n+    assert len(agent.memory.runs[session_id]) == 3\n+    assert len(agent.agent_session.memory[\"runs\"]) == 3\n+\n+    # New agent instance\n+    agent = Agent(\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        storage=agent_storage,\n+        add_history_to_messages=True,\n+        num_history_runs=3,\n+    )\n+\n+    agent.run(\"What have I asked so far?\", session_id=session_id)\n+    assert len(agent.memory.runs[session_id]) == 4\n+    assert len(agent.agent_session.memory[\"runs\"]) == 4\n+\n+\n def test_agent_runs_in_memory_legacy(chat_agent):\n     chat_agent.memory = AgentMemory()\n     session_id = \"test_session\"\ndiff --git a/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py b/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py\nindex 63eaedceef..53880fbbad 100644\n--- a/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py\n+++ b/libs/agno/tests/integration/teams/test_team_with_storage_and_memory.py\n@@ -349,3 +349,61 @@ async def test_correct_sessions_in_db(route_team, team_storage, agent_storage):\n     assert agent_sessions[0].memory[\"runs\"][0][\"session_id\"] == session_id\n     assert agent_sessions[0].memory[\"runs\"][1][\"session_id\"] == session_id\n     assert agent_sessions[0].memory[\"runs\"][0][\"run_id\"] != agent_sessions[0].memory[\"runs\"][1][\"run_id\"]\n+\n+\n+@pytest.mark.asyncio\n+async def test_cache_session_behavior(team_storage, memory):\n+    \"\"\"Test that cache_session=False removes sessions from memory after storage.\"\"\"\n+    user_id = \"test_user@example.com\"\n+    session_id = \"test_session_123\"\n+\n+    # Create a team with cache_session=False\n+    team = Team(\n+        name=\"Cache Test Team\",\n+        mode=\"coordinate\",\n+        model=OpenAIChat(id=\"gpt-4o-mini\"),\n+        members=[],\n+        storage=team_storage,\n+        memory=memory,\n+        cache_session=False,  # This is the key setting we're testing\n+    )\n+\n+    # Clear memory for this specific test case\n+    memory.clear()\n+\n+    # Verify memory is empty initially\n+    assert memory.runs == {}, \"Memory should be empty initially\"\n+\n+    # Run a message\n+    response = await team.arun(\"Hello, how are you?\", user_id=user_id, session_id=session_id)\n+\n+    # Verify the response was successful\n+    assert response is not None\n+    assert response.content is not None\n+\n+    # Verify the session was stored in storage\n+    team_session = team_storage.read(session_id=session_id)\n+    assert team_session is not None, \"Session should be stored in database\"\n+    assert team_session.session_id == session_id\n+    assert team_session.user_id == user_id\n+\n+    # Verify that the session was removed from memory (cache_session=False)\n+    assert session_id not in memory.runs, f\"Session {session_id} should not be in memory when cache_session=False\"\n+    assert session_id not in team.memory.runs, f\"Session {session_id} should not be in memory when cache_session=False\"\n+    assert len(memory.runs) == 0, \"Memory should be empty after run when cache_session=False\"\n+\n+    # Run another message to verify the pattern continues\n+    response2 = await team.arun(\"What's the weather like?\", user_id=user_id, session_id=session_id)\n+\n+    # Verify the response was successful\n+    assert response2 is not None\n+    assert response2.content is not None\n+\n+    # Verify memory is still empty\n+    assert session_id not in memory.runs, f\"Session {session_id} should still not be in memory\"\n+    assert len(memory.runs) == 0, \"Memory should still be empty after second run\"\n+\n+    # Verify storage still has the session data\n+    team_session_updated = team_storage.read(session_id=session_id)\n+    assert team_session_updated is not None, \"Session should still be in storage\"\n+    assert len(team_session_updated.memory[\"runs\"]) == 2, \"Storage should have both runs\"\n", "problem_statement": "[Bug] memory leak\n### Description\n\nAgno 1.7+ with memory v2 introduces severe memory usage. \n\nWe have noticed that identical agent (a team of agents) uses significantly more memory than previous version.\n\nUpon deeper inspection it might be an issue with changes introduced to deepcopy\n\nAre you aware of those issues and is there any fix to overcome that?\n\n```\n  1. Potential Flawed Deepcopy Implementation:\n  # The problematic code in Memory v2:\n  def __deepcopy__(self, memo):\n      # PROBLEM: These create shared references\n      if k in {\"db\", \"memory_manager\", \"summary_manager\", \"team_context\"}:\n          setattr(copied_obj, k, v)  # \u274c SHARED - NEVER GETS GC'ed\n\n  2. Aggressive DB Refreshing:\n  # Memory v1: Optional refresh\n  def get_user_memories(self, refresh_from_db: bool = True):\n      if refresh_from_db:\n          self.refresh_from_db()\n\n  # Memory v2: ALWAYS refreshes\n  def get_user_memories(self, user_id: Optional[str] = None):\n      self.refresh_from_db(user_id=user_id)  # \u274c NO CHOICE\n```\n\n  Why Memory v2 potentially causes Memory Leaks\n\n  1. Each agent deepcopy retains shared database connections\n  2. Multiple memory managers per agent instance\n  3. Constant database refreshes without connection pooling\n  4. Complex object graphs that prevent garbage collection\n\nCould this be the case?\n\nThank you\n\n### Steps to Reproduce\n\n-\n\n### Agent Configuration (if applicable)\n\n_No response_\n\n### Expected Behavior\n\nnormal memory usage\n\n### Actual Behavior\n\nmemory goes beyond 2gb+ and never comes down\n\n### Screenshots or Logs (if applicable)\n\n_No response_\n\n### Environment\n\n```markdown\n-\n```\n\n### Possible Solutions (optional)\n\n_No response_\n\n### Additional Context\n\n_No response_\n", "hints_text": "@introvert thanks for raising, we are aware of it yes. \n\nLooking into it\n\n@introvert I am doing some tests and seeing a sensible growth in memory. Can you please provide your team config so I can get a more representative test going? I am interested in solving this for sure.\n@dirkbrnd we have a team of agents and a similar config\n\n## Team and Agent Configuration\n\n```python\n# Team Definition\nTeam(\n    name=\"Team of agents\",\n    team_id=\"team_id\",\n    mode=\"coordinate\",\n    model=OpenAIChat(id=AGENT_MODEL),\n    members=[\n        agent1,\n        agent2,\n        agent3,\n    ],\n    tools=[ReasoningTools(add_instructions=True)],\n    instructions=,\n    markdown=True,\n    enable_agentic_context=False,\n    add_datetime_to_instructions=True,\n    debug_mode=AGENT_DEBUG_MODE,\n    show_tool_calls=AGENT_SHOW_TOOL_CALLS,\n    stream_intermediate_steps=AGENT_SHOW_TOOL_CALLS,\n    memory=Memory(\n        db=PostgresMemoryDb(\n            table_name=\"leader_memory\",\n            db_url=DB_URL,\n            schema=DB_SCHEMA\n        )\n    ),\n    show_members_responses=True,\n    enable_user_memories=True,\n    enable_session_summaries=False,\n    storage=storage,\n    share_member_interactions=False,\n    add_history_to_messages=True,\n    read_team_history=False,\n    num_history_runs=1,\n)\n\n# Agent 1 Definition\nAgent(\n    name=\"Agent 1\",\n    role=\"Role of Agent 1\",\n    model=OpenAIChat(id=AGENT_MODEL),\n    tools=tools,\n    description=\"You are the agent N1\",\n    instructions=instructions,\n    show_tool_calls=AGENT_SHOW_TOOL_CALLS,\n    stream_intermediate_steps=AGENT_SHOW_TOOL_CALLS,\n    debug_mode=AGENT_DEBUG_MODE,\n    search_knowledge=True,\n    update_knowledge=False,\n    memory=Memory(\n        db=PostgresMemoryDb(\n            table_name=\"agent1_agent_memory\",\n            db_url=DB_URL,\n            schema=DB_SCHEMA\n        )\n    ),\n    enable_user_memories=False,\n    enable_session_summaries=False,\n    knowledge=knowledge_base,\n    add_history_to_messages=True,\n    num_history_responses=1,\n    read_tool_call_history=False,\n)\n\n# Default Storage\nstorage = PostgresStorage(\n    table_name=\"agent_sessions\",\n    db_url=DB_URL,\n    schema=DB_SCHEMA,\n    mode=\"team\",\n    auto_upgrade_schema=True\n)\n```\n\nWhat is concerning is that memory will grow exponentially, but only after the update to the new version (we were on 1.4 before) \n@introvert \nSo if you are using the same session ID for each run for a user (or not specifying, then it will by default be the same session), then the number of runs keep growing over time and these runs will continually be loaded from memory. \n\nI have a PR out that does various evaluations to check memory growth and I can see the memory growing, but not to extreme levels.  I also want to assure you that with our new major release we are working on, runs won't be in memory anymore and this issue will be mitigated. Also there won't therfore be \"DB refreshes\". \nMy task in the short-term is to see if there is anything we can do to alleviate this issue now.\n@dirkbrnd this happens with reasoning on new sessions. The sad situation is that we cannot upgrade and continue development or deployments with this bug, so we will need to find an alternative.\n\nDo you have any suggestions what we could do to mitigate this?\n\nWhen is the updated version planned?\nI just released 1.7.2, it would help if you could check whether you see the same issues. In the mean time I will check reasoning as well, that was not in my initial investigation.\nUnfortunately the new version doesn't mitigate the issue. Any ideas what else we could do?\nStill taking a look at this, sorry for the delay!\n@introvert I think I finally found the issue.\n\nThe number of sessions and runs will continue to grow over time with prolonged use of a team/agent. Especially with a team it goes quicker.\n\nI linked the fix PR above if you wanted to test\nI can't fix this by default, but it will be fixed when we go to 2.0.0 soon. \nFor now it is fixed if you set `cache_session=False` on your team level. See the PR for details.\n\n", "all_hints_text": "@introvert thanks for raising, we are aware of it yes. \n\nLooking into it\n\n@introvert I am doing some tests and seeing a sensible growth in memory. Can you please provide your team config so I can get a more representative test going? I am interested in solving this for sure.\n@dirkbrnd we have a team of agents and a similar config\n\n## Team and Agent Configuration\n\n```python\n# Team Definition\nTeam(\n    name=\"Team of agents\",\n    team_id=\"team_id\",\n    mode=\"coordinate\",\n    model=OpenAIChat(id=AGENT_MODEL),\n    members=[\n        agent1,\n        agent2,\n        agent3,\n    ],\n    tools=[ReasoningTools(add_instructions=True)],\n    instructions=,\n    markdown=True,\n    enable_agentic_context=False,\n    add_datetime_to_instructions=True,\n    debug_mode=AGENT_DEBUG_MODE,\n    show_tool_calls=AGENT_SHOW_TOOL_CALLS,\n    stream_intermediate_steps=AGENT_SHOW_TOOL_CALLS,\n    memory=Memory(\n        db=PostgresMemoryDb(\n            table_name=\"leader_memory\",\n            db_url=DB_URL,\n            schema=DB_SCHEMA\n        )\n    ),\n    show_members_responses=True,\n    enable_user_memories=True,\n    enable_session_summaries=False,\n    storage=storage,\n    share_member_interactions=False,\n    add_history_to_messages=True,\n    read_team_history=False,\n    num_history_runs=1,\n)\n\n# Agent 1 Definition\nAgent(\n    name=\"Agent 1\",\n    role=\"Role of Agent 1\",\n    model=OpenAIChat(id=AGENT_MODEL),\n    tools=tools,\n    description=\"You are the agent N1\",\n    instructions=instructions,\n    show_tool_calls=AGENT_SHOW_TOOL_CALLS,\n    stream_intermediate_steps=AGENT_SHOW_TOOL_CALLS,\n    debug_mode=AGENT_DEBUG_MODE,\n    search_knowledge=True,\n    update_knowledge=False,\n    memory=Memory(\n        db=PostgresMemoryDb(\n            table_name=\"agent1_agent_memory\",\n            db_url=DB_URL,\n            schema=DB_SCHEMA\n        )\n    ),\n    enable_user_memories=False,\n    enable_session_summaries=False,\n    knowledge=knowledge_base,\n    add_history_to_messages=True,\n    num_history_responses=1,\n    read_tool_call_history=False,\n)\n\n# Default Storage\nstorage = PostgresStorage(\n    table_name=\"agent_sessions\",\n    db_url=DB_URL,\n    schema=DB_SCHEMA,\n    mode=\"team\",\n    auto_upgrade_schema=True\n)\n```\n\nWhat is concerning is that memory will grow exponentially, but only after the update to the new version (we were on 1.4 before) \n@introvert \nSo if you are using the same session ID for each run for a user (or not specifying, then it will by default be the same session), then the number of runs keep growing over time and these runs will continually be loaded from memory. \n\nI have a PR out that does various evaluations to check memory growth and I can see the memory growing, but not to extreme levels.  I also want to assure you that with our new major release we are working on, runs won't be in memory anymore and this issue will be mitigated. Also there won't therfore be \"DB refreshes\". \nMy task in the short-term is to see if there is anything we can do to alleviate this issue now.\n@dirkbrnd this happens with reasoning on new sessions. The sad situation is that we cannot upgrade and continue development or deployments with this bug, so we will need to find an alternative.\n\nDo you have any suggestions what we could do to mitigate this?\n\nWhen is the updated version planned?\nI just released 1.7.2, it would help if you could check whether you see the same issues. In the mean time I will check reasoning as well, that was not in my initial investigation.\nUnfortunately the new version doesn't mitigate the issue. Any ideas what else we could do?\nStill taking a look at this, sorry for the delay!\n@introvert I think I finally found the issue.\n\nThe number of sessions and runs will continue to grow over time with prolonged use of a team/agent. Especially with a team it goes quicker.\n\nI linked the fix PR above if you wanted to test\nI can't fix this by default, but it will be fixed when we go to 2.0.0 soon. \nFor now it is fixed if you set `cache_session=False` on your team level. See the PR for details.\nPlease let me know if the proposed solution works for you.\n\nBasically if you do `Team(..., cache_session=False)` it should be resolved. This is also supported on agents.\nI'll close this for now. Let me know if you still experience issues.\n\n", "commit_urls": ["https://github.com/agno-agi/agno/commit/13e9532d7099ffba25f61d8f098d3105f3f496fe", "https://github.com/agno-agi/agno/commit/3b61a9bda4d73cc24e8d110f09becde519e9aaa9", "https://github.com/agno-agi/agno/commit/be78c4e30a5c400a6910a42a29e90d0576785c6c", "https://github.com/agno-agi/agno/commit/2eb456393a031bb58228e52da3e0c897ed6d5b9a", "https://github.com/agno-agi/agno/commit/52a569e2a92ff3f3c360817203db83d9005cc000", "https://github.com/agno-agi/agno/commit/52ff46cd8b2ac948577cf26ee230c384122bc735", "https://github.com/agno-agi/agno/commit/66bdfc13b6256e2670c02678f3ca635e824602d3", "https://github.com/agno-agi/agno/commit/bdd61204d9faa13d6afe53ff7bda4a46ff523dfa"], "created_at": "2025-07-17T12:28:33Z", "classification": "Efficiency"}
{"repo": "celery/celery", "pull_number": 9799, "instance_id": "celery__celery-9799", "issue_numbers": [8882], "base_commit": "6fca4fb03a29f394d787f11491c2287086626154", "patch": "diff --git a/celery/app/trace.py b/celery/app/trace.py\nindex 2e8cf8a3181..b6289709365 100644\n--- a/celery/app/trace.py\n+++ b/celery/app/trace.py\n@@ -190,6 +190,7 @@ def handle_retry(self, task, req, store_errors=True, **kwargs):\n         # the exception raised is the Retry semi-predicate,\n         # and it's exc' attribute is the original exception raised (if any).\n         type_, _, tb = sys.exc_info()\n+        einfo = None\n         try:\n             reason = self.retval\n             einfo = ExceptionInfo((type_, reason, tb))\n@@ -205,39 +206,56 @@ def handle_retry(self, task, req, store_errors=True, **kwargs):\n                 'name': get_task_name(req, task.name),\n                 'exc': str(reason),\n             })\n+            # MEMORY LEAK FIX: Clear traceback frames to prevent memory retention (Issue #8882)\n+            traceback_clear(einfo.exception)\n             return einfo\n         finally:\n-            del tb\n+            # MEMORY LEAK FIX: Clean up direct traceback reference to prevent\n+            # retention of frame objects and their local variables (Issue #8882)\n+            if tb is not None:\n+                del tb\n \n     def handle_failure(self, task, req, store_errors=True, call_errbacks=True):\n         \"\"\"Handle exception.\"\"\"\n         orig_exc = self.retval\n+        tb_ref = None\n \n-        exc = get_pickleable_exception(orig_exc)\n-        if exc.__traceback__ is None:\n-            # `get_pickleable_exception` may have created a new exception without\n-            # a traceback.\n-            _, _, exc.__traceback__ = sys.exc_info()\n-\n-        exc_type = get_pickleable_etype(type(orig_exc))\n-\n-        # make sure we only send pickleable exceptions back to parent.\n-        einfo = ExceptionInfo(exc_info=(exc_type, exc, exc.__traceback__))\n-\n-        task.backend.mark_as_failure(\n-            req.id, exc, einfo.traceback,\n-            request=req, store_result=store_errors,\n-            call_errbacks=call_errbacks,\n-        )\n-\n-        task.on_failure(exc, req.id, req.args, req.kwargs, einfo)\n-        signals.task_failure.send(sender=task, task_id=req.id,\n-                                  exception=exc, args=req.args,\n-                                  kwargs=req.kwargs,\n-                                  traceback=exc.__traceback__,\n-                                  einfo=einfo)\n-        self._log_error(task, req, einfo)\n-        return einfo\n+        try:\n+            exc = get_pickleable_exception(orig_exc)\n+            if exc.__traceback__ is None:\n+                # `get_pickleable_exception` may have created a new exception without\n+                # a traceback.\n+                _, _, tb_ref = sys.exc_info()\n+                exc.__traceback__ = tb_ref\n+\n+            exc_type = get_pickleable_etype(type(orig_exc))\n+\n+            # make sure we only send pickleable exceptions back to parent.\n+            einfo = ExceptionInfo(exc_info=(exc_type, exc, exc.__traceback__))\n+\n+            task.backend.mark_as_failure(\n+                req.id, exc, einfo.traceback,\n+                request=req, store_result=store_errors,\n+                call_errbacks=call_errbacks,\n+            )\n+\n+            task.on_failure(exc, req.id, req.args, req.kwargs, einfo)\n+            signals.task_failure.send(sender=task, task_id=req.id,\n+                                      exception=exc, args=req.args,\n+                                      kwargs=req.kwargs,\n+                                      traceback=exc.__traceback__,\n+                                      einfo=einfo)\n+            self._log_error(task, req, einfo)\n+            # MEMORY LEAK FIX: Clear traceback frames to prevent memory retention (Issue #8882)\n+            traceback_clear(exc)\n+            # Note: We return einfo, so we can't clean it up here\n+            # The calling function is responsible for cleanup\n+            return einfo\n+        finally:\n+            # MEMORY LEAK FIX: Clean up any direct traceback references we may have created\n+            # to prevent retention of frame objects and their local variables (Issue #8882)\n+            if tb_ref is not None:\n+                del tb_ref\n \n     def _log_error(self, task, req, einfo):\n         eobj = einfo.exception = get_pickled_exception(einfo.exception)\n@@ -270,6 +288,12 @@ def _log_error(self, task, req, einfo):\n \n \n def traceback_clear(exc=None):\n+    \"\"\"Clear traceback frames to prevent memory leaks.\n+\n+    MEMORY LEAK FIX: This function helps break reference cycles between\n+    traceback objects and frame objects that can prevent garbage collection.\n+    Clearing frames releases local variables that may be holding large objects.\n+    \"\"\"\n     # Cleared Tb, but einfo still has a reference to Traceback.\n     # exc cleans up the Traceback at the last moment that can be revealed.\n     tb = None\n@@ -283,8 +307,10 @@ def traceback_clear(exc=None):\n \n     while tb is not None:\n         try:\n+            # MEMORY LEAK FIX: tb.tb_frame.clear() clears ALL frame data including\n+            # local variables, which is more efficient than accessing f_locals separately.\n+            # Removed redundant tb.tb_frame.f_locals access that was creating unnecessary references.\n             tb.tb_frame.clear()\n-            tb.tb_frame.f_locals\n         except RuntimeError:\n             # Ignore the exception raised if the frame is still executing.\n             pass\n@@ -456,18 +482,22 @@ def trace_task(uuid, args, kwargs, request=None):\n                     I, R = Info(REJECTED, exc), ExceptionInfo(internal=True)\n                     state, retval = I.state, I.retval\n                     I.handle_reject(task, task_request)\n+                    # MEMORY LEAK FIX: Clear traceback frames to prevent memory retention (Issue #8882)\n                     traceback_clear(exc)\n                 except Ignore as exc:\n                     I, R = Info(IGNORED, exc), ExceptionInfo(internal=True)\n                     state, retval = I.state, I.retval\n                     I.handle_ignore(task, task_request)\n+                    # MEMORY LEAK FIX: Clear traceback frames to prevent memory retention (Issue #8882)\n                     traceback_clear(exc)\n                 except Retry as exc:\n                     I, R, state, retval = on_error(\n                         task_request, exc, RETRY, call_errbacks=False)\n+                    # MEMORY LEAK FIX: Clear traceback frames to prevent memory retention (Issue #8882)\n                     traceback_clear(exc)\n                 except Exception as exc:\n                     I, R, state, retval = on_error(task_request, exc)\n+                    # MEMORY LEAK FIX: Clear traceback frames to prevent memory retention (Issue #8882)\n                     traceback_clear(exc)\n                 except BaseException:\n                     raise\n@@ -522,6 +552,8 @@ def trace_task(uuid, args, kwargs, request=None):\n                         )\n                     except EncodeError as exc:\n                         I, R, state, retval = on_error(task_request, exc)\n+                        # MEMORY LEAK FIX: Clear traceback frames to prevent memory retention (Issue #8882)\n+                        traceback_clear(exc)\n                     else:\n                         Rstr = saferepr(R, resultrepr_maxsize)\n                         T = monotonic() - time_start\n@@ -591,6 +623,8 @@ def trace_task(task, uuid, args, kwargs, request=None, **opts):\n \n def _signal_internal_error(task, uuid, args, kwargs, request, exc):\n     \"\"\"Send a special `internal_error` signal to the app for outside body errors.\"\"\"\n+    tb = None\n+    einfo = None\n     try:\n         _, _, tb = sys.exc_info()\n         einfo = ExceptionInfo()\n@@ -607,7 +641,16 @@ def _signal_internal_error(task, uuid, args, kwargs, request, exc):\n             einfo=einfo,\n         )\n     finally:\n-        del tb\n+        # MEMORY LEAK FIX: Clean up local references to prevent memory leaks (Issue #8882)\n+        # Both 'tb' and 'einfo' can hold references to frame objects and their local variables.\n+        # Explicitly clearing these prevents reference cycles that block garbage collection.\n+        if tb is not None:\n+            del tb\n+        if einfo is not None:\n+            # Clear traceback frames to ensure consistent cleanup\n+            traceback_clear(einfo.exception)\n+            # Break potential reference cycles by deleting the einfo object\n+            del einfo\n \n \n def trace_task_ret(name, uuid, request, body, content_type,\n", "test_patch": "diff --git a/t/integration/test_mem_leak_in_exception_handling.py b/t/integration/test_mem_leak_in_exception_handling.py\nnew file mode 100644\nindex 00000000000..6ec38d0bfc3\n--- /dev/null\n+++ b/t/integration/test_mem_leak_in_exception_handling.py\n@@ -0,0 +1,261 @@\n+\"\"\"\n+Integration tests for memory leak issue #8882.\n+\n+These tests reproduce memory leak scenarios that occur when Celery tasks\n+raise unhandled exceptions, causing ExceptionInfo objects to not be\n+properly garbage collected.\n+\"\"\"\n+\n+import gc\n+import logging\n+import os\n+import tracemalloc\n+\n+from celery import Celery\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class MemoryLeakUnhandledExceptionsTest:\n+    \"\"\"Test class for memory leak scenarios with unhandled exceptions.\"\"\"\n+\n+    def __init__(self):\n+        self.app = Celery('test_memory_leak')\n+        self.app.conf.update(\n+            broker_url='memory://',\n+            result_backend='cache+memory://',\n+            task_always_eager=True,\n+            task_eager_propagates=True,\n+            task_store_eager_result=True,\n+        )\n+        self.setup_tasks()\n+\n+    def setup_tasks(self):\n+        \"\"\"Setup test tasks.\"\"\"\n+\n+        @self.app.task\n+        def task_success():\n+            \"\"\"Task that completes successfully - baseline for memory comparison.\"\"\"\n+            return \"success\"\n+\n+        @self.app.task\n+        def task_unhandled_exception():\n+            \"\"\"Task that raises an unhandled RuntimeError exception.\"\"\"\n+            raise RuntimeError(\"Unhandled exception for memory leak test\")\n+\n+        @self.app.task(bind=True, max_retries=3)\n+        def task_retry_then_fail(self):\n+            \"\"\"Task that retries multiple times and eventually fails with unhandled exception.\"\"\"\n+            if self.request.retries < self.max_retries:\n+                raise self.retry(countdown=0.001)\n+            raise RuntimeError(\"Final retry failure - unhandled exception\")\n+\n+        @self.app.task\n+        def task_nested_exception_stack():\n+            \"\"\"Task that raises exception through deeply nested function calls.\"\"\"\n+            def deep_level_5():\n+                local_data = {\"level\": 5, \"data\": list(range(100))}  # noqa: F841\n+                raise ValueError(\"Deep nested exception at level 5\")\n+\n+            def deep_level_4():\n+                local_data = {\"level\": 4, \"nested\": {\"data\": list(range(50))}}  # noqa: F841\n+                deep_level_5()\n+\n+            def deep_level_3():\n+                local_data = [1, 2, 3, {\"nested\": True}]  # noqa: F841\n+                deep_level_4()\n+\n+            def deep_level_2():\n+                deep_level_3()\n+\n+            def deep_level_1():\n+                deep_level_2()\n+\n+            deep_level_1()\n+\n+        self.task_success = task_success\n+        self.task_unhandled_exception = task_unhandled_exception\n+        self.task_retry_then_fail = task_retry_then_fail\n+        self.task_nested_exception_stack = task_nested_exception_stack\n+\n+\n+def get_memory_usage():\n+    \"\"\"\n+    Get current memory usage in bytes.\n+\n+    Returns RSS (total process memory) if psutil is available,\n+    otherwise returns Python heap allocations via tracemalloc.\n+    Note: These measurements are not directly comparable.\n+    \"\"\"\n+    try:\n+        import psutil\n+        process = psutil.Process(os.getpid())\n+        return process.memory_info().rss\n+    except ImportError:\n+        # Fallback to tracemalloc if psutil not available\n+        current, peak = tracemalloc.get_traced_memory()\n+        return current\n+\n+\n+def test_mem_leak_unhandled_exceptions():\n+    \"\"\"Test that reproduces the memory leak when tasks raise unhandled exceptions.\"\"\"\n+\n+    # Setup\n+    test_instance = MemoryLeakUnhandledExceptionsTest()\n+\n+    # Enable memory tracing\n+    tracemalloc.start()\n+\n+    # Warm up - run some successful tasks first\n+    for _ in range(50):\n+        try:\n+            test_instance.task_success.apply()\n+        except Exception:\n+            pass\n+\n+    # Force garbage collection and get baseline memory\n+    gc.collect()\n+    baseline_memory = get_memory_usage()\n+\n+    # Run many failing tasks - this should demonstrate the leak\n+    exception_count = 0\n+    for _ in range(500):  # Reduced from 1000 to make test faster\n+        try:\n+            test_instance.task_unhandled_exception.apply()\n+        except Exception:\n+            exception_count += 1\n+\n+    # Force garbage collection\n+    gc.collect()\n+    after_exceptions_memory = get_memory_usage()\n+\n+    # Run successful tasks again to ensure the leak is from exceptions\n+    for _ in range(50):\n+        try:\n+            test_instance.task_success.apply()\n+        except Exception:\n+            pass\n+\n+    gc.collect()\n+    final_memory = get_memory_usage()\n+\n+    # Calculate memory increase\n+    memory_increase = after_exceptions_memory - baseline_memory\n+\n+    # Stop tracing\n+    tracemalloc.stop()\n+\n+    # Log memory statistics for debugging\n+    logger.debug(\"--- Memory Statistics ---\")  # Separator for better readability\n+    logger.debug(f\"Baseline memory: {baseline_memory / 1024 / 1024:.2f} MB\")\n+    logger.debug(f\"After exceptions: {after_exceptions_memory / 1024 / 1024:.2f} MB\")\n+    logger.debug(f\"Final memory: {final_memory / 1024 / 1024:.2f} MB\")\n+    logger.debug(f\"Memory increase: {memory_increase / 1024 / 1024:.2f} MB\")\n+    logger.debug(f\"Exceptions processed: {exception_count}\")\n+\n+    # The test should demonstrate a significant memory increase\n+    # This threshold may need adjustment based on the system\n+    memory_increase_mb = memory_increase / 1024 / 1024\n+\n+    # Verify the memory leak is fixed - memory increase should be minimal\n+    # Before fix: >70MB for 1000 tasks (~70KB/task)\n+    # After fix: <5MB for 500 tasks (<10KB/task)\n+    threshold_percent = float(os.getenv(\"MEMORY_LEAK_THRESHOLD_PERCENT\", 10))  # Default: 10% increase\n+    memory_threshold_mb = baseline_memory / 1024 / 1024 * (threshold_percent / 100)\n+    assert memory_increase_mb < memory_threshold_mb, (\n+        f\"Memory leak still exists! Expected <{memory_threshold_mb:.2f}MB increase \"\n+        f\"(based on {threshold_percent}% of baseline), \"\n+        f\"but got {memory_increase_mb:.2f}MB. \"\n+        f\"This indicates the memory leak fix is not working properly.\"\n+    )\n+\n+\n+def test_mem_leak_retry_failures():\n+    \"\"\"Test memory leak with task retry and eventual failure scenarios.\"\"\"\n+\n+    test_instance = MemoryLeakUnhandledExceptionsTest()\n+\n+    # Enable memory tracing\n+    tracemalloc.start()\n+\n+    # Get baseline\n+    gc.collect()\n+    baseline_memory = get_memory_usage()\n+\n+    # Run tasks that retry and eventually fail\n+    for _ in range(100):  # Fewer iterations since retries are expensive\n+        try:\n+            test_instance.task_retry_then_fail.apply()\n+        except Exception:\n+            pass\n+\n+    gc.collect()\n+    after_retries_memory = get_memory_usage()\n+\n+    # Stop tracing\n+    tracemalloc.stop()\n+\n+    # Calculate memory increase\n+    memory_increase = after_retries_memory - baseline_memory\n+    memory_increase_mb = memory_increase / 1024 / 1024\n+\n+    logger.debug(\"\")  # New line for better readability\n+    logger.debug(f\"Baseline memory: {baseline_memory / 1024 / 1024:.2f} MB\")\n+    logger.debug(f\"After retries: {after_retries_memory / 1024 / 1024:.2f} MB\")\n+    logger.debug(f\"Memory increase: {memory_increase_mb:.2f} MB\")\n+\n+    # Retries should not show significant memory increase if fix is working\n+    assert memory_increase_mb < 3, (\n+        f\"Memory leak in retry scenarios! Expected <3MB increase for 100 retry tasks, \"\n+        f\"but got {memory_increase_mb:.2f}MB\"\n+    )\n+\n+\n+def test_mem_leak_nested_exception_stacks():\n+    \"\"\"Test memory leak with deeply nested exception stacks and local variables.\"\"\"\n+\n+    test_instance = MemoryLeakUnhandledExceptionsTest()\n+\n+    # Enable memory tracing\n+    tracemalloc.start()\n+\n+    # Get baseline\n+    gc.collect()\n+    baseline_memory = get_memory_usage()\n+\n+    # Run tasks with complex exception stacks\n+    for _ in range(200):\n+        try:\n+            test_instance.task_nested_exception_stack.apply()\n+        except Exception:\n+            pass\n+\n+    gc.collect()\n+    after_complex_memory = get_memory_usage()\n+\n+    # Stop tracing\n+    tracemalloc.stop()\n+\n+    # Calculate memory increase\n+    memory_increase = after_complex_memory - baseline_memory\n+    memory_increase_mb = memory_increase / 1024 / 1024\n+\n+    logger.debug(\"Memory usage results:\")\n+    logger.debug(f\"Baseline memory: {baseline_memory / 1024 / 1024:.2f} MB\")\n+    logger.debug(f\"After complex exceptions: {after_complex_memory / 1024 / 1024:.2f} MB\")\n+    logger.debug(f\"Memory increase: {memory_increase_mb:.2f} MB\")\n+\n+    # Complex exceptions should not show significant memory increase if fix is working\n+    assert memory_increase_mb < 4, (\n+        f\"Memory leak in nested exception scenarios! Expected <4MB increase for 200 nested tasks, \"\n+        f\"but got {memory_increase_mb:.2f}MB\"\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    # Allow running these tests standalone for debugging\n+    print(\"Running memory leak integration tests...\")\n+    test_mem_leak_unhandled_exceptions()\n+    test_mem_leak_retry_failures()\n+    test_mem_leak_nested_exception_stacks()\n+    print(\"Memory leak integration tests completed\")\ndiff --git a/t/unit/app/test_trace.py b/t/unit/app/test_trace.py\nnew file mode 100644\nindex 00000000000..b2796971fdf\n--- /dev/null\n+++ b/t/unit/app/test_trace.py\n@@ -0,0 +1,134 @@\n+\"\"\"Unit tests for celery.app.trace module.\"\"\"\n+\n+import sys\n+\n+from celery.app.trace import traceback_clear\n+\n+\n+class test_traceback_clear:\n+    \"\"\"Unit tests for traceback_clear function.\"\"\"\n+\n+    def test_uses_exc_argument(self):\n+        \"\"\"Test that traceback_clear(exc) correctly uses the exc argument.\n+\n+        This test proves that the reported issue about traceback_clear not using\n+        the exc argument is NOT valid. The function does use the exc argument correctly.\n+        \"\"\"\n+        # Create exception with traceback\n+        def create_exception_with_traceback():\n+            \"\"\"Create an exception with a traceback for testing.\"\"\"\n+            try:\n+                # Create a nested call stack to have frames to clear\n+                def inner_function():\n+                    x = \"some_local_variable\" * 1000  # Create local variable  # noqa: F841\n+                    y = list(range(1000))  # Another local variable  # noqa: F841\n+                    raise ValueError(\"Test exception with traceback\")\n+\n+                def outer_function():\n+                    z = \"outer_local_variable\" * 1000  # Local variable in outer frame  # noqa: F841\n+                    inner_function()\n+\n+                outer_function()\n+            except Exception as e:\n+                return e\n+\n+        # Test 1: traceback_clear(exc) with provided exception\n+        exc = create_exception_with_traceback()\n+\n+        # Verify exception has traceback\n+        exc_tb = getattr(exc, '__traceback__', None)\n+        assert exc_tb is not None, \"Exception should have traceback\"\n+\n+        # Count initial frames\n+        initial_frames = []\n+        tb = exc_tb\n+        while tb is not None:\n+            initial_frames.append(tb.tb_frame)\n+            tb = tb.tb_next\n+\n+        assert len(initial_frames) > 0, \"Should have traceback frames\"\n+\n+        # Verify frames have local variables before clearing\n+        frame_locals_before = []\n+        for frame in initial_frames:\n+            frame_locals_before.append(len(frame.f_locals))\n+\n+        assert any(count > 0 for count in frame_locals_before), \"Frames should have local variables\"\n+\n+        # Call traceback_clear with the exception - this should use exc argument\n+        traceback_clear(exc)\n+\n+        # Verify frames are cleared\n+        exc_tb_after = getattr(exc, '__traceback__', None)\n+        assert exc_tb_after is not None, \"Traceback should still exist after clearing\"\n+\n+        tb = exc_tb_after\n+        frames_after = []\n+        while tb is not None:\n+            frames_after.append(tb.tb_frame)\n+            tb = tb.tb_next\n+\n+        # Check that frame locals are cleared\n+        cleared_count = 0\n+        for frame in frames_after:\n+            if len(frame.f_locals) == 0:\n+                cleared_count += 1\n+\n+        assert cleared_count == len(frames_after), \"All frames should be cleared\"\n+\n+        # Verify the function actually used the exc argument by checking traceback still exists\n+        assert getattr(exc, '__traceback__', None) is not None, (\n+            \"Traceback should still exist but frames should be cleared\"\n+        )\n+\n+    def test_without_exc_argument(self):\n+        \"\"\"Test traceback_clear() without exc argument uses sys.exc_info().\"\"\"\n+        try:\n+            def test_function():\n+                local_var = \"test\" * 1000  # noqa: F841\n+                raise RuntimeError(\"Test exception\")\n+\n+            test_function()\n+        except Exception:\n+            # Now we're in except block with active traceback\n+            _, _, tb_before = sys.exc_info()\n+            assert tb_before is not None, \"Should have active traceback\"\n+\n+            # Call traceback_clear without argument - should use sys.exc_info()\n+            traceback_clear()\n+            # Test passes if no exception is raised\n+\n+    def test_with_none(self):\n+        \"\"\"Test traceback_clear(None) uses sys.exc_info() fallback.\"\"\"\n+        try:\n+            def test_function():\n+                local_var = \"test\" * 1000  # noqa: F841\n+                raise RuntimeError(\"Test exception\")\n+\n+            test_function()\n+        except Exception:\n+            # Call with None - should fall back to sys.exc_info()\n+            traceback_clear(None)\n+            # Test passes if no exception is raised\n+\n+    def test_with_exception_no_traceback(self):\n+        \"\"\"Test traceback_clear with exception that has no __traceback__.\"\"\"\n+        # Create exception without traceback\n+        exc = ValueError(\"Test exception\")\n+\n+        # Should not raise exception\n+        traceback_clear(exc)\n+\n+    def test_handles_runtime_error(self):\n+        \"\"\"Test that traceback_clear handles RuntimeError when frame is executing.\"\"\"\n+        # This test is mainly for coverage - RuntimeError handling is internal\n+        # and difficult to trigger in normal circumstances\n+        try:\n+            def test_function():\n+                local_var = \"test\" * 1000  # noqa: F841\n+                raise RuntimeError(\"Test exception\")\n+\n+            test_function()\n+        except Exception as exc:\n+            # Should not raise exception even if RuntimeError occurs internally\n+            traceback_clear(exc)\n", "problem_statement": "Memory Leak on Unhandled Exceptions\n<!--\r\nPlease fill this template entirely and do not erase parts of it.\r\nWe reserve the right to close without a response\r\nbug reports which are incomplete.\r\n-->\r\n# Checklist\r\n<!--\r\nTo check an item on the list replace [ ] with [x].\r\n-->\r\n- [X] I have verified that the issue exists against the `main` branch of Celery.\r\n- [ ] This has already been asked to the [discussions forum](https://github.com/celery/celery/discussions) first.\r\n- [X] I have read the relevant section in the\r\n  [contribution guide](https://docs.celeryq.dev/en/main/contributing.html#other-bugs)\r\n  on reporting bugs.\r\n- [X] I have checked the [issues list](https://github.com/celery/celery/issues?q=is%3Aissue+label%3A%22Issue+Type%3A+Bug+Report%22+-label%3A%22Category%3A+Documentation%22)\r\n  for similar or identical bug reports.\r\n- [X] I have checked the [pull requests list](https://github.com/celery/celery/pulls?q=is%3Apr+label%3A%22PR+Type%3A+Bugfix%22+-label%3A%22Category%3A+Documentation%22)\r\n  for existing proposed fixes.\r\n- [X] I have checked the [commit log](https://github.com/celery/celery/commits/main)\r\n  to find out if the bug was already fixed in the main branch.\r\n- [X] I have included all related issues and possible duplicate issues\r\n  in this issue (If there are none, check this box anyway).\r\n\r\n## Mandatory Debugging Information\r\n\r\n- [X] I have included the output of ``celery -A proj report`` in the issue.\r\n    (if you are not able to do this, then at least specify the Celery\r\n     version affected).\r\n- [X] I have verified that the issue exists against the `main` branch of Celery.\r\n- [X] I have included the contents of ``pip freeze`` in the issue.\r\n- [X] I have included all the versions of all the external dependencies required\r\n  to reproduce this bug.\r\n\r\n## Optional Debugging Information\r\n<!--\r\nTry some of the below if you think they are relevant.\r\nIt will help us figure out the scope of the bug and how many users it affects.\r\n-->\r\n- [ ] I have tried reproducing the issue on more than one Python version\r\n  and/or implementation.\r\n- [X] I have tried reproducing the issue on more than one message broker and/or\r\n  result backend.\r\n- [ ] I have tried reproducing the issue on more than one version of the message\r\n  broker and/or result backend.\r\n- [ ] I have tried reproducing the issue on more than one operating system.\r\n- [ ] I have tried reproducing the issue on more than one workers pool.\r\n- [ ] I have tried reproducing the issue with autoscaling, retries,\r\n  ETA/Countdown & rate limits disabled.\r\n- [ ] I have tried reproducing the issue after downgrading\r\n  and/or upgrading Celery and its dependencies.\r\n\r\n## Related Issues and Possible Duplicates\r\n<!--\r\nPlease make sure to search and mention any related issues\r\nor possible duplicates to this issue as requested by the checklist above.\r\n\r\nThis may or may not include issues in other repositories that the Celery project\r\nmaintains or other repositories that are dependencies of Celery.\r\n\r\nIf you don't know how to mention issues, please refer to Github's documentation\r\non the subject: https://help.github.com/en/articles/autolinked-references-and-urls#issues-and-pull-requests\r\n-->\r\n\r\n#### Related Issues\r\n\r\n- None\r\n\r\n#### Possible Duplicates\r\n\r\n- None\r\n\r\n## Environment & Settings\r\n<!-- Include the contents of celery --version below -->\r\n**Celery version**: 5.3.6 (emerald-rush)\r\n<!-- Include the output of celery -A proj report below -->\r\n<details>\r\n<summary><b><code>celery report</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\nsoftware -> celery:5.3.6 (emerald-rush) kombu:5.3.5 py:3.11.8\r\n            billiard:4.2.0 py-amqp:5.2.0\r\nplatform -> system:Linux arch:64bit, ELF\r\n            kernel version:6.2.0-1017-aws imp:CPython\r\nloader   -> celery.loaders.app.AppLoader\r\nsettings -> transport:pyamqp results:disabled\r\n\r\nbroker_url: 'amqp://guest:********@localhost:5672//'\r\ndeprecated_settings: None\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Steps to Reproduce\r\n\r\n## Required Dependencies\r\n<!-- Please fill the required dependencies to reproduce this issue -->\r\n- **Minimal Python Version**: N/A or Unknown\r\n- **Minimal Celery Version**: N/A or Unknown\r\n- **Minimal Kombu Version**: N/A or Unknown\r\n- **Minimal Broker Version**: N/A or Unknown\r\n- **Minimal Result Backend Version**: N/A or Unknown\r\n- **Minimal OS and/or Kernel Version**: N/A or Unknown\r\n- **Minimal Broker Client Version**: N/A or Unknown\r\n- **Minimal Result Backend Client Version**: N/A or Unknown\r\n\r\n### Python Packages\r\n<!-- Please fill the contents of pip freeze below -->\r\n<details>\r\n<summary><b><code>pip freeze</code> Output:</b></summary>\r\n<p>\r\n\r\n```\r\namqp==5.2.0\r\nbilliard==4.2.0\r\ncelery==5.3.6\r\nclick==8.1.7\r\nclick-didyoumean==0.3.0\r\nclick-plugins==1.1.1\r\nclick-repl==0.3.0\r\nkombu==5.3.5\r\nprompt-toolkit==3.0.43\r\npsutil==5.9.8\r\npython-dateutil==2.8.2\r\nsix==1.16.0\r\ntzdata==2024.1\r\nvine==5.1.0\r\nwcwidth==0.2.13\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Other Dependencies\r\n<!--\r\nPlease provide system dependencies, configuration files\r\nand other dependency information if applicable\r\n-->\r\n<details>\r\n<p>\r\nN/A\r\n</p>\r\n</details>\r\n\r\n## Minimally Reproducible Test Case\r\n<!--\r\nPlease provide a reproducible test case.\r\nRefer to the Reporting Bugs section in our contribution guide.\r\n\r\nWe prefer submitting test cases in the form of a PR to our integration test suite.\r\nIf you can provide one, please mention the PR number below.\r\nIf not, please attach the most minimal code example required to reproduce the issue below.\r\nIf the test case is too large, please include a link to a gist or a repository below.\r\n-->\r\n\r\n<details>\r\n<p>\r\n\r\n```python\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n# Expected Behavior\r\n<!-- Describe in detail what you expect to happen -->\r\n\r\nTasks raising unhandled exceptions should not consume excessive worker memory.\r\n\r\n# Actual Behavior\r\n<!--\r\nDescribe in detail what actually happened.\r\nPlease include a backtrace and surround it with triple backticks (```).\r\nIn addition, include the Celery daemon logs, the broker logs,\r\nthe result backend logs and system logs below if they will help us debug\r\nthe issue.\r\n-->\r\nI first observed this issue in our production environment where unhandled tasks exceptions would appear to \"leak\" memory in the worker process.  Our production enviroment is AWS/SQS with Django backend.  I was able to isolate the behavior in a very minimal out-of-the-box celery example against rabbitmq broker (exactly as shown in the getting started).\r\n\r\nHere is my tasks.py\r\n```python\r\nfrom celery import Celery\r\n\r\napp = Celery('tasks', broker='pyamqp://guest@localhost//')\r\n\r\n\r\n@app.task\r\ndef ok():\r\n    pass\r\n\r\n\r\n@app.task\r\ndef bad():\r\n    raise RuntimeError(\"err\")\r\n\r\n\r\n@app.task(bind=True)\r\ndef again(self):\r\n    if self.request.retries <  self.max_retries:\r\n        raise self.retry(countdown=0.1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import argparse\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"task\")\r\n    parser.add_argument(\"--count\", type=int, default=1)\r\n    args = parser.parse_args()\r\n\r\n    task = app.tasks.get(f\"tasks.{args.task}\")\r\n    for _ in range(args.count):\r\n        task.delay()\r\n```\r\n\r\nI start rabbit mq per the example (only adding --rm)\r\n```bash\r\ndocker run -d --rm -p 5672:5672 rabbitmq\r\n```\r\n\r\nIn one shell I run the celery job\r\n```bash\r\nenv/bin/celery -A tasks worker --loglevel=INFO --concurrency=1\r\n```\r\n\r\nIn another shell, monitor the worker RSS memory.  I leave this to you, but `ps fax | grep celery` to find the worker pid, then`top -p {worker_pid}` to monitor works well enough.  In top the RSS is denoted in the RES column.\r\n\r\nIn another shell I issue jobs per the small `__main__` above in tasks.py\r\n\r\nUse the `ok` task as a control to observe that the memory does not increase\r\n```bash\r\nenv/bin/python tasks ok --count 1000\r\n```\r\nAt this point, the process should be fully loaded and I observe the worker's RSS at `34896KB`\r\n\r\nWhen issuing a single tasks.bad generally 2-300K memory may be consumed and not returned.  The first run consumes more likely heating up some python code that was not run yet.\r\n```bash\r\nenv/bin/python tasks bad\r\n```\r\nNow the worker's RSS is at `35792KB`\r\n\r\nIn a tight loop issue 1000 of `tasks.bad`, excessive memory is allocated\r\n```bash\r\nenv/bin/python tasks bad --count 1000\r\n```\r\nNow the worker's RSS is at `107168KB`\r\n\r\nIn this environment if I issue another batch of bad, the RSS tops out around 116MB\r\n\r\nI know these types of python issues (or even could be an python-ism) are difficult to deal with, but after some internal discussion we thought to minimally bring it to your attention and raise awareness to others. \r\n\r\nUsing code inspection I theorized similar behavior for a Retry (see My Analysis below).  You can run the same experiment again and this time instead of tasks.bad, use tasks.again and you should seem similar phenomenon only to a lesser extent.\r\n\r\nIn my production environment we see much larger jumps in RSS per unhandled exception.\r\n \r\n# My Analysis\r\n\r\nI did some sleuthing and I believe the issue centers on the billiard serialization of the traceback which I assume is returned to the parent process.  This is the einfo object `from billiard.einfo import ExceptionInfo`.  I believe some python 3.11 specific code aggravates the issue.\r\n\r\n[Here](https://github.com/celery/celery/blob/06e91d913c424ddb862d9a5b50a5d3da0acdd217/celery/app/trace.py#L225) celery builds the einfo object\r\n\r\n[Here](https://github.com/celery/billiard/blob/8bffa2c2067f43da746c8f4e890c5a007bab0a12/billiard/einfo.py#L28) is some python 3.11 specific billiard code which collects bytecode info for the traceback frames.  My analysis was this was the majority of bytes allocated (I'm not expert here but that's my quick understanding). I used memray along with the `-P solo` option to observe the memory allocation flamegraph.\r\n\r\nThis einfo object is propagated up through the stack.  I could avoid the problem if lower down the call stack I remove reference to the einfo.  e.g. set R=None before the return at line [575](https://github.com/celery/celery/blob/06e91d913c424ddb862d9a5b50a5d3da0acdd217/celery/app/trace.py#L575).  As the object propagates up the stack, that does not fix the issue.  For instance setting `R=None` at the end of `fast_trace_task` line [654](https://github.com/celery/celery/blob/06e91d913c424ddb862d9a5b50a5d3da0acdd217/celery/app/trace.py#L654) does not fix the issue.\r\n\r\nThe allocations seem proportional to the complexity of the traceback.  In our production environment, the \"leaks\" are much bigger.\r\n\r\n**Warning** here are some hypothesis based on observation and a few days recent study into this issue.  Don't take this as truth, but it might lead someone in the right direction.\r\nI believe the object is \"lost\" to the gc reference counter at some point in the call stack, and a more complicated gc collection becomes necessary.  If I insert a `gc.collect()` somewhere in the call stack it seems to mitigate the issue.  I believe so many objects are allocated/freed quickly while also allocation this set of einfo object components (and not freed), this tends to moves the einfo to the generation 2 management within the gc.  Once the allocations are made on these smaller size objects the memory is not freed back to the OS.\r\n\r\nIn our own deployment if I put `gc.collect()` in handle_success and handle_failure, then I do not observe the \"leak\".  We are not internally convinced at this solution yet.\n", "hints_text": "Recently  I encountered the same problem - the worker's memory was not released upon an exception. A fairly large amount of data was generated in the task - about a hundred megabytes, then an error occurred. Each time an error occurred, the worker's resident memory increased by one hundred megabytes and was not released. Worker memory usage was limited to four gigabytes, and by running tasks it was possible to reach this value, after which the worker crashed.\n\nAfter some investigating, I found that the answer indeed lies in traceback. From the [python documentation](https://docs.python.org/3/reference/compound_stmts.html#:~:text=Exceptions%20are%20cleared%20because%20with%20the%20traceback%20attached%20to%20them%2C%20they%20form%20a%20reference%20cycle%20with%20the%20stack%20frame%2C%20keeping%20all%20locals%20in%20that%20frame%20alive%20until%20the%20next%20garbage%20collection%20occurs.):\n\n> Exceptions are cleared because with the traceback attached to them, they form a reference cycle with the stack frame, keeping all locals in that frame alive until the next garbage collection occurs.\n\nCelery creates variables with an exception object, which creates a reference cycle, but actually it takes this into account and cleans up the traceback: \n\nhttps://github.com/celery/celery/blob/d1c35bbdf014f13f4ab698d75e3ea381a017b090/celery/app/trace.py#L272-L291\n\nHowever, the point seems to be that it doesn't take into account that exceptions can be nested.\n\n\nLet's imagine the following situation:\n\n```\ndef foo():\n    foo_data = ... # there are some local data\n    raise SomeException()\n\n\n@app.task\ndef actual_task():\n    task_data = ... # there are some local data\n    try:\n        foo()\n    except Exception:\n        raise SomeAnotherException()\n\n```\nSo `clear_traceback` function will clear `task_data`, but not `foo_data`.\nTo clear `foo_data`, `clear_traceback` must not only clear `exc.__traceback__`, but also `exc.__context__.__traceback__` if `__context__` is not `None`. Exception in `__context__` might have its own `__context__`, so is seems that clearing must be done recursively, something like this in the end of  `traceback_clear`:\n\n```\nif exc.__context__ is not None:\n    traceback_clear(exc.__context__)\n```\n\nIn the example of ralphie0112358 there are not any nested exceptions, and I could not reproduced the issue with it. But when I added a nested exception with the generation of a large amount of data, the situation with unreleased memory was consistently reproduced.\nPerhaps the increase in memory in ralphie0112358 example was not related to the problem I described, but the increase in memory in production was.\n\nIs seems that for now the solution is indeed to call `gc.collect()` directly or not to use nested exceptions.\n\nAnyway, hopefully this will be fixed in future versions.\n> Is seems that for now the solution is indeed to call `gc.collect()` directly or not to use nested exceptions.\n> \n> Anyway, hopefully this will be fixed in future versions.\n\nwould you mind come with a prospective fix in a draft pr, please?\n\n", "all_hints_text": "Recently  I encountered the same problem - the worker's memory was not released upon an exception. A fairly large amount of data was generated in the task - about a hundred megabytes, then an error occurred. Each time an error occurred, the worker's resident memory increased by one hundred megabytes and was not released. Worker memory usage was limited to four gigabytes, and by running tasks it was possible to reach this value, after which the worker crashed.\n\nAfter some investigating, I found that the answer indeed lies in traceback. From the [python documentation](https://docs.python.org/3/reference/compound_stmts.html#:~:text=Exceptions%20are%20cleared%20because%20with%20the%20traceback%20attached%20to%20them%2C%20they%20form%20a%20reference%20cycle%20with%20the%20stack%20frame%2C%20keeping%20all%20locals%20in%20that%20frame%20alive%20until%20the%20next%20garbage%20collection%20occurs.):\n\n> Exceptions are cleared because with the traceback attached to them, they form a reference cycle with the stack frame, keeping all locals in that frame alive until the next garbage collection occurs.\n\nCelery creates variables with an exception object, which creates a reference cycle, but actually it takes this into account and cleans up the traceback: \n\nhttps://github.com/celery/celery/blob/d1c35bbdf014f13f4ab698d75e3ea381a017b090/celery/app/trace.py#L272-L291\n\nHowever, the point seems to be that it doesn't take into account that exceptions can be nested.\n\n\nLet's imagine the following situation:\n\n```\ndef foo():\n    foo_data = ... # there are some local data\n    raise SomeException()\n\n\n@app.task\ndef actual_task():\n    task_data = ... # there are some local data\n    try:\n        foo()\n    except Exception:\n        raise SomeAnotherException()\n\n```\nSo `clear_traceback` function will clear `task_data`, but not `foo_data`.\nTo clear `foo_data`, `clear_traceback` must not only clear `exc.__traceback__`, but also `exc.__context__.__traceback__` if `__context__` is not `None`. Exception in `__context__` might have its own `__context__`, so is seems that clearing must be done recursively, something like this in the end of  `traceback_clear`:\n\n```\nif exc.__context__ is not None:\n    traceback_clear(exc.__context__)\n```\n\nIn the example of ralphie0112358 there are not any nested exceptions, and I could not reproduced the issue with it. But when I added a nested exception with the generation of a large amount of data, the situation with unreleased memory was consistently reproduced.\nPerhaps the increase in memory in ralphie0112358 example was not related to the problem I described, but the increase in memory in production was.\n\nIs seems that for now the solution is indeed to call `gc.collect()` directly or not to use nested exceptions.\n\nAnyway, hopefully this will be fixed in future versions.\n> Is seems that for now the solution is indeed to call `gc.collect()` directly or not to use nested exceptions.\n> \n> Anyway, hopefully this will be fixed in future versions.\n\nwould you mind come with a prospective fix in a draft pr, please?\n\n", "commit_urls": ["https://github.com/celery/celery/commit/1ca9ff4f3e9075a7fe2b1b208f0face55a7a8e8a", "https://github.com/celery/celery/commit/9e9098061a4d501150440554ef59f950dd7dce5b", "https://github.com/celery/celery/commit/c4fc2fa4241b521d43d78efefe7e77c18d9bd7d6", "https://github.com/celery/celery/commit/fa053d30bed31417b95617443f840ad06fd7d62d", "https://github.com/celery/celery/commit/d1add442079e29fe2ac5c24a6e18a2a69b2cf174", "https://github.com/celery/celery/commit/4235739ebf5e0c4397d2b01846b0761f5e8d2460", "https://github.com/celery/celery/commit/09a31c4677dd1413cea045fd6abcce85b765dfee", "https://github.com/celery/celery/commit/36229e5e5f87b1a74eab75981648ffab67f8cb48", "https://github.com/celery/celery/commit/ddf25b8ed7afcca3b2ace2deb038d524485683ed", "https://github.com/celery/celery/commit/f2e24b3db474ea8b236be18f8c486de37010f720", "https://github.com/celery/celery/commit/16cd9bb28612f2d850c1dec05c85ff57760eaca0", "https://github.com/celery/celery/commit/d68f7408f6a34ed7abc8f1456133af26537e37f1", "https://github.com/celery/celery/commit/3c19d6482bba6d1e83ff5456d72ba8d434c3e8de", "https://github.com/celery/celery/commit/6ebb474201e8374c8ac3be3ea68f160369e09a9d", "https://github.com/celery/celery/commit/ae32c461431a56b4a50f2375f92cda94d623847e", "https://github.com/celery/celery/commit/142fd892115c42bcc7af9ae255827a2b0d8127ba", "https://github.com/celery/celery/commit/f03efa7cb6ef7f2de5f2a3bdb8bff8eed966ab2f", "https://github.com/celery/celery/commit/6fbb93026b5e2b31787efeb6e19b720c7959ecd5", "https://github.com/celery/celery/commit/b2494a0a7d7ef5cfc386ae2fefa8da2b5d0ae01f", "https://github.com/celery/celery/commit/2edf77245da21bd49cbcfb98b418f3abb57fa379", "https://github.com/celery/celery/commit/860a1f18de50c9f6c327091a908f5641edaa84f4", "https://github.com/celery/celery/commit/c7913d9741e817956ad3a5e6a8dbee8ac1a15621", "https://github.com/celery/celery/commit/63c83129dd117be855c90c12a51be3820e7c0580", "https://github.com/celery/celery/commit/ca2d22204a47d7c7d553ae1408a099ab10caf055"], "created_at": "2025-07-05T08:53:28Z", "classification": "Efficiency"}
{"repo": "dgtlmoon/changedetection.io", "pull_number": 3220, "instance_id": "dgtlmoon__changedetection.io-3220", "issue_numbers": [3219], "base_commit": "7b8d335c433e55fcb4ce2c1b3c596e02dbd67e3b", "patch": "diff --git a/.dockerignore b/.dockerignore\nindex 2f88d7d3398..14fba462a46 100644\n--- a/.dockerignore\n+++ b/.dockerignore\n@@ -29,3 +29,35 @@ venv/\n \n # Visual Studio\n .vscode/\n+\n+# Test and development files\n+test-datastore/\n+tests/\n+docs/\n+*.md\n+!README.md\n+\n+# Temporary and log files\n+*.log\n+*.tmp\n+tmp/\n+temp/\n+\n+# Training data and large files\n+train-data/\n+works-data/\n+\n+# Container files\n+Dockerfile*\n+docker-compose*.yml\n+.dockerignore\n+\n+# Development certificates and keys\n+*.pem\n+*.key\n+*.crt\n+profile_output.prof\n+\n+# Large binary files that shouldn't be in container\n+*.pdf\n+chrome.json\n\\ No newline at end of file\ndiff --git a/changedetectionio/__init__.py b/changedetectionio/__init__.py\nindex 0cc7225076c..c34ab04b714 100644\n--- a/changedetectionio/__init__.py\n+++ b/changedetectionio/__init__.py\n@@ -10,10 +10,11 @@\n import getopt\n import platform\n import signal\n-import socket\n+\n import sys\n-from werkzeug.serving import run_simple\n \n+# Eventlet completely removed - using threading mode for SocketIO\n+# This provides better Python 3.12+ compatibility and eliminates eventlet/asyncio conflicts\n from changedetectionio import store\n from changedetectionio.flask_app import changedetection_app\n from loguru import logger\n@@ -28,22 +29,34 @@ def get_version():\n # Parent wrapper or OS sends us a SIGTERM/SIGINT, do everything required for a clean shutdown\n def sigshutdown_handler(_signo, _stack_frame):\n     name = signal.Signals(_signo).name\n-    logger.critical(f'Shutdown: Got Signal - {name} ({_signo}), Saving DB to disk and calling shutdown')\n-    datastore.sync_to_json()\n-    logger.success('Sync JSON to disk complete.')\n+    logger.critical(f'Shutdown: Got Signal - {name} ({_signo}), Fast shutdown initiated')\n+    \n+    # Set exit flag immediately to stop all loops\n+    app.config.exit.set()\n+    datastore.stop_thread = True\n+    \n+    # Shutdown workers immediately\n+    try:\n+        from changedetectionio import worker_handler\n+        worker_handler.shutdown_workers()\n+    except Exception as e:\n+        logger.error(f\"Error shutting down workers: {str(e)}\")\n     \n-    # Shutdown socketio server if available\n+    # Shutdown socketio server fast\n     from changedetectionio.flask_app import socketio_server\n     if socketio_server and hasattr(socketio_server, 'shutdown'):\n         try:\n-            logger.info(\"Shutting down Socket.IO server...\")\n             socketio_server.shutdown()\n         except Exception as e:\n             logger.error(f\"Error shutting down Socket.IO server: {str(e)}\")\n     \n-    # Set flags for clean shutdown\n-    datastore.stop_thread = True\n-    app.config.exit.set()\n+    # Save data quickly\n+    try:\n+        datastore.sync_to_json()\n+        logger.success('Fast sync to disk complete.')\n+    except Exception as e:\n+        logger.error(f\"Error syncing to disk: {str(e)}\")\n+    \n     sys.exit()\n \n def main():\n@@ -52,9 +65,9 @@ def main():\n \n     datastore_path = None\n     do_cleanup = False\n-    host = ''\n+    host = \"0.0.0.0\"\n     ipv6_enabled = False\n-    port = os.environ.get('PORT') or 5000\n+    port = int(os.environ.get('PORT', 5000))\n     ssl_mode = False\n \n     # On Windows, create and use a default path.\n@@ -150,6 +163,11 @@ def main():\n \n     app = changedetection_app(app_config, datastore)\n \n+    # Get the SocketIO instance from the Flask app (created in flask_app.py)\n+    from changedetectionio.flask_app import socketio_server\n+    global socketio\n+    socketio = socketio_server\n+\n     signal.signal(signal.SIGTERM, sigshutdown_handler)\n     signal.signal(signal.SIGINT, sigshutdown_handler)\n     \n@@ -174,10 +192,11 @@ def sigusr_clean_handler(_signo, _stack_frame):\n \n \n     @app.context_processor\n-    def inject_version():\n+    def inject_template_globals():\n         return dict(right_sticky=\"v{}\".format(datastore.data['version_tag']),\n                     new_version_available=app.config['NEW_VERSION_AVAILABLE'],\n-                    has_password=datastore.data['settings']['application']['password'] != False\n+                    has_password=datastore.data['settings']['application']['password'] != False,\n+                    socket_io_enabled=datastore.data['settings']['application']['ui'].get('socket_io_enabled', True)\n                     )\n \n     # Monitored websites will not receive a Referer header when a user clicks on an outgoing link.\n@@ -201,87 +220,21 @@ def hide_referrer(response):\n         from werkzeug.middleware.proxy_fix import ProxyFix\n         app.wsgi_app = ProxyFix(app.wsgi_app, x_prefix=1, x_host=1)\n \n-    s_type = socket.AF_INET6 if ipv6_enabled else socket.AF_INET\n-\n-    # Get socketio_server from flask_app\n-    from changedetectionio.flask_app import socketio_server\n-\n-    if socketio_server and datastore.data['settings']['application']['ui'].get('open_diff_in_new_tab'):\n-        logger.info(\"Starting server with Socket.IO support (using threading)...\")\n \n-        # Use Flask-SocketIO's run method with error handling for Werkzeug warning\n-        # This is the cleanest approach that works with all Flask-SocketIO versions\n-        # Use '0.0.0.0' as the default host if none is specified\n-        # This will listen on all available interfaces\n-        listen_host = '0.0.0.0' if host == '' else host\n-        logger.info(f\"Using host: {listen_host} and port: {port}\")\n+    # SocketIO instance is already initialized in flask_app.py\n \n-        try:\n-            # First try with the allow_unsafe_werkzeug parameter (newer versions)\n-            if ssl_mode:\n-                socketio_server.run(\n-                    app,\n-                    host=listen_host,\n-                    port=int(port),\n-                    certfile='cert.pem',\n-                    keyfile='privkey.pem',\n-                    debug=False,\n-                    use_reloader=False,\n-                    allow_unsafe_werkzeug=True  # Only in newer versions\n-                )\n-            else:\n-                socketio_server.run(\n-                    app,\n-                    host=listen_host,\n-                    port=int(port),\n-                    debug=False,\n-                    use_reloader=False,\n-                    allow_unsafe_werkzeug=True  # Only in newer versions\n-                )\n-        except TypeError:\n-            # If allow_unsafe_werkzeug is not a valid parameter, try without it\n-            logger.info(\"Falling back to basic run method without allow_unsafe_werkzeug\")\n-            # Override the werkzeug safety check by setting an environment variable\n-            os.environ['WERKZEUG_RUN_MAIN'] = 'true'\n-            if ssl_mode:\n-                socketio_server.run(\n-                    app,\n-                    host=listen_host,\n-                    port=int(port),\n-                    certfile='cert.pem',\n-                    keyfile='privkey.pem',\n-                    debug=False,\n-                    use_reloader=False\n-                )\n-            else:\n-                socketio_server.run(\n-                    app,\n-                    host=listen_host,\n-                    port=int(port),\n-                    debug=False,\n-                    use_reloader=False\n-                )\n+    # Launch using SocketIO run method for proper integration (if enabled)\n+    if socketio_server:\n+        if ssl_mode:\n+            socketio.run(app, host=host, port=int(port), debug=False, \n+                        certfile='cert.pem', keyfile='privkey.pem', allow_unsafe_werkzeug=True)\n+        else:\n+            socketio.run(app, host=host, port=int(port), debug=False, allow_unsafe_werkzeug=True)\n     else:\n-        logger.warning(\"Socket.IO server not initialized, falling back to standard WSGI server\")\n-        # Fallback to standard WSGI server if socketio_server is not available\n-        listen_host = '0.0.0.0' if host == '' else host\n+        # Run Flask app without Socket.IO if disabled\n+        logger.info(\"Starting Flask app without Socket.IO server\")\n         if ssl_mode:\n-            # Use Werkzeug's run_simple with SSL support\n-            run_simple(\n-                hostname=listen_host,\n-                port=int(port),\n-                application=app,\n-                use_reloader=False,\n-                use_debugger=False,\n-                ssl_context=('cert.pem', 'privkey.pem')\n-            )\n+            app.run(host=host, port=int(port), debug=False, \n+                   ssl_context=('cert.pem', 'privkey.pem'))\n         else:\n-            # Use Werkzeug's run_simple for standard HTTP\n-            run_simple(\n-                hostname=listen_host,\n-                port=int(port),\n-                application=app,\n-                use_reloader=False,\n-                use_debugger=False\n-            )\n-\n+            app.run(host=host, port=int(port), debug=False)\ndiff --git a/changedetectionio/api/Watch.py b/changedetectionio/api/Watch.py\nindex 1a815670d8b..c6011934978 100644\n--- a/changedetectionio/api/Watch.py\n+++ b/changedetectionio/api/Watch.py\n@@ -3,6 +3,7 @@\n \n from flask_expects_json import expects_json\n from changedetectionio import queuedWatchMetaData\n+from changedetectionio import worker_handler\n from flask_restful import abort, Resource\n from flask import request, make_response\n import validators\n@@ -47,7 +48,7 @@ def get(self, uuid):\n             abort(404, message='No watch exists with the UUID of {}'.format(uuid))\n \n         if request.args.get('recheck'):\n-            self.update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n+            worker_handler.queue_item_async_safe(self.update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n             return \"OK\", 200\n         if request.args.get('paused', '') == 'paused':\n             self.datastore.data['watching'].get(uuid).pause()\n@@ -236,7 +237,7 @@ def post(self):\n \n         new_uuid = self.datastore.add_watch(url=url, extras=extras, tag=tags)\n         if new_uuid:\n-            self.update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': new_uuid}))\n+            worker_handler.queue_item_async_safe(self.update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': new_uuid}))\n             return {'uuid': new_uuid}, 201\n         else:\n             return \"Invalid or unsupported URL\", 400\n@@ -291,7 +292,7 @@ def get(self):\n \n         if request.args.get('recheck_all'):\n             for uuid in self.datastore.data['watching'].keys():\n-                self.update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n+                worker_handler.queue_item_async_safe(self.update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n             return {'status': \"OK\"}, 200\n \n         return list, 200\n\\ No newline at end of file\ndiff --git a/changedetectionio/async_update_worker.py b/changedetectionio/async_update_worker.py\nnew file mode 100644\nindex 00000000000..ae0501b0926\n--- /dev/null\n+++ b/changedetectionio/async_update_worker.py\n@@ -0,0 +1,449 @@\n+from .processors.exceptions import ProcessorException\n+import changedetectionio.content_fetchers.exceptions as content_fetchers_exceptions\n+from changedetectionio.processors.text_json_diff.processor import FilterNotFoundInResponse\n+from changedetectionio import html_tools\n+from changedetectionio.flask_app import watch_check_update\n+\n+import asyncio\n+import importlib\n+import os\n+import time\n+\n+from loguru import logger\n+\n+# Async version of update_worker\n+# Processes jobs from AsyncSignalPriorityQueue instead of threaded queue\n+\n+async def async_update_worker(worker_id, q, notification_q, app, datastore):\n+    \"\"\"\n+    Async worker function that processes watch check jobs from the queue.\n+    \n+    Args:\n+        worker_id: Unique identifier for this worker\n+        q: AsyncSignalPriorityQueue containing jobs to process\n+        notification_q: Standard queue for notifications\n+        app: Flask application instance\n+        datastore: Application datastore\n+    \"\"\"\n+    # Set a descriptive name for this task\n+    task = asyncio.current_task()\n+    if task:\n+        task.set_name(f\"async-worker-{worker_id}\")\n+    \n+    logger.info(f\"Starting async worker {worker_id}\")\n+    \n+    while not app.config.exit.is_set():\n+        update_handler = None\n+        watch = None\n+\n+        try:\n+            # Use asyncio wait_for to make queue.get() cancellable\n+            queued_item_data = await asyncio.wait_for(q.get(), timeout=1.0)\n+        except asyncio.TimeoutError:\n+            # No jobs available, continue loop\n+            continue\n+        except Exception as e:\n+            logger.error(f\"Worker {worker_id} error getting queue item: {e}\")\n+            await asyncio.sleep(0.1)\n+            continue\n+        \n+        uuid = queued_item_data.item.get('uuid')\n+        fetch_start_time = round(time.time())\n+        \n+        # Mark this UUID as being processed\n+        from changedetectionio import worker_handler\n+        worker_handler.set_uuid_processing(uuid, processing=True)\n+        \n+        try:\n+            if uuid in list(datastore.data['watching'].keys()) and datastore.data['watching'][uuid].get('url'):\n+                changed_detected = False\n+                contents = b''\n+                process_changedetection_results = True\n+                update_obj = {}\n+\n+                # Clear last errors\n+                datastore.data['watching'][uuid]['browser_steps_last_error_step'] = None\n+                datastore.data['watching'][uuid]['last_checked'] = fetch_start_time\n+\n+                watch = datastore.data['watching'].get(uuid)\n+\n+                logger.info(f\"Worker {worker_id} processing watch UUID {uuid} Priority {queued_item_data.priority} URL {watch['url']}\")\n+\n+                try:\n+                    watch_check_update.send(watch_uuid=uuid)\n+\n+                    # Processor is what we are using for detecting the \"Change\"\n+                    processor = watch.get('processor', 'text_json_diff')\n+\n+                    # Init a new 'difference_detection_processor'\n+                    processor_module_name = f\"changedetectionio.processors.{processor}.processor\"\n+                    try:\n+                        processor_module = importlib.import_module(processor_module_name)\n+                    except ModuleNotFoundError as e:\n+                        print(f\"Processor module '{processor}' not found.\")\n+                        raise e\n+\n+                    update_handler = processor_module.perform_site_check(datastore=datastore,\n+                                                                         watch_uuid=uuid)\n+\n+                    # All fetchers are now async, so call directly\n+                    await update_handler.call_browser()\n+\n+                    # Run change detection (this is synchronous)\n+                    changed_detected, update_obj, contents = update_handler.run_changedetection(watch=watch)\n+\n+                except PermissionError as e:\n+                    logger.critical(f\"File permission error updating file, watch: {uuid}\")\n+                    logger.critical(str(e))\n+                    process_changedetection_results = False\n+\n+                except ProcessorException as e:\n+                    if e.screenshot:\n+                        watch.save_screenshot(screenshot=e.screenshot)\n+                    if e.xpath_data:\n+                        watch.save_xpath_data(data=e.xpath_data)\n+                    datastore.update_watch(uuid=uuid, update_obj={'last_error': e.message})\n+                    process_changedetection_results = False\n+\n+                except content_fetchers_exceptions.ReplyWithContentButNoText as e:\n+                    extra_help = \"\"\n+                    if e.has_filters:\n+                        has_img = html_tools.include_filters(include_filters='img',\n+                                                             html_content=e.html_content)\n+                        if has_img:\n+                            extra_help = \", it's possible that the filters you have give an empty result or contain only an image.\"\n+                        else:\n+                            extra_help = \", it's possible that the filters were found, but contained no usable text.\"\n+\n+                    datastore.update_watch(uuid=uuid, update_obj={\n+                        'last_error': f\"Got HTML content but no text found (With {e.status_code} reply code){extra_help}\"\n+                    })\n+\n+                    if e.screenshot:\n+                        watch.save_screenshot(screenshot=e.screenshot, as_error=True)\n+\n+                    if e.xpath_data:\n+                        watch.save_xpath_data(data=e.xpath_data)\n+                        \n+                    process_changedetection_results = False\n+\n+                except content_fetchers_exceptions.Non200ErrorCodeReceived as e:\n+                    if e.status_code == 403:\n+                        err_text = \"Error - 403 (Access denied) received\"\n+                    elif e.status_code == 404:\n+                        err_text = \"Error - 404 (Page not found) received\"\n+                    elif e.status_code == 407:\n+                        err_text = \"Error - 407 (Proxy authentication required) received, did you need a username and password for the proxy?\"\n+                    elif e.status_code == 500:\n+                        err_text = \"Error - 500 (Internal server error) received from the web site\"\n+                    else:\n+                        extra = ' (Access denied or blocked)' if str(e.status_code).startswith('4') else ''\n+                        err_text = f\"Error - Request returned a HTTP error code {e.status_code}{extra}\"\n+\n+                    if e.screenshot:\n+                        watch.save_screenshot(screenshot=e.screenshot, as_error=True)\n+                    if e.xpath_data:\n+                        watch.save_xpath_data(data=e.xpath_data, as_error=True)\n+                    if e.page_text:\n+                        watch.save_error_text(contents=e.page_text)\n+\n+                    datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text})\n+                    process_changedetection_results = False\n+\n+                except FilterNotFoundInResponse as e:\n+                    if not datastore.data['watching'].get(uuid):\n+                        continue\n+\n+                    err_text = \"Warning, no filters were found, no change detection ran - Did the page change layout? update your Visual Filter if necessary.\"\n+                    datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text})\n+\n+                    # Filter wasnt found, but we should still update the visual selector so that they can have a chance to set it up again\n+                    if e.screenshot:\n+                        watch.save_screenshot(screenshot=e.screenshot)\n+\n+                    if e.xpath_data:\n+                        watch.save_xpath_data(data=e.xpath_data)\n+\n+                    # Only when enabled, send the notification\n+                    if watch.get('filter_failure_notification_send', False):\n+                        c = watch.get('consecutive_filter_failures', 0)\n+                        c += 1\n+                        # Send notification if we reached the threshold?\n+                        threshold = datastore.data['settings']['application'].get('filter_failure_notification_threshold_attempts', 0)\n+                        logger.debug(f\"Filter for {uuid} not found, consecutive_filter_failures: {c} of threshold {threshold}\")\n+                        if c >= threshold:\n+                            if not watch.get('notification_muted'):\n+                                logger.debug(f\"Sending filter failed notification for {uuid}\")\n+                                await send_filter_failure_notification(uuid, notification_q, datastore)\n+                            c = 0\n+                            logger.debug(f\"Reset filter failure count back to zero\")\n+\n+                        datastore.update_watch(uuid=uuid, update_obj={'consecutive_filter_failures': c})\n+                    else:\n+                        logger.trace(f\"{uuid} - filter_failure_notification_send not enabled, skipping\")\n+\n+                    process_changedetection_results = False\n+\n+                except content_fetchers_exceptions.checksumFromPreviousCheckWasTheSame as e:\n+                    # Yes fine, so nothing todo, don't continue to process.\n+                    process_changedetection_results = False\n+                    changed_detected = False\n+                    \n+                except content_fetchers_exceptions.BrowserConnectError as e:\n+                    datastore.update_watch(uuid=uuid,\n+                                         update_obj={'last_error': e.msg})\n+                    process_changedetection_results = False\n+                    \n+                except content_fetchers_exceptions.BrowserFetchTimedOut as e:\n+                    datastore.update_watch(uuid=uuid,\n+                                         update_obj={'last_error': e.msg})\n+                    process_changedetection_results = False\n+                    \n+                except content_fetchers_exceptions.BrowserStepsStepException as e:\n+                    if not datastore.data['watching'].get(uuid):\n+                        continue\n+\n+                    error_step = e.step_n + 1\n+                    from playwright._impl._errors import TimeoutError, Error\n+\n+                    # Generally enough info for TimeoutError (couldnt locate the element after default seconds)\n+                    err_text = f\"Browser step at position {error_step} could not run, check the watch, add a delay if necessary, view Browser Steps to see screenshot at that step.\"\n+\n+                    if e.original_e.name == \"TimeoutError\":\n+                        # Just the first line is enough, the rest is the stack trace\n+                        err_text += \" Could not find the target.\"\n+                    else:\n+                        # Other Error, more info is good.\n+                        err_text += \" \" + str(e.original_e).splitlines()[0]\n+\n+                    logger.debug(f\"BrowserSteps exception at step {error_step} {str(e.original_e)}\")\n+\n+                    datastore.update_watch(uuid=uuid,\n+                                         update_obj={'last_error': err_text,\n+                                                   'browser_steps_last_error_step': error_step})\n+\n+                    if watch.get('filter_failure_notification_send', False):\n+                        c = watch.get('consecutive_filter_failures', 0)\n+                        c += 1\n+                        # Send notification if we reached the threshold?\n+                        threshold = datastore.data['settings']['application'].get('filter_failure_notification_threshold_attempts', 0)\n+                        logger.error(f\"Step for {uuid} not found, consecutive_filter_failures: {c}\")\n+                        if threshold > 0 and c >= threshold:\n+                            if not watch.get('notification_muted'):\n+                                await send_step_failure_notification(watch_uuid=uuid, step_n=e.step_n, notification_q=notification_q, datastore=datastore)\n+                            c = 0\n+\n+                        datastore.update_watch(uuid=uuid, update_obj={'consecutive_filter_failures': c})\n+\n+                    process_changedetection_results = False\n+\n+                except content_fetchers_exceptions.EmptyReply as e:\n+                    # Some kind of custom to-str handler in the exception handler that does this?\n+                    err_text = \"EmptyReply - try increasing 'Wait seconds before extracting text', Status Code {}\".format(e.status_code)\n+                    datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text,\n+                                                                'last_check_status': e.status_code})\n+                    process_changedetection_results = False\n+                    \n+                except content_fetchers_exceptions.ScreenshotUnavailable as e:\n+                    err_text = \"Screenshot unavailable, page did not render fully in the expected time or page was too long - try increasing 'Wait seconds before extracting text'\"\n+                    datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text,\n+                                                                'last_check_status': e.status_code})\n+                    process_changedetection_results = False\n+                    \n+                except content_fetchers_exceptions.JSActionExceptions as e:\n+                    err_text = \"Error running JS Actions - Page request - \"+e.message\n+                    if e.screenshot:\n+                        watch.save_screenshot(screenshot=e.screenshot, as_error=True)\n+                    datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text,\n+                                                                'last_check_status': e.status_code})\n+                    process_changedetection_results = False\n+                    \n+                except content_fetchers_exceptions.PageUnloadable as e:\n+                    err_text = \"Page request from server didnt respond correctly\"\n+                    if e.message:\n+                        err_text = \"{} - {}\".format(err_text, e.message)\n+\n+                    if e.screenshot:\n+                        watch.save_screenshot(screenshot=e.screenshot, as_error=True)\n+\n+                    datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text,\n+                                                                'last_check_status': e.status_code,\n+                                                                'has_ldjson_price_data': None})\n+                    process_changedetection_results = False\n+                    \n+                except content_fetchers_exceptions.BrowserStepsInUnsupportedFetcher as e:\n+                    err_text = \"This watch has Browser Steps configured and so it cannot run with the 'Basic fast Plaintext/HTTP Client', either remove the Browser Steps or select a Chrome fetcher.\"\n+                    datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text})\n+                    process_changedetection_results = False\n+                    logger.error(f\"Exception (BrowserStepsInUnsupportedFetcher) reached processing watch UUID: {uuid}\")\n+\n+                except Exception as e:\n+                    logger.error(f\"Worker {worker_id} exception processing watch UUID: {uuid}\")\n+                    logger.error(str(e))\n+                    datastore.update_watch(uuid=uuid, update_obj={'last_error': \"Exception: \" + str(e)})\n+                    process_changedetection_results = False\n+\n+                else:\n+                    if not datastore.data['watching'].get(uuid):\n+                        continue\n+\n+                    update_obj['content-type'] = update_handler.fetcher.get_all_headers().get('content-type', '').lower()\n+\n+                    if not watch.get('ignore_status_codes'):\n+                        update_obj['consecutive_filter_failures'] = 0\n+\n+                    update_obj['last_error'] = False\n+                    cleanup_error_artifacts(uuid, datastore)\n+\n+                if not datastore.data['watching'].get(uuid):\n+                    continue\n+\n+                if process_changedetection_results:\n+                    # Extract title if needed\n+                    if datastore.data['settings']['application'].get('extract_title_as_title') or watch['extract_title_as_title']:\n+                        if not watch['title'] or not len(watch['title']):\n+                            try:\n+                                update_obj['title'] = html_tools.extract_element(find='title', html_content=update_handler.fetcher.content)\n+                                logger.info(f\"UUID: {uuid} Extract <title> updated title to '{update_obj['title']}\")\n+                            except Exception as e:\n+                                logger.warning(f\"UUID: {uuid} Extract <title> as watch title was enabled, but couldn't find a <title>.\")\n+\n+                    try:\n+                        datastore.update_watch(uuid=uuid, update_obj=update_obj)\n+\n+                        if changed_detected or not watch.history_n:\n+                            if update_handler.screenshot:\n+                                watch.save_screenshot(screenshot=update_handler.screenshot)\n+\n+                            if update_handler.xpath_data:\n+                                watch.save_xpath_data(data=update_handler.xpath_data)\n+\n+                            # Ensure unique timestamp for history\n+                            if watch.newest_history_key and int(fetch_start_time) == int(watch.newest_history_key):\n+                                logger.warning(f\"Timestamp {fetch_start_time} already exists, waiting 1 seconds\")\n+                                fetch_start_time += 1\n+                                await asyncio.sleep(1)\n+\n+                            watch.save_history_text(contents=contents,\n+                                                    timestamp=int(fetch_start_time),\n+                                                    snapshot_id=update_obj.get('previous_md5', 'none'))\n+\n+                            empty_pages_are_a_change = datastore.data['settings']['application'].get('empty_pages_are_a_change', False)\n+                            if update_handler.fetcher.content or (not update_handler.fetcher.content and empty_pages_are_a_change):\n+                                watch.save_last_fetched_html(contents=update_handler.fetcher.content, timestamp=int(fetch_start_time))\n+\n+                            # Send notifications on second+ check\n+                            if watch.history_n >= 2:\n+                                logger.info(f\"Change detected in UUID {uuid} - {watch['url']}\")\n+                                if not watch.get('notification_muted'):\n+                                    await send_content_changed_notification(uuid, notification_q, datastore)\n+\n+                    except Exception as e:\n+                        logger.critical(f\"Worker {worker_id} exception in process_changedetection_results\")\n+                        logger.critical(str(e))\n+                        datastore.update_watch(uuid=uuid, update_obj={'last_error': str(e)})\n+\n+                # Always record attempt count\n+                count = watch.get('check_count', 0) + 1\n+\n+                # Record server header\n+                try:\n+                    server_header = update_handler.fetcher.headers.get('server', '').strip().lower()[:255]\n+                    datastore.update_watch(uuid=uuid, update_obj={'remote_server_reply': server_header})\n+                except Exception as e:\n+                    pass\n+\n+                datastore.update_watch(uuid=uuid, update_obj={'fetch_time': round(time.time() - fetch_start_time, 3),\n+                                                               'check_count': count})\n+\n+        except Exception as e:\n+            logger.error(f\"Worker {worker_id} unexpected error processing {uuid}: {e}\")\n+            logger.error(f\"Worker {worker_id} traceback:\", exc_info=True)\n+            \n+            # Also update the watch with error information\n+            if datastore and uuid in datastore.data['watching']:\n+                datastore.update_watch(uuid=uuid, update_obj={'last_error': f\"Worker error: {str(e)}\"})\n+        \n+        finally:\n+            # Always cleanup - this runs whether there was an exception or not\n+            if uuid:\n+                try:\n+                    # Mark UUID as no longer being processed\n+                    worker_handler.set_uuid_processing(uuid, processing=False)\n+                    \n+                    # Send completion signal\n+                    if watch:\n+                        #logger.info(f\"Worker {worker_id} sending completion signal for UUID {watch['uuid']}\")\n+                        watch_check_update.send(watch_uuid=watch['uuid'])\n+\n+                    update_handler = None\n+                    logger.debug(f\"Worker {worker_id} completed watch {uuid} in {time.time()-fetch_start_time:.2f}s\")\n+                except Exception as cleanup_error:\n+                    logger.error(f\"Worker {worker_id} error during cleanup: {cleanup_error}\")\n+            \n+            # Brief pause before continuing to avoid tight error loops (only on error)\n+            if 'e' in locals():\n+                await asyncio.sleep(1.0)\n+            else:\n+                # Small yield for normal completion\n+                await asyncio.sleep(0.01)\n+\n+        # Check if we should exit\n+        if app.config.exit.is_set():\n+            break\n+\n+    # Check if we're in pytest environment - if so, be more gentle with logging\n+    import sys\n+    in_pytest = \"pytest\" in sys.modules or \"PYTEST_CURRENT_TEST\" in os.environ\n+    \n+    if not in_pytest:\n+        logger.info(f\"Worker {worker_id} shutting down\")\n+\n+\n+def cleanup_error_artifacts(uuid, datastore):\n+    \"\"\"Helper function to clean up error artifacts\"\"\"\n+    cleanup_files = [\"last-error-screenshot.png\", \"last-error.txt\"]\n+    for f in cleanup_files:\n+        full_path = os.path.join(datastore.datastore_path, uuid, f)\n+        if os.path.isfile(full_path):\n+            os.unlink(full_path)\n+\n+\n+\n+async def send_content_changed_notification(watch_uuid, notification_q, datastore):\n+    \"\"\"Helper function to queue notifications using the new notification service\"\"\"\n+    try:\n+        from changedetectionio.notification_service import create_notification_service\n+        \n+        # Create notification service instance\n+        notification_service = create_notification_service(datastore, notification_q)\n+        \n+        notification_service.send_content_changed_notification(watch_uuid)\n+    except Exception as e:\n+        logger.error(f\"Error sending notification for {watch_uuid}: {e}\")\n+\n+\n+async def send_filter_failure_notification(watch_uuid, notification_q, datastore):\n+    \"\"\"Helper function to send filter failure notifications using the new notification service\"\"\"\n+    try:\n+        from changedetectionio.notification_service import create_notification_service\n+        \n+        # Create notification service instance\n+        notification_service = create_notification_service(datastore, notification_q)\n+        \n+        notification_service.send_filter_failure_notification(watch_uuid)\n+    except Exception as e:\n+        logger.error(f\"Error sending filter failure notification for {watch_uuid}: {e}\")\n+\n+\n+async def send_step_failure_notification(watch_uuid, step_n, notification_q, datastore):\n+    \"\"\"Helper function to send step failure notifications using the new notification service\"\"\"\n+    try:\n+        from changedetectionio.notification_service import create_notification_service\n+        \n+        # Create notification service instance\n+        notification_service = create_notification_service(datastore, notification_q)\n+        \n+        notification_service.send_step_failure_notification(watch_uuid, step_n)\n+    except Exception as e:\n+        logger.error(f\"Error sending step failure notification for {watch_uuid}: {e}\")\n\\ No newline at end of file\ndiff --git a/changedetectionio/blueprint/browser_steps/__init__.py b/changedetectionio/blueprint/browser_steps/__init__.py\nindex f7907c7c0f8..f0de3057ce4 100644\n--- a/changedetectionio/blueprint/browser_steps/__init__.py\n+++ b/changedetectionio/blueprint/browser_steps/__init__.py\n@@ -25,35 +25,53 @@\n import json\n import hashlib\n from flask import Response\n+import asyncio\n+import threading\n+\n+def run_async_in_browser_loop(coro):\n+    \"\"\"Run async coroutine using the existing async worker event loop\"\"\"\n+    from changedetectionio import worker_handler\n+    \n+    # Use the existing async worker event loop instead of creating a new one\n+    if worker_handler.USE_ASYNC_WORKERS and worker_handler.async_loop and not worker_handler.async_loop.is_closed():\n+        logger.debug(\"Browser steps using existing async worker event loop\")\n+        future = asyncio.run_coroutine_threadsafe(coro, worker_handler.async_loop)\n+        return future.result()\n+    else:\n+        # Fallback: create a new event loop (for sync workers or if async loop not available)\n+        logger.debug(\"Browser steps creating temporary event loop\")\n+        loop = asyncio.new_event_loop()\n+        asyncio.set_event_loop(loop)\n+        try:\n+            return loop.run_until_complete(coro)\n+        finally:\n+            loop.close()\n \n def construct_blueprint(datastore: ChangeDetectionStore):\n     browser_steps_blueprint = Blueprint('browser_steps', __name__, template_folder=\"templates\")\n \n-    def start_browsersteps_session(watch_uuid):\n-        from . import nonContext\n+    async def start_browsersteps_session(watch_uuid):\n         from . import browser_steps\n         import time\n-        global io_interface_context\n+        from playwright.async_api import async_playwright\n \n         # We keep the playwright session open for many minutes\n         keepalive_seconds = int(os.getenv('BROWSERSTEPS_MINUTES_KEEPALIVE', 10)) * 60\n \n         browsersteps_start_session = {'start_time': time.time()}\n \n-        # You can only have one of these running\n-        # This should be very fine to leave running for the life of the application\n-        # @idea - Make it global so the pool of watch fetchers can use it also\n-        if not io_interface_context:\n-            io_interface_context = nonContext.c_sync_playwright()\n-            # Start the Playwright context, which is actually a nodejs sub-process and communicates over STDIN/STDOUT pipes\n-            io_interface_context = io_interface_context.start()\n+        # Create a new async playwright instance for browser steps\n+        playwright_instance = async_playwright()\n+        playwright_context = await playwright_instance.start()\n \n         keepalive_ms = ((keepalive_seconds + 3) * 1000)\n         base_url = os.getenv('PLAYWRIGHT_DRIVER_URL', '').strip('\"')\n         a = \"?\" if not '?' in base_url else '&'\n         base_url += a + f\"timeout={keepalive_ms}\"\n \n-        browsersteps_start_session['browser'] = io_interface_context.chromium.connect_over_cdp(base_url)\n+        browser = await playwright_context.chromium.connect_over_cdp(base_url, timeout=keepalive_ms)\n+        browsersteps_start_session['browser'] = browser\n+        browsersteps_start_session['playwright_context'] = playwright_context\n \n         proxy_id = datastore.get_preferred_proxy_for_watch(uuid=watch_uuid)\n         proxy = None\n@@ -75,15 +93,20 @@ def start_browsersteps_session(watch_uuid):\n                 logger.debug(f\"Browser Steps: UUID {watch_uuid} selected proxy {proxy_url}\")\n \n         # Tell Playwright to connect to Chrome and setup a new session via our stepper interface\n-        browsersteps_start_session['browserstepper'] = browser_steps.browsersteps_live_ui(\n-            playwright_browser=browsersteps_start_session['browser'],\n+        browserstepper = browser_steps.browsersteps_live_ui(\n+            playwright_browser=browser,\n             proxy=proxy,\n             start_url=datastore.data['watching'][watch_uuid].link,\n             headers=datastore.data['watching'][watch_uuid].get('headers')\n         )\n+        \n+        # Initialize the async connection\n+        await browserstepper.connect(proxy=proxy)\n+        \n+        browsersteps_start_session['browserstepper'] = browserstepper\n \n         # For test\n-        #browsersteps_start_session['browserstepper'].action_goto_url(value=\"http://example.com?time=\"+str(time.time()))\n+        #await browsersteps_start_session['browserstepper'].action_goto_url(value=\"http://example.com?time=\"+str(time.time()))\n \n         return browsersteps_start_session\n \n@@ -92,7 +115,7 @@ def start_browsersteps_session(watch_uuid):\n     @browser_steps_blueprint.route(\"/browsersteps_start_session\", methods=['GET'])\n     def browsersteps_start_session():\n         # A new session was requested, return sessionID\n-\n+        import asyncio\n         import uuid\n         browsersteps_session_id = str(uuid.uuid4())\n         watch_uuid = request.args.get('uuid')\n@@ -104,7 +127,10 @@ def browsersteps_start_session():\n         logger.debug(\"browser_steps.py connecting\")\n \n         try:\n-            browsersteps_sessions[browsersteps_session_id] = start_browsersteps_session(watch_uuid)\n+            # Run the async function in the dedicated browser steps event loop\n+            browsersteps_sessions[browsersteps_session_id] = run_async_in_browser_loop(\n+                start_browsersteps_session(watch_uuid)\n+            )\n         except Exception as e:\n             if 'ECONNREFUSED' in str(e):\n                 return make_response('Unable to start the Playwright Browser session, is sockpuppetbrowser running? Network configuration is OK?', 401)\n@@ -169,9 +195,14 @@ def browsersteps_ui_update():\n             is_last_step = strtobool(request.form.get('is_last_step'))\n \n             try:\n-                browsersteps_sessions[browsersteps_session_id]['browserstepper'].call_action(action_name=step_operation,\n-                                         selector=step_selector,\n-                                         optional_value=step_optional_value)\n+                # Run the async call_action method in the dedicated browser steps event loop\n+                run_async_in_browser_loop(\n+                    browsersteps_sessions[browsersteps_session_id]['browserstepper'].call_action(\n+                        action_name=step_operation,\n+                        selector=step_selector,\n+                        optional_value=step_optional_value\n+                    )\n+                )\n \n             except Exception as e:\n                 logger.error(f\"Exception when calling step operation {step_operation} {str(e)}\")\n@@ -185,7 +216,11 @@ def browsersteps_ui_update():\n \n         # Screenshots and other info only needed on requesting a step (POST)\n         try:\n-            (screenshot, xpath_data) = browsersteps_sessions[browsersteps_session_id]['browserstepper'].get_current_state()\n+            # Run the async get_current_state method in the dedicated browser steps event loop\n+            (screenshot, xpath_data) = run_async_in_browser_loop(\n+                browsersteps_sessions[browsersteps_session_id]['browserstepper'].get_current_state()\n+            )\n+                \n             if is_last_step:\n                 watch = datastore.data['watching'].get(uuid)\n                 u = browsersteps_sessions[browsersteps_session_id]['browserstepper'].page.url\n@@ -199,7 +234,6 @@ def browsersteps_ui_update():\n             return make_response(\"Error fetching screenshot and element data - \" + str(e), 401)\n \n         # SEND THIS BACK TO THE BROWSER\n-\n         output = {\n             \"screenshot\": f\"data:image/jpeg;base64,{base64.b64encode(screenshot).decode('ascii')}\",\n             \"xpath_data\": xpath_data,\ndiff --git a/changedetectionio/blueprint/browser_steps/browser_steps.py b/changedetectionio/blueprint/browser_steps/browser_steps.py\nindex d380d565670..e0a3cb2c1cf 100644\n--- a/changedetectionio/blueprint/browser_steps/browser_steps.py\n+++ b/changedetectionio/blueprint/browser_steps/browser_steps.py\n@@ -63,7 +63,7 @@ def __init__(self, start_url):\n         self.start_url = start_url\n \n     # Convert and perform \"Click Button\" for example\n-    def call_action(self, action_name, selector=None, optional_value=None):\n+    async def call_action(self, action_name, selector=None, optional_value=None):\n         if self.page is None:\n             logger.warning(\"Cannot call action on None page object\")\n             return\n@@ -93,73 +93,74 @@ def call_action(self, action_name, selector=None, optional_value=None):\n             optional_value = jinja_render(template_str=optional_value)\n \n \n-        action_handler(selector, optional_value)\n+        await action_handler(selector, optional_value)\n         # Safely wait for timeout\n-        self.page.wait_for_timeout(1.5 * 1000)\n+        await self.page.wait_for_timeout(1.5 * 1000)\n         logger.debug(f\"Call action done in {time.time()-now:.2f}s\")\n \n-    def action_goto_url(self, selector=None, value=None):\n+    async def action_goto_url(self, selector=None, value=None):\n         if not value:\n             logger.warning(\"No URL provided for goto_url action\")\n             return None\n             \n         now = time.time()\n-        response = self.page.goto(value, timeout=0, wait_until='load')\n+        response = await self.page.goto(value, timeout=0, wait_until='load')\n         logger.debug(f\"Time to goto URL {time.time()-now:.2f}s\")\n         return response\n \n     # Incase they request to go back to the start\n-    def action_goto_site(self, selector=None, value=None):\n-        return self.action_goto_url(value=self.start_url)\n+    async def action_goto_site(self, selector=None, value=None):\n+        return await self.action_goto_url(value=self.start_url)\n \n-    def action_click_element_containing_text(self, selector=None, value=''):\n+    async def action_click_element_containing_text(self, selector=None, value=''):\n         logger.debug(\"Clicking element containing text\")\n         if not value or not len(value.strip()):\n             return\n             \n         elem = self.page.get_by_text(value)\n-        if elem.count():\n-            elem.first.click(delay=randint(200, 500), timeout=self.action_timeout)\n+        if await elem.count():\n+            await elem.first.click(delay=randint(200, 500), timeout=self.action_timeout)\n \n \n-    def action_click_element_containing_text_if_exists(self, selector=None, value=''):\n+    async def action_click_element_containing_text_if_exists(self, selector=None, value=''):\n         logger.debug(\"Clicking element containing text if exists\")\n         if not value or not len(value.strip()):\n             return\n             \n         elem = self.page.get_by_text(value)\n-        logger.debug(f\"Clicking element containing text - {elem.count()} elements found\")\n-        if elem.count():\n-            elem.first.click(delay=randint(200, 500), timeout=self.action_timeout)\n+        count = await elem.count()\n+        logger.debug(f\"Clicking element containing text - {count} elements found\")\n+        if count:\n+            await elem.first.click(delay=randint(200, 500), timeout=self.action_timeout)\n                 \n \n-    def action_enter_text_in_field(self, selector, value):\n+    async def action_enter_text_in_field(self, selector, value):\n         if not selector or not len(selector.strip()):\n             return\n \n-        self.page.fill(selector, value, timeout=self.action_timeout)\n+        await self.page.fill(selector, value, timeout=self.action_timeout)\n \n-    def action_execute_js(self, selector, value):\n+    async def action_execute_js(self, selector, value):\n         if not value:\n             return None\n             \n-        return self.page.evaluate(value)\n+        return await self.page.evaluate(value)\n \n-    def action_click_element(self, selector, value):\n+    async def action_click_element(self, selector, value):\n         logger.debug(\"Clicking element\")\n         if not selector or not len(selector.strip()):\n             return\n \n-        self.page.click(selector=selector, timeout=self.action_timeout + 20 * 1000, delay=randint(200, 500))\n+        await self.page.click(selector=selector, timeout=self.action_timeout + 20 * 1000, delay=randint(200, 500))\n \n-    def action_click_element_if_exists(self, selector, value):\n+    async def action_click_element_if_exists(self, selector, value):\n         import playwright._impl._errors as _api_types\n         logger.debug(\"Clicking element if exists\")\n         if not selector or not len(selector.strip()):\n             return\n             \n         try:\n-            self.page.click(selector, timeout=self.action_timeout, delay=randint(200, 500))\n+            await self.page.click(selector, timeout=self.action_timeout, delay=randint(200, 500))\n         except _api_types.TimeoutError:\n             return\n         except _api_types.Error:\n@@ -167,7 +168,7 @@ def action_click_element_if_exists(self, selector, value):\n             return\n                 \n \n-    def action_click_x_y(self, selector, value):\n+    async def action_click_x_y(self, selector, value):\n         if not value or not re.match(r'^\\s?\\d+\\s?,\\s?\\d+\\s?$', value):\n             logger.warning(\"'Click X,Y' step should be in the format of '100 , 90'\")\n             return\n@@ -177,42 +178,42 @@ def action_click_x_y(self, selector, value):\n             x = int(float(x.strip()))\n             y = int(float(y.strip()))\n             \n-            self.page.mouse.click(x=x, y=y, delay=randint(200, 500))\n+            await self.page.mouse.click(x=x, y=y, delay=randint(200, 500))\n                 \n         except Exception as e:\n             logger.error(f\"Error parsing x,y coordinates: {str(e)}\")\n \n-    def action__select_by_option_text(self, selector, value):\n+    async def action__select_by_option_text(self, selector, value):\n         if not selector or not len(selector.strip()):\n             return\n \n-        self.page.select_option(selector, label=value, timeout=self.action_timeout)\n+        await self.page.select_option(selector, label=value, timeout=self.action_timeout)\n \n-    def action_scroll_down(self, selector, value):\n+    async def action_scroll_down(self, selector, value):\n         # Some sites this doesnt work on for some reason\n-        self.page.mouse.wheel(0, 600)\n-        self.page.wait_for_timeout(1000)\n+        await self.page.mouse.wheel(0, 600)\n+        await self.page.wait_for_timeout(1000)\n \n-    def action_wait_for_seconds(self, selector, value):\n+    async def action_wait_for_seconds(self, selector, value):\n         try:\n             seconds = float(value.strip()) if value else 1.0\n-            self.page.wait_for_timeout(seconds * 1000)\n+            await self.page.wait_for_timeout(seconds * 1000)\n         except (ValueError, TypeError) as e:\n             logger.error(f\"Invalid value for wait_for_seconds: {str(e)}\")\n \n-    def action_wait_for_text(self, selector, value):\n+    async def action_wait_for_text(self, selector, value):\n         if not value:\n             return\n             \n         import json\n         v = json.dumps(value)\n-        self.page.wait_for_function(\n+        await self.page.wait_for_function(\n             f'document.querySelector(\"body\").innerText.includes({v});',\n             timeout=30000\n         )\n             \n \n-    def action_wait_for_text_in_element(self, selector, value):\n+    async def action_wait_for_text_in_element(self, selector, value):\n         if not selector or not value:\n             return\n             \n@@ -220,49 +221,49 @@ def action_wait_for_text_in_element(self, selector, value):\n         s = json.dumps(selector)\n         v = json.dumps(value)\n         \n-        self.page.wait_for_function(\n+        await self.page.wait_for_function(\n             f'document.querySelector({s}).innerText.includes({v});',\n             timeout=30000\n         )\n \n     # @todo - in the future make some popout interface to capture what needs to be set\n     # https://playwright.dev/python/docs/api/class-keyboard\n-    def action_press_enter(self, selector, value):\n-        self.page.keyboard.press(\"Enter\", delay=randint(200, 500))\n+    async def action_press_enter(self, selector, value):\n+        await self.page.keyboard.press(\"Enter\", delay=randint(200, 500))\n             \n \n-    def action_press_page_up(self, selector, value):\n-        self.page.keyboard.press(\"PageUp\", delay=randint(200, 500))\n+    async def action_press_page_up(self, selector, value):\n+        await self.page.keyboard.press(\"PageUp\", delay=randint(200, 500))\n \n-    def action_press_page_down(self, selector, value):\n-        self.page.keyboard.press(\"PageDown\", delay=randint(200, 500))\n+    async def action_press_page_down(self, selector, value):\n+        await self.page.keyboard.press(\"PageDown\", delay=randint(200, 500))\n \n-    def action_check_checkbox(self, selector, value):\n+    async def action_check_checkbox(self, selector, value):\n         if not selector:\n             return\n \n-        self.page.locator(selector).check(timeout=self.action_timeout)\n+        await self.page.locator(selector).check(timeout=self.action_timeout)\n \n-    def action_uncheck_checkbox(self, selector, value):\n+    async def action_uncheck_checkbox(self, selector, value):\n         if not selector:\n             return\n             \n-        self.page.locator(selector).uncheck(timeout=self.action_timeout)\n+        await self.page.locator(selector).uncheck(timeout=self.action_timeout)\n             \n \n-    def action_remove_elements(self, selector, value):\n+    async def action_remove_elements(self, selector, value):\n         \"\"\"Removes all elements matching the given selector from the DOM.\"\"\"\n         if not selector:\n             return\n             \n-        self.page.locator(selector).evaluate_all(\"els => els.forEach(el => el.remove())\")\n+        await self.page.locator(selector).evaluate_all(\"els => els.forEach(el => el.remove())\")\n \n-    def action_make_all_child_elements_visible(self, selector, value):\n+    async def action_make_all_child_elements_visible(self, selector, value):\n         \"\"\"Recursively makes all child elements inside the given selector fully visible.\"\"\"\n         if not selector:\n             return\n             \n-        self.page.locator(selector).locator(\"*\").evaluate_all(\"\"\"\n+        await self.page.locator(selector).locator(\"*\").evaluate_all(\"\"\"\n             els => els.forEach(el => {\n                 el.style.display = 'block';   // Forces it to be displayed\n                 el.style.visibility = 'visible';   // Ensures it's not hidden\n@@ -307,21 +308,22 @@ def __init__(self, playwright_browser, proxy=None, headers=None, start_url=None)\n         self.playwright_browser = playwright_browser\n         self.start_url = start_url\n         self._is_cleaned_up = False\n-        if self.context is None:\n-            self.connect(proxy=proxy)\n+        self.proxy = proxy\n+        # Note: connect() is now async and must be called separately\n \n     def __del__(self):\n         # Ensure cleanup happens if object is garbage collected\n-        self.cleanup()\n+        # Note: cleanup is now async, so we can only mark as cleaned up here\n+        self._is_cleaned_up = True\n \n     # Connect and setup a new context\n-    def connect(self, proxy=None):\n+    async def connect(self, proxy=None):\n         # Should only get called once - test that\n         keep_open = 1000 * 60 * 5\n         now = time.time()\n \n         # @todo handle multiple contexts, bind a unique id from the browser on each req?\n-        self.context = self.playwright_browser.new_context(\n+        self.context = await self.playwright_browser.new_context(\n             accept_downloads=False,  # Should never be needed\n             bypass_csp=True,  # This is needed to enable JavaScript execution on GitHub and others\n             extra_http_headers=self.headers,\n@@ -332,7 +334,7 @@ def connect(self, proxy=None):\n             user_agent=manage_user_agent(headers=self.headers),\n         )\n \n-        self.page = self.context.new_page()\n+        self.page = await self.context.new_page()\n \n         # self.page.set_default_navigation_timeout(keep_open)\n         self.page.set_default_timeout(keep_open)\n@@ -342,13 +344,15 @@ def connect(self, proxy=None):\n         self.page.on(\"console\", lambda msg: print(f\"Browser steps console - {msg.type}: {msg.text} {msg.args}\"))\n \n         logger.debug(f\"Time to browser setup {time.time()-now:.2f}s\")\n-        self.page.wait_for_timeout(1 * 1000)\n+        await self.page.wait_for_timeout(1 * 1000)\n \n     def mark_as_closed(self):\n         logger.debug(\"Page closed, cleaning up..\")\n-        self.cleanup()\n+        # Note: This is called from a sync context (event handler)\n+        # so we'll just mark as cleaned up and let __del__ handle the rest\n+        self._is_cleaned_up = True\n \n-    def cleanup(self):\n+    async def cleanup(self):\n         \"\"\"Properly clean up all resources to prevent memory leaks\"\"\"\n         if self._is_cleaned_up:\n             return\n@@ -359,7 +363,7 @@ def cleanup(self):\n         if hasattr(self, 'page') and self.page is not None:\n             try:\n                 # Force garbage collection before closing\n-                self.page.request_gc()\n+                await self.page.request_gc()\n             except Exception as e:\n                 logger.debug(f\"Error during page garbage collection: {str(e)}\")\n                 \n@@ -370,7 +374,7 @@ def cleanup(self):\n                 logger.debug(f\"Error removing event listeners: {str(e)}\")\n                 \n             try:\n-                self.page.close()\n+                await self.page.close()\n             except Exception as e:\n                 logger.debug(f\"Error closing page: {str(e)}\")\n             \n@@ -379,7 +383,7 @@ def cleanup(self):\n         # Clean up context\n         if hasattr(self, 'context') and self.context is not None:\n             try:\n-                self.context.close()\n+                await self.context.close()\n             except Exception as e:\n                 logger.debug(f\"Error closing context: {str(e)}\")\n             \n@@ -401,12 +405,12 @@ def has_expired(self):\n             \n         return False\n \n-    def get_current_state(self):\n+    async def get_current_state(self):\n         \"\"\"Return the screenshot and interactive elements mapping, generally always called after action_()\"\"\"\n         import importlib.resources\n         import json\n         # because we for now only run browser steps in playwright mode (not puppeteer mode)\n-        from changedetectionio.content_fetchers.playwright import capture_full_page\n+        from changedetectionio.content_fetchers.playwright import capture_full_page_async\n \n         # Safety check - don't proceed if resources are cleaned up\n         if self._is_cleaned_up or self.page is None:\n@@ -416,29 +420,29 @@ def get_current_state(self):\n         xpath_element_js = importlib.resources.files(\"changedetectionio.content_fetchers.res\").joinpath('xpath_element_scraper.js').read_text()\n \n         now = time.time()\n-        self.page.wait_for_timeout(1 * 1000)\n+        await self.page.wait_for_timeout(1 * 1000)\n \n         screenshot = None\n         xpath_data = None\n         \n         try:\n             # Get screenshot first\n-            screenshot = capture_full_page(page=self.page)\n+            screenshot = await capture_full_page_async(page=self.page)\n             logger.debug(f\"Time to get screenshot from browser {time.time() - now:.2f}s\")\n \n             # Then get interactive elements\n             now = time.time()\n-            self.page.evaluate(\"var include_filters=''\")\n-            self.page.request_gc()\n+            await self.page.evaluate(\"var include_filters=''\")\n+            await self.page.request_gc()\n \n             scan_elements = 'a,button,input,select,textarea,i,th,td,p,li,h1,h2,h3,h4,div,span'\n \n             MAX_TOTAL_HEIGHT = int(os.getenv(\"SCREENSHOT_MAX_HEIGHT\", SCREENSHOT_MAX_HEIGHT_DEFAULT))\n-            xpath_data = json.loads(self.page.evaluate(xpath_element_js, {\n+            xpath_data = json.loads(await self.page.evaluate(xpath_element_js, {\n                 \"visualselector_xpath_selectors\": scan_elements,\n                 \"max_height\": MAX_TOTAL_HEIGHT\n             }))\n-            self.page.request_gc()\n+            await self.page.request_gc()\n \n             # Sort elements by size\n             xpath_data['size_pos'] = sorted(xpath_data['size_pos'], key=lambda k: k['width'] * k['height'], reverse=True)\n@@ -448,13 +452,13 @@ def get_current_state(self):\n             logger.error(f\"Error getting current state: {str(e)}\")\n             # Attempt recovery - force garbage collection\n             try:\n-                self.page.request_gc()\n+                await self.page.request_gc()\n             except:\n                 pass\n         \n         # Request garbage collection one final time\n         try:\n-            self.page.request_gc()\n+            await self.page.request_gc()\n         except:\n             pass\n             \ndiff --git a/changedetectionio/blueprint/browser_steps/nonContext.py b/changedetectionio/blueprint/browser_steps/nonContext.py\ndeleted file mode 100644\nindex 93abe269639..00000000000\n--- a/changedetectionio/blueprint/browser_steps/nonContext.py\n+++ /dev/null\n@@ -1,17 +0,0 @@\n-from playwright.sync_api import PlaywrightContextManager\n-\n-# So playwright wants to run as a context manager, but we do something horrible and hacky\n-# we are holding the session open for as long as possible, then shutting it down, and opening a new one\n-# So it means we don't get to use PlaywrightContextManager' __enter__ __exit__\n-# To work around this, make goodbye() act the same as the __exit__()\n-#\n-# But actually I think this is because the context is opened correctly with __enter__() but we timeout the connection\n-# then theres some lock condition where we cant destroy it without it hanging\n-\n-class c_PlaywrightContextManager(PlaywrightContextManager):\n-\n-    def goodbye(self) -> None:\n-        self.__exit__()\n-\n-def c_sync_playwright() -> PlaywrightContextManager:\n-    return c_PlaywrightContextManager()\ndiff --git a/changedetectionio/blueprint/imports/__init__.py b/changedetectionio/blueprint/imports/__init__.py\nindex 2e5fddf5b4e..e6fbf760fdf 100644\n--- a/changedetectionio/blueprint/imports/__init__.py\n+++ b/changedetectionio/blueprint/imports/__init__.py\n@@ -1,6 +1,7 @@\n from flask import Blueprint, request, redirect, url_for, flash, render_template\n from changedetectionio.store import ChangeDetectionStore\n from changedetectionio.auth_decorator import login_optionally_required\n+from changedetectionio import worker_handler\n from changedetectionio.blueprint.imports.importer import (\n     import_url_list, \n     import_distill_io_json, \n@@ -24,7 +25,7 @@ def import_page():\n                 importer_handler = import_url_list()\n                 importer_handler.run(data=request.values.get('urls'), flash=flash, datastore=datastore, processor=request.values.get('processor', 'text_json_diff'))\n                 for uuid in importer_handler.new_uuids:\n-                    update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n+                    worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n \n                 if len(importer_handler.remaining_data) == 0:\n                     return redirect(url_for('watchlist.index'))\n@@ -37,7 +38,7 @@ def import_page():\n                 d_importer = import_distill_io_json()\n                 d_importer.run(data=request.values.get('distill-io'), flash=flash, datastore=datastore)\n                 for uuid in d_importer.new_uuids:\n-                    update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n+                    worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n \n             # XLSX importer\n             if request.files and request.files.get('xlsx_file'):\n@@ -60,7 +61,7 @@ def import_page():\n                     w_importer.run(data=file, flash=flash, datastore=datastore)\n \n                 for uuid in w_importer.new_uuids:\n-                    update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n+                    worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n \n         # Could be some remaining, or we could be on GET\n         form = forms.importForm(formdata=request.form if request.method == 'POST' else None)\ndiff --git a/changedetectionio/blueprint/price_data_follower/__init__.py b/changedetectionio/blueprint/price_data_follower/__init__.py\nindex 99841d71529..c2c6e768c8a 100644\n--- a/changedetectionio/blueprint/price_data_follower/__init__.py\n+++ b/changedetectionio/blueprint/price_data_follower/__init__.py\n@@ -4,6 +4,7 @@\n from flask_login import login_required\n from changedetectionio.store import ChangeDetectionStore\n from changedetectionio import queuedWatchMetaData\n+from changedetectionio import worker_handler\n from queue import PriorityQueue\n \n PRICE_DATA_TRACK_ACCEPT = 'accepted'\n@@ -19,7 +20,7 @@ def accept(uuid):\n         datastore.data['watching'][uuid]['track_ldjson_price_data'] = PRICE_DATA_TRACK_ACCEPT\n         datastore.data['watching'][uuid]['processor'] = 'restock_diff'\n         datastore.data['watching'][uuid].clear_watch()\n-        update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n+        worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n         return redirect(url_for(\"watchlist.index\"))\n \n     @login_required\ndiff --git a/changedetectionio/blueprint/settings/__init__.py b/changedetectionio/blueprint/settings/__init__.py\nindex 015cc274950..548a5b701f9 100644\n--- a/changedetectionio/blueprint/settings/__init__.py\n+++ b/changedetectionio/blueprint/settings/__init__.py\n@@ -67,7 +67,32 @@ def settings_page():\n                     del (app_update['password'])\n \n                 datastore.data['settings']['application'].update(app_update)\n+                \n+                # Handle dynamic worker count adjustment\n+                old_worker_count = datastore.data['settings']['requests'].get('workers', 1)\n+                new_worker_count = form.data['requests'].get('workers', 1)\n+                \n                 datastore.data['settings']['requests'].update(form.data['requests'])\n+                \n+                # Adjust worker count if it changed\n+                if new_worker_count != old_worker_count:\n+                    from changedetectionio import worker_handler\n+                    from changedetectionio.flask_app import update_q, notification_q, app, datastore as ds\n+                    \n+                    result = worker_handler.adjust_async_worker_count(\n+                        new_count=new_worker_count,\n+                        update_q=update_q,\n+                        notification_q=notification_q,\n+                        app=app,\n+                        datastore=ds\n+                    )\n+                    \n+                    if result['status'] == 'success':\n+                        flash(f\"Worker count adjusted: {result['message']}\", 'notice')\n+                    elif result['status'] == 'not_supported':\n+                        flash(\"Dynamic worker adjustment not supported for sync workers\", 'warning')\n+                    elif result['status'] == 'error':\n+                        flash(f\"Error adjusting workers: {result['message']}\", 'error')\n \n                 if not os.getenv(\"SALTED_PASS\", False) and len(form.application.form.password.encrypted_password):\n                     datastore.data['settings']['application']['password'] = form.application.form.password.encrypted_password\ndiff --git a/changedetectionio/blueprint/settings/templates/settings.html b/changedetectionio/blueprint/settings/templates/settings.html\nindex 5f302331973..88ebd6de77b 100644\n--- a/changedetectionio/blueprint/settings/templates/settings.html\n+++ b/changedetectionio/blueprint/settings/templates/settings.html\n@@ -135,6 +135,12 @@\n                         {{ render_field(form.application.form.webdriver_delay) }}\n                     </div>\n                 </fieldset>\n+                <div class=\"pure-control-group\">\n+                    {{ render_field(form.requests.form.workers) }}\n+                    {% set worker_info = get_worker_status_info() %}\n+                    <span class=\"pure-form-message-inline\">Number of concurrent workers to process watches. More workers = faster processing but higher memory usage.<br>\n+                    Currently running: <strong>{{ worker_info.count }}</strong> operational {{ worker_info.type }} workers{% if worker_info.active_workers > 0 %} ({{ worker_info.active_workers }} actively processing){% endif %}.</span>\n+                </div>\n                 <div class=\"pure-control-group inline-radio\">\n                     {{ render_field(form.requests.form.default_ua) }}\n                     <span class=\"pure-form-message-inline\">\n@@ -247,9 +253,9 @@ <h4>Chrome Extension</h4>\n                     <span class=\"pure-form-message-inline\">Enable this setting to open the diff page in a new tab. If disabled, the diff page will open in the current tab.</span>\n                 </div>\n                 <div class=\"pure-control-group\">\n-                    <span class=\"pure-form-message-inline\">Enable realtime updates in the UI</span>\n+                    {{ render_checkbox_field(form.application.form.ui.form.socket_io_enabled, class=\"socket_io_enabled\") }}\n+                    <span class=\"pure-form-message-inline\">Realtime UI Updates Enabled - (Restart required if this is changed)</span>\n                 </div>\n-\n             </div>\n             <div class=\"tab-pane-inner\" id=\"proxies\">\n                 <div id=\"recommended-proxy\">\ndiff --git a/changedetectionio/blueprint/ui/__init__.py b/changedetectionio/blueprint/ui/__init__.py\nindex c9061bf77ac..9ed40554a4b 100644\n--- a/changedetectionio/blueprint/ui/__init__.py\n+++ b/changedetectionio/blueprint/ui/__init__.py\n@@ -1,15 +1,13 @@\n import time\n from flask import Blueprint, request, redirect, url_for, flash, render_template, session\n from loguru import logger\n-from functools import wraps\n \n-from changedetectionio.blueprint.ui.ajax import constuct_ui_ajax_blueprint\n from changedetectionio.store import ChangeDetectionStore\n from changedetectionio.blueprint.ui.edit import construct_blueprint as construct_edit_blueprint\n from changedetectionio.blueprint.ui.notification import construct_blueprint as construct_notification_blueprint\n from changedetectionio.blueprint.ui.views import construct_blueprint as construct_views_blueprint\n \n-def construct_blueprint(datastore: ChangeDetectionStore, update_q, running_update_threads, queuedWatchMetaData, watch_check_update):\n+def construct_blueprint(datastore: ChangeDetectionStore, update_q, worker_handler, queuedWatchMetaData, watch_check_update):\n     ui_blueprint = Blueprint('ui', __name__, template_folder=\"templates\")\n     \n     # Register the edit blueprint\n@@ -24,9 +22,6 @@ def construct_blueprint(datastore: ChangeDetectionStore, update_q, running_updat\n     views_blueprint = construct_views_blueprint(datastore, update_q, queuedWatchMetaData, watch_check_update)\n     ui_blueprint.register_blueprint(views_blueprint)\n \n-    ui_ajax_blueprint = constuct_ui_ajax_blueprint(datastore, update_q, running_update_threads, queuedWatchMetaData, watch_check_update)\n-    ui_blueprint.register_blueprint(ui_ajax_blueprint)\n-\n     # Import the login decorator\n     from changedetectionio.auth_decorator import login_optionally_required\n \n@@ -100,7 +95,7 @@ def form_clone():\n         new_uuid = datastore.clone(uuid)\n \n         if not datastore.data['watching'].get(uuid).get('paused'):\n-            update_q.put(queuedWatchMetaData.PrioritizedItem(priority=5, item={'uuid': new_uuid}))\n+            worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=5, item={'uuid': new_uuid}))\n \n         flash('Cloned, you are editing the new watch.')\n \n@@ -116,13 +111,11 @@ def form_watch_checknow():\n \n         i = 0\n \n-        running_uuids = []\n-        for t in running_update_threads:\n-            running_uuids.append(t.current_uuid)\n+        running_uuids = worker_handler.get_running_uuids()\n \n         if uuid:\n             if uuid not in running_uuids:\n-                update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n+                worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n                 i += 1\n \n         else:\n@@ -139,7 +132,7 @@ def form_watch_checknow():\n                         if tag != None and tag not in watch['tags']:\n                             continue\n \n-                        update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': watch_uuid}))\n+                        worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': watch_uuid}))\n                         i += 1\n \n         if i == 1:\n@@ -197,7 +190,7 @@ def form_watch_list_checkbox_operations():\n             for uuid in uuids:\n                 if datastore.data['watching'].get(uuid):\n                     # Recheck and require a full reprocessing\n-                    update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n+                    worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n             flash(\"{} watches queued for rechecking\".format(len(uuids)))\n \n         elif (op == 'clear-errors'):\ndiff --git a/changedetectionio/blueprint/ui/ajax.py b/changedetectionio/blueprint/ui/ajax.py\ndeleted file mode 100644\nindex bbe3464d6ae..00000000000\n--- a/changedetectionio/blueprint/ui/ajax.py\n+++ /dev/null\n@@ -1,35 +0,0 @@\n-import time\n-\n-from blinker import signal\n-from flask import Blueprint, request, redirect, url_for, flash, render_template, session\n-\n-\n-from changedetectionio.store import ChangeDetectionStore\n-\n-def constuct_ui_ajax_blueprint(datastore: ChangeDetectionStore, update_q, running_update_threads, queuedWatchMetaData, watch_check_update):\n-    ui_ajax_blueprint = Blueprint('ajax', __name__, template_folder=\"templates\", url_prefix='/ajax')\n-\n-    # Import the login decorator\n-    from changedetectionio.auth_decorator import login_optionally_required\n-\n-    @ui_ajax_blueprint.route(\"/toggle\", methods=['POST'])\n-    @login_optionally_required\n-    def ajax_toggler():\n-        op = request.values.get('op')\n-        uuid = request.values.get('uuid')\n-        if op and datastore.data['watching'].get(uuid):\n-            if op == 'pause':\n-                datastore.data['watching'][uuid].toggle_pause()\n-            elif op == 'mute':\n-                datastore.data['watching'][uuid].toggle_mute()\n-            elif op == 'recheck':\n-                update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n-\n-            watch_check_update = signal('watch_check_update')\n-            if watch_check_update:\n-                watch_check_update.send(watch_uuid=uuid)\n-\n-        return 'OK'\n-\n-\n-    return ui_ajax_blueprint\ndiff --git a/changedetectionio/blueprint/ui/edit.py b/changedetectionio/blueprint/ui/edit.py\nindex b491d854908..bdee4725625 100644\n--- a/changedetectionio/blueprint/ui/edit.py\n+++ b/changedetectionio/blueprint/ui/edit.py\n@@ -9,6 +9,7 @@\n from changedetectionio.store import ChangeDetectionStore\n from changedetectionio.auth_decorator import login_optionally_required\n from changedetectionio.time_handler import is_within_schedule\n+from changedetectionio import worker_handler\n \n def construct_blueprint(datastore: ChangeDetectionStore, update_q, queuedWatchMetaData):\n     edit_blueprint = Blueprint('ui_edit', __name__, template_folder=\"../ui/templates\")\n@@ -201,7 +202,7 @@ def edit_page(uuid):\n             #############################\n             if not datastore.data['watching'][uuid].get('paused') and is_in_schedule:\n                 # Queue the watch for immediate recheck, with a higher priority\n-                update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n+                worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n \n             # Diff page [edit] link should go back to diff page\n             if request.args.get(\"next\") and request.args.get(\"next\") == 'diff':\ndiff --git a/changedetectionio/blueprint/ui/views.py b/changedetectionio/blueprint/ui/views.py\nindex efcdc03a491..7954a19710e 100644\n--- a/changedetectionio/blueprint/ui/views.py\n+++ b/changedetectionio/blueprint/ui/views.py\n@@ -7,6 +7,7 @@\n from changedetectionio.store import ChangeDetectionStore\n from changedetectionio.auth_decorator import login_optionally_required\n from changedetectionio import html_tools\n+from changedetectionio import worker_handler\n \n def construct_blueprint(datastore: ChangeDetectionStore, update_q, queuedWatchMetaData, watch_check_update):\n     views_blueprint = Blueprint('ui_views', __name__, template_folder=\"../ui/templates\")\n@@ -212,7 +213,7 @@ def form_quick_watch_add():\n                 return redirect(url_for('ui.ui_edit.edit_page', uuid=new_uuid, unpause_on_save=1, tag=request.args.get('tag')))\n             else:\n                 # Straight into the queue.\n-                update_q.put(queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': new_uuid}))\n+                worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': new_uuid}))\n                 flash(\"Watch added.\")\n \n         return redirect(url_for('watchlist.index', tag=request.args.get('tag','')))\ndiff --git a/changedetectionio/blueprint/watchlist/__init__.py b/changedetectionio/blueprint/watchlist/__init__.py\nindex bd3b6c988d4..8cd5423afef 100644\n--- a/changedetectionio/blueprint/watchlist/__init__.py\n+++ b/changedetectionio/blueprint/watchlist/__init__.py\n@@ -78,7 +78,6 @@ def index():\n             active_tag=active_tag,\n             active_tag_uuid=active_tag_uuid,\n             app_rss_token=datastore.data['settings']['application'].get('rss_access_token'),\n-            ajax_toggle_url=url_for('ui.ajax.ajax_toggler'),\n             datastore=datastore,\n             errored_count=errored_count,\n             form=form,\ndiff --git a/changedetectionio/blueprint/watchlist/templates/watch-overview.html b/changedetectionio/blueprint/watchlist/templates/watch-overview.html\nindex 49fd2bd3ed0..728a204cb65 100644\n--- a/changedetectionio/blueprint/watchlist/templates/watch-overview.html\n+++ b/changedetectionio/blueprint/watchlist/templates/watch-overview.html\n@@ -1,11 +1,15 @@\n-{% extends 'base.html' %}\n-{% block content %}\n-{% from '_helpers.html' import render_simple_field, render_field, render_nolabel_field, sort_by_title %}\n+{%- extends 'base.html' -%}\n+{%- block content -%}\n+{%- from '_helpers.html' import render_simple_field, render_field, render_nolabel_field, sort_by_title -%}\n <script src=\"{{url_for('static_content', group='js', filename='jquery-3.6.0.min.js')}}\"></script>\n <script src=\"{{url_for('static_content', group='js', filename='watch-overview.js')}}\" defer></script>\n <script>let nowtimeserver={{ now_time_server }};</script>\n-<script>let ajax_toggle_url=\"{{ ajax_toggle_url }}\";</script>\n-\n+<script>\n+// Initialize Feather icons after the page loads\n+document.addEventListener('DOMContentLoaded', function() {\n+    feather.replace();\n+});\n+</script>\n <style>\n .checking-now .last-checked {\n     background-image: linear-gradient(to bottom, transparent 0%, rgba(0,0,0,0.05) 40%, rgba(0,0,0,0.1) 100%);\n@@ -39,139 +43,143 @@\n     <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\" >\n     <input type=\"hidden\" id=\"op_extradata\" name=\"op_extradata\" value=\"\" >\n     <div id=\"checkbox-operations\">\n-        <button class=\"pure-button button-secondary button-xsmall\"  name=\"op\" value=\"pause\">Pause</button>\n-        <button class=\"pure-button button-secondary button-xsmall\"  name=\"op\" value=\"unpause\">UnPause</button>\n-        <button class=\"pure-button button-secondary button-xsmall\"  name=\"op\" value=\"mute\">Mute</button>\n-        <button class=\"pure-button button-secondary button-xsmall\"  name=\"op\" value=\"unmute\">UnMute</button>\n-        <button class=\"pure-button button-secondary button-xsmall\" name=\"op\" value=\"recheck\">Recheck</button>\n-        <button class=\"pure-button button-secondary button-xsmall\" name=\"op\" value=\"assign-tag\" id=\"checkbox-assign-tag\">Tag</button>\n-        <button class=\"pure-button button-secondary button-xsmall\" name=\"op\" value=\"mark-viewed\">Mark viewed</button>\n-        <button class=\"pure-button button-secondary button-xsmall\" name=\"op\" value=\"notification-default\">Use default notification</button>\n-        <button class=\"pure-button button-secondary button-xsmall\" name=\"op\" value=\"clear-errors\">Clear errors</button>\n-        <button class=\"pure-button button-secondary button-xsmall\" style=\"background: #dd4242;\" name=\"op\" value=\"clear-history\">Clear/reset history</button>\n-        <button class=\"pure-button button-secondary button-xsmall\" style=\"background: #dd4242;\" name=\"op\" value=\"delete\">Delete</button>\n+        <button class=\"pure-button button-secondary button-xsmall\"  name=\"op\" value=\"pause\"><i data-feather=\"pause\" style=\"width: 14px; height: 14px; stroke: white; margin-right: 4px;\"></i>Pause</button>\n+        <button class=\"pure-button button-secondary button-xsmall\"  name=\"op\" value=\"unpause\"><i data-feather=\"play\" style=\"width: 14px; height: 14px; stroke: white; margin-right: 4px;\"></i>UnPause</button>\n+        <button class=\"pure-button button-secondary button-xsmall\"  name=\"op\" value=\"mute\"><i data-feather=\"volume-x\" style=\"width: 14px; height: 14px; stroke: white; margin-right: 4px;\"></i>Mute</button>\n+        <button class=\"pure-button button-secondary button-xsmall\"  name=\"op\" value=\"unmute\"><i data-feather=\"volume-2\" style=\"width: 14px; height: 14px; stroke: white; margin-right: 4px;\"></i>UnMute</button>\n+        <button class=\"pure-button button-secondary button-xsmall\" name=\"op\" value=\"recheck\"><i data-feather=\"refresh-cw\" style=\"width: 14px; height: 14px; stroke: white; margin-right: 4px;\"></i>Recheck</button>\n+        <button class=\"pure-button button-secondary button-xsmall\" name=\"op\" value=\"assign-tag\" id=\"checkbox-assign-tag\"><i data-feather=\"tag\" style=\"width: 14px; height: 14px; stroke: white; margin-right: 4px;\"></i>Tag</button>\n+        <button class=\"pure-button button-secondary button-xsmall\" name=\"op\" value=\"mark-viewed\"><i data-feather=\"eye\" style=\"width: 14px; height: 14px; stroke: white; margin-right: 4px;\"></i>Mark viewed</button>\n+        <button class=\"pure-button button-secondary button-xsmall\" name=\"op\" value=\"notification-default\"><i data-feather=\"bell\" style=\"width: 14px; height: 14px; stroke: white; margin-right: 4px;\"></i>Use default notification</button>\n+        <button class=\"pure-button button-secondary button-xsmall\" name=\"op\" value=\"clear-errors\"><i data-feather=\"x-circle\" style=\"width: 14px; height: 14px; stroke: white; margin-right: 4px;\"></i>Clear errors</button>\n+        <button class=\"pure-button button-secondary button-xsmall\" style=\"background: #dd4242;\" name=\"op\" value=\"clear-history\"><i data-feather=\"trash-2\" style=\"width: 14px; height: 14px; stroke: white; margin-right: 4px;\"></i>Clear/reset history</button>\n+        <button class=\"pure-button button-secondary button-xsmall\" style=\"background: #dd4242;\" name=\"op\" value=\"delete\"><i data-feather=\"trash\" style=\"width: 14px; height: 14px; stroke: white; margin-right: 4px;\"></i>Delete</button>\n     </div>\n-    {% if watches|length >= pagination.per_page %}\n+    {%- if watches|length >= pagination.per_page -%}\n         {{ pagination.info }}\n-    {% endif %}\n-    {% if search_q %}<div id=\"search-result-info\">Searching \"<strong><i>{{search_q}}</i></strong>\"</div>{% endif %}\n+    {%- endif -%}\n+    {%- if search_q -%}<div id=\"search-result-info\">Searching \"<strong><i>{{search_q}}</i></strong>\"</div>{%- endif -%}\n     <div>\n         <a href=\"{{url_for('watchlist.index')}}\" class=\"pure-button button-tag {{'active' if not active_tag_uuid }}\">All</a>\n \n     <!-- tag list -->\n-    {% for uuid, tag in tags %}\n-        {% if tag != \"\" %}\n+    {%- for uuid, tag in tags -%}\n+        {%- if tag != \"\" -%}\n             <a href=\"{{url_for('watchlist.index', tag=uuid) }}\" class=\"pure-button button-tag {{'active' if active_tag_uuid == uuid }}\">{{ tag.title }}</a>\n-        {% endif %}\n-    {% endfor %}\n+        {%- endif -%}\n+    {%- endfor -%}\n     </div>\n \n-    {% set sort_order = sort_order or 'asc' %}\n-    {% set sort_attribute = sort_attribute or 'last_changed'  %}\n-    {% set pagination_page = request.args.get('page', 0) %}\n-    {% set cols_required = 6 %}\n-    {% set any_has_restock_price_processor = datastore.any_watches_have_processor_by_name(\"restock_diff\") %}\n-    {% if any_has_restock_price_processor %}\n-        {% set cols_required = cols_required + 1 %}\n-    {% endif %}\n+    {%- set sort_order = sort_order or 'asc' -%}\n+    {%- set sort_attribute = sort_attribute or 'last_changed'  -%}\n+    {%- set pagination_page = request.args.get('page', 0) -%}\n+    {%- set cols_required = 6 -%}\n+    {%- set any_has_restock_price_processor = datastore.any_watches_have_processor_by_name(\"restock_diff\") -%}\n+    {%- if any_has_restock_price_processor -%}\n+        {%- set cols_required = cols_required + 1 -%}\n+    {%- endif -%}\n \n     <div id=\"watch-table-wrapper\">\n \n         <table class=\"pure-table pure-table-striped watch-table\">\n             <thead>\n             <tr>\n-                {% set link_order = \"desc\" if sort_order  == 'asc' else \"asc\" %}\n-                {% set arrow_span = \"\" %}\n+                {%- set link_order = \"desc\" if sort_order  == 'asc' else \"asc\" -%}\n+                {%- set arrow_span = \"\" -%}\n                 <th><input style=\"vertical-align: middle\" type=\"checkbox\" id=\"check-all\" > <a class=\"{{ 'active '+link_order if sort_attribute == 'date_created' else 'inactive' }}\"  href=\"{{url_for('watchlist.index', sort='date_created', order=link_order, tag=active_tag_uuid)}}\"># <span class='arrow {{link_order}}'></span></a></th>\n                 <th class=\"empty-cell\"></th>\n                 <th><a class=\"{{ 'active '+link_order if sort_attribute == 'label' else 'inactive' }}\" href=\"{{url_for('watchlist.index', sort='label', order=link_order, tag=active_tag_uuid)}}\">Website <span class='arrow {{link_order}}'></span></a></th>\n-             {% if any_has_restock_price_processor %}\n+             {%- if any_has_restock_price_processor -%}\n                 <th>Restock &amp; Price</th>\n-             {% endif %}\n+             {%- endif -%}\n                 <th><a class=\"{{ 'active '+link_order if sort_attribute == 'last_checked' else 'inactive' }}\" href=\"{{url_for('watchlist.index', sort='last_checked', order=link_order, tag=active_tag_uuid)}}\"><span class=\"hide-on-mobile\">Last</span> Checked <span class='arrow {{link_order}}'></span></a></th>\n                 <th><a class=\"{{ 'active '+link_order if sort_attribute == 'last_changed' else 'inactive' }}\" href=\"{{url_for('watchlist.index', sort='last_changed', order=link_order, tag=active_tag_uuid)}}\"><span class=\"hide-on-mobile\">Last</span> Changed <span class='arrow {{link_order}}'></span></a></th>\n                 <th class=\"empty-cell\"></th>\n             </tr>\n             </thead>\n             <tbody>\n-            {% if not watches|length %}\n+            {%- if not watches|length -%}\n             <tr>\n                 <td colspan=\"{{ cols_required }}\" style=\"text-wrap: wrap;\">No website watches configured, please add a URL in the box above, or <a href=\"{{ url_for('imports.import_page')}}\" >import a list</a>.</td>\n             </tr>\n-            {% endif %}\n-            {% for watch in (watches|sort(attribute=sort_attribute, reverse=sort_order == 'asc'))|pagination_slice(skip=pagination.skip) %}\n-\n-                {% set checking_now = is_checking_now(watch) %}\n-            <tr id=\"{{ watch.uuid }}\" data-watch-uuid=\"{{ watch.uuid }}\"\n-                class=\"{{ loop.cycle('pure-table-odd', 'pure-table-even') }} processor-{{ watch['processor'] }}\n-                {# realtime.js also sets these vars on the row for update #}\n-                {% if watch.compile_error_texts()|length >2 %}has-error{% endif %}\n-                {% if watch.paused is defined and watch.paused != False %}paused{% endif %}\n-                {% if watch.has_unviewed %}unviewed{% endif %}\n-                {% if watch.has_restock_info %} has-restock-info {% if watch['restock']['in_stock'] %}in-stock{% else %}not-in-stock{% endif %} {% else %}no-restock-info{% endif %}\n-                {% if watch.uuid in queued_uuids %}queued{% endif %}\n-                {% if checking_now %}checking-now{% endif %}\n-                {% if watch.notification_muted %}notification_muted{% endif %}\n-                \">\n+            {%- endif -%}\n+            {%- for watch in (watches|sort(attribute=sort_attribute, reverse=sort_order == 'asc'))|pagination_slice(skip=pagination.skip) -%}\n+                {%- set checking_now = is_checking_now(watch) -%}\n+                {%- set history_n = watch.history_n -%}\n+                {#  Mirror in changedetectionio/static/js/realtime.js for the frontend #}\n+                {%- set row_classes = [\n+                    loop.cycle('pure-table-odd', 'pure-table-even'),\n+                    'processor-' ~ watch['processor'],\n+                    'has-error' if watch.compile_error_texts()|length > 2 else '',\n+                    'paused' if watch.paused is defined and watch.paused != False else '',\n+                    'unviewed' if watch.has_unviewed else '',\n+                    'has-restock-info' if watch.has_restock_info else 'no-restock-info',\n+                    'in-stock' if watch.has_restock_info and watch['restock']['in_stock'] else '',\n+                    'not-in-stock' if watch.has_restock_info and not watch['restock']['in_stock'] else '',\n+                    'queued' if watch.uuid in queued_uuids else '',\n+                    'checking-now' if checking_now else '',\n+                    'notification_muted' if watch.notification_muted else '',\n+                    'single-history' if history_n == 1 else '',\n+                    'multiple-history' if history_n >= 2 else ''\n+                ] -%}\n+            <tr id=\"{{ watch.uuid }}\" data-watch-uuid=\"{{ watch.uuid }}\" class=\"{{ row_classes | reject('equalto', '') | join(' ') }}\">\n                 <td class=\"inline checkbox-uuid\" ><input name=\"uuids\"  type=\"checkbox\" value=\"{{ watch.uuid}} \" > <span>{{ loop.index+pagination.skip }}</span></td>\n                 <td class=\"inline watch-controls\">\n                     <a class=\"ajax-op state-off pause-toggle\" data-op=\"pause\" href=\"{{url_for('watchlist.index', op='pause', uuid=watch.uuid, tag=active_tag_uuid)}}\"><img src=\"{{url_for('static_content', group='images', filename='pause.svg')}}\" alt=\"Pause checks\" title=\"Pause checks\" class=\"icon icon-pause\" ></a>\n                     <a class=\"ajax-op state-on pause-toggle\"  data-op=\"pause\" style=\"display: none\" href=\"{{url_for('watchlist.index', op='pause', uuid=watch.uuid, tag=active_tag_uuid)}}\"><img src=\"{{url_for('static_content', group='images', filename='play.svg')}}\" alt=\"UnPause checks\" title=\"UnPause checks\" class=\"icon icon-unpause\" ></a>\n-\n                     <a class=\"ajax-op state-off mute-toggle\" data-op=\"mute\" href=\"{{url_for('watchlist.index', op='mute', uuid=watch.uuid, tag=active_tag_uuid)}}\"><img src=\"{{url_for('static_content', group='images', filename='bell-off.svg')}}\" alt=\"Mute notification\" title=\"Mute notification\" class=\"icon icon-mute\" ></a>\n                     <a class=\"ajax-op state-on mute-toggle\" data-op=\"mute\"  style=\"display: none\" href=\"{{url_for('watchlist.index', op='mute', uuid=watch.uuid, tag=active_tag_uuid)}}\"><img src=\"{{url_for('static_content', group='images', filename='bell-off.svg')}}\" alt=\"UnMute notification\" title=\"UnMute notification\" class=\"icon icon-mute\" ></a>\n                 </td>\n                 <td class=\"title-col inline\">{{watch.title if watch.title is not none and watch.title|length > 0 else watch.url}}\n-                    <a class=\"external\" target=\"_blank\" rel=\"noopener\" href=\"{{ watch.link.replace('source:','') }}\"></a>\n+                    <a class=\"external\" target=\"_blank\" rel=\"noopener\" href=\"{{ watch.link.replace('source:','') }}\"><i data-feather=\"external-link\"></i></a>\n                     <a class=\"link-spread\" href=\"{{url_for('ui.form_share_put_watch', uuid=watch.uuid)}}\"><img src=\"{{url_for('static_content', group='images', filename='spread.svg')}}\" class=\"status-icon icon icon-spread\" title=\"Create a link to share watch config with others\" ></a>\n \n-                    {% if watch.get_fetch_backend == \"html_webdriver\"\n+                    {%- if watch.get_fetch_backend == \"html_webdriver\"\n                          or ( watch.get_fetch_backend == \"system\" and system_default_fetcher == 'html_webdriver'  )\n                          or \"extra_browser_\" in watch.get_fetch_backend\n-                    %}\n+                    -%}\n                     <img class=\"status-icon\" src=\"{{url_for('static_content', group='images', filename='google-chrome-icon.png')}}\" alt=\"Using a Chrome browser\" title=\"Using a Chrome browser\" >\n-                    {% endif %}\n+                    {%- endif -%}\n \n-                    {% if watch.is_pdf  %}<img class=\"status-icon\" src=\"{{url_for('static_content', group='images', filename='pdf-icon.svg')}}\" alt=\"Converting PDF to text\" >{% endif %}\n-                    {% if watch.has_browser_steps %}<img class=\"status-icon status-browsersteps\" src=\"{{url_for('static_content', group='images', filename='steps.svg')}}\" alt=\"Browser Steps is enabled\" >{% endif %}\n+                    {%- if watch.is_pdf  -%}<img class=\"status-icon\" src=\"{{url_for('static_content', group='images', filename='pdf-icon.svg')}}\" alt=\"Converting PDF to text\" >{%- endif -%}\n+                    {%- if watch.has_browser_steps -%}<img class=\"status-icon status-browsersteps\" src=\"{{url_for('static_content', group='images', filename='steps.svg')}}\" alt=\"Browser Steps is enabled\" >{%- endif -%}\n \n                     <div class=\"error-text\" style=\"display:none;\">{{ watch.compile_error_texts(has_proxies=datastore.proxy_list)|safe }}</div>\n \n-                    {% if watch['processor'] == 'text_json_diff'  %}\n-                        {% if watch['has_ldjson_price_data'] and not watch['track_ldjson_price_data']  %}\n+                    {%- if watch['processor'] == 'text_json_diff'  -%}\n+                        {%- if watch['has_ldjson_price_data'] and not watch['track_ldjson_price_data']  -%}\n                         <div class=\"ldjson-price-track-offer\">Switch to Restock & Price watch mode? <a href=\"{{url_for('price_data_follower.accept', uuid=watch.uuid)}}\" class=\"pure-button button-xsmall\">Yes</a> <a href=\"{{url_for('price_data_follower.reject', uuid=watch.uuid)}}\" class=\"\">No</a></div>\n-                        {% endif %}\n-                    {% endif %}\n-                    {% if watch['processor'] == 'restock_diff' %}\n+                        {%- endif -%}\n+                    {%- endif -%}\n+                    {%- if watch['processor'] == 'restock_diff' -%}\n                         <span class=\"tracking-ldjson-price-data\" title=\"Automatically following embedded price information\"><img src=\"{{url_for('static_content', group='images', filename='price-tag-icon.svg')}}\"  class=\"status-icon price-follow-tag-icon\" > Price</span>\n-                    {% endif %}\n-                    {% for watch_tag_uuid, watch_tag in datastore.get_all_tags_for_watch(watch['uuid']).items() %}\n+                    {%- endif -%}\n+                    {%- for watch_tag_uuid, watch_tag in datastore.get_all_tags_for_watch(watch['uuid']).items() -%}\n                       <span class=\"watch-tag-list\">{{ watch_tag.title }}</span>\n-                    {% endfor %}\n+                    {%- endfor -%}\n                 </td>\n-            <!-- @todo make it so any watch handler obj can expose this --->\n-{% if any_has_restock_price_processor %}\n+{%- if any_has_restock_price_processor -%}\n                 <td class=\"restock-and-price\">\n-                    {% if watch['processor'] == 'restock_diff'  %}\n-                        {% if watch.has_restock_info %}\n+                    {%- if watch['processor'] == 'restock_diff'  -%}\n+                        {%- if watch.has_restock_info -%}\n                             <span class=\"restock-label {{'in-stock' if watch['restock']['in_stock'] else 'not-in-stock' }}\" title=\"Detecting restock and price\">\n                                 <!-- maybe some object watch['processor'][restock_diff] or.. -->\n-                                 {% if watch['restock']['in_stock'] %} In stock {% else %} Not in stock {% endif %}\n+                                 {%- if watch['restock']['in_stock']-%}  In stock {%- else-%}  Not in stock {%- endif -%}\n                             </span>\n-                        {% endif %}\n+                        {%- endif -%}\n \n-                        {% if watch.get('restock') and watch['restock']['price'] != None %}\n-                            {% if watch['restock']['price'] != None %}\n+                        {%- if watch.get('restock') and watch['restock']['price'] != None -%}\n+                            {%- if watch['restock']['price'] != None -%}\n                                 <span class=\"restock-label price\" title=\"Price\">\n                                 {{ watch['restock']['price']|format_number_locale }} {{ watch['restock']['currency'] }}\n                                 </span>\n-                            {% endif %}\n-                        {% elif not watch.has_restock_info %}\n+                            {%- endif -%}\n+                        {%- elif not watch.has_restock_info -%}\n                             <span class=\"restock-label error\">No information</span>\n-                        {% endif %}\n-                    {% endif %}\n+                        {%- endif -%}\n+                    {%- endif -%}\n                 </td>\n-{% endif %}\n+{%- endif -%}\n             {#last_checked becomes fetch-start-time#}\n                 <td class=\"last-checked\" data-timestamp=\"{{ watch.last_checked }}\" data-fetchduration={{ watch.fetch_time }} data-eta_complete=\"{{ watch.last_checked+watch.fetch_time }}\" >\n                     <div class=\"spinner-wrapper\" style=\"display:none;\" >\n@@ -179,51 +187,34 @@\n                     </div>\n                     <span class=\"innertext\">{{watch|format_last_checked_time|safe}}</span>\n                 </td>\n-\n-\n-                <td class=\"last-changed\" data-timestamp=\"{{ watch.last_changed }}\">{% if watch.history_n >=2 and watch.last_changed >0 %}\n+                <td class=\"last-changed\" data-timestamp=\"{{ watch.last_changed }}\">{%- if watch.history_n >=2 and watch.last_changed >0 -%}\n                     {{watch.last_changed|format_timestamp_timeago}}\n-                    {% else %}\n+                    {%- else -%}\n                     Not yet\n-                    {% endif %}\n+                    {%- endif -%}\n                 </td>\n                 <td>\n+                    {%- set target_attr = ' target=\"' ~ watch.uuid ~ '\"' if datastore.data['settings']['application']['ui'].get('open_diff_in_new_tab') else '' -%}\n                     <a href=\"\" class=\"already-in-queue-button recheck pure-button pure-button-primary\" style=\"display: none;\" disabled=\"disabled\">Queued</a>\n-\n                     <a href=\"{{ url_for('ui.form_watch_checknow', uuid=watch.uuid, tag=request.args.get('tag')) }}\" data-op='recheck' class=\"ajax-op recheck pure-button pure-button-primary\">Recheck</a>\n                     <a href=\"{{ url_for('ui.ui_edit.edit_page', uuid=watch.uuid, tag=active_tag_uuid)}}#general\" class=\"pure-button pure-button-primary\">Edit</a>\n-\n-                    {% if watch.history_n >= 2 %}\n-\n-                        {% set open_diff_in_new_tab = datastore.data['settings']['application']['ui'].get('open_diff_in_new_tab') %}\n-                        {% set target_attr = ' target=\"' ~ watch.uuid ~ '\"' if open_diff_in_new_tab else '' %}\n-\n-                        {% if watch.has_unviewed %}\n-                           <a href=\"{{ url_for('ui.ui_views.diff_history_page', uuid=watch.uuid, from_version=watch.get_from_version_based_on_last_viewed) }}\" {{target_attr}} class=\"pure-button pure-button-primary diff-link\">History</a>\n-                        {% else %}\n-                           <a href=\"{{ url_for('ui.ui_views.diff_history_page', uuid=watch.uuid)}}\" {{target_attr}} class=\"pure-button pure-button-primary diff-link\">History</a>\n-                        {% endif %}\n-\n-                    {% else %}\n-                        {% if watch.history_n == 1 or (watch.history_n ==0 and watch.error_text_ctime )%}\n-                            <a href=\"{{ url_for('ui.ui_views.preview_page', uuid=watch.uuid)}}\" {{target_attr}} class=\"pure-button pure-button-primary\">Preview</a>\n-                        {% endif %}\n-                    {% endif %}\n+                    <a href=\"{{ url_for('ui.ui_views.diff_history_page', uuid=watch.uuid)}}\" {{target_attr}} class=\"pure-button pure-button-primary history-link\" style=\"display: none;\">History</a>\n+                    <a href=\"{{ url_for('ui.ui_views.preview_page', uuid=watch.uuid)}}\" {{target_attr}} class=\"pure-button pure-button-primary preview-link\" style=\"display: none;\">Preview</a>\n                 </td>\n             </tr>\n-            {% endfor %}\n+            {%- endfor -%}\n             </tbody>\n         </table>\n         <ul id=\"post-list-buttons\">\n-            <li id=\"post-list-with-errors\" class=\"{% if errored_count %}has-error{% endif %}\" style=\"display: none;\" >\n+            <li id=\"post-list-with-errors\" class=\"{%- if errored_count -%}has-error{%- endif -%}\" style=\"display: none;\" >\n                 <a href=\"{{url_for('watchlist.index', with_errors=1, tag=request.args.get('tag')) }}\" class=\"pure-button button-tag button-error\">With errors ({{ errored_count }})</a>\n             </li>\n-            <li id=\"post-list-mark-views\" class=\"{% if has_unviewed %}has-unviewed{% endif %}\" style=\"display: none;\" >\n+            <li id=\"post-list-mark-views\" class=\"{%- if has_unviewed -%}has-unviewed{%- endif -%}\" style=\"display: none;\" >\n                 <a href=\"{{url_for('ui.mark_all_viewed',with_errors=request.args.get('with_errors',0)) }}\" class=\"pure-button button-tag \" id=\"mark-all-viewed\">Mark all viewed</a>\n             </li>\n             <li>\n                <a href=\"{{ url_for('ui.form_watch_checknow', tag=active_tag_uuid, with_errors=request.args.get('with_errors',0)) }}\" class=\"pure-button button-tag\" id=\"recheck-all\">Recheck\n-                all {% if active_tag_uuid %} in \"{{active_tag.title}}\"{%endif%}</a>\n+                all {%- if active_tag_uuid-%}  in \"{{active_tag.title}}\"{%endif%}</a>\n             </li>\n             <li>\n                 <a href=\"{{ url_for('rss.feed', tag=active_tag_uuid, token=app_rss_token)}}\"><img alt=\"RSS Feed\" id=\"feed-icon\" src=\"{{url_for('static_content', group='images', filename='generic_feed-icon.svg')}}\" height=\"15\"></a>\n@@ -233,4 +224,4 @@\n     </div>\n     </form>\n </div>\n-{% endblock %}\n\\ No newline at end of file\n+{%- endblock -%}\n\\ No newline at end of file\ndiff --git a/changedetectionio/content_fetchers/base.py b/changedetectionio/content_fetchers/base.py\nindex bfa7e83cdfb..1abce26dec7 100644\n--- a/changedetectionio/content_fetchers/base.py\n+++ b/changedetectionio/content_fetchers/base.py\n@@ -68,7 +68,7 @@ def get_error(self):\n         return self.error\n \n     @abstractmethod\n-    def run(self,\n+    async def run(self,\n             url,\n             timeout,\n             request_headers,\n@@ -122,7 +122,7 @@ def browser_steps_get_valid_steps(self):\n \n         return None\n \n-    def iterate_browser_steps(self, start_url=None):\n+    async def iterate_browser_steps(self, start_url=None):\n         from changedetectionio.blueprint.browser_steps.browser_steps import steppable_browser_interface\n         from playwright._impl._errors import TimeoutError, Error\n         from changedetectionio.safe_jinja import render as jinja_render\n@@ -136,8 +136,8 @@ def iterate_browser_steps(self, start_url=None):\n             for step in valid_steps:\n                 step_n += 1\n                 logger.debug(f\">> Iterating check - browser Step n {step_n} - {step['operation']}...\")\n-                self.screenshot_step(\"before-\" + str(step_n))\n-                self.save_step_html(\"before-\" + str(step_n))\n+                await self.screenshot_step(\"before-\" + str(step_n))\n+                await self.save_step_html(\"before-\" + str(step_n))\n \n                 try:\n                     optional_value = step['optional_value']\n@@ -148,11 +148,11 @@ def iterate_browser_steps(self, start_url=None):\n                     if '{%' in step['selector'] or '{{' in step['selector']:\n                         selector = jinja_render(template_str=step['selector'])\n \n-                    getattr(interface, \"call_action\")(action_name=step['operation'],\n+                    await getattr(interface, \"call_action\")(action_name=step['operation'],\n                                                       selector=selector,\n                                                       optional_value=optional_value)\n-                    self.screenshot_step(step_n)\n-                    self.save_step_html(step_n)\n+                    await self.screenshot_step(step_n)\n+                    await self.save_step_html(step_n)\n                 except (Error, TimeoutError) as e:\n                     logger.debug(str(e))\n                     # Stop processing here\ndiff --git a/changedetectionio/content_fetchers/playwright.py b/changedetectionio/content_fetchers/playwright.py\nindex bb8ade1846c..c5b5bd31f05 100644\n--- a/changedetectionio/content_fetchers/playwright.py\n+++ b/changedetectionio/content_fetchers/playwright.py\n@@ -9,15 +9,15 @@\n from changedetectionio.content_fetchers.base import Fetcher, manage_user_agent\n from changedetectionio.content_fetchers.exceptions import PageUnloadable, Non200ErrorCodeReceived, EmptyReply, ScreenshotUnavailable\n \n-def capture_full_page(page):\n+async def capture_full_page_async(page):\n     import os\n     import time\n     from multiprocessing import Process, Pipe\n \n     start = time.time()\n \n-    page_height = page.evaluate(\"document.documentElement.scrollHeight\")\n-    page_width = page.evaluate(\"document.documentElement.scrollWidth\")\n+    page_height = await page.evaluate(\"document.documentElement.scrollHeight\")\n+    page_width = await page.evaluate(\"document.documentElement.scrollWidth\")\n     original_viewport = page.viewport_size\n \n     logger.debug(f\"Playwright viewport size {page.viewport_size} page height {page_height} page width {page_width}\")\n@@ -32,23 +32,23 @@ def capture_full_page(page):\n             step_size = page_height # Incase page is bigger than default viewport but smaller than proposed step size\n         logger.debug(f\"Setting bigger viewport to step through large page width W{page.viewport_size['width']}xH{step_size} because page_height > viewport_size\")\n         # Set viewport to a larger size to capture more content at once\n-        page.set_viewport_size({'width': page.viewport_size['width'], 'height': step_size})\n+        await page.set_viewport_size({'width': page.viewport_size['width'], 'height': step_size})\n \n     # Capture screenshots in chunks up to the max total height\n     while y < min(page_height, SCREENSHOT_MAX_TOTAL_HEIGHT):\n-        page.request_gc()\n-        page.evaluate(f\"window.scrollTo(0, {y})\")\n-        page.request_gc()\n-        screenshot_chunks.append(page.screenshot(\n+        await page.request_gc()\n+        await page.evaluate(f\"window.scrollTo(0, {y})\")\n+        await page.request_gc()\n+        screenshot_chunks.append(await page.screenshot(\n             type=\"jpeg\",\n             full_page=False,\n             quality=int(os.getenv(\"SCREENSHOT_QUALITY\", 72))\n         ))\n         y += step_size\n-        page.request_gc()\n+        await page.request_gc()\n \n     # Restore original viewport size\n-    page.set_viewport_size({'width': original_viewport['width'], 'height': original_viewport['height']})\n+    await page.set_viewport_size({'width': original_viewport['width'], 'height': original_viewport['height']})\n \n     # If we have multiple chunks, stitch them together\n     if len(screenshot_chunks) > 1:\n@@ -73,7 +73,6 @@ def capture_full_page(page):\n \n     return screenshot_chunks[0]\n \n-\n class fetcher(Fetcher):\n     fetcher_description = \"Playwright {}/Javascript\".format(\n         os.getenv(\"PLAYWRIGHT_BROWSER_TYPE\", 'chromium').capitalize()\n@@ -124,9 +123,9 @@ def __init__(self, proxy_override=None, custom_browser_connection_url=None):\n                 self.proxy['username'] = parsed.username\n                 self.proxy['password'] = parsed.password\n \n-    def screenshot_step(self, step_n=''):\n+    async def screenshot_step(self, step_n=''):\n         super().screenshot_step(step_n=step_n)\n-        screenshot = capture_full_page(page=self.page)\n+        screenshot = await capture_full_page_async(page=self.page)\n \n \n         if self.browser_steps_screenshot_path is not None:\n@@ -135,15 +134,15 @@ def screenshot_step(self, step_n=''):\n             with open(destination, 'wb') as f:\n                 f.write(screenshot)\n \n-    def save_step_html(self, step_n):\n+    async def save_step_html(self, step_n):\n         super().save_step_html(step_n=step_n)\n-        content = self.page.content()\n+        content = await self.page.content()\n         destination = os.path.join(self.browser_steps_screenshot_path, 'step_{}.html'.format(step_n))\n         logger.debug(f\"Saving step HTML to {destination}\")\n         with open(destination, 'w') as f:\n             f.write(content)\n \n-    def run(self,\n+    async def run(self,\n             url,\n             timeout,\n             request_headers,\n@@ -154,26 +153,26 @@ def run(self,\n             is_binary=False,\n             empty_pages_are_a_change=False):\n \n-        from playwright.sync_api import sync_playwright\n+        from playwright.async_api import async_playwright\n         import playwright._impl._errors\n         import time\n         self.delete_browser_steps_screenshots()\n         response = None\n \n-        with sync_playwright() as p:\n+        async with async_playwright() as p:\n             browser_type = getattr(p, self.browser_type)\n \n             # Seemed to cause a connection Exception even tho I can see it connect\n             # self.browser = browser_type.connect(self.command_executor, timeout=timeout*1000)\n             # 60,000 connection timeout only\n-            browser = browser_type.connect_over_cdp(self.browser_connection_url, timeout=60000)\n+            browser = await browser_type.connect_over_cdp(self.browser_connection_url, timeout=60000)\n \n             # SOCKS5 with authentication is not supported (yet)\n             # https://github.com/microsoft/playwright/issues/10567\n \n             # Set user agent to prevent Cloudflare from blocking the browser\n             # Use the default one configured in the App.py model that's passed from fetch_site_status.py\n-            context = browser.new_context(\n+            context = await browser.new_context(\n                 accept_downloads=False,  # Should never be needed\n                 bypass_csp=True,  # This is needed to enable JavaScript execution on GitHub and others\n                 extra_http_headers=request_headers,\n@@ -183,7 +182,7 @@ def run(self,\n                 user_agent=manage_user_agent(headers=request_headers),\n             )\n \n-            self.page = context.new_page()\n+            self.page = await context.new_page()\n \n             # Listen for all console events and handle errors\n             self.page.on(\"console\", lambda msg: logger.debug(f\"Playwright console: Watch URL: {url} {msg.type}: {msg.text} {msg.args}\"))\n@@ -193,32 +192,37 @@ def run(self,\n             browsersteps_interface = steppable_browser_interface(start_url=url)\n             browsersteps_interface.page = self.page\n \n-            response = browsersteps_interface.action_goto_url(value=url)\n+            response = await browsersteps_interface.action_goto_url(value=url)\n \n             if response is None:\n-                context.close()\n-                browser.close()\n+                await context.close()\n+                await browser.close()\n                 logger.debug(\"Content Fetcher > Response object from the browser communication was none\")\n                 raise EmptyReply(url=url, status_code=None)\n \n-            self.headers = response.all_headers()\n+            # In async_playwright, all_headers() returns a coroutine\n+            try:\n+                self.headers = await response.all_headers()\n+            except TypeError:\n+                # Fallback for sync version\n+                self.headers = response.all_headers()\n \n             try:\n                 if self.webdriver_js_execute_code is not None and len(self.webdriver_js_execute_code):\n-                    browsersteps_interface.action_execute_js(value=self.webdriver_js_execute_code, selector=None)\n+                    await browsersteps_interface.action_execute_js(value=self.webdriver_js_execute_code, selector=None)\n             except playwright._impl._errors.TimeoutError as e:\n-                context.close()\n-                browser.close()\n+                await context.close()\n+                await browser.close()\n                 # This can be ok, we will try to grab what we could retrieve\n                 pass\n             except Exception as e:\n                 logger.debug(f\"Content Fetcher > Other exception when executing custom JS code {str(e)}\")\n-                context.close()\n-                browser.close()\n+                await context.close()\n+                await browser.close()\n                 raise PageUnloadable(url=url, status_code=None, message=str(e))\n \n             extra_wait = int(os.getenv(\"WEBDRIVER_DELAY_BEFORE_CONTENT_READY\", 5)) + self.render_extract_delay\n-            self.page.wait_for_timeout(extra_wait * 1000)\n+            await self.page.wait_for_timeout(extra_wait * 1000)\n \n             try:\n                 self.status_code = response.status\n@@ -226,48 +230,48 @@ def run(self,\n                 # https://github.com/dgtlmoon/changedetection.io/discussions/2122#discussioncomment-8241962\n                 logger.critical(f\"Response from the browser/Playwright did not have a status_code! Response follows.\")\n                 logger.critical(response)\n-                context.close()\n-                browser.close()\n+                await context.close()\n+                await browser.close()\n                 raise PageUnloadable(url=url, status_code=None, message=str(e))\n \n             if self.status_code != 200 and not ignore_status_codes:\n-                screenshot = capture_full_page(self.page)\n+                screenshot = await capture_full_page_async(self.page)\n                 raise Non200ErrorCodeReceived(url=url, status_code=self.status_code, screenshot=screenshot)\n \n-            if not empty_pages_are_a_change and len(self.page.content().strip()) == 0:\n+            if not empty_pages_are_a_change and len((await self.page.content()).strip()) == 0:\n                 logger.debug(\"Content Fetcher > Content was empty, empty_pages_are_a_change = False\")\n-                context.close()\n-                browser.close()\n+                await context.close()\n+                await browser.close()\n                 raise EmptyReply(url=url, status_code=response.status)\n \n             # Run Browser Steps here\n             if self.browser_steps_get_valid_steps():\n-                self.iterate_browser_steps(start_url=url)\n+                await self.iterate_browser_steps(start_url=url)\n \n-            self.page.wait_for_timeout(extra_wait * 1000)\n+            await self.page.wait_for_timeout(extra_wait * 1000)\n \n             now = time.time()\n             # So we can find an element on the page where its selector was entered manually (maybe not xPath etc)\n             if current_include_filters is not None:\n-                self.page.evaluate(\"var include_filters={}\".format(json.dumps(current_include_filters)))\n+                await self.page.evaluate(\"var include_filters={}\".format(json.dumps(current_include_filters)))\n             else:\n-                self.page.evaluate(\"var include_filters=''\")\n-            self.page.request_gc()\n+                await self.page.evaluate(\"var include_filters=''\")\n+            await self.page.request_gc()\n \n             # request_gc before and after evaluate to free up memory\n             # @todo browsersteps etc\n             MAX_TOTAL_HEIGHT = int(os.getenv(\"SCREENSHOT_MAX_HEIGHT\", SCREENSHOT_MAX_HEIGHT_DEFAULT))\n-            self.xpath_data = self.page.evaluate(XPATH_ELEMENT_JS, {\n+            self.xpath_data = await self.page.evaluate(XPATH_ELEMENT_JS, {\n                 \"visualselector_xpath_selectors\": visualselector_xpath_selectors,\n                 \"max_height\": MAX_TOTAL_HEIGHT\n             })\n-            self.page.request_gc()\n+            await self.page.request_gc()\n \n-            self.instock_data = self.page.evaluate(INSTOCK_DATA_JS)\n-            self.page.request_gc()\n+            self.instock_data = await self.page.evaluate(INSTOCK_DATA_JS)\n+            await self.page.request_gc()\n \n-            self.content = self.page.content()\n-            self.page.request_gc()\n+            self.content = await self.page.content()\n+            await self.page.request_gc()\n             logger.debug(f\"Scrape xPath element data in browser done in {time.time() - now:.2f}s\")\n \n             # Bug 3 in Playwright screenshot handling\n@@ -279,7 +283,7 @@ def run(self,\n             # acceptable screenshot quality here\n             try:\n                 # The actual screenshot - this always base64 and needs decoding! horrible! huge CPU usage\n-                self.screenshot = capture_full_page(page=self.page)\n+                self.screenshot = await capture_full_page_async(page=self.page)\n \n             except Exception as e:\n                 # It's likely the screenshot was too long/big and something crashed\n@@ -287,30 +291,30 @@ def run(self,\n             finally:\n                 # Request garbage collection one more time before closing\n                 try:\n-                    self.page.request_gc()\n+                    await self.page.request_gc()\n                 except:\n                     pass\n                 \n                 # Clean up resources properly\n                 try:\n-                    self.page.request_gc()\n+                    await self.page.request_gc()\n                 except:\n                     pass\n \n                 try:\n-                    self.page.close()\n+                    await self.page.close()\n                 except:\n                     pass\n                 self.page = None\n \n                 try:\n-                    context.close()\n+                    await context.close()\n                 except:\n                     pass\n                 context = None\n \n                 try:\n-                    browser.close()\n+                    await browser.close()\n                 except:\n                     pass\n                 browser = None\ndiff --git a/changedetectionio/content_fetchers/puppeteer.py b/changedetectionio/content_fetchers/puppeteer.py\nindex 22b6856909c..d62d308d54d 100644\n--- a/changedetectionio/content_fetchers/puppeteer.py\n+++ b/changedetectionio/content_fetchers/puppeteer.py\n@@ -310,15 +310,15 @@ async def fetch_page(self,\n     async def main(self, **kwargs):\n         await self.fetch_page(**kwargs)\n \n-    def run(self, url, timeout, request_headers, request_body, request_method, ignore_status_codes=False,\n+    async def run(self, url, timeout, request_headers, request_body, request_method, ignore_status_codes=False,\n             current_include_filters=None, is_binary=False, empty_pages_are_a_change=False):\n \n         #@todo make update_worker async which could run any of these content_fetchers within memory and time constraints\n-        max_time = os.getenv('PUPPETEER_MAX_PROCESSING_TIMEOUT_SECONDS', 180)\n+        max_time = int(os.getenv('PUPPETEER_MAX_PROCESSING_TIMEOUT_SECONDS', 180))\n \n-        # This will work in 3.10 but not >= 3.11 because 3.11 wants tasks only\n+        # Now we run this properly in async context since we're called from async worker\n         try:\n-            asyncio.run(asyncio.wait_for(self.main(\n+            await asyncio.wait_for(self.main(\n                 url=url,\n                 timeout=timeout,\n                 request_headers=request_headers,\n@@ -328,7 +328,7 @@ def run(self, url, timeout, request_headers, request_body, request_method, ignor\n                 current_include_filters=current_include_filters,\n                 is_binary=is_binary,\n                 empty_pages_are_a_change=empty_pages_are_a_change\n-            ), timeout=max_time))\n+            ), timeout=max_time)\n         except asyncio.TimeoutError:\n             raise(BrowserFetchTimedOut(msg=f\"Browser connected but was unable to process the page in {max_time} seconds.\"))\n \ndiff --git a/changedetectionio/content_fetchers/requests.py b/changedetectionio/content_fetchers/requests.py\nindex 70b6c319a32..aba5ed0d041 100644\n--- a/changedetectionio/content_fetchers/requests.py\n+++ b/changedetectionio/content_fetchers/requests.py\n@@ -1,6 +1,7 @@\n from loguru import logger\n import hashlib\n import os\n+import asyncio\n from changedetectionio import strtobool\n from changedetectionio.content_fetchers.exceptions import BrowserStepsInUnsupportedFetcher, EmptyReply, Non200ErrorCodeReceived\n from changedetectionio.content_fetchers.base import Fetcher\n@@ -15,7 +16,7 @@ def __init__(self, proxy_override=None, custom_browser_connection_url=None):\n         self.proxy_override = proxy_override\n         # browser_connection_url is none because its always 'launched locally'\n \n-    def run(self,\n+    def _run_sync(self,\n             url,\n             timeout,\n             request_headers,\n@@ -25,6 +26,7 @@ def run(self,\n             current_include_filters=None,\n             is_binary=False,\n             empty_pages_are_a_change=False):\n+        \"\"\"Synchronous version of run - the original requests implementation\"\"\"\n \n         import chardet\n         import requests\n@@ -36,7 +38,6 @@ def run(self,\n         proxies = {}\n \n         # Allows override the proxy on a per-request basis\n-\n         # https://requests.readthedocs.io/en/latest/user/advanced/#socks\n         # Should also work with `socks5://user:pass@host:port` type syntax.\n \n@@ -100,9 +101,38 @@ def run(self,\n         else:\n             self.content = r.text\n \n-\n         self.raw_content = r.content\n \n+    async def run(self,\n+            url,\n+            timeout,\n+            request_headers,\n+            request_body,\n+            request_method,\n+            ignore_status_codes=False,\n+            current_include_filters=None,\n+            is_binary=False,\n+            empty_pages_are_a_change=False):\n+        \"\"\"Async wrapper that runs the synchronous requests code in a thread pool\"\"\"\n+        \n+        loop = asyncio.get_event_loop()\n+        \n+        # Run the synchronous _run_sync in a thread pool to avoid blocking the event loop\n+        await loop.run_in_executor(\n+            None,  # Use default ThreadPoolExecutor\n+            lambda: self._run_sync(\n+                url=url,\n+                timeout=timeout,\n+                request_headers=request_headers,\n+                request_body=request_body,\n+                request_method=request_method,\n+                ignore_status_codes=ignore_status_codes,\n+                current_include_filters=current_include_filters,\n+                is_binary=is_binary,\n+                empty_pages_are_a_change=empty_pages_are_a_change\n+            )\n+        )\n+\n     def quit(self, watch=None):\n \n         # In case they switched to `requests` fetcher from something else\ndiff --git a/changedetectionio/content_fetchers/webdriver_selenium.py b/changedetectionio/content_fetchers/webdriver_selenium.py\nindex 180d6332e8a..48897d7aa87 100644\n--- a/changedetectionio/content_fetchers/webdriver_selenium.py\n+++ b/changedetectionio/content_fetchers/webdriver_selenium.py\n@@ -47,7 +47,7 @@ def __init__(self, proxy_override=None, custom_browser_connection_url=None):\n             self.proxy_url = k.strip()\n \n \n-    def run(self,\n+    async def run(self,\n             url,\n             timeout,\n             request_headers,\n@@ -58,77 +58,86 @@ def run(self,\n             is_binary=False,\n             empty_pages_are_a_change=False):\n \n-        from selenium.webdriver.chrome.options import Options as ChromeOptions\n-        # request_body, request_method unused for now, until some magic in the future happens.\n+        import asyncio\n+        \n+        # Wrap the entire selenium operation in a thread executor\n+        def _run_sync():\n+            from selenium.webdriver.chrome.options import Options as ChromeOptions\n+            # request_body, request_method unused for now, until some magic in the future happens.\n+\n+            options = ChromeOptions()\n+\n+            # Load Chrome options from env\n+            CHROME_OPTIONS = [\n+                line.strip()\n+                for line in os.getenv(\"CHROME_OPTIONS\", \"\").strip().splitlines()\n+                if line.strip()\n+            ]\n+\n+            for opt in CHROME_OPTIONS:\n+                options.add_argument(opt)\n+\n+            # 1. proxy_config /Proxy(proxy_config) selenium object is REALLY unreliable\n+            # 2. selenium-wire cant be used because the websocket version conflicts with pypeteer-ng\n+            # 3. selenium only allows ONE runner at a time by default!\n+            # 4. driver must use quit() or it will continue to block/hold the selenium process!!\n+\n+            if self.proxy_url:\n+                options.add_argument(f'--proxy-server={self.proxy_url}')\n+\n+            from selenium.webdriver.remote.remote_connection import RemoteConnection\n+            from selenium.webdriver.remote.webdriver import WebDriver as RemoteWebDriver\n+            driver = None\n+            try:\n+                # Create the RemoteConnection and set timeout (e.g., 30 seconds)\n+                remote_connection = RemoteConnection(\n+                    self.browser_connection_url,\n+                )\n+                remote_connection.set_timeout(30)  # seconds\n+\n+                # Now create the driver with the RemoteConnection\n+                driver = RemoteWebDriver(\n+                    command_executor=remote_connection,\n+                    options=options\n+                )\n+\n+                driver.set_page_load_timeout(int(os.getenv(\"WEBDRIVER_PAGELOAD_TIMEOUT\", 45)))\n+            except Exception as e:\n+                if driver:\n+                    driver.quit()\n+                raise e\n+\n+            try:\n+                driver.get(url)\n+\n+                if not \"--window-size\" in os.getenv(\"CHROME_OPTIONS\", \"\"):\n+                    driver.set_window_size(1280, 1024)\n \n-        options = ChromeOptions()\n-\n-        # Load Chrome options from env\n-        CHROME_OPTIONS = [\n-            line.strip()\n-            for line in os.getenv(\"CHROME_OPTIONS\", \"\").strip().splitlines()\n-            if line.strip()\n-        ]\n+                driver.implicitly_wait(int(os.getenv(\"WEBDRIVER_DELAY_BEFORE_CONTENT_READY\", 5)))\n \n-        for opt in CHROME_OPTIONS:\n-            options.add_argument(opt)\n-\n-        # 1. proxy_config /Proxy(proxy_config) selenium object is REALLY unreliable\n-        # 2. selenium-wire cant be used because the websocket version conflicts with pypeteer-ng\n-        # 3. selenium only allows ONE runner at a time by default!\n-        # 4. driver must use quit() or it will continue to block/hold the selenium process!!\n-\n-        if self.proxy_url:\n-            options.add_argument(f'--proxy-server={self.proxy_url}')\n-\n-        from selenium.webdriver.remote.remote_connection import RemoteConnection\n-        from selenium.webdriver.remote.webdriver import WebDriver as RemoteWebDriver\n-        driver = None\n-        try:\n-            # Create the RemoteConnection and set timeout (e.g., 30 seconds)\n-            remote_connection = RemoteConnection(\n-                self.browser_connection_url,\n-            )\n-            remote_connection.set_timeout(30)  # seconds\n-\n-            # Now create the driver with the RemoteConnection\n-            driver = RemoteWebDriver(\n-                command_executor=remote_connection,\n-                options=options\n-            )\n-\n-            driver.set_page_load_timeout(int(os.getenv(\"WEBDRIVER_PAGELOAD_TIMEOUT\", 45)))\n-        except Exception as e:\n-            if driver:\n+                if self.webdriver_js_execute_code is not None:\n+                    driver.execute_script(self.webdriver_js_execute_code)\n+                    # Selenium doesn't automatically wait for actions as good as Playwright, so wait again\n+                    driver.implicitly_wait(int(os.getenv(\"WEBDRIVER_DELAY_BEFORE_CONTENT_READY\", 5)))\n+\n+                # @todo - how to check this? is it possible?\n+                self.status_code = 200\n+                # @todo somehow we should try to get this working for WebDriver\n+                # raise EmptyReply(url=url, status_code=r.status_code)\n+\n+                # @todo - dom wait loaded?\n+                import time\n+                time.sleep(int(os.getenv(\"WEBDRIVER_DELAY_BEFORE_CONTENT_READY\", 5)) + self.render_extract_delay)\n+                self.content = driver.page_source\n+                self.headers = {}\n+                self.screenshot = driver.get_screenshot_as_png()\n+            except Exception as e:\n                 driver.quit()\n-            raise e\n-\n-        try:\n-            driver.get(url)\n-\n-            if not \"--window-size\" in os.getenv(\"CHROME_OPTIONS\", \"\"):\n-                driver.set_window_size(1280, 1024)\n-\n-            driver.implicitly_wait(int(os.getenv(\"WEBDRIVER_DELAY_BEFORE_CONTENT_READY\", 5)))\n-\n-            if self.webdriver_js_execute_code is not None:\n-                driver.execute_script(self.webdriver_js_execute_code)\n-                # Selenium doesn't automatically wait for actions as good as Playwright, so wait again\n-                driver.implicitly_wait(int(os.getenv(\"WEBDRIVER_DELAY_BEFORE_CONTENT_READY\", 5)))\n+                raise e\n \n-            # @todo - how to check this? is it possible?\n-            self.status_code = 200\n-            # @todo somehow we should try to get this working for WebDriver\n-            # raise EmptyReply(url=url, status_code=r.status_code)\n-\n-            # @todo - dom wait loaded?\n-            time.sleep(int(os.getenv(\"WEBDRIVER_DELAY_BEFORE_CONTENT_READY\", 5)) + self.render_extract_delay)\n-            self.content = driver.page_source\n-            self.headers = {}\n-            self.screenshot = driver.get_screenshot_as_png()\n-        except Exception as e:\n             driver.quit()\n-            raise e\n \n-        driver.quit()\n+        # Run the selenium operations in a thread pool to avoid blocking the event loop\n+        loop = asyncio.get_event_loop()\n+        await loop.run_in_executor(None, _run_sync)\n \ndiff --git a/changedetectionio/custom_queue.py b/changedetectionio/custom_queue.py\nindex f5566fa587f..feb1fbcdabc 100644\n--- a/changedetectionio/custom_queue.py\n+++ b/changedetectionio/custom_queue.py\n@@ -1,4 +1,5 @@\n import queue\n+import asyncio\n from blinker import signal\n from loguru import logger\n \n@@ -50,3 +51,450 @@ def get(self, block=True, timeout=None):\n         except Exception as e:\n             logger.critical(f\"Exception: {e}\")\n         return item\n+    \n+    def get_uuid_position(self, target_uuid):\n+        \"\"\"\n+        Find the position of a watch UUID in the priority queue.\n+        Optimized for large queues - O(n) complexity instead of O(n log n).\n+        \n+        Args:\n+            target_uuid: The UUID to search for\n+            \n+        Returns:\n+            dict: Contains position info or None if not found\n+                - position: 0-based position in queue (0 = next to be processed)\n+                - total_items: total number of items in queue\n+                - priority: the priority value of the found item\n+        \"\"\"\n+        with self.mutex:\n+            queue_list = list(self.queue)\n+            total_items = len(queue_list)\n+            \n+            if total_items == 0:\n+                return {\n+                    'position': None,\n+                    'total_items': 0,\n+                    'priority': None,\n+                    'found': False\n+                }\n+            \n+            # Find the target item and its priority first - O(n)\n+            target_item = None\n+            target_priority = None\n+            \n+            for item in queue_list:\n+                if (hasattr(item, 'item') and \n+                    isinstance(item.item, dict) and \n+                    item.item.get('uuid') == target_uuid):\n+                    target_item = item\n+                    target_priority = item.priority\n+                    break\n+            \n+            if target_item is None:\n+                return {\n+                    'position': None,\n+                    'total_items': total_items,\n+                    'priority': None,\n+                    'found': False\n+                }\n+            \n+            # Count how many items have higher priority (lower numbers) - O(n)\n+            position = 0\n+            for item in queue_list:\n+                # Items with lower priority numbers are processed first\n+                if item.priority < target_priority:\n+                    position += 1\n+                elif item.priority == target_priority and item != target_item:\n+                    # For same priority, count items that come before this one\n+                    # (Note: this is approximate since heap order isn't guaranteed for equal priorities)\n+                    position += 1\n+            \n+            return {\n+                'position': position,\n+                'total_items': total_items,\n+                'priority': target_priority,\n+                'found': True\n+            }\n+    \n+    def get_all_queued_uuids(self, limit=None, offset=0):\n+        \"\"\"\n+        Get UUIDs currently in the queue with their positions.\n+        For large queues, use limit/offset for pagination.\n+        \n+        Args:\n+            limit: Maximum number of items to return (None = all)\n+            offset: Number of items to skip (for pagination)\n+        \n+        Returns:\n+            dict: Contains items and metadata\n+                - items: List of dicts with uuid, position, and priority\n+                - total_items: Total number of items in queue\n+                - returned_items: Number of items returned\n+                - has_more: Whether there are more items after this page\n+        \"\"\"\n+        with self.mutex:\n+            queue_list = list(self.queue)\n+            total_items = len(queue_list)\n+            \n+            if total_items == 0:\n+                return {\n+                    'items': [],\n+                    'total_items': 0,\n+                    'returned_items': 0,\n+                    'has_more': False\n+                }\n+            \n+            # For very large queues, warn about performance\n+            if total_items > 1000 and limit is None:\n+                logger.warning(f\"Getting all {total_items} queued items without limit - this may be slow\")\n+            \n+            # Sort only if we need exact positions (expensive for large queues)\n+            if limit is not None and limit <= 100:\n+                # For small requests, we can afford to sort\n+                queue_items = sorted(queue_list)\n+                end_idx = min(offset + limit, len(queue_items)) if limit else len(queue_items)\n+                items_to_process = queue_items[offset:end_idx]\n+                \n+                result = []\n+                for position, item in enumerate(items_to_process, start=offset):\n+                    if (hasattr(item, 'item') and \n+                        isinstance(item.item, dict) and \n+                        'uuid' in item.item):\n+                        \n+                        result.append({\n+                            'uuid': item.item['uuid'],\n+                            'position': position,\n+                            'priority': item.priority\n+                        })\n+                \n+                return {\n+                    'items': result,\n+                    'total_items': total_items,\n+                    'returned_items': len(result),\n+                    'has_more': (offset + len(result)) < total_items\n+                }\n+            else:\n+                # For large requests, return items with approximate positions\n+                # This is much faster O(n) instead of O(n log n)\n+                result = []\n+                processed = 0\n+                skipped = 0\n+                \n+                for item in queue_list:\n+                    if (hasattr(item, 'item') and \n+                        isinstance(item.item, dict) and \n+                        'uuid' in item.item):\n+                        \n+                        if skipped < offset:\n+                            skipped += 1\n+                            continue\n+                        \n+                        if limit and processed >= limit:\n+                            break\n+                        \n+                        # Approximate position based on priority comparison\n+                        approx_position = sum(1 for other in queue_list if other.priority < item.priority)\n+                        \n+                        result.append({\n+                            'uuid': item.item['uuid'],\n+                            'position': approx_position,  # Approximate\n+                            'priority': item.priority\n+                        })\n+                        processed += 1\n+                \n+                return {\n+                    'items': result,\n+                    'total_items': total_items,\n+                    'returned_items': len(result),\n+                    'has_more': (offset + len(result)) < total_items,\n+                    'note': 'Positions are approximate for performance with large queues'\n+                }\n+    \n+    def get_queue_summary(self):\n+        \"\"\"\n+        Get a quick summary of queue state without expensive operations.\n+        O(n) complexity - fast even for large queues.\n+        \n+        Returns:\n+            dict: Queue summary statistics\n+        \"\"\"\n+        with self.mutex:\n+            queue_list = list(self.queue)\n+            total_items = len(queue_list)\n+            \n+            if total_items == 0:\n+                return {\n+                    'total_items': 0,\n+                    'priority_breakdown': {},\n+                    'immediate_items': 0,\n+                    'clone_items': 0,\n+                    'scheduled_items': 0\n+                }\n+            \n+            # Count items by priority type - O(n)\n+            immediate_items = 0  # priority 1\n+            clone_items = 0      # priority 5  \n+            scheduled_items = 0  # priority > 100 (timestamps)\n+            priority_counts = {}\n+            \n+            for item in queue_list:\n+                priority = item.priority\n+                priority_counts[priority] = priority_counts.get(priority, 0) + 1\n+                \n+                if priority == 1:\n+                    immediate_items += 1\n+                elif priority == 5:\n+                    clone_items += 1\n+                elif priority > 100:\n+                    scheduled_items += 1\n+            \n+            return {\n+                'total_items': total_items,\n+                'priority_breakdown': priority_counts,\n+                'immediate_items': immediate_items,\n+                'clone_items': clone_items,\n+                'scheduled_items': scheduled_items,\n+                'min_priority': min(priority_counts.keys()) if priority_counts else None,\n+                'max_priority': max(priority_counts.keys()) if priority_counts else None\n+            }\n+\n+\n+class AsyncSignalPriorityQueue(asyncio.PriorityQueue):\n+    \"\"\"\n+    Async version of SignalPriorityQueue that sends signals when items are added/removed.\n+    \n+    This class extends asyncio.PriorityQueue and maintains the same signal behavior\n+    as the synchronous version for real-time UI updates.\n+    \"\"\"\n+    \n+    def __init__(self, maxsize=0):\n+        super().__init__(maxsize)\n+        try:\n+            self.queue_length_signal = signal('queue_length')\n+        except Exception as e:\n+            logger.critical(f\"Exception: {e}\")\n+\n+    async def put(self, item):\n+        # Call the parent's put method first\n+        await super().put(item)\n+        \n+        # After putting the item in the queue, check if it has a UUID and emit signal\n+        if hasattr(item, 'item') and isinstance(item.item, dict) and 'uuid' in item.item:\n+            uuid = item.item['uuid']\n+            # Get the signal and send it if it exists\n+            watch_check_update = signal('watch_check_update')\n+            if watch_check_update:\n+                # Send the watch_uuid parameter\n+                watch_check_update.send(watch_uuid=uuid)\n+        \n+        # Send queue_length signal with current queue size\n+        try:\n+            if self.queue_length_signal:\n+                self.queue_length_signal.send(length=self.qsize())\n+        except Exception as e:\n+            logger.critical(f\"Exception: {e}\")\n+\n+    async def get(self):\n+        # Call the parent's get method first\n+        item = await super().get()\n+        \n+        # Send queue_length signal with current queue size\n+        try:\n+            if self.queue_length_signal:\n+                self.queue_length_signal.send(length=self.qsize())\n+        except Exception as e:\n+            logger.critical(f\"Exception: {e}\")\n+        return item\n+    \n+    @property\n+    def queue(self):\n+        \"\"\"\n+        Provide compatibility with sync PriorityQueue.queue access\n+        Returns the internal queue for template access\n+        \"\"\"\n+        return self._queue if hasattr(self, '_queue') else []\n+    \n+    def get_uuid_position(self, target_uuid):\n+        \"\"\"\n+        Find the position of a watch UUID in the async priority queue.\n+        Optimized for large queues - O(n) complexity instead of O(n log n).\n+        \n+        Args:\n+            target_uuid: The UUID to search for\n+            \n+        Returns:\n+            dict: Contains position info or None if not found\n+                - position: 0-based position in queue (0 = next to be processed)\n+                - total_items: total number of items in queue\n+                - priority: the priority value of the found item\n+        \"\"\"\n+        queue_list = list(self._queue)\n+        total_items = len(queue_list)\n+        \n+        if total_items == 0:\n+            return {\n+                'position': None,\n+                'total_items': 0,\n+                'priority': None,\n+                'found': False\n+            }\n+        \n+        # Find the target item and its priority first - O(n)\n+        target_item = None\n+        target_priority = None\n+        \n+        for item in queue_list:\n+            if (hasattr(item, 'item') and \n+                isinstance(item.item, dict) and \n+                item.item.get('uuid') == target_uuid):\n+                target_item = item\n+                target_priority = item.priority\n+                break\n+        \n+        if target_item is None:\n+            return {\n+                'position': None,\n+                'total_items': total_items,\n+                'priority': None,\n+                'found': False\n+            }\n+        \n+        # Count how many items have higher priority (lower numbers) - O(n)\n+        position = 0\n+        for item in queue_list:\n+            if item.priority < target_priority:\n+                position += 1\n+            elif item.priority == target_priority and item != target_item:\n+                position += 1\n+        \n+        return {\n+            'position': position,\n+            'total_items': total_items,\n+            'priority': target_priority,\n+            'found': True\n+        }\n+    \n+    def get_all_queued_uuids(self, limit=None, offset=0):\n+        \"\"\"\n+        Get UUIDs currently in the async queue with their positions.\n+        For large queues, use limit/offset for pagination.\n+        \n+        Args:\n+            limit: Maximum number of items to return (None = all)\n+            offset: Number of items to skip (for pagination)\n+        \n+        Returns:\n+            dict: Contains items and metadata (same structure as sync version)\n+        \"\"\"\n+        queue_list = list(self._queue)\n+        total_items = len(queue_list)\n+        \n+        if total_items == 0:\n+            return {\n+                'items': [],\n+                'total_items': 0,\n+                'returned_items': 0,\n+                'has_more': False\n+            }\n+        \n+        # Same logic as sync version but without mutex\n+        if limit is not None and limit <= 100:\n+            queue_items = sorted(queue_list)\n+            end_idx = min(offset + limit, len(queue_items)) if limit else len(queue_items)\n+            items_to_process = queue_items[offset:end_idx]\n+            \n+            result = []\n+            for position, item in enumerate(items_to_process, start=offset):\n+                if (hasattr(item, 'item') and \n+                    isinstance(item.item, dict) and \n+                    'uuid' in item.item):\n+                    \n+                    result.append({\n+                        'uuid': item.item['uuid'],\n+                        'position': position,\n+                        'priority': item.priority\n+                    })\n+            \n+            return {\n+                'items': result,\n+                'total_items': total_items,\n+                'returned_items': len(result),\n+                'has_more': (offset + len(result)) < total_items\n+            }\n+        else:\n+            # Fast approximate positions for large queues\n+            result = []\n+            processed = 0\n+            skipped = 0\n+            \n+            for item in queue_list:\n+                if (hasattr(item, 'item') and \n+                    isinstance(item.item, dict) and \n+                    'uuid' in item.item):\n+                    \n+                    if skipped < offset:\n+                        skipped += 1\n+                        continue\n+                    \n+                    if limit and processed >= limit:\n+                        break\n+                    \n+                    approx_position = sum(1 for other in queue_list if other.priority < item.priority)\n+                    \n+                    result.append({\n+                        'uuid': item.item['uuid'],\n+                        'position': approx_position,\n+                        'priority': item.priority\n+                    })\n+                    processed += 1\n+            \n+            return {\n+                'items': result,\n+                'total_items': total_items,\n+                'returned_items': len(result),\n+                'has_more': (offset + len(result)) < total_items,\n+                'note': 'Positions are approximate for performance with large queues'\n+            }\n+    \n+    def get_queue_summary(self):\n+        \"\"\"\n+        Get a quick summary of async queue state.\n+        O(n) complexity - fast even for large queues.\n+        \"\"\"\n+        queue_list = list(self._queue)\n+        total_items = len(queue_list)\n+        \n+        if total_items == 0:\n+            return {\n+                'total_items': 0,\n+                'priority_breakdown': {},\n+                'immediate_items': 0,\n+                'clone_items': 0,\n+                'scheduled_items': 0\n+            }\n+        \n+        immediate_items = 0\n+        clone_items = 0\n+        scheduled_items = 0\n+        priority_counts = {}\n+        \n+        for item in queue_list:\n+            priority = item.priority\n+            priority_counts[priority] = priority_counts.get(priority, 0) + 1\n+            \n+            if priority == 1:\n+                immediate_items += 1\n+            elif priority == 5:\n+                clone_items += 1\n+            elif priority > 100:\n+                scheduled_items += 1\n+        \n+        return {\n+            'total_items': total_items,\n+            'priority_breakdown': priority_counts,\n+            'immediate_items': immediate_items,\n+            'clone_items': clone_items,\n+            'scheduled_items': scheduled_items,\n+            'min_priority': min(priority_counts.keys()) if priority_counts else None,\n+            'max_priority': max(priority_counts.keys()) if priority_counts else None\n+        }\ndiff --git a/changedetectionio/flask_app.py b/changedetectionio/flask_app.py\nindex fe3c0b973c3..164c0c8d10d 100644\n--- a/changedetectionio/flask_app.py\n+++ b/changedetectionio/flask_app.py\n@@ -4,6 +4,7 @@\n import locale\n import os\n import queue\n+import sys\n import threading\n import time\n import timeago\n@@ -11,7 +12,8 @@\n \n from changedetectionio.strtobool import strtobool\n from threading import Event\n-from changedetectionio.custom_queue import SignalPriorityQueue\n+from changedetectionio.custom_queue import SignalPriorityQueue, AsyncSignalPriorityQueue\n+from changedetectionio import worker_handler\n \n from flask import (\n     Flask,\n@@ -45,12 +47,11 @@\n datastore = None\n \n # Local\n-running_update_threads = []\n ticker_thread = None\n-\n extra_stylesheets = []\n \n-update_q = SignalPriorityQueue()\n+# Use async queue by default, keep sync for backward compatibility  \n+update_q = AsyncSignalPriorityQueue() if worker_handler.USE_ASYNC_WORKERS else SignalPriorityQueue()\n notification_q = queue.Queue()\n MAX_QUEUE_SIZE = 2000\n \n@@ -145,10 +146,32 @@ def _jinja2_filter_format_number_locale(value: float) -> str:\n \n @app.template_global('is_checking_now')\n def _watch_is_checking_now(watch_obj, format=\"%Y-%m-%d %H:%M:%S\"):\n-    # Worker thread tells us which UUID it is currently processing.\n-    for t in running_update_threads:\n-        if t.current_uuid == watch_obj['uuid']:\n-            return True\n+    return worker_handler.is_watch_running(watch_obj['uuid'])\n+\n+@app.template_global('get_watch_queue_position')\n+def _get_watch_queue_position(watch_obj):\n+    \"\"\"Get the position of a watch in the queue\"\"\"\n+    uuid = watch_obj['uuid']\n+    return update_q.get_uuid_position(uuid)\n+\n+@app.template_global('get_current_worker_count')\n+def _get_current_worker_count():\n+    \"\"\"Get the current number of operational workers\"\"\"\n+    return worker_handler.get_worker_count()\n+\n+@app.template_global('get_worker_status_info')\n+def _get_worker_status_info():\n+    \"\"\"Get detailed worker status information for display\"\"\"\n+    status = worker_handler.get_worker_status()\n+    running_uuids = worker_handler.get_running_uuids()\n+    \n+    return {\n+        'count': status['worker_count'],\n+        'type': status['worker_type'],\n+        'active_workers': len(running_uuids),\n+        'processing_watches': running_uuids,\n+        'loop_running': status.get('async_loop_running', None)\n+    }\n \n \n # We use the whole watch object from the store/JSON so we can see if there's some related status in terms of a thread\n@@ -470,16 +493,21 @@ def static_content(group, filename):\n \n     # watchlist UI buttons etc\n     import changedetectionio.blueprint.ui as ui\n-    app.register_blueprint(ui.construct_blueprint(datastore, update_q, running_update_threads, queuedWatchMetaData, watch_check_update))\n+    app.register_blueprint(ui.construct_blueprint(datastore, update_q, worker_handler, queuedWatchMetaData, watch_check_update))\n \n     import changedetectionio.blueprint.watchlist as watchlist\n     app.register_blueprint(watchlist.construct_blueprint(datastore=datastore, update_q=update_q, queuedWatchMetaData=queuedWatchMetaData), url_prefix='')\n \n-    # Initialize Socket.IO server\n-    from changedetectionio.realtime.socket_server import init_socketio\n-    global socketio_server\n-    socketio_server = init_socketio(app, datastore)\n-    logger.info(\"Socket.IO server initialized\")\n+    # Initialize Socket.IO server conditionally based on settings\n+    socket_io_enabled = datastore.data['settings']['application']['ui'].get('socket_io_enabled', True)\n+    if socket_io_enabled:\n+        from changedetectionio.realtime.socket_server import init_socketio\n+        global socketio_server\n+        socketio_server = init_socketio(app, datastore)\n+        logger.info(\"Socket.IO server initialized\")\n+    else:\n+        logger.info(\"Socket.IO server disabled via settings\")\n+        socketio_server = None\n \n     # Memory cleanup endpoint\n     @app.route('/gc-cleanup', methods=['GET'])\n@@ -491,12 +519,91 @@ def gc_cleanup():\n         result = memory_cleanup(app)\n         return jsonify({\"status\": \"success\", \"message\": \"Memory cleanup completed\", \"result\": result})\n \n+    # Worker health check endpoint\n+    @app.route('/worker-health', methods=['GET'])\n+    @login_optionally_required\n+    def worker_health():\n+        from flask import jsonify\n+        \n+        expected_workers = int(os.getenv(\"FETCH_WORKERS\", datastore.data['settings']['requests']['workers']))\n+        \n+        # Get basic status\n+        status = worker_handler.get_worker_status()\n+        \n+        # Perform health check\n+        health_result = worker_handler.check_worker_health(\n+            expected_count=expected_workers,\n+            update_q=update_q,\n+            notification_q=notification_q,\n+            app=app,\n+            datastore=datastore\n+        )\n+        \n+        return jsonify({\n+            \"status\": \"success\",\n+            \"worker_status\": status,\n+            \"health_check\": health_result,\n+            \"expected_workers\": expected_workers\n+        })\n+\n+    # Queue status endpoint\n+    @app.route('/queue-status', methods=['GET'])\n+    @login_optionally_required\n+    def queue_status():\n+        from flask import jsonify, request\n+        \n+        # Get specific UUID position if requested\n+        target_uuid = request.args.get('uuid')\n+        \n+        if target_uuid:\n+            position_info = update_q.get_uuid_position(target_uuid)\n+            return jsonify({\n+                \"status\": \"success\",\n+                \"uuid\": target_uuid,\n+                \"queue_position\": position_info\n+            })\n+        else:\n+            # Get pagination parameters\n+            limit = request.args.get('limit', type=int)\n+            offset = request.args.get('offset', type=int, default=0)\n+            summary_only = request.args.get('summary', type=bool, default=False)\n+            \n+            if summary_only:\n+                # Fast summary for large queues\n+                summary = update_q.get_queue_summary()\n+                return jsonify({\n+                    \"status\": \"success\",\n+                    \"queue_summary\": summary\n+                })\n+            else:\n+                # Get queued items with pagination support\n+                if limit is None:\n+                    # Default limit for large queues to prevent performance issues\n+                    queue_size = update_q.qsize()\n+                    if queue_size > 100:\n+                        limit = 50\n+                        logger.warning(f\"Large queue ({queue_size} items) detected, limiting to {limit} items. Use ?limit=N for more.\")\n+                \n+                all_queued = update_q.get_all_queued_uuids(limit=limit, offset=offset)\n+                return jsonify({\n+                    \"status\": \"success\",\n+                    \"queue_size\": update_q.qsize(),\n+                    \"queued_data\": all_queued\n+                })\n+\n+    # Start the async workers during app initialization\n+    # Can be overridden by ENV or use the default settings\n+    n_workers = int(os.getenv(\"FETCH_WORKERS\", datastore.data['settings']['requests']['workers']))\n+    logger.info(f\"Starting {n_workers} workers during app initialization\")\n+    worker_handler.start_workers(n_workers, update_q, notification_q, app, datastore)\n+\n     # @todo handle ctrl break\n     ticker_thread = threading.Thread(target=ticker_thread_check_time_launch_checks).start()\n     threading.Thread(target=notification_runner).start()\n \n+    in_pytest = \"pytest\" in sys.modules or \"PYTEST_CURRENT_TEST\" in os.environ\n     # Check for new release version, but not when running in test/build or pytest\n-    if not os.getenv(\"GITHUB_REF\", False) and not strtobool(os.getenv('DISABLE_VERSION_CHECK', 'no')):\n+    if not os.getenv(\"GITHUB_REF\", False) and not strtobool(os.getenv('DISABLE_VERSION_CHECK', 'no')) and not in_pytest:\n         threading.Thread(target=check_for_new_version).start()\n \n     # Return the Flask app - the Socket.IO will be attached to it but initialized separately\n@@ -588,27 +695,35 @@ def notification_runner():\n # Threaded runner, look for new watches to feed into the Queue.\n def ticker_thread_check_time_launch_checks():\n     import random\n-    from changedetectionio import update_worker\n     proxy_last_called_time = {}\n+    last_health_check = 0\n \n     recheck_time_minimum_seconds = int(os.getenv('MINIMUM_SECONDS_RECHECK_TIME', 3))\n     logger.debug(f\"System env MINIMUM_SECONDS_RECHECK_TIME {recheck_time_minimum_seconds}\")\n \n-    # Spin up Workers that do the fetching\n-    # Can be overriden by ENV or use the default settings\n-    n_workers = int(os.getenv(\"FETCH_WORKERS\", datastore.data['settings']['requests']['workers']))\n-    for _ in range(n_workers):\n-        new_worker = update_worker.update_worker(update_q, notification_q, app, datastore)\n-        running_update_threads.append(new_worker)\n-        new_worker.start()\n+    # Workers are now started during app initialization, not here\n \n     while not app.config.exit.is_set():\n \n+        # Periodic worker health check (every 60 seconds)\n+        now = time.time()\n+        if now - last_health_check > 60:\n+            expected_workers = int(os.getenv(\"FETCH_WORKERS\", datastore.data['settings']['requests']['workers']))\n+            health_result = worker_handler.check_worker_health(\n+                expected_count=expected_workers,\n+                update_q=update_q,\n+                notification_q=notification_q,\n+                app=app,\n+                datastore=datastore\n+            )\n+            \n+            if health_result['status'] != 'healthy':\n+                logger.warning(f\"Worker health check: {health_result['message']}\")\n+                \n+            last_health_check = now\n+\n         # Get a list of watches by UUID that are currently fetching data\n-        running_uuids = []\n-        for t in running_update_threads:\n-            if t.current_uuid:\n-                running_uuids.append(t.current_uuid)\n+        running_uuids = worker_handler.get_running_uuids()\n \n         # Re #232 - Deepcopy the data incase it changes while we're iterating through it all\n         watch_uuid_list = []\n@@ -711,7 +826,7 @@ def ticker_thread_check_time_launch_checks():\n                         f\"{now - watch['last_checked']:0.2f}s since last checked\")\n \n                     # Into the queue with you\n-                    update_q.put(queuedWatchMetaData.PrioritizedItem(priority=priority, item={'uuid': uuid}))\n+                    worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=priority, item={'uuid': uuid}))\n \n                     # Reset for next time\n                     watch.jitter_seconds = 0\ndiff --git a/changedetectionio/forms.py b/changedetectionio/forms.py\nindex cf7512229d6..5d9dafa95aa 100644\n--- a/changedetectionio/forms.py\n+++ b/changedetectionio/forms.py\n@@ -719,6 +719,12 @@ class globalSettingsRequestForm(Form):\n     jitter_seconds = IntegerField('Random jitter seconds \u00b1 check',\n                                   render_kw={\"style\": \"width: 5em;\"},\n                                   validators=[validators.NumberRange(min=0, message=\"Should contain zero or more seconds\")])\n+    \n+    workers = IntegerField('Number of fetch workers',\n+                          render_kw={\"style\": \"width: 5em;\"},\n+                          validators=[validators.NumberRange(min=1, max=50,\n+                                                             message=\"Should be between 1 and 50\")])\n+    \n     extra_proxies = FieldList(FormField(SingleExtraProxy), min_entries=5)\n     extra_browsers = FieldList(FormField(SingleExtraBrowser), min_entries=5)\n \n@@ -733,6 +739,7 @@ def validate_extra_proxies(self, extra_validators=None):\n \n class globalSettingsApplicationUIForm(Form):\n     open_diff_in_new_tab = BooleanField('Open diff page in a new tab', default=True, validators=[validators.Optional()])\n+    socket_io_enabled = BooleanField('Realtime UI Updates Enabled', default=True, validators=[validators.Optional()])\n \n # datastore.data['settings']['application']..\n class globalSettingsApplicationForm(commonSettingsForm):\ndiff --git a/changedetectionio/model/App.py b/changedetectionio/model/App.py\nindex 348090177f3..8de5b0aa3be 100644\n--- a/changedetectionio/model/App.py\n+++ b/changedetectionio/model/App.py\n@@ -62,6 +62,7 @@ class model(dict):\n                     'timezone': None, # Default IANA timezone name\n                     'ui': {\n                         'open_diff_in_new_tab': True,\n+                        'socket_io_enabled': True\n                     },\n                 }\n             }\ndiff --git a/changedetectionio/notification_service.py b/changedetectionio/notification_service.py\nnew file mode 100644\nindex 00000000000..5f3136b8112\n--- /dev/null\n+++ b/changedetectionio/notification_service.py\n@@ -0,0 +1,246 @@\n+#!/usr/bin/env python3\n+\n+\"\"\"\n+Notification Service Module\n+Extracted from update_worker.py to provide standalone notification functionality\n+for both sync and async workers\n+\"\"\"\n+\n+import time\n+from loguru import logger\n+\n+\n+class NotificationService:\n+    \"\"\"\n+    Standalone notification service that handles all notification functionality\n+    previously embedded in the update_worker class\n+    \"\"\"\n+    \n+    def __init__(self, datastore, notification_q):\n+        self.datastore = datastore\n+        self.notification_q = notification_q\n+    \n+    def queue_notification_for_watch(self, n_object, watch):\n+        \"\"\"\n+        Queue a notification for a watch with full diff rendering and template variables\n+        \"\"\"\n+        from changedetectionio import diff\n+        from changedetectionio.notification import default_notification_format_for_watch\n+\n+        dates = []\n+        trigger_text = ''\n+\n+        now = time.time()\n+\n+        if watch:\n+            watch_history = watch.history\n+            dates = list(watch_history.keys())\n+            trigger_text = watch.get('trigger_text', [])\n+\n+        # Add text that was triggered\n+        if len(dates):\n+            snapshot_contents = watch.get_history_snapshot(dates[-1])\n+        else:\n+            snapshot_contents = \"No snapshot/history available, the watch should fetch atleast once.\"\n+\n+        # If we ended up here with \"System default\"\n+        if n_object.get('notification_format') == default_notification_format_for_watch:\n+            n_object['notification_format'] = self.datastore.data['settings']['application'].get('notification_format')\n+\n+        html_colour_enable = False\n+        # HTML needs linebreak, but MarkDown and Text can use a linefeed\n+        if n_object.get('notification_format') == 'HTML':\n+            line_feed_sep = \"<br>\"\n+            # Snapshot will be plaintext on the disk, convert to some kind of HTML\n+            snapshot_contents = snapshot_contents.replace('\\n', line_feed_sep)\n+        elif n_object.get('notification_format') == 'HTML Color':\n+            line_feed_sep = \"<br>\"\n+            # Snapshot will be plaintext on the disk, convert to some kind of HTML\n+            snapshot_contents = snapshot_contents.replace('\\n', line_feed_sep)\n+            html_colour_enable = True\n+        else:\n+            line_feed_sep = \"\\n\"\n+\n+        triggered_text = ''\n+        if len(trigger_text):\n+            from . import html_tools\n+            triggered_text = html_tools.get_triggered_text(content=snapshot_contents, trigger_text=trigger_text)\n+            if triggered_text:\n+                triggered_text = line_feed_sep.join(triggered_text)\n+\n+        # Could be called as a 'test notification' with only 1 snapshot available\n+        prev_snapshot = \"Example text: example test\\nExample text: change detection is cool\\nExample text: some more examples\\n\"\n+        current_snapshot = \"Example text: example test\\nExample text: change detection is fantastic\\nExample text: even more examples\\nExample text: a lot more examples\"\n+\n+        if len(dates) > 1:\n+            prev_snapshot = watch.get_history_snapshot(dates[-2])\n+            current_snapshot = watch.get_history_snapshot(dates[-1])\n+\n+        n_object.update({\n+            'current_snapshot': snapshot_contents,\n+            'diff': diff.render_diff(prev_snapshot, current_snapshot, line_feed_sep=line_feed_sep, html_colour=html_colour_enable),\n+            'diff_added': diff.render_diff(prev_snapshot, current_snapshot, include_removed=False, line_feed_sep=line_feed_sep),\n+            'diff_full': diff.render_diff(prev_snapshot, current_snapshot, include_equal=True, line_feed_sep=line_feed_sep, html_colour=html_colour_enable),\n+            'diff_patch': diff.render_diff(prev_snapshot, current_snapshot, line_feed_sep=line_feed_sep, patch_format=True),\n+            'diff_removed': diff.render_diff(prev_snapshot, current_snapshot, include_added=False, line_feed_sep=line_feed_sep),\n+            'notification_timestamp': now,\n+            'screenshot': watch.get_screenshot() if watch and watch.get('notification_screenshot') else None,\n+            'triggered_text': triggered_text,\n+            'uuid': watch.get('uuid') if watch else None,\n+            'watch_url': watch.get('url') if watch else None,\n+        })\n+\n+        if watch:\n+            n_object.update(watch.extra_notification_token_values())\n+\n+        logger.trace(f\"Main rendered notification placeholders (diff_added etc) calculated in {time.time()-now:.3f}s\")\n+        logger.debug(\"Queued notification for sending\")\n+        self.notification_q.put(n_object)\n+\n+    def _check_cascading_vars(self, var_name, watch):\n+        \"\"\"\n+        Check notification variables in cascading priority:\n+        Individual watch settings > Tag settings > Global settings\n+        \"\"\"\n+        from changedetectionio.notification import (\n+            default_notification_format_for_watch,\n+            default_notification_body,\n+            default_notification_title\n+        )\n+\n+        # Would be better if this was some kind of Object where Watch can reference the parent datastore etc\n+        v = watch.get(var_name)\n+        if v and not watch.get('notification_muted'):\n+            if var_name == 'notification_format' and v == default_notification_format_for_watch:\n+                return self.datastore.data['settings']['application'].get('notification_format')\n+\n+            return v\n+\n+        tags = self.datastore.get_all_tags_for_watch(uuid=watch.get('uuid'))\n+        if tags:\n+            for tag_uuid, tag in tags.items():\n+                v = tag.get(var_name)\n+                if v and not tag.get('notification_muted'):\n+                    return v\n+\n+        if self.datastore.data['settings']['application'].get(var_name):\n+            return self.datastore.data['settings']['application'].get(var_name)\n+\n+        # Otherwise could be defaults\n+        if var_name == 'notification_format':\n+            return default_notification_format_for_watch\n+        if var_name == 'notification_body':\n+            return default_notification_body\n+        if var_name == 'notification_title':\n+            return default_notification_title\n+\n+        return None\n+\n+    def send_content_changed_notification(self, watch_uuid):\n+        \"\"\"\n+        Send notification when content changes are detected\n+        \"\"\"\n+        n_object = {}\n+        watch = self.datastore.data['watching'].get(watch_uuid)\n+        if not watch:\n+            return\n+\n+        watch_history = watch.history\n+        dates = list(watch_history.keys())\n+        # Theoretically it's possible that this could be just 1 long,\n+        # - In the case that the timestamp key was not unique\n+        if len(dates) == 1:\n+            raise ValueError(\n+                \"History index had 2 or more, but only 1 date loaded, timestamps were not unique? maybe two of the same timestamps got written, needs more delay?\"\n+            )\n+\n+        # Should be a better parent getter in the model object\n+\n+        # Prefer - Individual watch settings > Tag settings >  Global settings (in that order)\n+        n_object['notification_urls'] = self._check_cascading_vars('notification_urls', watch)\n+        n_object['notification_title'] = self._check_cascading_vars('notification_title', watch)\n+        n_object['notification_body'] = self._check_cascading_vars('notification_body', watch)\n+        n_object['notification_format'] = self._check_cascading_vars('notification_format', watch)\n+\n+        # (Individual watch) Only prepare to notify if the rules above matched\n+        queued = False\n+        if n_object and n_object.get('notification_urls'):\n+            queued = True\n+\n+            count = watch.get('notification_alert_count', 0) + 1\n+            self.datastore.update_watch(uuid=watch_uuid, update_obj={'notification_alert_count': count})\n+\n+            self.queue_notification_for_watch(n_object=n_object, watch=watch)\n+\n+        return queued\n+\n+    def send_filter_failure_notification(self, watch_uuid):\n+        \"\"\"\n+        Send notification when CSS/XPath filters fail consecutively\n+        \"\"\"\n+        threshold = self.datastore.data['settings']['application'].get('filter_failure_notification_threshold_attempts')\n+        watch = self.datastore.data['watching'].get(watch_uuid)\n+        if not watch:\n+            return\n+\n+        n_object = {'notification_title': 'Changedetection.io - Alert - CSS/xPath filter was not present in the page',\n+                    'notification_body': \"Your configured CSS/xPath filters of '{}' for {{{{watch_url}}}} did not appear on the page after {} attempts, did the page change layout?\\n\\nLink: {{{{base_url}}}}/edit/{{{{watch_uuid}}}}\\n\\nThanks - Your omniscient changedetection.io installation :)\\n\".format(\n+                        \", \".join(watch['include_filters']),\n+                        threshold),\n+                    'notification_format': 'text'}\n+\n+        if len(watch['notification_urls']):\n+            n_object['notification_urls'] = watch['notification_urls']\n+\n+        elif len(self.datastore.data['settings']['application']['notification_urls']):\n+            n_object['notification_urls'] = self.datastore.data['settings']['application']['notification_urls']\n+\n+        # Only prepare to notify if the rules above matched\n+        if 'notification_urls' in n_object:\n+            n_object.update({\n+                'watch_url': watch['url'],\n+                'uuid': watch_uuid,\n+                'screenshot': None\n+            })\n+            self.notification_q.put(n_object)\n+            logger.debug(f\"Sent filter not found notification for {watch_uuid}\")\n+        else:\n+            logger.debug(f\"NOT sending filter not found notification for {watch_uuid} - no notification URLs\")\n+\n+    def send_step_failure_notification(self, watch_uuid, step_n):\n+        \"\"\"\n+        Send notification when browser steps fail consecutively\n+        \"\"\"\n+        watch = self.datastore.data['watching'].get(watch_uuid, False)\n+        if not watch:\n+            return\n+        threshold = self.datastore.data['settings']['application'].get('filter_failure_notification_threshold_attempts')\n+        n_object = {'notification_title': \"Changedetection.io - Alert - Browser step at position {} could not be run\".format(step_n+1),\n+                    'notification_body': \"Your configured browser step at position {} for {{{{watch_url}}}} \"\n+                                         \"did not appear on the page after {} attempts, did the page change layout? \"\n+                                         \"Does it need a delay added?\\n\\nLink: {{{{base_url}}}}/edit/{{{{watch_uuid}}}}\\n\\n\"\n+                                         \"Thanks - Your omniscient changedetection.io installation :)\\n\".format(step_n+1, threshold),\n+                    'notification_format': 'text'}\n+\n+        if len(watch['notification_urls']):\n+            n_object['notification_urls'] = watch['notification_urls']\n+\n+        elif len(self.datastore.data['settings']['application']['notification_urls']):\n+            n_object['notification_urls'] = self.datastore.data['settings']['application']['notification_urls']\n+\n+        # Only prepare to notify if the rules above matched\n+        if 'notification_urls' in n_object:\n+            n_object.update({\n+                'watch_url': watch['url'],\n+                'uuid': watch_uuid\n+            })\n+            self.notification_q.put(n_object)\n+            logger.error(f\"Sent step not found notification for {watch_uuid}\")\n+\n+\n+# Convenience functions for creating notification service instances\n+def create_notification_service(datastore, notification_q):\n+    \"\"\"\n+    Factory function to create a NotificationService instance\n+    \"\"\"\n+    return NotificationService(datastore, notification_q)\n\\ No newline at end of file\ndiff --git a/changedetectionio/processors/__init__.py b/changedetectionio/processors/__init__.py\nindex e7c97a16292..2ae6df4d6ee 100644\n--- a/changedetectionio/processors/__init__.py\n+++ b/changedetectionio/processors/__init__.py\n@@ -27,7 +27,7 @@ def __init__(self, *args, datastore, watch_uuid, **kwargs):\n         # Generic fetcher that should be extended (requests, playwright etc)\n         self.fetcher = Fetcher()\n \n-    def call_browser(self, preferred_proxy_id=None):\n+    async def call_browser(self, preferred_proxy_id=None):\n \n         from requests.structures import CaseInsensitiveDict\n \n@@ -147,16 +147,17 @@ def call_browser(self, preferred_proxy_id=None):\n         # And here we go! call the right browser with browser-specific settings\n         empty_pages_are_a_change = self.datastore.data['settings']['application'].get('empty_pages_are_a_change', False)\n \n-        self.fetcher.run(url=url,\n-                         timeout=timeout,\n-                         request_headers=request_headers,\n-                         request_body=request_body,\n-                         request_method=request_method,\n-                         ignore_status_codes=ignore_status_codes,\n-                         current_include_filters=self.watch.get('include_filters'),\n-                         is_binary=is_binary,\n-                         empty_pages_are_a_change=empty_pages_are_a_change\n-                         )\n+        # All fetchers are now async\n+        await self.fetcher.run(url=url,\n+                               timeout=timeout,\n+                               request_headers=request_headers,\n+                               request_body=request_body,\n+                               request_method=request_method,\n+                               ignore_status_codes=ignore_status_codes,\n+                               current_include_filters=self.watch.get('include_filters'),\n+                               is_binary=is_binary,\n+                               empty_pages_are_a_change=empty_pages_are_a_change\n+                               )\n \n         #@todo .quit here could go on close object, so we can run JS if change-detected\n         self.fetcher.quit(watch=self.watch)\ndiff --git a/changedetectionio/realtime/README.md b/changedetectionio/realtime/README.md\nnew file mode 100644\nindex 00000000000..a391ad2a57c\n--- /dev/null\n+++ b/changedetectionio/realtime/README.md\n@@ -0,0 +1,124 @@\n+# Real-time Socket.IO Implementation\n+\n+This directory contains the Socket.IO implementation for changedetection.io's real-time updates.\n+\n+## Architecture Overview\n+\n+The real-time system provides live updates to the web interface for:\n+- Watch status changes (checking, completed, errors)\n+- Queue length updates  \n+- General statistics updates\n+\n+## Current Implementation\n+\n+### Socket.IO Configuration\n+- **Async Mode**: `threading` (default) or `gevent` (optional via SOCKETIO_MODE env var)\n+- **Server**: Flask-SocketIO with threading support\n+- **Background Tasks**: Python threading with daemon threads\n+\n+### Async Worker Integration\n+- **Workers**: Async workers using asyncio for watch processing\n+- **Queue**: AsyncSignalPriorityQueue for job distribution\n+- **Signals**: Blinker signals for real-time updates between workers and Socket.IO\n+\n+### Environment Variables\n+- `SOCKETIO_MODE=threading` (default, recommended)\n+- `SOCKETIO_MODE=gevent` (optional, has cross-platform limitations)\n+\n+## Architecture Decision: Why Threading Mode?\n+\n+### Previous Issues with Eventlet\n+**Eventlet was completely removed** due to fundamental compatibility issues:\n+\n+1. **Monkey Patching Conflicts**: `eventlet.monkey_patch()` globally replaced Python's threading/socket modules, causing conflicts with:\n+   - Playwright's synchronous browser automation\n+   - Async worker event loops\n+   - Various Python libraries expecting real threading\n+\n+2. **Python 3.12+ Compatibility**: Eventlet had issues with newer Python versions and asyncio integration\n+\n+3. **CVE-2023-29483**: Security vulnerability in eventlet's dnspython dependency\n+\n+### Current Solution Benefits\n+\u2705 **Threading Mode Advantages**:\n+- Full compatibility with async workers and Playwright\n+- No monkey patching - uses standard Python threading\n+- Better Python 3.12+ support\n+- Cross-platform compatibility (Windows, macOS, Linux)\n+- No external async library dependencies\n+- Fast shutdown capabilities\n+\n+\u2705 **Optional Gevent Support**:\n+- Available via `SOCKETIO_MODE=gevent` for high-concurrency scenarios\n+- Cross-platform limitations documented in requirements.txt\n+- Not recommended as default due to Windows socket limits and macOS ARM build issues\n+\n+## Socket.IO Mode Configuration\n+\n+### Threading Mode (Default)\n+```python\n+# Enabled automatically\n+async_mode = 'threading'\n+socketio = SocketIO(app, async_mode='threading')\n+```\n+\n+### Gevent Mode (Optional)\n+```bash\n+# Set environment variable\n+export SOCKETIO_MODE=gevent\n+```\n+\n+## Background Tasks\n+\n+### Queue Polling\n+- **Threading Mode**: `threading.Thread` with `threading.Event` for shutdown\n+- **Signal Handling**: Blinker signals for watch state changes\n+- **Real-time Updates**: Direct Socket.IO `emit()` calls to connected clients\n+\n+### Worker Integration\n+- **Async Workers**: Run in separate asyncio event loop thread\n+- **Communication**: AsyncSignalPriorityQueue bridges async workers and Socket.IO\n+- **Updates**: Real-time updates sent when workers complete tasks\n+\n+## Files in This Directory\n+\n+- `socket_server.py`: Main Socket.IO initialization and event handling\n+- `events.py`: Watch operation event handlers  \n+- `__init__.py`: Module initialization\n+\n+## Production Deployment\n+\n+### Recommended WSGI Servers\n+For production with Socket.IO threading mode:\n+- **Gunicorn**: `gunicorn --worker-class eventlet changedetection:app` (if using gevent mode)\n+- **uWSGI**: With threading support\n+- **Docker**: Built-in Flask server works well for containerized deployments\n+\n+### Performance Considerations\n+- Threading mode: Better memory usage, standard Python threading\n+- Gevent mode: Higher concurrency but platform limitations\n+- Async workers: Separate from Socket.IO, provides scalability\n+\n+## Environment Variables\n+\n+| Variable | Default | Description |\n+|----------|---------|-------------|\n+| `SOCKETIO_MODE` | `threading` | Socket.IO async mode (`threading` or `gevent`) |\n+| `FETCH_WORKERS` | `10` | Number of async workers for watch processing |\n+| `CHANGEDETECTION_HOST` | `0.0.0.0` | Server bind address |\n+| `CHANGEDETECTION_PORT` | `5000` | Server port |\n+\n+## Debugging Tips\n+\n+1. **Socket.IO Issues**: Check browser dev tools for WebSocket connection errors\n+2. **Threading Issues**: Monitor with `ps -T` to check thread count  \n+3. **Worker Issues**: Use `/worker-health` endpoint to check async worker status\n+4. **Queue Issues**: Use `/queue-status` endpoint to monitor job queue\n+5. **Performance**: Use `/gc-cleanup` endpoint to trigger memory cleanup\n+\n+## Migration Notes\n+\n+If upgrading from eventlet-based versions:\n+- Remove any `EVENTLET_*` environment variables\n+- No code changes needed - Socket.IO mode is automatically configured\n+- Optional: Set `SOCKETIO_MODE=gevent` if high concurrency is required and platform supports it\n\\ No newline at end of file\ndiff --git a/changedetectionio/realtime/events.py b/changedetectionio/realtime/events.py\nnew file mode 100644\nindex 00000000000..a68ea99cf13\n--- /dev/null\n+++ b/changedetectionio/realtime/events.py\n@@ -0,0 +1,58 @@\n+from flask_socketio import emit\n+from loguru import logger\n+from blinker import signal\n+\n+\n+def register_watch_operation_handlers(socketio, datastore):\n+    \"\"\"Register Socket.IO event handlers for watch operations\"\"\"\n+    \n+    @socketio.on('watch_operation')\n+    def handle_watch_operation(data):\n+        \"\"\"Handle watch operations like pause, mute, recheck via Socket.IO\"\"\"\n+        try:\n+            op = data.get('op')\n+            uuid = data.get('uuid')\n+            \n+            logger.debug(f\"Socket.IO: Received watch operation '{op}' for UUID {uuid}\")\n+            \n+            if not op or not uuid:\n+                emit('operation_result', {'success': False, 'error': 'Missing operation or UUID'})\n+                return\n+            \n+            # Check if watch exists\n+            if not datastore.data['watching'].get(uuid):\n+                emit('operation_result', {'success': False, 'error': 'Watch not found'})\n+                return\n+            \n+            watch = datastore.data['watching'][uuid]\n+            \n+            # Perform the operation\n+            if op == 'pause':\n+                watch.toggle_pause()\n+                logger.info(f\"Socket.IO: Toggled pause for watch {uuid}\")\n+            elif op == 'mute':\n+                watch.toggle_mute()\n+                logger.info(f\"Socket.IO: Toggled mute for watch {uuid}\")\n+            elif op == 'recheck':\n+                # Import here to avoid circular imports\n+                from changedetectionio.flask_app import update_q\n+                from changedetectionio import queuedWatchMetaData\n+                from changedetectionio import worker_handler\n+                \n+                worker_handler.queue_item_async_safe(update_q, queuedWatchMetaData.PrioritizedItem(priority=1, item={'uuid': uuid}))\n+                logger.info(f\"Socket.IO: Queued recheck for watch {uuid}\")\n+            else:\n+                emit('operation_result', {'success': False, 'error': f'Unknown operation: {op}'})\n+                return\n+            \n+            # Send signal to update UI\n+            watch_check_update = signal('watch_check_update')\n+            if watch_check_update:\n+                watch_check_update.send(watch_uuid=uuid)\n+            \n+            # Send success response to client\n+            emit('operation_result', {'success': True, 'operation': op, 'uuid': uuid})\n+            \n+        except Exception as e:\n+            logger.error(f\"Socket.IO error in handle_watch_operation: {str(e)}\")\n+            emit('operation_result', {'success': False, 'error': str(e)})\ndiff --git a/changedetectionio/realtime/socket_server.py b/changedetectionio/realtime/socket_server.py\nindex 9f7c39cf845..c9241486ae3 100644\n--- a/changedetectionio/realtime/socket_server.py\n+++ b/changedetectionio/realtime/socket_server.py\n@@ -8,8 +8,10 @@\n \n from changedetectionio import strtobool\n \n+\n class SignalHandler:\n     \"\"\"A standalone class to receive signals\"\"\"\n+\n     def __init__(self, socketio_instance, datastore):\n         self.socketio_instance = socketio_instance\n         self.datastore = datastore\n@@ -17,19 +19,22 @@ def __init__(self, socketio_instance, datastore):\n         # Connect to the watch_check_update signal\n         from changedetectionio.flask_app import watch_check_update as wcc\n         wcc.connect(self.handle_signal, weak=False)\n-        logger.info(\"SignalHandler: Connected to signal from direct import\")\n-        \n+        #        logger.info(\"SignalHandler: Connected to signal from direct import\")\n+\n         # Connect to the queue_length signal\n         queue_length_signal = signal('queue_length')\n         queue_length_signal.connect(self.handle_queue_length, weak=False)\n-        logger.info(\"SignalHandler: Connected to queue_length signal\")\n+        #       logger.info(\"SignalHandler: Connected to queue_length signal\")\n \n+        # Create and start the queue update thread using standard threading\n+        import threading\n+        self.polling_emitter_thread = threading.Thread(\n+            target=self.polling_emit_running_or_queued_watches_threaded, \n+            daemon=True\n+        )\n+        self.polling_emitter_thread.start()\n+        logger.info(\"Started polling thread using threading (eventlet-free)\")\n \n-        # Create and start the queue update thread using gevent\n-        import gevent\n-        logger.info(\"Using gevent for polling thread\")\n-        self.polling_emitter_thread = gevent.spawn(self.polling_emit_running_or_queued_watches)\n-        \n         # Store the thread reference in socketio for clean shutdown\n         self.socketio_instance.polling_emitter_thread = self.polling_emitter_thread\n \n@@ -44,7 +49,7 @@ def handle_signal(self, *args, **kwargs):\n             watch = self.datastore.data['watching'].get(watch_uuid)\n             if watch:\n                 if app_context:\n-                    #note\n+                    # note\n                     with app_context.app_context():\n                         with app_context.test_request_context():\n                             # Forward to handle_watch_update with the watch parameter\n@@ -61,59 +66,85 @@ def handle_queue_length(self, *args, **kwargs):\n         try:\n             queue_length = kwargs.get('length', 0)\n             logger.debug(f\"SignalHandler: Queue length update received: {queue_length}\")\n-            \n+\n             # Emit the queue size to all connected clients\n             self.socketio_instance.emit(\"queue_size\", {\n                 \"q_length\": queue_length,\n                 \"event_timestamp\": time.time()\n             })\n-            \n+\n         except Exception as e:\n             logger.error(f\"Socket.IO error in handle_queue_length: {str(e)}\")\n \n \n-    def polling_emit_running_or_queued_watches(self):\n-        \"\"\"Greenlet that periodically updates the browser/frontend with current state of who is being checked or queued\n-        This is because sometimes the browser page could reload (like on clicking on a link) but the data is old\n-        \"\"\"\n-        logger.info(\"Queue update greenlet started\")\n-\n-        # Import the watch_check_update signal, update_q, and running_update_threads here to avoid circular imports\n-        from changedetectionio.flask_app import app, running_update_threads\n+    def polling_emit_running_or_queued_watches_threaded(self):\n+        \"\"\"Threading version of polling for Windows compatibility\"\"\"\n+        import time\n+        import threading\n+        logger.info(\"Queue update thread started (threading mode)\")\n+        \n+        # Import here to avoid circular imports\n+        from changedetectionio.flask_app import app\n+        from changedetectionio import worker_handler\n         watch_check_update = signal('watch_check_update')\n         \n-        # Use gevent sleep for non-blocking operation\n-        from gevent import sleep as gevent_sleep\n-\n-        # Get the stop event from the socketio instance\n-        stop_event = self.socketio_instance.stop_event if hasattr(self.socketio_instance, 'stop_event') else None\n-\n-        # Run until explicitly stopped\n-        while stop_event is None or not stop_event.is_set():\n+        # Track previous state to avoid unnecessary emissions\n+        previous_running_uuids = set()\n+        \n+        # Run until app shutdown - check exit flag more frequently for fast shutdown\n+        exit_event = getattr(app.config, 'exit', threading.Event())\n+        \n+        while not exit_event.is_set():\n             try:\n-                # For each item in the queue, send a signal, so we update the UI\n-                for t in running_update_threads:\n-                    if hasattr(t, 'current_uuid') and t.current_uuid:\n-                        logger.trace(f\"Sending update for {t.current_uuid}\")\n-                        # Send with app_context to ensure proper URL generation\n+                # Get current running UUIDs from async workers\n+                running_uuids = set(worker_handler.get_running_uuids())\n+                \n+                # Only send updates for UUIDs that changed state\n+                newly_running = running_uuids - previous_running_uuids\n+                no_longer_running = previous_running_uuids - running_uuids\n+                \n+                # Send updates for newly running UUIDs (but exit fast if shutdown requested)\n+                for uuid in newly_running:\n+                    if exit_event.is_set():\n+                        break\n+                    logger.trace(f\"Threading polling: UUID {uuid} started processing\")\n+                    with app.app_context():\n+                        watch_check_update.send(app_context=app, watch_uuid=uuid)\n+                    time.sleep(0.01)  # Small yield\n+                \n+                # Send updates for UUIDs that finished processing (but exit fast if shutdown requested)\n+                if not exit_event.is_set():\n+                    for uuid in no_longer_running:\n+                        if exit_event.is_set():\n+                            break\n+                        logger.trace(f\"Threading polling: UUID {uuid} finished processing\")\n                         with app.app_context():\n-                            watch_check_update.send(app_context=app, watch_uuid=t.current_uuid)\n-                        # Yield control back to gevent after each send to prevent blocking\n-                        gevent_sleep(0.1)  # Small sleep to yield control\n-                    \n-                    # Check if we need to stop in the middle of processing\n-                    if stop_event is not None and stop_event.is_set():\n+                            watch_check_update.send(app_context=app, watch_uuid=uuid)\n+                        time.sleep(0.01)  # Small yield\n+                \n+                # Update tracking for next iteration\n+                previous_running_uuids = running_uuids\n+                \n+                # Sleep between polling cycles, but check exit flag every 0.5 seconds for fast shutdown\n+                for _ in range(20):  # 20 * 0.5 = 10 seconds total\n+                    if exit_event.is_set():\n                         break\n-\n-                # Sleep between polling/update cycles\n-                gevent_sleep(2)\n-\n+                    time.sleep(0.5)\n+                \n             except Exception as e:\n-                logger.error(f\"Error in queue update greenlet: {str(e)}\")\n-                # Sleep a bit to avoid flooding logs in case of persistent error\n-                gevent_sleep(0.5)\n-\n-        logger.info(\"Queue update greenlet stopped\")\n+                logger.error(f\"Error in threading polling: {str(e)}\")\n+                # Even during error recovery, check for exit quickly\n+                for _ in range(1):  # 1 * 0.5 = 0.5 seconds\n+                    if exit_event.is_set():\n+                        break\n+                    time.sleep(0.5)\n+        \n+        # Check if we're in pytest environment - if so, be more gentle with logging\n+        import sys\n+        in_pytest = \"pytest\" in sys.modules or \"PYTEST_CURRENT_TEST\" in os.environ\n+        \n+        if not in_pytest:\n+            logger.info(\"Queue update thread stopped (threading mode)\")\n \n \n def handle_watch_update(socketio, **kwargs):\n@@ -123,14 +154,12 @@ def handle_watch_update(socketio, **kwargs):\n         datastore = kwargs.get('datastore')\n \n         # Emit the watch update to all connected clients\n-        from changedetectionio.flask_app import running_update_threads, update_q\n+        from changedetectionio.flask_app import update_q\n         from changedetectionio.flask_app import _jinja2_filter_datetime\n+        from changedetectionio import worker_handler\n \n         # Get list of watches that are currently running\n-        running_uuids = []\n-        for t in running_update_threads:\n-            if hasattr(t, 'current_uuid') and t.current_uuid:\n-                running_uuids.append(t.current_uuid)\n+        running_uuids = worker_handler.get_running_uuids()\n \n         # Get list of watches in the queue\n         queue_list = []\n@@ -143,26 +172,30 @@ def handle_watch_update(socketio, **kwargs):\n         error_texts = watch.compile_error_texts()\n \n         # Create a simplified watch data object to send to clients\n+        watch_uuid = watch.get('uuid')\n+        \n         watch_data = {\n-            'checking_now': True if watch.get('uuid') in running_uuids else False,\n+            'checking_now': True if watch_uuid in running_uuids else False,\n             'fetch_time': watch.get('fetch_time'),\n             'has_error': True if error_texts else False,\n             'last_changed': watch.get('last_changed'),\n             'last_checked': watch.get('last_checked'),\n             'error_text': error_texts,\n+            'history_n': watch.history_n,\n             'last_checked_text': _jinja2_filter_datetime(watch),\n-            'last_changed_text': timeago.format(int(watch['last_changed']), time.time()) if watch.history_n >= 2 and int(watch.get('last_changed', 0)) > 0 else 'Not yet',\n-            'queued': True if watch.get('uuid') in queue_list else False,\n+            'last_changed_text': timeago.format(int(watch['last_changed']), time.time()) if watch.history_n >= 2 and int(\n+                watch.get('last_changed', 0)) > 0 else 'Not yet',\n+            'queued': True if watch_uuid in queue_list else False,\n             'paused': True if watch.get('paused') else False,\n             'notification_muted': True if watch.get('notification_muted') else False,\n             'unviewed': watch.has_unviewed,\n-            'uuid': watch.get('uuid'),\n+            'uuid': watch_uuid,\n             'event_timestamp': time.time()\n         }\n \n-        errored_count =0\n-        for uuid, watch in datastore.data['watching'].items():\n-            if watch.get('last_error'):\n+        errored_count = 0\n+        for watch_uuid_iter, watch_iter in datastore.data['watching'].items():\n+            if watch_iter.get('last_error'):\n                 errored_count += 1\n \n         general_stats = {\n@@ -171,13 +204,13 @@ def handle_watch_update(socketio, **kwargs):\n         }\n \n         # Debug what's being emitted\n-        #logger.debug(f\"Emitting 'watch_update' event for {watch.get('uuid')}, data: {watch_data}\")\n-        \n+        # logger.debug(f\"Emitting 'watch_update' event for {watch.get('uuid')}, data: {watch_data}\")\n+\n         # Emit to all clients (no 'broadcast' parameter needed - it's the default behavior)\n         socketio.emit(\"watch_update\", {'watch': watch_data, 'general_stats': general_stats})\n-        \n-        # Log after successful emit\n-        #logger.info(f\"Socket.IO: Emitted update for watch {watch.get('uuid')}, Checking now: {watch_data['checking_now']}\")\n+\n+        # Log after successful emit - use watch_data['uuid'] to avoid variable shadowing issues\n+        logger.trace(f\"Socket.IO: Emitted update for watch {watch_data['uuid']}, Checking now: {watch_data['checking_now']}\")\n \n     except Exception as e:\n         logger.error(f\"Socket.IO error in handle_watch_update: {str(e)}\")\n@@ -185,35 +218,65 @@ def handle_watch_update(socketio, **kwargs):\n \n def init_socketio(app, datastore):\n     \"\"\"Initialize SocketIO with the main Flask app\"\"\"\n-    # Use the threading async_mode instead of eventlet\n-    # This avoids the need for monkey patching eventlet,\n-    # Which leads to problems with async playwright etc\n-    async_mode = 'gevent'\n-    logger.info(f\"Using {async_mode} mode for Socket.IO\")\n+    import platform\n+    import sys\n+    \n+    # Platform-specific async_mode selection for better stability\n+    system = platform.system().lower()\n+    python_version = sys.version_info\n+    \n+    # Check for SocketIO mode configuration via environment variable\n+    # Default is 'threading' for best cross-platform compatibility\n+    socketio_mode = os.getenv('SOCKETIO_MODE', 'threading').lower()\n+    \n+    if socketio_mode == 'gevent':\n+        # Use gevent mode (higher concurrency but platform limitations)\n+        try:\n+            import gevent\n+            async_mode = 'gevent'\n+            logger.info(f\"SOCKETIO_MODE=gevent: Using {async_mode} mode for Socket.IO\")\n+        except ImportError:\n+            async_mode = 'threading'\n+            logger.warning(f\"SOCKETIO_MODE=gevent but gevent not available, falling back to {async_mode} mode\")\n+    elif socketio_mode == 'threading':\n+        # Use threading mode (default - best compatibility)\n+        async_mode = 'threading'\n+        logger.info(f\"SOCKETIO_MODE=threading: Using {async_mode} mode for Socket.IO\")\n+    else:\n+        # Invalid mode specified, use default\n+        async_mode = 'threading'\n+        logger.warning(f\"Invalid SOCKETIO_MODE='{socketio_mode}', using default {async_mode} mode for Socket.IO\")\n+    \n+    # Log platform info for debugging\n+    logger.info(f\"Platform: {system}, Python: {python_version.major}.{python_version.minor}, Socket.IO mode: {async_mode}\")\n \n     # Restrict SocketIO CORS to same origin by default, can be overridden with env var\n     cors_origins = os.environ.get('SOCKETIO_CORS_ORIGINS', None)\n-    \n+\n     socketio = SocketIO(app,\n-                      async_mode=async_mode,\n-                      cors_allowed_origins=cors_origins,  # None means same-origin only\n-                      logger=strtobool(os.getenv('SOCKETIO_LOGGING', 'False')),\n-                      engineio_logger=strtobool(os.getenv('SOCKETIO_LOGGING', 'False')))\n+                        async_mode=async_mode,\n+                        cors_allowed_origins=cors_origins,  # None means same-origin only\n+                        logger=strtobool(os.getenv('SOCKETIO_LOGGING', 'False')),\n+                        engineio_logger=strtobool(os.getenv('SOCKETIO_LOGGING', 'False')))\n \n     # Set up event handlers\n+    logger.info(\"Socket.IO: Registering connect event handler\")\n+\n     @socketio.on('connect')\n     def handle_connect():\n         \"\"\"Handle client connection\"\"\"\n-        from changedetectionio.auth_decorator import login_optionally_required\n+        #        logger.info(\"Socket.IO: CONNECT HANDLER CALLED - Starting connection process\")\n         from flask import request\n         from flask_login import current_user\n         from changedetectionio.flask_app import update_q\n \n         # Access datastore from socketio\n         datastore = socketio.datastore\n+        #        logger.info(f\"Socket.IO: Current user authenticated: {current_user.is_authenticated if hasattr(current_user, 'is_authenticated') else 'No current_user'}\")\n \n         # Check if authentication is required and user is not authenticated\n         has_password_enabled = datastore.data['settings']['application'].get('password') or os.getenv(\"SALTED_PASS\", False)\n+        #        logger.info(f\"Socket.IO: Password enabled: {has_password_enabled}\")\n         if has_password_enabled and not current_user.is_authenticated:\n             logger.warning(\"Socket.IO: Rejecting unauthenticated connection\")\n             return False  # Reject the connection\n@@ -231,6 +294,7 @@ def handle_connect():\n \n         logger.info(\"Socket.IO: Client connected\")\n \n+    #    logger.info(\"Socket.IO: Registering disconnect event handler\")\n     @socketio.on('disconnect')\n     def handle_disconnect():\n         \"\"\"Handle client disconnection\"\"\"\n@@ -239,45 +303,40 @@ def handle_disconnect():\n     # Create a dedicated signal handler that will receive signals and emit them to clients\n     signal_handler = SignalHandler(socketio, datastore)\n \n+    # Register watch operation event handlers\n+    from .events import register_watch_operation_handlers\n+    register_watch_operation_handlers(socketio, datastore)\n+\n     # Store the datastore reference on the socketio object for later use\n     socketio.datastore = datastore\n-    \n-    # Create a stop event for our queue update thread using gevent Event\n-    import gevent.event\n-    stop_event = gevent.event.Event()\n-    socketio.stop_event = stop_event\n \n-    \n+    # No stop event needed for threading mode - threads check app.config.exit directly\n+\n     # Add a shutdown method to the socketio object\n     def shutdown():\n-        \"\"\"Shutdown the SocketIO server gracefully\"\"\"\n+        \"\"\"Shutdown the SocketIO server fast and aggressively\"\"\"\n         try:\n-            logger.info(\"Socket.IO: Shutting down server...\")\n-            \n-            # Signal the queue update thread to stop\n-            if hasattr(socketio, 'stop_event'):\n-                socketio.stop_event.set()\n-                logger.info(\"Socket.IO: Signaled queue update thread to stop\")\n-            \n-            # Wait for the greenlet to exit (with timeout)\n+            logger.info(\"Socket.IO: Fast shutdown initiated...\")\n+\n+            # For threading mode, give the thread a very short time to exit gracefully\n             if hasattr(socketio, 'polling_emitter_thread'):\n-                try:\n-                    # For gevent greenlets\n-                    socketio.polling_emitter_thread.join(timeout=5)\n-                    logger.info(\"Socket.IO: Queue update greenlet joined successfully\")\n-                except Exception as e:\n-                    logger.error(f\"Error joining greenlet: {str(e)}\")\n-                    logger.info(\"Socket.IO: Queue update greenlet did not exit in time\")\n-            \n-            # Close any remaining client connections\n-            #if hasattr(socketio, 'server'):\n-            #    socketio.server.disconnect()\n-            logger.info(\"Socket.IO: Server shutdown complete\")\n+                if socketio.polling_emitter_thread.is_alive():\n+                    logger.info(\"Socket.IO: Waiting 1 second for polling thread to stop...\")\n+                    socketio.polling_emitter_thread.join(timeout=1.0)  # Only 1 second timeout\n+                    if socketio.polling_emitter_thread.is_alive():\n+                        logger.info(\"Socket.IO: Polling thread still running after timeout - continuing with shutdown\")\n+                    else:\n+                        logger.info(\"Socket.IO: Polling thread stopped quickly\")\n+                else:\n+                    logger.info(\"Socket.IO: Polling thread already stopped\")\n+\n+            logger.info(\"Socket.IO: Fast shutdown complete\")\n         except Exception as e:\n             logger.error(f\"Socket.IO error during shutdown: {str(e)}\")\n-    \n+\n     # Attach the shutdown method to the socketio object\n     socketio.shutdown = shutdown\n \n     logger.info(\"Socket.IO initialized and attached to main Flask app\")\n+    logger.info(f\"Socket.IO: Registered event handlers: {socketio.handlers if hasattr(socketio, 'handlers') else 'No handlers found'}\")\n     return socketio\ndiff --git a/changedetectionio/static/js/realtime.js b/changedetectionio/static/js/realtime.js\nindex 1ff1d19e3dd..5f6984c9cfc 100644\n--- a/changedetectionio/static/js/realtime.js\n+++ b/changedetectionio/static/js/realtime.js\n@@ -2,20 +2,20 @@\n \n $(document).ready(function () {\n \n-    function bindAjaxHandlerButtonsEvents() {\n-        $('.ajax-op').on('click.ajaxHandlerNamespace', function (e) {\n+    function bindSocketHandlerButtonsEvents(socket) {\n+        $('.ajax-op').on('click.socketHandlerNamespace', function (e) {\n             e.preventDefault();\n-            $.ajax({\n-                type: \"POST\",\n-                url: ajax_toggle_url,\n-                data: {'op': $(this).data('op'), 'uuid': $(this).closest('tr').data('watch-uuid')},\n-                statusCode: {\n-                    400: function () {\n-                        // More than likely the CSRF token was lost when the server restarted\n-                        alert(\"There was a problem processing the request, please reload the page.\");\n-                    }\n-                }\n+            const op = $(this).data('op');\n+            const uuid = $(this).closest('tr').data('watch-uuid');\n+            \n+            console.log(`Socket.IO: Sending watch operation '${op}' for UUID ${uuid}`);\n+            \n+            // Emit the operation via Socket.IO\n+            socket.emit('watch_operation', {\n+                'op': op,\n+                'uuid': uuid\n             });\n+            \n             return false;\n         });\n     }\n@@ -38,7 +38,7 @@ $(document).ready(function () {\n             socket.on('connect', function () {\n                 console.log('Socket.IO connected with path:', socketio_url);\n                 console.log('Socket transport:', socket.io.engine.transport.name);\n-                bindAjaxHandlerButtonsEvents();\n+                bindSocketHandlerButtonsEvents(socket);\n             });\n \n             socket.on('connect_error', function(error) {\n@@ -55,7 +55,7 @@ $(document).ready(function () {\n \n             socket.on('disconnect', function (reason) {\n                 console.log('Socket.IO disconnected, reason:', reason);\n-                $('.ajax-op').off('.ajaxHandlerNamespace')\n+                $('.ajax-op').off('.socketHandlerNamespace')\n             });\n \n             socket.on('queue_size', function (data) {\n@@ -63,6 +63,16 @@ $(document).ready(function () {\n                 // Update queue size display if implemented in the UI\n             })\n \n+            // Listen for operation results\n+            socket.on('operation_result', function (data) {\n+                if (data.success) {\n+                    console.log(`Socket.IO: Operation '${data.operation}' completed successfully for UUID ${data.uuid}`);\n+                } else {\n+                    console.error(`Socket.IO: Operation failed: ${data.error}`);\n+                    alert(\"There was a problem processing the request: \" + data.error);\n+                }\n+            });\n+\n             // Listen for periodically emitted watch data\n             console.log('Adding watch_update event listener');\n \n@@ -87,6 +97,8 @@ $(document).ready(function () {\n                     $($watchRow).toggleClass('has-error', watch.has_error);\n                     $($watchRow).toggleClass('notification_muted', watch.notification_muted);\n                     $($watchRow).toggleClass('paused', watch.paused);\n+                    $($watchRow).toggleClass('single-history', watch.history_n === 1);\n+                    $($watchRow).toggleClass('multiple-history', watch.history_n >= 2);\n \n                     $('td.title-col .error-text', $watchRow).html(watch.error_text)\n \ndiff --git a/changedetectionio/static/js/socket.io.min.js b/changedetectionio/static/js/socket.io.min.js\nindex bb469c4e0ee..7738dec17d5 100644\n--- a/changedetectionio/static/js/socket.io.min.js\n+++ b/changedetectionio/static/js/socket.io.min.js\n@@ -1,7 +1,7 @@\n /*!\n- * Socket.IO v4.6.0\n- * (c) 2014-2023 Guillermo Rauch\n+ * Socket.IO v3.1.3\n+ * (c) 2014-2021 Guillermo Rauch\n  * Released under the MIT License.\n  */\n-!function(t,e){\"object\"==typeof exports&&\"undefined\"!=typeof module?module.exports=e():\"function\"==typeof define&&define.amd?define(e):(t=\"undefined\"!=typeof globalThis?globalThis:t||self).io=e()}(this,(function(){\"use strict\";function t(e){return t=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t},t(e)}function e(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}function n(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}function r(t,e,r){return e&&n(t.prototype,e),r&&n(t,r),Object.defineProperty(t,\"prototype\",{writable:!1}),t}function i(){return i=Object.assign?Object.assign.bind():function(t){for(var e=1;e<arguments.length;e++){var n=arguments[e];for(var r in n)Object.prototype.hasOwnProperty.call(n,r)&&(t[r]=n[r])}return t},i.apply(this,arguments)}function o(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function\");t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),Object.defineProperty(t,\"prototype\",{writable:!1}),e&&a(t,e)}function s(t){return s=Object.setPrototypeOf?Object.getPrototypeOf.bind():function(t){return t.__proto__||Object.getPrototypeOf(t)},s(t)}function a(t,e){return a=Object.setPrototypeOf?Object.setPrototypeOf.bind():function(t,e){return t.__proto__=e,t},a(t,e)}function c(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(t){return!1}}function u(t,e,n){return u=c()?Reflect.construct.bind():function(t,e,n){var r=[null];r.push.apply(r,e);var i=new(Function.bind.apply(t,r));return n&&a(i,n.prototype),i},u.apply(null,arguments)}function h(t){var e=\"function\"==typeof Map?new Map:void 0;return h=function(t){if(null===t||(n=t,-1===Function.toString.call(n).indexOf(\"[native code]\")))return t;var n;if(\"function\"!=typeof t)throw new TypeError(\"Super expression must either be null or a function\");if(void 0!==e){if(e.has(t))return e.get(t);e.set(t,r)}function r(){return u(t,arguments,s(this).constructor)}return r.prototype=Object.create(t.prototype,{constructor:{value:r,enumerable:!1,writable:!0,configurable:!0}}),a(r,t)},h(t)}function f(t){if(void 0===t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return t}function l(t,e){if(e&&(\"object\"==typeof e||\"function\"==typeof e))return e;if(void 0!==e)throw new TypeError(\"Derived constructors may only return object or undefined\");return f(t)}function p(t){var e=c();return function(){var n,r=s(t);if(e){var i=s(this).constructor;n=Reflect.construct(r,arguments,i)}else n=r.apply(this,arguments);return l(this,n)}}function d(t,e){for(;!Object.prototype.hasOwnProperty.call(t,e)&&null!==(t=s(t)););return t}function y(){return y=\"undefined\"!=typeof Reflect&&Reflect.get?Reflect.get.bind():function(t,e,n){var r=d(t,e);if(r){var i=Object.getOwnPropertyDescriptor(r,e);return i.get?i.get.call(arguments.length<3?t:n):i.value}},y.apply(this,arguments)}function v(t,e){(null==e||e>t.length)&&(e=t.length);for(var n=0,r=new Array(e);n<e;n++)r[n]=t[n];return r}function g(t,e){var n=\"undefined\"!=typeof Symbol&&t[Symbol.iterator]||t[\"@@iterator\"];if(!n){if(Array.isArray(t)||(n=function(t,e){if(t){if(\"string\"==typeof t)return v(t,e);var n=Object.prototype.toString.call(t).slice(8,-1);return\"Object\"===n&&t.constructor&&(n=t.constructor.name),\"Map\"===n||\"Set\"===n?Array.from(t):\"Arguments\"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)?v(t,e):void 0}}(t))||e&&t&&\"number\"==typeof t.length){n&&(t=n);var r=0,i=function(){};return{s:i,n:function(){return r>=t.length?{done:!0}:{done:!1,value:t[r++]}},e:function(t){throw t},f:i}}throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\")}var o,s=!0,a=!1;return{s:function(){n=n.call(t)},n:function(){var t=n.next();return s=t.done,t},e:function(t){a=!0,o=t},f:function(){try{s||null==n.return||n.return()}finally{if(a)throw o}}}}var m=Object.create(null);m.open=\"0\",m.close=\"1\",m.ping=\"2\",m.pong=\"3\",m.message=\"4\",m.upgrade=\"5\",m.noop=\"6\";var k=Object.create(null);Object.keys(m).forEach((function(t){k[m[t]]=t}));for(var b={type:\"error\",data:\"parser error\"},w=\"function\"==typeof Blob||\"undefined\"!=typeof Blob&&\"[object BlobConstructor]\"===Object.prototype.toString.call(Blob),_=\"function\"==typeof ArrayBuffer,E=function(t,e,n){var r,i=t.type,o=t.data;return w&&o instanceof Blob?e?n(o):O(o,n):_&&(o instanceof ArrayBuffer||(r=o,\"function\"==typeof ArrayBuffer.isView?ArrayBuffer.isView(r):r&&r.buffer instanceof ArrayBuffer))?e?n(o):O(new Blob([o]),n):n(m[i]+(o||\"\"))},O=function(t,e){var n=new FileReader;return n.onload=function(){var t=n.result.split(\",\")[1];e(\"b\"+t)},n.readAsDataURL(t)},A=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\",R=\"undefined\"==typeof Uint8Array?[]:new Uint8Array(256),T=0;T<A.length;T++)R[A.charCodeAt(T)]=T;var C=\"function\"==typeof ArrayBuffer,B=function(t,e){if(\"string\"!=typeof t)return{type:\"message\",data:N(t,e)};var n=t.charAt(0);return\"b\"===n?{type:\"message\",data:S(t.substring(1),e)}:k[n]?t.length>1?{type:k[n],data:t.substring(1)}:{type:k[n]}:b},S=function(t,e){if(C){var n=function(t){var e,n,r,i,o,s=.75*t.length,a=t.length,c=0;\"=\"===t[t.length-1]&&(s--,\"=\"===t[t.length-2]&&s--);var u=new ArrayBuffer(s),h=new Uint8Array(u);for(e=0;e<a;e+=4)n=R[t.charCodeAt(e)],r=R[t.charCodeAt(e+1)],i=R[t.charCodeAt(e+2)],o=R[t.charCodeAt(e+3)],h[c++]=n<<2|r>>4,h[c++]=(15&r)<<4|i>>2,h[c++]=(3&i)<<6|63&o;return u}(t);return N(n,e)}return{base64:!0,data:t}},N=function(t,e){return\"blob\"===e&&t instanceof ArrayBuffer?new Blob([t]):t},x=String.fromCharCode(30);function L(t){if(t)return function(t){for(var e in L.prototype)t[e]=L.prototype[e];return t}(t)}L.prototype.on=L.prototype.addEventListener=function(t,e){return this._callbacks=this._callbacks||{},(this._callbacks[\"$\"+t]=this._callbacks[\"$\"+t]||[]).push(e),this},L.prototype.once=function(t,e){function n(){this.off(t,n),e.apply(this,arguments)}return n.fn=e,this.on(t,n),this},L.prototype.off=L.prototype.removeListener=L.prototype.removeAllListeners=L.prototype.removeEventListener=function(t,e){if(this._callbacks=this._callbacks||{},0==arguments.length)return this._callbacks={},this;var n,r=this._callbacks[\"$\"+t];if(!r)return this;if(1==arguments.length)return delete this._callbacks[\"$\"+t],this;for(var i=0;i<r.length;i++)if((n=r[i])===e||n.fn===e){r.splice(i,1);break}return 0===r.length&&delete this._callbacks[\"$\"+t],this},L.prototype.emit=function(t){this._callbacks=this._callbacks||{};for(var e=new Array(arguments.length-1),n=this._callbacks[\"$\"+t],r=1;r<arguments.length;r++)e[r-1]=arguments[r];if(n){r=0;for(var i=(n=n.slice(0)).length;r<i;++r)n[r].apply(this,e)}return this},L.prototype.emitReserved=L.prototype.emit,L.prototype.listeners=function(t){return this._callbacks=this._callbacks||{},this._callbacks[\"$\"+t]||[]},L.prototype.hasListeners=function(t){return!!this.listeners(t).length};var P=\"undefined\"!=typeof self?self:\"undefined\"!=typeof window?window:Function(\"return this\")();function j(t){for(var e=arguments.length,n=new Array(e>1?e-1:0),r=1;r<e;r++)n[r-1]=arguments[r];return n.reduce((function(e,n){return t.hasOwnProperty(n)&&(e[n]=t[n]),e}),{})}var q=P.setTimeout,I=P.clearTimeout;function D(t,e){e.useNativeTimers?(t.setTimeoutFn=q.bind(P),t.clearTimeoutFn=I.bind(P)):(t.setTimeoutFn=P.setTimeout.bind(P),t.clearTimeoutFn=P.clearTimeout.bind(P))}var F,M=function(t){o(i,t);var n=p(i);function i(t,r,o){var s;return e(this,i),(s=n.call(this,t)).description=r,s.context=o,s.type=\"TransportError\",s}return r(i)}(h(Error)),U=function(t){o(i,t);var n=p(i);function i(t){var r;return e(this,i),(r=n.call(this)).writable=!1,D(f(r),t),r.opts=t,r.query=t.query,r.socket=t.socket,r}return r(i,[{key:\"onError\",value:function(t,e,n){return y(s(i.prototype),\"emitReserved\",this).call(this,\"error\",new M(t,e,n)),this}},{key:\"open\",value:function(){return this.readyState=\"opening\",this.doOpen(),this}},{key:\"close\",value:function(){return\"opening\"!==this.readyState&&\"open\"!==this.readyState||(this.doClose(),this.onClose()),this}},{key:\"send\",value:function(t){\"open\"===this.readyState&&this.write(t)}},{key:\"onOpen\",value:function(){this.readyState=\"open\",this.writable=!0,y(s(i.prototype),\"emitReserved\",this).call(this,\"open\")}},{key:\"onData\",value:function(t){var e=B(t,this.socket.binaryType);this.onPacket(e)}},{key:\"onPacket\",value:function(t){y(s(i.prototype),\"emitReserved\",this).call(this,\"packet\",t)}},{key:\"onClose\",value:function(t){this.readyState=\"closed\",y(s(i.prototype),\"emitReserved\",this).call(this,\"close\",t)}},{key:\"pause\",value:function(t){}}]),i}(L),V=\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-_\".split(\"\"),H={},K=0,Y=0;function z(t){var e=\"\";do{e=V[t%64]+e,t=Math.floor(t/64)}while(t>0);return e}function W(){var t=z(+new Date);return t!==F?(K=0,F=t):t+\".\"+z(K++)}for(;Y<64;Y++)H[V[Y]]=Y;function $(t){var e=\"\";for(var n in t)t.hasOwnProperty(n)&&(e.length&&(e+=\"&\"),e+=encodeURIComponent(n)+\"=\"+encodeURIComponent(t[n]));return e}function J(t){for(var e={},n=t.split(\"&\"),r=0,i=n.length;r<i;r++){var o=n[r].split(\"=\");e[decodeURIComponent(o[0])]=decodeURIComponent(o[1])}return e}var Q=!1;try{Q=\"undefined\"!=typeof XMLHttpRequest&&\"withCredentials\"in new XMLHttpRequest}catch(t){}var X=Q;function G(t){var e=t.xdomain;try{if(\"undefined\"!=typeof XMLHttpRequest&&(!e||X))return new XMLHttpRequest}catch(t){}if(!e)try{return new(P[[\"Active\"].concat(\"Object\").join(\"X\")])(\"Microsoft.XMLHTTP\")}catch(t){}}function Z(){}var tt=null!=new G({xdomain:!1}).responseType,et=function(t){o(s,t);var n=p(s);function s(t){var r;if(e(this,s),(r=n.call(this,t)).polling=!1,\"undefined\"!=typeof location){var i=\"https:\"===location.protocol,o=location.port;o||(o=i?\"443\":\"80\"),r.xd=\"undefined\"!=typeof location&&t.hostname!==location.hostname||o!==t.port,r.xs=t.secure!==i}var a=t&&t.forceBase64;return r.supportsBinary=tt&&!a,r}return r(s,[{key:\"name\",get:function(){return\"polling\"}},{key:\"doOpen\",value:function(){this.poll()}},{key:\"pause\",value:function(t){var e=this;this.readyState=\"pausing\";var n=function(){e.readyState=\"paused\",t()};if(this.polling||!this.writable){var r=0;this.polling&&(r++,this.once(\"pollComplete\",(function(){--r||n()}))),this.writable||(r++,this.once(\"drain\",(function(){--r||n()})))}else n()}},{key:\"poll\",value:function(){this.polling=!0,this.doPoll(),this.emitReserved(\"poll\")}},{key:\"onData\",value:function(t){var e=this;(function(t,e){for(var n=t.split(x),r=[],i=0;i<n.length;i++){var o=B(n[i],e);if(r.push(o),\"error\"===o.type)break}return r})(t,this.socket.binaryType).forEach((function(t){if(\"opening\"===e.readyState&&\"open\"===t.type&&e.onOpen(),\"close\"===t.type)return e.onClose({description:\"transport closed by the server\"}),!1;e.onPacket(t)})),\"closed\"!==this.readyState&&(this.polling=!1,this.emitReserved(\"pollComplete\"),\"open\"===this.readyState&&this.poll())}},{key:\"doClose\",value:function(){var t=this,e=function(){t.write([{type:\"close\"}])};\"open\"===this.readyState?e():this.once(\"open\",e)}},{key:\"write\",value:function(t){var e=this;this.writable=!1,function(t,e){var n=t.length,r=new Array(n),i=0;t.forEach((function(t,o){E(t,!1,(function(t){r[o]=t,++i===n&&e(r.join(x))}))}))}(t,(function(t){e.doWrite(t,(function(){e.writable=!0,e.emitReserved(\"drain\")}))}))}},{key:\"uri\",value:function(){var t=this.query||{},e=this.opts.secure?\"https\":\"http\",n=\"\";!1!==this.opts.timestampRequests&&(t[this.opts.timestampParam]=W()),this.supportsBinary||t.sid||(t.b64=1),this.opts.port&&(\"https\"===e&&443!==Number(this.opts.port)||\"http\"===e&&80!==Number(this.opts.port))&&(n=\":\"+this.opts.port);var r=$(t);return e+\"://\"+(-1!==this.opts.hostname.indexOf(\":\")?\"[\"+this.opts.hostname+\"]\":this.opts.hostname)+n+this.opts.path+(r.length?\"?\"+r:\"\")}},{key:\"request\",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};return i(t,{xd:this.xd,xs:this.xs},this.opts),new nt(this.uri(),t)}},{key:\"doWrite\",value:function(t,e){var n=this,r=this.request({method:\"POST\",data:t});r.on(\"success\",e),r.on(\"error\",(function(t,e){n.onError(\"xhr post error\",t,e)}))}},{key:\"doPoll\",value:function(){var t=this,e=this.request();e.on(\"data\",this.onData.bind(this)),e.on(\"error\",(function(e,n){t.onError(\"xhr poll error\",e,n)})),this.pollXhr=e}}]),s}(U),nt=function(t){o(i,t);var n=p(i);function i(t,r){var o;return e(this,i),D(f(o=n.call(this)),r),o.opts=r,o.method=r.method||\"GET\",o.uri=t,o.async=!1!==r.async,o.data=void 0!==r.data?r.data:null,o.create(),o}return r(i,[{key:\"create\",value:function(){var t=this,e=j(this.opts,\"agent\",\"pfx\",\"key\",\"passphrase\",\"cert\",\"ca\",\"ciphers\",\"rejectUnauthorized\",\"autoUnref\");e.xdomain=!!this.opts.xd,e.xscheme=!!this.opts.xs;var n=this.xhr=new G(e);try{n.open(this.method,this.uri,this.async);try{if(this.opts.extraHeaders)for(var r in n.setDisableHeaderCheck&&n.setDisableHeaderCheck(!0),this.opts.extraHeaders)this.opts.extraHeaders.hasOwnProperty(r)&&n.setRequestHeader(r,this.opts.extraHeaders[r])}catch(t){}if(\"POST\"===this.method)try{n.setRequestHeader(\"Content-type\",\"text/plain;charset=UTF-8\")}catch(t){}try{n.setRequestHeader(\"Accept\",\"*/*\")}catch(t){}\"withCredentials\"in n&&(n.withCredentials=this.opts.withCredentials),this.opts.requestTimeout&&(n.timeout=this.opts.requestTimeout),n.onreadystatechange=function(){4===n.readyState&&(200===n.status||1223===n.status?t.onLoad():t.setTimeoutFn((function(){t.onError(\"number\"==typeof n.status?n.status:0)}),0))},n.send(this.data)}catch(e){return void this.setTimeoutFn((function(){t.onError(e)}),0)}\"undefined\"!=typeof document&&(this.index=i.requestsCount++,i.requests[this.index]=this)}},{key:\"onError\",value:function(t){this.emitReserved(\"error\",t,this.xhr),this.cleanup(!0)}},{key:\"cleanup\",value:function(t){if(void 0!==this.xhr&&null!==this.xhr){if(this.xhr.onreadystatechange=Z,t)try{this.xhr.abort()}catch(t){}\"undefined\"!=typeof document&&delete i.requests[this.index],this.xhr=null}}},{key:\"onLoad\",value:function(){var t=this.xhr.responseText;null!==t&&(this.emitReserved(\"data\",t),this.emitReserved(\"success\"),this.cleanup())}},{key:\"abort\",value:function(){this.cleanup()}}]),i}(L);if(nt.requestsCount=0,nt.requests={},\"undefined\"!=typeof document)if(\"function\"==typeof attachEvent)attachEvent(\"onunload\",rt);else if(\"function\"==typeof addEventListener){addEventListener(\"onpagehide\"in P?\"pagehide\":\"unload\",rt,!1)}function rt(){for(var t in nt.requests)nt.requests.hasOwnProperty(t)&&nt.requests[t].abort()}var it=\"function\"==typeof Promise&&\"function\"==typeof Promise.resolve?function(t){return Promise.resolve().then(t)}:function(t,e){return e(t,0)},ot=P.WebSocket||P.MozWebSocket,st=\"undefined\"!=typeof navigator&&\"string\"==typeof navigator.product&&\"reactnative\"===navigator.product.toLowerCase(),at=function(t){o(i,t);var n=p(i);function i(t){var r;return e(this,i),(r=n.call(this,t)).supportsBinary=!t.forceBase64,r}return r(i,[{key:\"name\",get:function(){return\"websocket\"}},{key:\"doOpen\",value:function(){if(this.check()){var t=this.uri(),e=this.opts.protocols,n=st?{}:j(this.opts,\"agent\",\"perMessageDeflate\",\"pfx\",\"key\",\"passphrase\",\"cert\",\"ca\",\"ciphers\",\"rejectUnauthorized\",\"localAddress\",\"protocolVersion\",\"origin\",\"maxPayload\",\"family\",\"checkServerIdentity\");this.opts.extraHeaders&&(n.headers=this.opts.extraHeaders);try{this.ws=st?new ot(t,e,n):e?new ot(t,e):new ot(t)}catch(t){return this.emitReserved(\"error\",t)}this.ws.binaryType=this.socket.binaryType||\"arraybuffer\",this.addEventListeners()}}},{key:\"addEventListeners\",value:function(){var t=this;this.ws.onopen=function(){t.opts.autoUnref&&t.ws._socket.unref(),t.onOpen()},this.ws.onclose=function(e){return t.onClose({description:\"websocket connection closed\",context:e})},this.ws.onmessage=function(e){return t.onData(e.data)},this.ws.onerror=function(e){return t.onError(\"websocket error\",e)}}},{key:\"write\",value:function(t){var e=this;this.writable=!1;for(var n=function(n){var r=t[n],i=n===t.length-1;E(r,e.supportsBinary,(function(t){try{e.ws.send(t)}catch(t){}i&&it((function(){e.writable=!0,e.emitReserved(\"drain\")}),e.setTimeoutFn)}))},r=0;r<t.length;r++)n(r)}},{key:\"doClose\",value:function(){void 0!==this.ws&&(this.ws.close(),this.ws=null)}},{key:\"uri\",value:function(){var t=this.query||{},e=this.opts.secure?\"wss\":\"ws\",n=\"\";this.opts.port&&(\"wss\"===e&&443!==Number(this.opts.port)||\"ws\"===e&&80!==Number(this.opts.port))&&(n=\":\"+this.opts.port),this.opts.timestampRequests&&(t[this.opts.timestampParam]=W()),this.supportsBinary||(t.b64=1);var r=$(t);return e+\"://\"+(-1!==this.opts.hostname.indexOf(\":\")?\"[\"+this.opts.hostname+\"]\":this.opts.hostname)+n+this.opts.path+(r.length?\"?\"+r:\"\")}},{key:\"check\",value:function(){return!!ot}}]),i}(U),ct={websocket:at,polling:et},ut=/^(?:(?![^:@\\/?#]+:[^:@\\/]*@)(http|https|ws|wss):\\/\\/)?((?:(([^:@\\/?#]*)(?::([^:@\\/?#]*))?)?@)?((?:[a-f0-9]{0,4}:){2,7}[a-f0-9]{0,4}|[^:\\/?#]*)(?::(\\d*))?)(((\\/(?:[^?#](?![^?#\\/]*\\.[^?#\\/.]+(?:[?#]|$)))*\\/?)?([^?#\\/]*))(?:\\?([^#]*))?(?:#(.*))?)/,ht=[\"source\",\"protocol\",\"authority\",\"userInfo\",\"user\",\"password\",\"host\",\"port\",\"relative\",\"path\",\"directory\",\"file\",\"query\",\"anchor\"];function ft(t){var e=t,n=t.indexOf(\"[\"),r=t.indexOf(\"]\");-1!=n&&-1!=r&&(t=t.substring(0,n)+t.substring(n,r).replace(/:/g,\";\")+t.substring(r,t.length));for(var i,o,s=ut.exec(t||\"\"),a={},c=14;c--;)a[ht[c]]=s[c]||\"\";return-1!=n&&-1!=r&&(a.source=e,a.host=a.host.substring(1,a.host.length-1).replace(/;/g,\":\"),a.authority=a.authority.replace(\"[\",\"\").replace(\"]\",\"\").replace(/;/g,\":\"),a.ipv6uri=!0),a.pathNames=function(t,e){var n=/\\/{2,9}/g,r=e.replace(n,\"/\").split(\"/\");\"/\"!=e.slice(0,1)&&0!==e.length||r.splice(0,1);\"/\"==e.slice(-1)&&r.splice(r.length-1,1);return r}(0,a.path),a.queryKey=(i=a.query,o={},i.replace(/(?:^|&)([^&=]*)=?([^&]*)/g,(function(t,e,n){e&&(o[e]=n)})),o),a}var lt=function(n){o(a,n);var s=p(a);function a(n){var r,o=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return e(this,a),(r=s.call(this)).writeBuffer=[],n&&\"object\"===t(n)&&(o=n,n=null),n?(n=ft(n),o.hostname=n.host,o.secure=\"https\"===n.protocol||\"wss\"===n.protocol,o.port=n.port,n.query&&(o.query=n.query)):o.host&&(o.hostname=ft(o.host).host),D(f(r),o),r.secure=null!=o.secure?o.secure:\"undefined\"!=typeof location&&\"https:\"===location.protocol,o.hostname&&!o.port&&(o.port=r.secure?\"443\":\"80\"),r.hostname=o.hostname||(\"undefined\"!=typeof location?location.hostname:\"localhost\"),r.port=o.port||(\"undefined\"!=typeof location&&location.port?location.port:r.secure?\"443\":\"80\"),r.transports=o.transports||[\"polling\",\"websocket\"],r.writeBuffer=[],r.prevBufferLen=0,r.opts=i({path:\"/engine.io\",agent:!1,withCredentials:!1,upgrade:!0,timestampParam:\"t\",rememberUpgrade:!1,addTrailingSlash:!0,rejectUnauthorized:!0,perMessageDeflate:{threshold:1024},transportOptions:{},closeOnBeforeunload:!0},o),r.opts.path=r.opts.path.replace(/\\/$/,\"\")+(r.opts.addTrailingSlash?\"/\":\"\"),\"string\"==typeof r.opts.query&&(r.opts.query=J(r.opts.query)),r.id=null,r.upgrades=null,r.pingInterval=null,r.pingTimeout=null,r.pingTimeoutTimer=null,\"function\"==typeof addEventListener&&(r.opts.closeOnBeforeunload&&(r.beforeunloadEventListener=function(){r.transport&&(r.transport.removeAllListeners(),r.transport.close())},addEventListener(\"beforeunload\",r.beforeunloadEventListener,!1)),\"localhost\"!==r.hostname&&(r.offlineEventListener=function(){r.onClose(\"transport close\",{description:\"network connection lost\"})},addEventListener(\"offline\",r.offlineEventListener,!1))),r.open(),r}return r(a,[{key:\"createTransport\",value:function(t){var e=i({},this.opts.query);e.EIO=4,e.transport=t,this.id&&(e.sid=this.id);var n=i({},this.opts.transportOptions[t],this.opts,{query:e,socket:this,hostname:this.hostname,secure:this.secure,port:this.port});return new ct[t](n)}},{key:\"open\",value:function(){var t,e=this;if(this.opts.rememberUpgrade&&a.priorWebsocketSuccess&&-1!==this.transports.indexOf(\"websocket\"))t=\"websocket\";else{if(0===this.transports.length)return void this.setTimeoutFn((function(){e.emitReserved(\"error\",\"No transports available\")}),0);t=this.transports[0]}this.readyState=\"opening\";try{t=this.createTransport(t)}catch(t){return this.transports.shift(),void this.open()}t.open(),this.setTransport(t)}},{key:\"setTransport\",value:function(t){var e=this;this.transport&&this.transport.removeAllListeners(),this.transport=t,t.on(\"drain\",this.onDrain.bind(this)).on(\"packet\",this.onPacket.bind(this)).on(\"error\",this.onError.bind(this)).on(\"close\",(function(t){return e.onClose(\"transport close\",t)}))}},{key:\"probe\",value:function(t){var e=this,n=this.createTransport(t),r=!1;a.priorWebsocketSuccess=!1;var i=function(){r||(n.send([{type:\"ping\",data:\"probe\"}]),n.once(\"packet\",(function(t){if(!r)if(\"pong\"===t.type&&\"probe\"===t.data){if(e.upgrading=!0,e.emitReserved(\"upgrading\",n),!n)return;a.priorWebsocketSuccess=\"websocket\"===n.name,e.transport.pause((function(){r||\"closed\"!==e.readyState&&(f(),e.setTransport(n),n.send([{type:\"upgrade\"}]),e.emitReserved(\"upgrade\",n),n=null,e.upgrading=!1,e.flush())}))}else{var i=new Error(\"probe error\");i.transport=n.name,e.emitReserved(\"upgradeError\",i)}})))};function o(){r||(r=!0,f(),n.close(),n=null)}var s=function(t){var r=new Error(\"probe error: \"+t);r.transport=n.name,o(),e.emitReserved(\"upgradeError\",r)};function c(){s(\"transport closed\")}function u(){s(\"socket closed\")}function h(t){n&&t.name!==n.name&&o()}var f=function(){n.removeListener(\"open\",i),n.removeListener(\"error\",s),n.removeListener(\"close\",c),e.off(\"close\",u),e.off(\"upgrading\",h)};n.once(\"open\",i),n.once(\"error\",s),n.once(\"close\",c),this.once(\"close\",u),this.once(\"upgrading\",h),n.open()}},{key:\"onOpen\",value:function(){if(this.readyState=\"open\",a.priorWebsocketSuccess=\"websocket\"===this.transport.name,this.emitReserved(\"open\"),this.flush(),\"open\"===this.readyState&&this.opts.upgrade)for(var t=0,e=this.upgrades.length;t<e;t++)this.probe(this.upgrades[t])}},{key:\"onPacket\",value:function(t){if(\"opening\"===this.readyState||\"open\"===this.readyState||\"closing\"===this.readyState)switch(this.emitReserved(\"packet\",t),this.emitReserved(\"heartbeat\"),t.type){case\"open\":this.onHandshake(JSON.parse(t.data));break;case\"ping\":this.resetPingTimeout(),this.sendPacket(\"pong\"),this.emitReserved(\"ping\"),this.emitReserved(\"pong\");break;case\"error\":var e=new Error(\"server error\");e.code=t.data,this.onError(e);break;case\"message\":this.emitReserved(\"data\",t.data),this.emitReserved(\"message\",t.data)}}},{key:\"onHandshake\",value:function(t){this.emitReserved(\"handshake\",t),this.id=t.sid,this.transport.query.sid=t.sid,this.upgrades=this.filterUpgrades(t.upgrades),this.pingInterval=t.pingInterval,this.pingTimeout=t.pingTimeout,this.maxPayload=t.maxPayload,this.onOpen(),\"closed\"!==this.readyState&&this.resetPingTimeout()}},{key:\"resetPingTimeout\",value:function(){var t=this;this.clearTimeoutFn(this.pingTimeoutTimer),this.pingTimeoutTimer=this.setTimeoutFn((function(){t.onClose(\"ping timeout\")}),this.pingInterval+this.pingTimeout),this.opts.autoUnref&&this.pingTimeoutTimer.unref()}},{key:\"onDrain\",value:function(){this.writeBuffer.splice(0,this.prevBufferLen),this.prevBufferLen=0,0===this.writeBuffer.length?this.emitReserved(\"drain\"):this.flush()}},{key:\"flush\",value:function(){if(\"closed\"!==this.readyState&&this.transport.writable&&!this.upgrading&&this.writeBuffer.length){var t=this.getWritablePackets();this.transport.send(t),this.prevBufferLen=t.length,this.emitReserved(\"flush\")}}},{key:\"getWritablePackets\",value:function(){if(!(this.maxPayload&&\"polling\"===this.transport.name&&this.writeBuffer.length>1))return this.writeBuffer;for(var t,e=1,n=0;n<this.writeBuffer.length;n++){var r=this.writeBuffer[n].data;if(r&&(e+=\"string\"==typeof(t=r)?function(t){for(var e=0,n=0,r=0,i=t.length;r<i;r++)(e=t.charCodeAt(r))<128?n+=1:e<2048?n+=2:e<55296||e>=57344?n+=3:(r++,n+=4);return n}(t):Math.ceil(1.33*(t.byteLength||t.size))),n>0&&e>this.maxPayload)return this.writeBuffer.slice(0,n);e+=2}return this.writeBuffer}},{key:\"write\",value:function(t,e,n){return this.sendPacket(\"message\",t,e,n),this}},{key:\"send\",value:function(t,e,n){return this.sendPacket(\"message\",t,e,n),this}},{key:\"sendPacket\",value:function(t,e,n,r){if(\"function\"==typeof e&&(r=e,e=void 0),\"function\"==typeof n&&(r=n,n=null),\"closing\"!==this.readyState&&\"closed\"!==this.readyState){(n=n||{}).compress=!1!==n.compress;var i={type:t,data:e,options:n};this.emitReserved(\"packetCreate\",i),this.writeBuffer.push(i),r&&this.once(\"flush\",r),this.flush()}}},{key:\"close\",value:function(){var t=this,e=function(){t.onClose(\"forced close\"),t.transport.close()},n=function n(){t.off(\"upgrade\",n),t.off(\"upgradeError\",n),e()},r=function(){t.once(\"upgrade\",n),t.once(\"upgradeError\",n)};return\"opening\"!==this.readyState&&\"open\"!==this.readyState||(this.readyState=\"closing\",this.writeBuffer.length?this.once(\"drain\",(function(){t.upgrading?r():e()})):this.upgrading?r():e()),this}},{key:\"onError\",value:function(t){a.priorWebsocketSuccess=!1,this.emitReserved(\"error\",t),this.onClose(\"transport error\",t)}},{key:\"onClose\",value:function(t,e){\"opening\"!==this.readyState&&\"open\"!==this.readyState&&\"closing\"!==this.readyState||(this.clearTimeoutFn(this.pingTimeoutTimer),this.transport.removeAllListeners(\"close\"),this.transport.close(),this.transport.removeAllListeners(),\"function\"==typeof removeEventListener&&(removeEventListener(\"beforeunload\",this.beforeunloadEventListener,!1),removeEventListener(\"offline\",this.offlineEventListener,!1)),this.readyState=\"closed\",this.id=null,this.emitReserved(\"close\",t,e),this.writeBuffer=[],this.prevBufferLen=0)}},{key:\"filterUpgrades\",value:function(t){for(var e=[],n=0,r=t.length;n<r;n++)~this.transports.indexOf(t[n])&&e.push(t[n]);return e}}]),a}(L);lt.protocol=4,lt.protocol;var pt=\"function\"==typeof ArrayBuffer,dt=Object.prototype.toString,yt=\"function\"==typeof Blob||\"undefined\"!=typeof Blob&&\"[object BlobConstructor]\"===dt.call(Blob),vt=\"function\"==typeof File||\"undefined\"!=typeof File&&\"[object FileConstructor]\"===dt.call(File);function gt(t){return pt&&(t instanceof ArrayBuffer||function(t){return\"function\"==typeof ArrayBuffer.isView?ArrayBuffer.isView(t):t.buffer instanceof ArrayBuffer}(t))||yt&&t instanceof Blob||vt&&t instanceof File}function mt(e,n){if(!e||\"object\"!==t(e))return!1;if(Array.isArray(e)){for(var r=0,i=e.length;r<i;r++)if(mt(e[r]))return!0;return!1}if(gt(e))return!0;if(e.toJSON&&\"function\"==typeof e.toJSON&&1===arguments.length)return mt(e.toJSON(),!0);for(var o in e)if(Object.prototype.hasOwnProperty.call(e,o)&&mt(e[o]))return!0;return!1}function kt(t){var e=[],n=t.data,r=t;return r.data=bt(n,e),r.attachments=e.length,{packet:r,buffers:e}}function bt(e,n){if(!e)return e;if(gt(e)){var r={_placeholder:!0,num:n.length};return n.push(e),r}if(Array.isArray(e)){for(var i=new Array(e.length),o=0;o<e.length;o++)i[o]=bt(e[o],n);return i}if(\"object\"===t(e)&&!(e instanceof Date)){var s={};for(var a in e)Object.prototype.hasOwnProperty.call(e,a)&&(s[a]=bt(e[a],n));return s}return e}function wt(t,e){return t.data=_t(t.data,e),delete t.attachments,t}function _t(e,n){if(!e)return e;if(e&&!0===e._placeholder){if(\"number\"==typeof e.num&&e.num>=0&&e.num<n.length)return n[e.num];throw new Error(\"illegal attachments\")}if(Array.isArray(e))for(var r=0;r<e.length;r++)e[r]=_t(e[r],n);else if(\"object\"===t(e))for(var i in e)Object.prototype.hasOwnProperty.call(e,i)&&(e[i]=_t(e[i],n));return e}var Et;!function(t){t[t.CONNECT=0]=\"CONNECT\",t[t.DISCONNECT=1]=\"DISCONNECT\",t[t.EVENT=2]=\"EVENT\",t[t.ACK=3]=\"ACK\",t[t.CONNECT_ERROR=4]=\"CONNECT_ERROR\",t[t.BINARY_EVENT=5]=\"BINARY_EVENT\",t[t.BINARY_ACK=6]=\"BINARY_ACK\"}(Et||(Et={}));var Ot=function(){function t(n){e(this,t),this.replacer=n}return r(t,[{key:\"encode\",value:function(t){return t.type!==Et.EVENT&&t.type!==Et.ACK||!mt(t)?[this.encodeAsString(t)]:this.encodeAsBinary({type:t.type===Et.EVENT?Et.BINARY_EVENT:Et.BINARY_ACK,nsp:t.nsp,data:t.data,id:t.id})}},{key:\"encodeAsString\",value:function(t){var e=\"\"+t.type;return t.type!==Et.BINARY_EVENT&&t.type!==Et.BINARY_ACK||(e+=t.attachments+\"-\"),t.nsp&&\"/\"!==t.nsp&&(e+=t.nsp+\",\"),null!=t.id&&(e+=t.id),null!=t.data&&(e+=JSON.stringify(t.data,this.replacer)),e}},{key:\"encodeAsBinary\",value:function(t){var e=kt(t),n=this.encodeAsString(e.packet),r=e.buffers;return r.unshift(n),r}}]),t}(),At=function(n){o(a,n);var i=p(a);function a(t){var n;return e(this,a),(n=i.call(this)).reviver=t,n}return r(a,[{key:\"add\",value:function(t){var e;if(\"string\"==typeof t){if(this.reconstructor)throw new Error(\"got plaintext data when reconstructing a packet\");var n=(e=this.decodeString(t)).type===Et.BINARY_EVENT;n||e.type===Et.BINARY_ACK?(e.type=n?Et.EVENT:Et.ACK,this.reconstructor=new Rt(e),0===e.attachments&&y(s(a.prototype),\"emitReserved\",this).call(this,\"decoded\",e)):y(s(a.prototype),\"emitReserved\",this).call(this,\"decoded\",e)}else{if(!gt(t)&&!t.base64)throw new Error(\"Unknown type: \"+t);if(!this.reconstructor)throw new Error(\"got binary data when not reconstructing a packet\");(e=this.reconstructor.takeBinaryData(t))&&(this.reconstructor=null,y(s(a.prototype),\"emitReserved\",this).call(this,\"decoded\",e))}}},{key:\"decodeString\",value:function(t){var e=0,n={type:Number(t.charAt(0))};if(void 0===Et[n.type])throw new Error(\"unknown packet type \"+n.type);if(n.type===Et.BINARY_EVENT||n.type===Et.BINARY_ACK){for(var r=e+1;\"-\"!==t.charAt(++e)&&e!=t.length;);var i=t.substring(r,e);if(i!=Number(i)||\"-\"!==t.charAt(e))throw new Error(\"Illegal attachments\");n.attachments=Number(i)}if(\"/\"===t.charAt(e+1)){for(var o=e+1;++e;){if(\",\"===t.charAt(e))break;if(e===t.length)break}n.nsp=t.substring(o,e)}else n.nsp=\"/\";var s=t.charAt(e+1);if(\"\"!==s&&Number(s)==s){for(var c=e+1;++e;){var u=t.charAt(e);if(null==u||Number(u)!=u){--e;break}if(e===t.length)break}n.id=Number(t.substring(c,e+1))}if(t.charAt(++e)){var h=this.tryParse(t.substr(e));if(!a.isPayloadValid(n.type,h))throw new Error(\"invalid payload\");n.data=h}return n}},{key:\"tryParse\",value:function(t){try{return JSON.parse(t,this.reviver)}catch(t){return!1}}},{key:\"destroy\",value:function(){this.reconstructor&&(this.reconstructor.finishedReconstruction(),this.reconstructor=null)}}],[{key:\"isPayloadValid\",value:function(e,n){switch(e){case Et.CONNECT:return\"object\"===t(n);case Et.DISCONNECT:return void 0===n;case Et.CONNECT_ERROR:return\"string\"==typeof n||\"object\"===t(n);case Et.EVENT:case Et.BINARY_EVENT:return Array.isArray(n)&&n.length>0;case Et.ACK:case Et.BINARY_ACK:return Array.isArray(n)}}}]),a}(L),Rt=function(){function t(n){e(this,t),this.packet=n,this.buffers=[],this.reconPack=n}return r(t,[{key:\"takeBinaryData\",value:function(t){if(this.buffers.push(t),this.buffers.length===this.reconPack.attachments){var e=wt(this.reconPack,this.buffers);return this.finishedReconstruction(),e}return null}},{key:\"finishedReconstruction\",value:function(){this.reconPack=null,this.buffers=[]}}]),t}(),Tt=Object.freeze({__proto__:null,protocol:5,get PacketType(){return Et},Encoder:Ot,Decoder:At});function Ct(t,e,n){return t.on(e,n),function(){t.off(e,n)}}var Bt=Object.freeze({connect:1,connect_error:1,disconnect:1,disconnecting:1,newListener:1,removeListener:1}),St=function(t){o(a,t);var n=p(a);function a(t,r,o){var s;return e(this,a),(s=n.call(this)).connected=!1,s.recovered=!1,s.receiveBuffer=[],s.sendBuffer=[],s._queue=[],s.ids=0,s.acks={},s.flags={},s.io=t,s.nsp=r,o&&o.auth&&(s.auth=o.auth),s._opts=i({},o),s.io._autoConnect&&s.open(),s}return r(a,[{key:\"disconnected\",get:function(){return!this.connected}},{key:\"subEvents\",value:function(){if(!this.subs){var t=this.io;this.subs=[Ct(t,\"open\",this.onopen.bind(this)),Ct(t,\"packet\",this.onpacket.bind(this)),Ct(t,\"error\",this.onerror.bind(this)),Ct(t,\"close\",this.onclose.bind(this))]}}},{key:\"active\",get:function(){return!!this.subs}},{key:\"connect\",value:function(){return this.connected||(this.subEvents(),this.io._reconnecting||this.io.open(),\"open\"===this.io._readyState&&this.onopen()),this}},{key:\"open\",value:function(){return this.connect()}},{key:\"send\",value:function(){for(var t=arguments.length,e=new Array(t),n=0;n<t;n++)e[n]=arguments[n];return e.unshift(\"message\"),this.emit.apply(this,e),this}},{key:\"emit\",value:function(t){if(Bt.hasOwnProperty(t))throw new Error('\"'+t.toString()+'\" is a reserved event name');for(var e=arguments.length,n=new Array(e>1?e-1:0),r=1;r<e;r++)n[r-1]=arguments[r];if(n.unshift(t),this._opts.retries&&!this.flags.fromQueue&&!this.flags.volatile)return this._addToQueue(n),this;var i={type:Et.EVENT,data:n,options:{}};if(i.options.compress=!1!==this.flags.compress,\"function\"==typeof n[n.length-1]){var o=this.ids++,s=n.pop();this._registerAckCallback(o,s),i.id=o}var a=this.io.engine&&this.io.engine.transport&&this.io.engine.transport.writable,c=this.flags.volatile&&(!a||!this.connected);return c||(this.connected?(this.notifyOutgoingListeners(i),this.packet(i)):this.sendBuffer.push(i)),this.flags={},this}},{key:\"_registerAckCallback\",value:function(t,e){var n,r=this,i=null!==(n=this.flags.timeout)&&void 0!==n?n:this._opts.ackTimeout;if(void 0!==i){var o=this.io.setTimeoutFn((function(){delete r.acks[t];for(var n=0;n<r.sendBuffer.length;n++)r.sendBuffer[n].id===t&&r.sendBuffer.splice(n,1);e.call(r,new Error(\"operation has timed out\"))}),i);this.acks[t]=function(){r.io.clearTimeoutFn(o);for(var t=arguments.length,n=new Array(t),i=0;i<t;i++)n[i]=arguments[i];e.apply(r,[null].concat(n))}}else this.acks[t]=e}},{key:\"emitWithAck\",value:function(t){for(var e=this,n=arguments.length,r=new Array(n>1?n-1:0),i=1;i<n;i++)r[i-1]=arguments[i];var o=void 0!==this.flags.timeout||void 0!==this._opts.ackTimeout;return new Promise((function(n,i){r.push((function(t,e){return o?t?i(t):n(e):n(t)})),e.emit.apply(e,[t].concat(r))}))}},{key:\"_addToQueue\",value:function(t){var e,n=this;\"function\"==typeof t[t.length-1]&&(e=t.pop());var r={id:this.ids++,tryCount:0,pending:!1,args:t,flags:i({fromQueue:!0},this.flags)};t.push((function(t){if(r===n._queue[0]){var i=null!==t;if(i)r.tryCount>n._opts.retries&&(n._queue.shift(),e&&e(t));else if(n._queue.shift(),e){for(var o=arguments.length,s=new Array(o>1?o-1:0),a=1;a<o;a++)s[a-1]=arguments[a];e.apply(void 0,[null].concat(s))}return r.pending=!1,n._drainQueue()}})),this._queue.push(r),this._drainQueue()}},{key:\"_drainQueue\",value:function(){if(0!==this._queue.length){var t=this._queue[0];if(!t.pending){t.pending=!0,t.tryCount++;var e=this.ids;this.ids=t.id,this.flags=t.flags,this.emit.apply(this,t.args),this.ids=e}}}},{key:\"packet\",value:function(t){t.nsp=this.nsp,this.io._packet(t)}},{key:\"onopen\",value:function(){var t=this;\"function\"==typeof this.auth?this.auth((function(e){t._sendConnectPacket(e)})):this._sendConnectPacket(this.auth)}},{key:\"_sendConnectPacket\",value:function(t){this.packet({type:Et.CONNECT,data:this._pid?i({pid:this._pid,offset:this._lastOffset},t):t})}},{key:\"onerror\",value:function(t){this.connected||this.emitReserved(\"connect_error\",t)}},{key:\"onclose\",value:function(t,e){this.connected=!1,delete this.id,this.emitReserved(\"disconnect\",t,e)}},{key:\"onpacket\",value:function(t){if(t.nsp===this.nsp)switch(t.type){case Et.CONNECT:t.data&&t.data.sid?this.onconnect(t.data.sid,t.data.pid):this.emitReserved(\"connect_error\",new Error(\"It seems you are trying to reach a Socket.IO server in v2.x with a v3.x client, but they are not compatible (more information here: https://socket.io/docs/v3/migrating-from-2-x-to-3-0/)\"));break;case Et.EVENT:case Et.BINARY_EVENT:this.onevent(t);break;case Et.ACK:case Et.BINARY_ACK:this.onack(t);break;case Et.DISCONNECT:this.ondisconnect();break;case Et.CONNECT_ERROR:this.destroy();var e=new Error(t.data.message);e.data=t.data.data,this.emitReserved(\"connect_error\",e)}}},{key:\"onevent\",value:function(t){var e=t.data||[];null!=t.id&&e.push(this.ack(t.id)),this.connected?this.emitEvent(e):this.receiveBuffer.push(Object.freeze(e))}},{key:\"emitEvent\",value:function(t){if(this._anyListeners&&this._anyListeners.length){var e,n=g(this._anyListeners.slice());try{for(n.s();!(e=n.n()).done;){e.value.apply(this,t)}}catch(t){n.e(t)}finally{n.f()}}y(s(a.prototype),\"emit\",this).apply(this,t),this._pid&&t.length&&\"string\"==typeof t[t.length-1]&&(this._lastOffset=t[t.length-1])}},{key:\"ack\",value:function(t){var e=this,n=!1;return function(){if(!n){n=!0;for(var r=arguments.length,i=new Array(r),o=0;o<r;o++)i[o]=arguments[o];e.packet({type:Et.ACK,id:t,data:i})}}}},{key:\"onack\",value:function(t){var e=this.acks[t.id];\"function\"==typeof e&&(e.apply(this,t.data),delete this.acks[t.id])}},{key:\"onconnect\",value:function(t,e){this.id=t,this.recovered=e&&this._pid===e,this._pid=e,this.connected=!0,this.emitBuffered(),this.emitReserved(\"connect\")}},{key:\"emitBuffered\",value:function(){var t=this;this.receiveBuffer.forEach((function(e){return t.emitEvent(e)})),this.receiveBuffer=[],this.sendBuffer.forEach((function(e){t.notifyOutgoingListeners(e),t.packet(e)})),this.sendBuffer=[]}},{key:\"ondisconnect\",value:function(){this.destroy(),this.onclose(\"io server disconnect\")}},{key:\"destroy\",value:function(){this.subs&&(this.subs.forEach((function(t){return t()})),this.subs=void 0),this.io._destroy(this)}},{key:\"disconnect\",value:function(){return this.connected&&this.packet({type:Et.DISCONNECT}),this.destroy(),this.connected&&this.onclose(\"io client disconnect\"),this}},{key:\"close\",value:function(){return this.disconnect()}},{key:\"compress\",value:function(t){return this.flags.compress=t,this}},{key:\"volatile\",get:function(){return this.flags.volatile=!0,this}},{key:\"timeout\",value:function(t){return this.flags.timeout=t,this}},{key:\"onAny\",value:function(t){return this._anyListeners=this._anyListeners||[],this._anyListeners.push(t),this}},{key:\"prependAny\",value:function(t){return this._anyListeners=this._anyListeners||[],this._anyListeners.unshift(t),this}},{key:\"offAny\",value:function(t){if(!this._anyListeners)return this;if(t){for(var e=this._anyListeners,n=0;n<e.length;n++)if(t===e[n])return e.splice(n,1),this}else this._anyListeners=[];return this}},{key:\"listenersAny\",value:function(){return this._anyListeners||[]}},{key:\"onAnyOutgoing\",value:function(t){return this._anyOutgoingListeners=this._anyOutgoingListeners||[],this._anyOutgoingListeners.push(t),this}},{key:\"prependAnyOutgoing\",value:function(t){return this._anyOutgoingListeners=this._anyOutgoingListeners||[],this._anyOutgoingListeners.unshift(t),this}},{key:\"offAnyOutgoing\",value:function(t){if(!this._anyOutgoingListeners)return this;if(t){for(var e=this._anyOutgoingListeners,n=0;n<e.length;n++)if(t===e[n])return e.splice(n,1),this}else this._anyOutgoingListeners=[];return this}},{key:\"listenersAnyOutgoing\",value:function(){return this._anyOutgoingListeners||[]}},{key:\"notifyOutgoingListeners\",value:function(t){if(this._anyOutgoingListeners&&this._anyOutgoingListeners.length){var e,n=g(this._anyOutgoingListeners.slice());try{for(n.s();!(e=n.n()).done;){e.value.apply(this,t.data)}}catch(t){n.e(t)}finally{n.f()}}}}]),a}(L);function Nt(t){t=t||{},this.ms=t.min||100,this.max=t.max||1e4,this.factor=t.factor||2,this.jitter=t.jitter>0&&t.jitter<=1?t.jitter:0,this.attempts=0}Nt.prototype.duration=function(){var t=this.ms*Math.pow(this.factor,this.attempts++);if(this.jitter){var e=Math.random(),n=Math.floor(e*this.jitter*t);t=0==(1&Math.floor(10*e))?t-n:t+n}return 0|Math.min(t,this.max)},Nt.prototype.reset=function(){this.attempts=0},Nt.prototype.setMin=function(t){this.ms=t},Nt.prototype.setMax=function(t){this.max=t},Nt.prototype.setJitter=function(t){this.jitter=t};var xt=function(n){o(s,n);var i=p(s);function s(n,r){var o,a;e(this,s),(o=i.call(this)).nsps={},o.subs=[],n&&\"object\"===t(n)&&(r=n,n=void 0),(r=r||{}).path=r.path||\"/socket.io\",o.opts=r,D(f(o),r),o.reconnection(!1!==r.reconnection),o.reconnectionAttempts(r.reconnectionAttempts||1/0),o.reconnectionDelay(r.reconnectionDelay||1e3),o.reconnectionDelayMax(r.reconnectionDelayMax||5e3),o.randomizationFactor(null!==(a=r.randomizationFactor)&&void 0!==a?a:.5),o.backoff=new Nt({min:o.reconnectionDelay(),max:o.reconnectionDelayMax(),jitter:o.randomizationFactor()}),o.timeout(null==r.timeout?2e4:r.timeout),o._readyState=\"closed\",o.uri=n;var c=r.parser||Tt;return o.encoder=new c.Encoder,o.decoder=new c.Decoder,o._autoConnect=!1!==r.autoConnect,o._autoConnect&&o.open(),o}return r(s,[{key:\"reconnection\",value:function(t){return arguments.length?(this._reconnection=!!t,this):this._reconnection}},{key:\"reconnectionAttempts\",value:function(t){return void 0===t?this._reconnectionAttempts:(this._reconnectionAttempts=t,this)}},{key:\"reconnectionDelay\",value:function(t){var e;return void 0===t?this._reconnectionDelay:(this._reconnectionDelay=t,null===(e=this.backoff)||void 0===e||e.setMin(t),this)}},{key:\"randomizationFactor\",value:function(t){var e;return void 0===t?this._randomizationFactor:(this._randomizationFactor=t,null===(e=this.backoff)||void 0===e||e.setJitter(t),this)}},{key:\"reconnectionDelayMax\",value:function(t){var e;return void 0===t?this._reconnectionDelayMax:(this._reconnectionDelayMax=t,null===(e=this.backoff)||void 0===e||e.setMax(t),this)}},{key:\"timeout\",value:function(t){return arguments.length?(this._timeout=t,this):this._timeout}},{key:\"maybeReconnectOnOpen\",value:function(){!this._reconnecting&&this._reconnection&&0===this.backoff.attempts&&this.reconnect()}},{key:\"open\",value:function(t){var e=this;if(~this._readyState.indexOf(\"open\"))return this;this.engine=new lt(this.uri,this.opts);var n=this.engine,r=this;this._readyState=\"opening\",this.skipReconnect=!1;var i=Ct(n,\"open\",(function(){r.onopen(),t&&t()})),o=Ct(n,\"error\",(function(n){r.cleanup(),r._readyState=\"closed\",e.emitReserved(\"error\",n),t?t(n):r.maybeReconnectOnOpen()}));if(!1!==this._timeout){var s=this._timeout;0===s&&i();var a=this.setTimeoutFn((function(){i(),n.close(),n.emit(\"error\",new Error(\"timeout\"))}),s);this.opts.autoUnref&&a.unref(),this.subs.push((function(){clearTimeout(a)}))}return this.subs.push(i),this.subs.push(o),this}},{key:\"connect\",value:function(t){return this.open(t)}},{key:\"onopen\",value:function(){this.cleanup(),this._readyState=\"open\",this.emitReserved(\"open\");var t=this.engine;this.subs.push(Ct(t,\"ping\",this.onping.bind(this)),Ct(t,\"data\",this.ondata.bind(this)),Ct(t,\"error\",this.onerror.bind(this)),Ct(t,\"close\",this.onclose.bind(this)),Ct(this.decoder,\"decoded\",this.ondecoded.bind(this)))}},{key:\"onping\",value:function(){this.emitReserved(\"ping\")}},{key:\"ondata\",value:function(t){try{this.decoder.add(t)}catch(t){this.onclose(\"parse error\",t)}}},{key:\"ondecoded\",value:function(t){var e=this;it((function(){e.emitReserved(\"packet\",t)}),this.setTimeoutFn)}},{key:\"onerror\",value:function(t){this.emitReserved(\"error\",t)}},{key:\"socket\",value:function(t,e){var n=this.nsps[t];return n||(n=new St(this,t,e),this.nsps[t]=n),this._autoConnect&&n.connect(),n}},{key:\"_destroy\",value:function(t){for(var e=0,n=Object.keys(this.nsps);e<n.length;e++){var r=n[e];if(this.nsps[r].active)return}this._close()}},{key:\"_packet\",value:function(t){for(var e=this.encoder.encode(t),n=0;n<e.length;n++)this.engine.write(e[n],t.options)}},{key:\"cleanup\",value:function(){this.subs.forEach((function(t){return t()})),this.subs.length=0,this.decoder.destroy()}},{key:\"_close\",value:function(){this.skipReconnect=!0,this._reconnecting=!1,this.onclose(\"forced close\"),this.engine&&this.engine.close()}},{key:\"disconnect\",value:function(){return this._close()}},{key:\"onclose\",value:function(t,e){this.cleanup(),this.backoff.reset(),this._readyState=\"closed\",this.emitReserved(\"close\",t,e),this._reconnection&&!this.skipReconnect&&this.reconnect()}},{key:\"reconnect\",value:function(){var t=this;if(this._reconnecting||this.skipReconnect)return this;var e=this;if(this.backoff.attempts>=this._reconnectionAttempts)this.backoff.reset(),this.emitReserved(\"reconnect_failed\"),this._reconnecting=!1;else{var n=this.backoff.duration();this._reconnecting=!0;var r=this.setTimeoutFn((function(){e.skipReconnect||(t.emitReserved(\"reconnect_attempt\",e.backoff.attempts),e.skipReconnect||e.open((function(n){n?(e._reconnecting=!1,e.reconnect(),t.emitReserved(\"reconnect_error\",n)):e.onreconnect()})))}),n);this.opts.autoUnref&&r.unref(),this.subs.push((function(){clearTimeout(r)}))}}},{key:\"onreconnect\",value:function(){var t=this.backoff.attempts;this._reconnecting=!1,this.backoff.reset(),this.emitReserved(\"reconnect\",t)}}]),s}(L),Lt={};function Pt(e,n){\"object\"===t(e)&&(n=e,e=void 0);var r,i=function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:\"\",n=arguments.length>2?arguments[2]:void 0,r=t;n=n||\"undefined\"!=typeof location&&location,null==t&&(t=n.protocol+\"//\"+n.host),\"string\"==typeof t&&(\"/\"===t.charAt(0)&&(t=\"/\"===t.charAt(1)?n.protocol+t:n.host+t),/^(https?|wss?):\\/\\//.test(t)||(t=void 0!==n?n.protocol+\"//\"+t:\"https://\"+t),r=ft(t)),r.port||(/^(http|ws)$/.test(r.protocol)?r.port=\"80\":/^(http|ws)s$/.test(r.protocol)&&(r.port=\"443\")),r.path=r.path||\"/\";var i=-1!==r.host.indexOf(\":\")?\"[\"+r.host+\"]\":r.host;return r.id=r.protocol+\"://\"+i+\":\"+r.port+e,r.href=r.protocol+\"://\"+i+(n&&n.port===r.port?\"\":\":\"+r.port),r}(e,(n=n||{}).path||\"/socket.io\"),o=i.source,s=i.id,a=i.path,c=Lt[s]&&a in Lt[s].nsps;return n.forceNew||n[\"force new connection\"]||!1===n.multiplex||c?r=new xt(o,n):(Lt[s]||(Lt[s]=new xt(o,n)),r=Lt[s]),i.query&&!n.query&&(n.query=i.queryKey),r.socket(i.path,n)}return i(Pt,{Manager:xt,Socket:St,io:Pt,connect:Pt}),Pt}));\n-//# sourceMappingURL=socket.io.min.js.map\n+!function(t,e){\"object\"==typeof exports&&\"object\"==typeof module?module.exports=e():\"function\"==typeof define&&define.amd?define([],e):\"object\"==typeof exports?exports.io=e():t.io=e()}(self,(function(){return function(t){var e={};function n(r){if(e[r])return e[r].exports;var o=e[r]={i:r,l:!1,exports:{}};return t[r].call(o.exports,o,o.exports,n),o.l=!0,o.exports}return n.m=t,n.c=e,n.d=function(t,e,r){n.o(t,e)||Object.defineProperty(t,e,{enumerable:!0,get:r})},n.r=function(t){\"undefined\"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(t,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(t,\"__esModule\",{value:!0})},n.t=function(t,e){if(1&e&&(t=n(t)),8&e)return t;if(4&e&&\"object\"==typeof t&&t&&t.__esModule)return t;var r=Object.create(null);if(n.r(r),Object.defineProperty(r,\"default\",{enumerable:!0,value:t}),2&e&&\"string\"!=typeof t)for(var o in t)n.d(r,o,function(e){return t[e]}.bind(null,o));return r},n.n=function(t){var e=t&&t.__esModule?function(){return t.default}:function(){return t};return n.d(e,\"a\",e),e},n.o=function(t,e){return Object.prototype.hasOwnProperty.call(t,e)},n.p=\"\",n(n.s=17)}([function(t,e,n){function r(t){if(t)return function(t){for(var e in r.prototype)t[e]=r.prototype[e];return t}(t)}t.exports=r,r.prototype.on=r.prototype.addEventListener=function(t,e){return this._callbacks=this._callbacks||{},(this._callbacks[\"$\"+t]=this._callbacks[\"$\"+t]||[]).push(e),this},r.prototype.once=function(t,e){function n(){this.off(t,n),e.apply(this,arguments)}return n.fn=e,this.on(t,n),this},r.prototype.off=r.prototype.removeListener=r.prototype.removeAllListeners=r.prototype.removeEventListener=function(t,e){if(this._callbacks=this._callbacks||{},0==arguments.length)return this._callbacks={},this;var n,r=this._callbacks[\"$\"+t];if(!r)return this;if(1==arguments.length)return delete this._callbacks[\"$\"+t],this;for(var o=0;o<r.length;o++)if((n=r[o])===e||n.fn===e){r.splice(o,1);break}return 0===r.length&&delete this._callbacks[\"$\"+t],this},r.prototype.emit=function(t){this._callbacks=this._callbacks||{};for(var e=new Array(arguments.length-1),n=this._callbacks[\"$\"+t],r=1;r<arguments.length;r++)e[r-1]=arguments[r];if(n){r=0;for(var o=(n=n.slice(0)).length;r<o;++r)n[r].apply(this,e)}return this},r.prototype.listeners=function(t){return this._callbacks=this._callbacks||{},this._callbacks[\"$\"+t]||[]},r.prototype.hasListeners=function(t){return!!this.listeners(t).length}},function(t,e,n){var r=n(23),o=n(24),i=String.fromCharCode(30);t.exports={protocol:4,encodePacket:r,encodePayload:function(t,e){var n=t.length,o=new Array(n),s=0;t.forEach((function(t,c){r(t,!1,(function(t){o[c]=t,++s===n&&e(o.join(i))}))}))},decodePacket:o,decodePayload:function(t,e){for(var n=t.split(i),r=[],s=0;s<n.length;s++){var c=o(n[s],e);if(r.push(c),\"error\"===c.type)break}return r}}},function(t,e){t.exports=\"undefined\"!=typeof self?self:\"undefined\"!=typeof window?window:Function(\"return this\")()},function(t,e,n){function r(t){return(r=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}function o(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}function i(t,e){return(i=Object.setPrototypeOf||function(t,e){return t.__proto__=e,t})(t,e)}function s(t){var e=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(t){return!1}}();return function(){var n,r=a(t);if(e){var o=a(this).constructor;n=Reflect.construct(r,arguments,o)}else n=r.apply(this,arguments);return c(this,n)}}function c(t,e){return!e||\"object\"!==r(e)&&\"function\"!=typeof e?function(t){if(void 0===t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return t}(t):e}function a(t){return(a=Object.setPrototypeOf?Object.getPrototypeOf:function(t){return t.__proto__||Object.getPrototypeOf(t)})(t)}var u=n(1),f=function(t){!function(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function\");t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),e&&i(t,e)}(a,t);var e,n,r,c=s(a);function a(t){var e;return function(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}(this,a),(e=c.call(this)).opts=t,e.query=t.query,e.readyState=\"\",e.socket=t.socket,e}return e=a,(n=[{key:\"onError\",value:function(t,e){var n=new Error(t);return n.type=\"TransportError\",n.description=e,this.emit(\"error\",n),this}},{key:\"open\",value:function(){return\"closed\"!==this.readyState&&\"\"!==this.readyState||(this.readyState=\"opening\",this.doOpen()),this}},{key:\"close\",value:function(){return\"opening\"!==this.readyState&&\"open\"!==this.readyState||(this.doClose(),this.onClose()),this}},{key:\"send\",value:function(t){if(\"open\"!==this.readyState)throw new Error(\"Transport not open\");this.write(t)}},{key:\"onOpen\",value:function(){this.readyState=\"open\",this.writable=!0,this.emit(\"open\")}},{key:\"onData\",value:function(t){var e=u.decodePacket(t,this.socket.binaryType);this.onPacket(e)}},{key:\"onPacket\",value:function(t){this.emit(\"packet\",t)}},{key:\"onClose\",value:function(){this.readyState=\"closed\",this.emit(\"close\")}}])&&o(e.prototype,n),r&&o(e,r),a}(n(0));t.exports=f},function(t,e){e.encode=function(t){var e=\"\";for(var n in t)t.hasOwnProperty(n)&&(e.length&&(e+=\"&\"),e+=encodeURIComponent(n)+\"=\"+encodeURIComponent(t[n]));return e},e.decode=function(t){for(var e={},n=t.split(\"&\"),r=0,o=n.length;r<o;r++){var i=n[r].split(\"=\");e[decodeURIComponent(i[0])]=decodeURIComponent(i[1])}return e}},function(t,e,n){\"use strict\";function r(t){return(r=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}function o(t,e,n){return(o=\"undefined\"!=typeof Reflect&&Reflect.get?Reflect.get:function(t,e,n){var r=function(t,e){for(;!Object.prototype.hasOwnProperty.call(t,e)&&null!==(t=a(t)););return t}(t,e);if(r){var o=Object.getOwnPropertyDescriptor(r,e);return o.get?o.get.call(n):o.value}})(t,e,n||t)}function i(t,e){return(i=Object.setPrototypeOf||function(t,e){return t.__proto__=e,t})(t,e)}function s(t){var e=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(t){return!1}}();return function(){var n,r=a(t);if(e){var o=a(this).constructor;n=Reflect.construct(r,arguments,o)}else n=r.apply(this,arguments);return c(this,n)}}function c(t,e){return!e||\"object\"!==r(e)&&\"function\"!=typeof e?function(t){if(void 0===t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return t}(t):e}function a(t){return(a=Object.setPrototypeOf?Object.getPrototypeOf:function(t){return t.__proto__||Object.getPrototypeOf(t)})(t)}function u(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}function f(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}function p(t,e,n){return e&&f(t.prototype,e),n&&f(t,n),t}Object.defineProperty(e,\"__esModule\",{value:!0}),e.Decoder=e.Encoder=e.PacketType=e.protocol=void 0;var l,h=n(0),y=n(29),d=n(15);e.protocol=5,function(t){t[t.CONNECT=0]=\"CONNECT\",t[t.DISCONNECT=1]=\"DISCONNECT\",t[t.EVENT=2]=\"EVENT\",t[t.ACK=3]=\"ACK\",t[t.CONNECT_ERROR=4]=\"CONNECT_ERROR\",t[t.BINARY_EVENT=5]=\"BINARY_EVENT\",t[t.BINARY_ACK=6]=\"BINARY_ACK\"}(l=e.PacketType||(e.PacketType={}));var v=function(){function t(){u(this,t)}return p(t,[{key:\"encode\",value:function(t){return t.type!==l.EVENT&&t.type!==l.ACK||!d.hasBinary(t)?[this.encodeAsString(t)]:(t.type=t.type===l.EVENT?l.BINARY_EVENT:l.BINARY_ACK,this.encodeAsBinary(t))}},{key:\"encodeAsString\",value:function(t){var e=\"\"+t.type;return t.type!==l.BINARY_EVENT&&t.type!==l.BINARY_ACK||(e+=t.attachments+\"-\"),t.nsp&&\"/\"!==t.nsp&&(e+=t.nsp+\",\"),null!=t.id&&(e+=t.id),null!=t.data&&(e+=JSON.stringify(t.data)),e}},{key:\"encodeAsBinary\",value:function(t){var e=y.deconstructPacket(t),n=this.encodeAsString(e.packet),r=e.buffers;return r.unshift(n),r}}]),t}();e.Encoder=v;var b=function(t){!function(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function\");t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),e&&i(t,e)}(n,t);var e=s(n);function n(){return u(this,n),e.call(this)}return p(n,[{key:\"add\",value:function(t){var e;if(\"string\"==typeof t)(e=this.decodeString(t)).type===l.BINARY_EVENT||e.type===l.BINARY_ACK?(this.reconstructor=new m(e),0===e.attachments&&o(a(n.prototype),\"emit\",this).call(this,\"decoded\",e)):o(a(n.prototype),\"emit\",this).call(this,\"decoded\",e);else{if(!d.isBinary(t)&&!t.base64)throw new Error(\"Unknown type: \"+t);if(!this.reconstructor)throw new Error(\"got binary data when not reconstructing a packet\");(e=this.reconstructor.takeBinaryData(t))&&(this.reconstructor=null,o(a(n.prototype),\"emit\",this).call(this,\"decoded\",e))}}},{key:\"decodeString\",value:function(t){var e=0,r={type:Number(t.charAt(0))};if(void 0===l[r.type])throw new Error(\"unknown packet type \"+r.type);if(r.type===l.BINARY_EVENT||r.type===l.BINARY_ACK){for(var o=e+1;\"-\"!==t.charAt(++e)&&e!=t.length;);var i=t.substring(o,e);if(i!=Number(i)||\"-\"!==t.charAt(e))throw new Error(\"Illegal attachments\");r.attachments=Number(i)}if(\"/\"===t.charAt(e+1)){for(var s=e+1;++e;){if(\",\"===t.charAt(e))break;if(e===t.length)break}r.nsp=t.substring(s,e)}else r.nsp=\"/\";var c=t.charAt(e+1);if(\"\"!==c&&Number(c)==c){for(var a=e+1;++e;){var u=t.charAt(e);if(null==u||Number(u)!=u){--e;break}if(e===t.length)break}r.id=Number(t.substring(a,e+1))}if(t.charAt(++e)){var f=function(t){try{return JSON.parse(t)}catch(t){return!1}}(t.substr(e));if(!n.isPayloadValid(r.type,f))throw new Error(\"invalid payload\");r.data=f}return r}},{key:\"destroy\",value:function(){this.reconstructor&&this.reconstructor.finishedReconstruction()}}],[{key:\"isPayloadValid\",value:function(t,e){switch(t){case l.CONNECT:return\"object\"===r(e);case l.DISCONNECT:return void 0===e;case l.CONNECT_ERROR:return\"string\"==typeof e||\"object\"===r(e);case l.EVENT:case l.BINARY_EVENT:return Array.isArray(e)&&e.length>0;case l.ACK:case l.BINARY_ACK:return Array.isArray(e)}}}]),n}(h);e.Decoder=b;var m=function(){function t(e){u(this,t),this.packet=e,this.buffers=[],this.reconPack=e}return p(t,[{key:\"takeBinaryData\",value:function(t){if(this.buffers.push(t),this.buffers.length===this.reconPack.attachments){var e=y.reconstructPacket(this.reconPack,this.buffers);return this.finishedReconstruction(),e}return null}},{key:\"finishedReconstruction\",value:function(){this.reconPack=null,this.buffers=[]}}]),t}()},function(t,e){var n=/^(?:(?![^:@]+:[^:@\\/]*@)(http|https|ws|wss):\\/\\/)?((?:(([^:@]*)(?::([^:@]*))?)?@)?((?:[a-f0-9]{0,4}:){2,7}[a-f0-9]{0,4}|[^:\\/?#]*)(?::(\\d*))?)(((\\/(?:[^?#](?![^?#\\/]*\\.[^?#\\/.]+(?:[?#]|$)))*\\/?)?([^?#\\/]*))(?:\\?([^#]*))?(?:#(.*))?)/,r=[\"source\",\"protocol\",\"authority\",\"userInfo\",\"user\",\"password\",\"host\",\"port\",\"relative\",\"path\",\"directory\",\"file\",\"query\",\"anchor\"];t.exports=function(t){var e=t,o=t.indexOf(\"[\"),i=t.indexOf(\"]\");-1!=o&&-1!=i&&(t=t.substring(0,o)+t.substring(o,i).replace(/:/g,\";\")+t.substring(i,t.length));for(var s,c,a=n.exec(t||\"\"),u={},f=14;f--;)u[r[f]]=a[f]||\"\";return-1!=o&&-1!=i&&(u.source=e,u.host=u.host.substring(1,u.host.length-1).replace(/;/g,\":\"),u.authority=u.authority.replace(\"[\",\"\").replace(\"]\",\"\").replace(/;/g,\":\"),u.ipv6uri=!0),u.pathNames=function(t,e){var n=e.replace(/\\/{2,9}/g,\"/\").split(\"/\");\"/\"!=e.substr(0,1)&&0!==e.length||n.splice(0,1);\"/\"==e.substr(e.length-1,1)&&n.splice(n.length-1,1);return n}(0,u.path),u.queryKey=(s=u.query,c={},s.replace(/(?:^|&)([^&=]*)=?([^&]*)/g,(function(t,e,n){e&&(c[e]=n)})),c),u}},function(t,e,n){\"use strict\";function r(t){return(r=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}function o(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}function i(t,e,n){return(i=\"undefined\"!=typeof Reflect&&Reflect.get?Reflect.get:function(t,e,n){var r=function(t,e){for(;!Object.prototype.hasOwnProperty.call(t,e)&&null!==(t=u(t)););return t}(t,e);if(r){var o=Object.getOwnPropertyDescriptor(r,e);return o.get?o.get.call(n):o.value}})(t,e,n||t)}function s(t,e){return(s=Object.setPrototypeOf||function(t,e){return t.__proto__=e,t})(t,e)}function c(t){var e=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(t){return!1}}();return function(){var n,r=u(t);if(e){var o=u(this).constructor;n=Reflect.construct(r,arguments,o)}else n=r.apply(this,arguments);return a(this,n)}}function a(t,e){return!e||\"object\"!==r(e)&&\"function\"!=typeof e?function(t){if(void 0===t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return t}(t):e}function u(t){return(u=Object.setPrototypeOf?Object.getPrototypeOf:function(t){return t.__proto__||Object.getPrototypeOf(t)})(t)}Object.defineProperty(e,\"__esModule\",{value:!0}),e.Manager=void 0;var f=n(19),p=n(14),l=n(0),h=n(5),y=n(16),d=n(30),v=function(t){!function(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function\");t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),e&&s(t,e)}(v,t);var e,n,a,l=c(v);function v(t,e){var n;!function(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}(this,v),(n=l.call(this)).nsps={},n.subs=[],t&&\"object\"===r(t)&&(e=t,t=void 0),(e=e||{}).path=e.path||\"/socket.io\",n.opts=e,n.reconnection(!1!==e.reconnection),n.reconnectionAttempts(e.reconnectionAttempts||1/0),n.reconnectionDelay(e.reconnectionDelay||1e3),n.reconnectionDelayMax(e.reconnectionDelayMax||5e3),n.randomizationFactor(e.randomizationFactor||.5),n.backoff=new d({min:n.reconnectionDelay(),max:n.reconnectionDelayMax(),jitter:n.randomizationFactor()}),n.timeout(null==e.timeout?2e4:e.timeout),n._readyState=\"closed\",n.uri=t;var o=e.parser||h;return n.encoder=new o.Encoder,n.decoder=new o.Decoder,n._autoConnect=!1!==e.autoConnect,n._autoConnect&&n.open(),n}return e=v,(n=[{key:\"reconnection\",value:function(t){return arguments.length?(this._reconnection=!!t,this):this._reconnection}},{key:\"reconnectionAttempts\",value:function(t){return void 0===t?this._reconnectionAttempts:(this._reconnectionAttempts=t,this)}},{key:\"reconnectionDelay\",value:function(t){var e;return void 0===t?this._reconnectionDelay:(this._reconnectionDelay=t,null===(e=this.backoff)||void 0===e||e.setMin(t),this)}},{key:\"randomizationFactor\",value:function(t){var e;return void 0===t?this._randomizationFactor:(this._randomizationFactor=t,null===(e=this.backoff)||void 0===e||e.setJitter(t),this)}},{key:\"reconnectionDelayMax\",value:function(t){var e;return void 0===t?this._reconnectionDelayMax:(this._reconnectionDelayMax=t,null===(e=this.backoff)||void 0===e||e.setMax(t),this)}},{key:\"timeout\",value:function(t){return arguments.length?(this._timeout=t,this):this._timeout}},{key:\"maybeReconnectOnOpen\",value:function(){!this._reconnecting&&this._reconnection&&0===this.backoff.attempts&&this.reconnect()}},{key:\"open\",value:function(t){var e=this;if(~this._readyState.indexOf(\"open\"))return this;this.engine=f(this.uri,this.opts);var n=this.engine,r=this;this._readyState=\"opening\",this.skipReconnect=!1;var o=y.on(n,\"open\",(function(){r.onopen(),t&&t()})),s=y.on(n,\"error\",(function(n){r.cleanup(),r._readyState=\"closed\",i(u(v.prototype),\"emit\",e).call(e,\"error\",n),t?t(n):r.maybeReconnectOnOpen()}));if(!1!==this._timeout){var c=this._timeout;0===c&&o();var a=setTimeout((function(){o(),n.close(),n.emit(\"error\",new Error(\"timeout\"))}),c);this.subs.push((function(){clearTimeout(a)}))}return this.subs.push(o),this.subs.push(s),this}},{key:\"connect\",value:function(t){return this.open(t)}},{key:\"onopen\",value:function(){this.cleanup(),this._readyState=\"open\",i(u(v.prototype),\"emit\",this).call(this,\"open\");var t=this.engine;this.subs.push(y.on(t,\"ping\",this.onping.bind(this)),y.on(t,\"data\",this.ondata.bind(this)),y.on(t,\"error\",this.onerror.bind(this)),y.on(t,\"close\",this.onclose.bind(this)),y.on(this.decoder,\"decoded\",this.ondecoded.bind(this)))}},{key:\"onping\",value:function(){i(u(v.prototype),\"emit\",this).call(this,\"ping\")}},{key:\"ondata\",value:function(t){this.decoder.add(t)}},{key:\"ondecoded\",value:function(t){i(u(v.prototype),\"emit\",this).call(this,\"packet\",t)}},{key:\"onerror\",value:function(t){i(u(v.prototype),\"emit\",this).call(this,\"error\",t)}},{key:\"socket\",value:function(t,e){var n=this.nsps[t];return n||(n=new p.Socket(this,t,e),this.nsps[t]=n),n}},{key:\"_destroy\",value:function(t){for(var e=0,n=Object.keys(this.nsps);e<n.length;e++){var r=n[e];if(this.nsps[r].active)return}this._close()}},{key:\"_packet\",value:function(t){for(var e=this.encoder.encode(t),n=0;n<e.length;n++)this.engine.write(e[n],t.options)}},{key:\"cleanup\",value:function(){this.subs.forEach((function(t){return t()})),this.subs.length=0,this.decoder.destroy()}},{key:\"_close\",value:function(){this.skipReconnect=!0,this._reconnecting=!1,\"opening\"===this._readyState&&this.cleanup(),this.backoff.reset(),this._readyState=\"closed\",this.engine&&this.engine.close()}},{key:\"disconnect\",value:function(){return this._close()}},{key:\"onclose\",value:function(t){this.cleanup(),this.backoff.reset(),this._readyState=\"closed\",i(u(v.prototype),\"emit\",this).call(this,\"close\",t),this._reconnection&&!this.skipReconnect&&this.reconnect()}},{key:\"reconnect\",value:function(){var t=this;if(this._reconnecting||this.skipReconnect)return this;var e=this;if(this.backoff.attempts>=this._reconnectionAttempts)this.backoff.reset(),i(u(v.prototype),\"emit\",this).call(this,\"reconnect_failed\"),this._reconnecting=!1;else{var n=this.backoff.duration();this._reconnecting=!0;var r=setTimeout((function(){e.skipReconnect||(i(u(v.prototype),\"emit\",t).call(t,\"reconnect_attempt\",e.backoff.attempts),e.skipReconnect||e.open((function(n){n?(e._reconnecting=!1,e.reconnect(),i(u(v.prototype),\"emit\",t).call(t,\"reconnect_error\",n)):e.onreconnect()})))}),n);this.subs.push((function(){clearTimeout(r)}))}}},{key:\"onreconnect\",value:function(){var t=this.backoff.attempts;this._reconnecting=!1,this.backoff.reset(),i(u(v.prototype),\"emit\",this).call(this,\"reconnect\",t)}}])&&o(e.prototype,n),a&&o(e,a),v}(l);e.Manager=v},function(t,e,n){var r=n(9),o=n(22),i=n(26),s=n(27);e.polling=function(t){var e=!1,n=!1,s=!1!==t.jsonp;if(\"undefined\"!=typeof location){var c=\"https:\"===location.protocol,a=location.port;a||(a=c?443:80),e=t.hostname!==location.hostname||a!==t.port,n=t.secure!==c}if(t.xdomain=e,t.xscheme=n,\"open\"in new r(t)&&!t.forceJSONP)return new o(t);if(!s)throw new Error(\"JSONP disabled\");return new i(t)},e.websocket=s},function(t,e,n){var r=n(21),o=n(2);t.exports=function(t){var e=t.xdomain,n=t.xscheme,i=t.enablesXDR;try{if(\"undefined\"!=typeof XMLHttpRequest&&(!e||r))return new XMLHttpRequest}catch(t){}try{if(\"undefined\"!=typeof XDomainRequest&&!n&&i)return new XDomainRequest}catch(t){}if(!e)try{return new(o[[\"Active\"].concat(\"Object\").join(\"X\")])(\"Microsoft.XMLHTTP\")}catch(t){}}},function(t,e,n){function r(t){return(r=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}function o(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}function i(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}function s(t,e){return(s=Object.setPrototypeOf||function(t,e){return t.__proto__=e,t})(t,e)}function c(t){var e=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(t){return!1}}();return function(){var n,r=u(t);if(e){var o=u(this).constructor;n=Reflect.construct(r,arguments,o)}else n=r.apply(this,arguments);return a(this,n)}}function a(t,e){return!e||\"object\"!==r(e)&&\"function\"!=typeof e?function(t){if(void 0===t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return t}(t):e}function u(t){return(u=Object.setPrototypeOf?Object.getPrototypeOf:function(t){return t.__proto__||Object.getPrototypeOf(t)})(t)}var f=n(3),p=n(4),l=n(1),h=n(12),y=function(t){!function(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function\");t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),e&&s(t,e)}(u,t);var e,n,r,a=c(u);function u(){return o(this,u),a.apply(this,arguments)}return e=u,(n=[{key:\"doOpen\",value:function(){this.poll()}},{key:\"pause\",value:function(t){var e=this;function n(){e.readyState=\"paused\",t()}if(this.readyState=\"pausing\",this.polling||!this.writable){var r=0;this.polling&&(r++,this.once(\"pollComplete\",(function(){--r||n()}))),this.writable||(r++,this.once(\"drain\",(function(){--r||n()})))}else n()}},{key:\"poll\",value:function(){this.polling=!0,this.doPoll(),this.emit(\"poll\")}},{key:\"onData\",value:function(t){var e=this;l.decodePayload(t,this.socket.binaryType).forEach((function(t,n,r){if(\"opening\"===e.readyState&&\"open\"===t.type&&e.onOpen(),\"close\"===t.type)return e.onClose(),!1;e.onPacket(t)})),\"closed\"!==this.readyState&&(this.polling=!1,this.emit(\"pollComplete\"),\"open\"===this.readyState&&this.poll())}},{key:\"doClose\",value:function(){var t=this;function e(){t.write([{type:\"close\"}])}\"open\"===this.readyState?e():this.once(\"open\",e)}},{key:\"write\",value:function(t){var e=this;this.writable=!1,l.encodePayload(t,(function(t){e.doWrite(t,(function(){e.writable=!0,e.emit(\"drain\")}))}))}},{key:\"uri\",value:function(){var t=this.query||{},e=this.opts.secure?\"https\":\"http\",n=\"\";return!1!==this.opts.timestampRequests&&(t[this.opts.timestampParam]=h()),this.supportsBinary||t.sid||(t.b64=1),t=p.encode(t),this.opts.port&&(\"https\"===e&&443!==Number(this.opts.port)||\"http\"===e&&80!==Number(this.opts.port))&&(n=\":\"+this.opts.port),t.length&&(t=\"?\"+t),e+\"://\"+(-1!==this.opts.hostname.indexOf(\":\")?\"[\"+this.opts.hostname+\"]\":this.opts.hostname)+n+this.opts.path+t}},{key:\"name\",get:function(){return\"polling\"}}])&&i(e.prototype,n),r&&i(e,r),u}(f);t.exports=y},function(t,e){var n=Object.create(null);n.open=\"0\",n.close=\"1\",n.ping=\"2\",n.pong=\"3\",n.message=\"4\",n.upgrade=\"5\",n.noop=\"6\";var r=Object.create(null);Object.keys(n).forEach((function(t){r[n[t]]=t}));t.exports={PACKET_TYPES:n,PACKET_TYPES_REVERSE:r,ERROR_PACKET:{type:\"error\",data:\"parser error\"}}},function(t,e,n){\"use strict\";var r,o=\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-_\".split(\"\"),i={},s=0,c=0;function a(t){var e=\"\";do{e=o[t%64]+e,t=Math.floor(t/64)}while(t>0);return e}function u(){var t=a(+new Date);return t!==r?(s=0,r=t):t+\".\"+a(s++)}for(;c<64;c++)i[o[c]]=c;u.encode=a,u.decode=function(t){var e=0;for(c=0;c<t.length;c++)e=64*e+i[t.charAt(c)];return e},t.exports=u},function(t,e){t.exports.pick=function(t){for(var e=arguments.length,n=new Array(e>1?e-1:0),r=1;r<e;r++)n[r-1]=arguments[r];return n.reduce((function(e,n){return t.hasOwnProperty(n)&&(e[n]=t[n]),e}),{})}},function(t,e,n){\"use strict\";function r(t){return(r=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}function o(t,e){var n;if(\"undefined\"==typeof Symbol||null==t[Symbol.iterator]){if(Array.isArray(t)||(n=function(t,e){if(!t)return;if(\"string\"==typeof t)return i(t,e);var n=Object.prototype.toString.call(t).slice(8,-1);\"Object\"===n&&t.constructor&&(n=t.constructor.name);if(\"Map\"===n||\"Set\"===n)return Array.from(t);if(\"Arguments\"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n))return i(t,e)}(t))||e&&t&&\"number\"==typeof t.length){n&&(t=n);var r=0,o=function(){};return{s:o,n:function(){return r>=t.length?{done:!0}:{done:!1,value:t[r++]}},e:function(t){throw t},f:o}}throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\")}var s,c=!0,a=!1;return{s:function(){n=t[Symbol.iterator]()},n:function(){var t=n.next();return c=t.done,t},e:function(t){a=!0,s=t},f:function(){try{c||null==n.return||n.return()}finally{if(a)throw s}}}}function i(t,e){(null==e||e>t.length)&&(e=t.length);for(var n=0,r=new Array(e);n<e;n++)r[n]=t[n];return r}function s(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}function c(t,e,n){return(c=\"undefined\"!=typeof Reflect&&Reflect.get?Reflect.get:function(t,e,n){var r=function(t,e){for(;!Object.prototype.hasOwnProperty.call(t,e)&&null!==(t=p(t)););return t}(t,e);if(r){var o=Object.getOwnPropertyDescriptor(r,e);return o.get?o.get.call(n):o.value}})(t,e,n||t)}function a(t,e){return(a=Object.setPrototypeOf||function(t,e){return t.__proto__=e,t})(t,e)}function u(t){var e=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(t){return!1}}();return function(){var n,r=p(t);if(e){var o=p(this).constructor;n=Reflect.construct(r,arguments,o)}else n=r.apply(this,arguments);return f(this,n)}}function f(t,e){return!e||\"object\"!==r(e)&&\"function\"!=typeof e?function(t){if(void 0===t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return t}(t):e}function p(t){return(p=Object.setPrototypeOf?Object.getPrototypeOf:function(t){return t.__proto__||Object.getPrototypeOf(t)})(t)}Object.defineProperty(e,\"__esModule\",{value:!0}),e.Socket=void 0;var l=n(5),h=n(0),y=n(16),d=Object.freeze({connect:1,connect_error:1,disconnect:1,disconnecting:1,newListener:1,removeListener:1}),v=function(t){!function(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function\");t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),e&&a(t,e)}(f,t);var e,n,r,i=u(f);function f(t,e,n){var r;return function(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}(this,f),(r=i.call(this)).receiveBuffer=[],r.sendBuffer=[],r.ids=0,r.acks={},r.flags={},r.io=t,r.nsp=e,r.ids=0,r.acks={},r.receiveBuffer=[],r.sendBuffer=[],r.connected=!1,r.disconnected=!0,r.flags={},n&&n.auth&&(r.auth=n.auth),r.io._autoConnect&&r.open(),r}return e=f,(n=[{key:\"subEvents\",value:function(){if(!this.subs){var t=this.io;this.subs=[y.on(t,\"open\",this.onopen.bind(this)),y.on(t,\"packet\",this.onpacket.bind(this)),y.on(t,\"error\",this.onerror.bind(this)),y.on(t,\"close\",this.onclose.bind(this))]}}},{key:\"connect\",value:function(){return this.connected||(this.subEvents(),this.io._reconnecting||this.io.open(),\"open\"===this.io._readyState&&this.onopen()),this}},{key:\"open\",value:function(){return this.connect()}},{key:\"send\",value:function(){for(var t=arguments.length,e=new Array(t),n=0;n<t;n++)e[n]=arguments[n];return e.unshift(\"message\"),this.emit.apply(this,e),this}},{key:\"emit\",value:function(t){if(d.hasOwnProperty(t))throw new Error('\"'+t+'\" is a reserved event name');for(var e=arguments.length,n=new Array(e>1?e-1:0),r=1;r<e;r++)n[r-1]=arguments[r];n.unshift(t);var o={type:l.PacketType.EVENT,data:n,options:{}};o.options.compress=!1!==this.flags.compress,\"function\"==typeof n[n.length-1]&&(this.acks[this.ids]=n.pop(),o.id=this.ids++);var i=this.io.engine&&this.io.engine.transport&&this.io.engine.transport.writable,s=this.flags.volatile&&(!i||!this.connected);return s||(this.connected?this.packet(o):this.sendBuffer.push(o)),this.flags={},this}},{key:\"packet\",value:function(t){t.nsp=this.nsp,this.io._packet(t)}},{key:\"onopen\",value:function(){var t=this;\"function\"==typeof this.auth?this.auth((function(e){t.packet({type:l.PacketType.CONNECT,data:e})})):this.packet({type:l.PacketType.CONNECT,data:this.auth})}},{key:\"onerror\",value:function(t){this.connected||c(p(f.prototype),\"emit\",this).call(this,\"connect_error\",t)}},{key:\"onclose\",value:function(t){this.connected=!1,this.disconnected=!0,delete this.id,c(p(f.prototype),\"emit\",this).call(this,\"disconnect\",t)}},{key:\"onpacket\",value:function(t){if(t.nsp===this.nsp)switch(t.type){case l.PacketType.CONNECT:if(t.data&&t.data.sid){var e=t.data.sid;this.onconnect(e)}else c(p(f.prototype),\"emit\",this).call(this,\"connect_error\",new Error(\"It seems you are trying to reach a Socket.IO server in v2.x with a v3.x client, but they are not compatible (more information here: https://socket.io/docs/v3/migrating-from-2-x-to-3-0/)\"));break;case l.PacketType.EVENT:case l.PacketType.BINARY_EVENT:this.onevent(t);break;case l.PacketType.ACK:case l.PacketType.BINARY_ACK:this.onack(t);break;case l.PacketType.DISCONNECT:this.ondisconnect();break;case l.PacketType.CONNECT_ERROR:var n=new Error(t.data.message);n.data=t.data.data,c(p(f.prototype),\"emit\",this).call(this,\"connect_error\",n)}}},{key:\"onevent\",value:function(t){var e=t.data||[];null!=t.id&&e.push(this.ack(t.id)),this.connected?this.emitEvent(e):this.receiveBuffer.push(Object.freeze(e))}},{key:\"emitEvent\",value:function(t){if(this._anyListeners&&this._anyListeners.length){var e,n=o(this._anyListeners.slice());try{for(n.s();!(e=n.n()).done;)e.value.apply(this,t)}catch(t){n.e(t)}finally{n.f()}}c(p(f.prototype),\"emit\",this).apply(this,t)}},{key:\"ack\",value:function(t){var e=this,n=!1;return function(){if(!n){n=!0;for(var r=arguments.length,o=new Array(r),i=0;i<r;i++)o[i]=arguments[i];e.packet({type:l.PacketType.ACK,id:t,data:o})}}}},{key:\"onack\",value:function(t){var e=this.acks[t.id];\"function\"==typeof e&&(e.apply(this,t.data),delete this.acks[t.id])}},{key:\"onconnect\",value:function(t){this.id=t,this.connected=!0,this.disconnected=!1,c(p(f.prototype),\"emit\",this).call(this,\"connect\"),this.emitBuffered()}},{key:\"emitBuffered\",value:function(){var t=this;this.receiveBuffer.forEach((function(e){return t.emitEvent(e)})),this.receiveBuffer=[],this.sendBuffer.forEach((function(e){return t.packet(e)})),this.sendBuffer=[]}},{key:\"ondisconnect\",value:function(){this.destroy(),this.onclose(\"io server disconnect\")}},{key:\"destroy\",value:function(){this.subs&&(this.subs.forEach((function(t){return t()})),this.subs=void 0),this.io._destroy(this)}},{key:\"disconnect\",value:function(){return this.connected&&this.packet({type:l.PacketType.DISCONNECT}),this.destroy(),this.connected&&this.onclose(\"io client disconnect\"),this}},{key:\"close\",value:function(){return this.disconnect()}},{key:\"compress\",value:function(t){return this.flags.compress=t,this}},{key:\"onAny\",value:function(t){return this._anyListeners=this._anyListeners||[],this._anyListeners.push(t),this}},{key:\"prependAny\",value:function(t){return this._anyListeners=this._anyListeners||[],this._anyListeners.unshift(t),this}},{key:\"offAny\",value:function(t){if(!this._anyListeners)return this;if(t){for(var e=this._anyListeners,n=0;n<e.length;n++)if(t===e[n])return e.splice(n,1),this}else this._anyListeners=[];return this}},{key:\"listenersAny\",value:function(){return this._anyListeners||[]}},{key:\"active\",get:function(){return!!this.subs}},{key:\"volatile\",get:function(){return this.flags.volatile=!0,this}}])&&s(e.prototype,n),r&&s(e,r),f}(h);e.Socket=v},function(t,e,n){\"use strict\";function r(t){return(r=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}Object.defineProperty(e,\"__esModule\",{value:!0}),e.hasBinary=e.isBinary=void 0;var o=\"function\"==typeof ArrayBuffer,i=Object.prototype.toString,s=\"function\"==typeof Blob||\"undefined\"!=typeof Blob&&\"[object BlobConstructor]\"===i.call(Blob),c=\"function\"==typeof File||\"undefined\"!=typeof File&&\"[object FileConstructor]\"===i.call(File);function a(t){return o&&(t instanceof ArrayBuffer||function(t){return\"function\"==typeof ArrayBuffer.isView?ArrayBuffer.isView(t):t.buffer instanceof ArrayBuffer}(t))||s&&t instanceof Blob||c&&t instanceof File}e.isBinary=a,e.hasBinary=function t(e,n){if(!e||\"object\"!==r(e))return!1;if(Array.isArray(e)){for(var o=0,i=e.length;o<i;o++)if(t(e[o]))return!0;return!1}if(a(e))return!0;if(e.toJSON&&\"function\"==typeof e.toJSON&&1===arguments.length)return t(e.toJSON(),!0);for(var s in e)if(Object.prototype.hasOwnProperty.call(e,s)&&t(e[s]))return!0;return!1}},function(t,e,n){\"use strict\";Object.defineProperty(e,\"__esModule\",{value:!0}),e.on=void 0,e.on=function(t,e,n){return t.on(e,n),function(){t.off(e,n)}}},function(t,e,n){\"use strict\";function r(t){return(r=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}Object.defineProperty(e,\"__esModule\",{value:!0}),e.Socket=e.io=e.Manager=e.protocol=void 0;var o=n(18),i=n(7),s=n(14);Object.defineProperty(e,\"Socket\",{enumerable:!0,get:function(){return s.Socket}}),t.exports=e=a;var c=e.managers={};function a(t,e){\"object\"===r(t)&&(e=t,t=void 0),e=e||{};var n,s=o.url(t,e.path),a=s.source,u=s.id,f=s.path,p=c[u]&&f in c[u].nsps;return e.forceNew||e[\"force new connection\"]||!1===e.multiplex||p?n=new i.Manager(a,e):(c[u]||(c[u]=new i.Manager(a,e)),n=c[u]),s.query&&!e.query&&(e.query=s.queryKey),n.socket(s.path,e)}e.io=a;var u=n(5);Object.defineProperty(e,\"protocol\",{enumerable:!0,get:function(){return u.protocol}}),e.connect=a;var f=n(7);Object.defineProperty(e,\"Manager\",{enumerable:!0,get:function(){return f.Manager}})},function(t,e,n){\"use strict\";Object.defineProperty(e,\"__esModule\",{value:!0}),e.url=void 0;var r=n(6);e.url=function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:\"\",n=arguments.length>2?arguments[2]:void 0,o=t;n=n||\"undefined\"!=typeof location&&location,null==t&&(t=n.protocol+\"//\"+n.host),\"string\"==typeof t&&(\"/\"===t.charAt(0)&&(t=\"/\"===t.charAt(1)?n.protocol+t:n.host+t),/^(https?|wss?):\\/\\//.test(t)||(t=void 0!==n?n.protocol+\"//\"+t:\"https://\"+t),o=r(t)),o.port||(/^(http|ws)$/.test(o.protocol)?o.port=\"80\":/^(http|ws)s$/.test(o.protocol)&&(o.port=\"443\")),o.path=o.path||\"/\";var i=-1!==o.host.indexOf(\":\"),s=i?\"[\"+o.host+\"]\":o.host;return o.id=o.protocol+\"://\"+s+\":\"+o.port+e,o.href=o.protocol+\"://\"+s+(n&&n.port===o.port?\"\":\":\"+o.port),o}},function(t,e,n){var r=n(20);t.exports=function(t,e){return new r(t,e)},t.exports.Socket=r,t.exports.protocol=r.protocol,t.exports.Transport=n(3),t.exports.transports=n(8),t.exports.parser=n(1)},function(t,e,n){function r(){return(r=Object.assign||function(t){for(var e=1;e<arguments.length;e++){var n=arguments[e];for(var r in n)Object.prototype.hasOwnProperty.call(n,r)&&(t[r]=n[r])}return t}).apply(this,arguments)}function o(t){return(o=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}function i(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}function s(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}function c(t,e){return(c=Object.setPrototypeOf||function(t,e){return t.__proto__=e,t})(t,e)}function a(t){var e=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(t){return!1}}();return function(){var n,r=f(t);if(e){var o=f(this).constructor;n=Reflect.construct(r,arguments,o)}else n=r.apply(this,arguments);return u(this,n)}}function u(t,e){return!e||\"object\"!==o(e)&&\"function\"!=typeof e?function(t){if(void 0===t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return t}(t):e}function f(t){return(f=Object.setPrototypeOf?Object.getPrototypeOf:function(t){return t.__proto__||Object.getPrototypeOf(t)})(t)}var p=n(8),l=n(0),h=n(1),y=n(6),d=n(4),v=function(t){!function(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function\");t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),e&&c(t,e)}(l,t);var e,n,u,f=a(l);function l(t){var e,n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return i(this,l),e=f.call(this),t&&\"object\"===o(t)&&(n=t,t=null),t?(t=y(t),n.hostname=t.host,n.secure=\"https\"===t.protocol||\"wss\"===t.protocol,n.port=t.port,t.query&&(n.query=t.query)):n.host&&(n.hostname=y(n.host).host),e.secure=null!=n.secure?n.secure:\"undefined\"!=typeof location&&\"https:\"===location.protocol,n.hostname&&!n.port&&(n.port=e.secure?\"443\":\"80\"),e.hostname=n.hostname||(\"undefined\"!=typeof location?location.hostname:\"localhost\"),e.port=n.port||(\"undefined\"!=typeof location&&location.port?location.port:e.secure?443:80),e.transports=n.transports||[\"polling\",\"websocket\"],e.readyState=\"\",e.writeBuffer=[],e.prevBufferLen=0,e.opts=r({path:\"/engine.io\",agent:!1,withCredentials:!1,upgrade:!0,jsonp:!0,timestampParam:\"t\",rememberUpgrade:!1,rejectUnauthorized:!0,perMessageDeflate:{threshold:1024},transportOptions:{}},n),e.opts.path=e.opts.path.replace(/\\/$/,\"\")+\"/\",\"string\"==typeof e.opts.query&&(e.opts.query=d.decode(e.opts.query)),e.id=null,e.upgrades=null,e.pingInterval=null,e.pingTimeout=null,e.pingTimeoutTimer=null,\"function\"==typeof addEventListener&&addEventListener(\"beforeunload\",(function(){e.transport&&(e.transport.removeAllListeners(),e.transport.close())}),!1),e.open(),e}return e=l,(n=[{key:\"createTransport\",value:function(t){var e=function(t){var e={};for(var n in t)t.hasOwnProperty(n)&&(e[n]=t[n]);return e}(this.opts.query);e.EIO=h.protocol,e.transport=t,this.id&&(e.sid=this.id);var n=r({},this.opts.transportOptions[t],this.opts,{query:e,socket:this,hostname:this.hostname,secure:this.secure,port:this.port});return new p[t](n)}},{key:\"open\",value:function(){var t;if(this.opts.rememberUpgrade&&l.priorWebsocketSuccess&&-1!==this.transports.indexOf(\"websocket\"))t=\"websocket\";else{if(0===this.transports.length){var e=this;return void setTimeout((function(){e.emit(\"error\",\"No transports available\")}),0)}t=this.transports[0]}this.readyState=\"opening\";try{t=this.createTransport(t)}catch(t){return this.transports.shift(),void this.open()}t.open(),this.setTransport(t)}},{key:\"setTransport\",value:function(t){var e=this;this.transport&&this.transport.removeAllListeners(),this.transport=t,t.on(\"drain\",(function(){e.onDrain()})).on(\"packet\",(function(t){e.onPacket(t)})).on(\"error\",(function(t){e.onError(t)})).on(\"close\",(function(){e.onClose(\"transport close\")}))}},{key:\"probe\",value:function(t){var e=this.createTransport(t,{probe:1}),n=!1,r=this;function o(){if(r.onlyBinaryUpgrades){var t=!this.supportsBinary&&r.transport.supportsBinary;n=n||t}n||(e.send([{type:\"ping\",data:\"probe\"}]),e.once(\"packet\",(function(t){if(!n)if(\"pong\"===t.type&&\"probe\"===t.data){if(r.upgrading=!0,r.emit(\"upgrading\",e),!e)return;l.priorWebsocketSuccess=\"websocket\"===e.name,r.transport.pause((function(){n||\"closed\"!==r.readyState&&(f(),r.setTransport(e),e.send([{type:\"upgrade\"}]),r.emit(\"upgrade\",e),e=null,r.upgrading=!1,r.flush())}))}else{var o=new Error(\"probe error\");o.transport=e.name,r.emit(\"upgradeError\",o)}})))}function i(){n||(n=!0,f(),e.close(),e=null)}function s(t){var n=new Error(\"probe error: \"+t);n.transport=e.name,i(),r.emit(\"upgradeError\",n)}function c(){s(\"transport closed\")}function a(){s(\"socket closed\")}function u(t){e&&t.name!==e.name&&i()}function f(){e.removeListener(\"open\",o),e.removeListener(\"error\",s),e.removeListener(\"close\",c),r.removeListener(\"close\",a),r.removeListener(\"upgrading\",u)}l.priorWebsocketSuccess=!1,e.once(\"open\",o),e.once(\"error\",s),e.once(\"close\",c),this.once(\"close\",a),this.once(\"upgrading\",u),e.open()}},{key:\"onOpen\",value:function(){if(this.readyState=\"open\",l.priorWebsocketSuccess=\"websocket\"===this.transport.name,this.emit(\"open\"),this.flush(),\"open\"===this.readyState&&this.opts.upgrade&&this.transport.pause)for(var t=0,e=this.upgrades.length;t<e;t++)this.probe(this.upgrades[t])}},{key:\"onPacket\",value:function(t){if(\"opening\"===this.readyState||\"open\"===this.readyState||\"closing\"===this.readyState)switch(this.emit(\"packet\",t),this.emit(\"heartbeat\"),t.type){case\"open\":this.onHandshake(JSON.parse(t.data));break;case\"ping\":this.resetPingTimeout(),this.sendPacket(\"pong\"),this.emit(\"pong\");break;case\"error\":var e=new Error(\"server error\");e.code=t.data,this.onError(e);break;case\"message\":this.emit(\"data\",t.data),this.emit(\"message\",t.data)}}},{key:\"onHandshake\",value:function(t){this.emit(\"handshake\",t),this.id=t.sid,this.transport.query.sid=t.sid,this.upgrades=this.filterUpgrades(t.upgrades),this.pingInterval=t.pingInterval,this.pingTimeout=t.pingTimeout,this.onOpen(),\"closed\"!==this.readyState&&this.resetPingTimeout()}},{key:\"resetPingTimeout\",value:function(){var t=this;clearTimeout(this.pingTimeoutTimer),this.pingTimeoutTimer=setTimeout((function(){t.onClose(\"ping timeout\")}),this.pingInterval+this.pingTimeout)}},{key:\"onDrain\",value:function(){this.writeBuffer.splice(0,this.prevBufferLen),this.prevBufferLen=0,0===this.writeBuffer.length?this.emit(\"drain\"):this.flush()}},{key:\"flush\",value:function(){\"closed\"!==this.readyState&&this.transport.writable&&!this.upgrading&&this.writeBuffer.length&&(this.transport.send(this.writeBuffer),this.prevBufferLen=this.writeBuffer.length,this.emit(\"flush\"))}},{key:\"write\",value:function(t,e,n){return this.sendPacket(\"message\",t,e,n),this}},{key:\"send\",value:function(t,e,n){return this.sendPacket(\"message\",t,e,n),this}},{key:\"sendPacket\",value:function(t,e,n,r){if(\"function\"==typeof e&&(r=e,e=void 0),\"function\"==typeof n&&(r=n,n=null),\"closing\"!==this.readyState&&\"closed\"!==this.readyState){(n=n||{}).compress=!1!==n.compress;var o={type:t,data:e,options:n};this.emit(\"packetCreate\",o),this.writeBuffer.push(o),r&&this.once(\"flush\",r),this.flush()}}},{key:\"close\",value:function(){var t=this;function e(){t.onClose(\"forced close\"),t.transport.close()}function n(){t.removeListener(\"upgrade\",n),t.removeListener(\"upgradeError\",n),e()}function r(){t.once(\"upgrade\",n),t.once(\"upgradeError\",n)}return\"opening\"!==this.readyState&&\"open\"!==this.readyState||(this.readyState=\"closing\",this.writeBuffer.length?this.once(\"drain\",(function(){this.upgrading?r():e()})):this.upgrading?r():e()),this}},{key:\"onError\",value:function(t){l.priorWebsocketSuccess=!1,this.emit(\"error\",t),this.onClose(\"transport error\",t)}},{key:\"onClose\",value:function(t,e){\"opening\"!==this.readyState&&\"open\"!==this.readyState&&\"closing\"!==this.readyState||(clearTimeout(this.pingIntervalTimer),clearTimeout(this.pingTimeoutTimer),this.transport.removeAllListeners(\"close\"),this.transport.close(),this.transport.removeAllListeners(),this.readyState=\"closed\",this.id=null,this.emit(\"close\",t,e),this.writeBuffer=[],this.prevBufferLen=0)}},{key:\"filterUpgrades\",value:function(t){for(var e=[],n=0,r=t.length;n<r;n++)~this.transports.indexOf(t[n])&&e.push(t[n]);return e}}])&&s(e.prototype,n),u&&s(e,u),l}(l);v.priorWebsocketSuccess=!1,v.protocol=h.protocol,t.exports=v},function(t,e){try{t.exports=\"undefined\"!=typeof XMLHttpRequest&&\"withCredentials\"in new XMLHttpRequest}catch(e){t.exports=!1}},function(t,e,n){function r(t){return(r=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}function o(){return(o=Object.assign||function(t){for(var e=1;e<arguments.length;e++){var n=arguments[e];for(var r in n)Object.prototype.hasOwnProperty.call(n,r)&&(t[r]=n[r])}return t}).apply(this,arguments)}function i(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}function s(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}function c(t,e,n){return e&&s(t.prototype,e),n&&s(t,n),t}function a(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function\");t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),e&&u(t,e)}function u(t,e){return(u=Object.setPrototypeOf||function(t,e){return t.__proto__=e,t})(t,e)}function f(t){var e=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(t){return!1}}();return function(){var n,r=l(t);if(e){var o=l(this).constructor;n=Reflect.construct(r,arguments,o)}else n=r.apply(this,arguments);return p(this,n)}}function p(t,e){return!e||\"object\"!==r(e)&&\"function\"!=typeof e?function(t){if(void 0===t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return t}(t):e}function l(t){return(l=Object.setPrototypeOf?Object.getPrototypeOf:function(t){return t.__proto__||Object.getPrototypeOf(t)})(t)}var h=n(9),y=n(10),d=n(0),v=n(13).pick,b=n(2);function m(){}var g=null!=new h({xdomain:!1}).responseType,k=function(t){a(n,t);var e=f(n);function n(t){var r;if(i(this,n),r=e.call(this,t),\"undefined\"!=typeof location){var o=\"https:\"===location.protocol,s=location.port;s||(s=o?443:80),r.xd=\"undefined\"!=typeof location&&t.hostname!==location.hostname||s!==t.port,r.xs=t.secure!==o}var c=t&&t.forceBase64;return r.supportsBinary=g&&!c,r}return c(n,[{key:\"request\",value:function(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};return o(t,{xd:this.xd,xs:this.xs},this.opts),new w(this.uri(),t)}},{key:\"doWrite\",value:function(t,e){var n=this.request({method:\"POST\",data:t}),r=this;n.on(\"success\",e),n.on(\"error\",(function(t){r.onError(\"xhr post error\",t)}))}},{key:\"doPoll\",value:function(){var t=this.request(),e=this;t.on(\"data\",(function(t){e.onData(t)})),t.on(\"error\",(function(t){e.onError(\"xhr poll error\",t)})),this.pollXhr=t}}]),n}(y),w=function(t){a(n,t);var e=f(n);function n(t,r){var o;return i(this,n),(o=e.call(this)).opts=r,o.method=r.method||\"GET\",o.uri=t,o.async=!1!==r.async,o.data=void 0!==r.data?r.data:null,o.create(),o}return c(n,[{key:\"create\",value:function(){var t=v(this.opts,\"agent\",\"enablesXDR\",\"pfx\",\"key\",\"passphrase\",\"cert\",\"ca\",\"ciphers\",\"rejectUnauthorized\");t.xdomain=!!this.opts.xd,t.xscheme=!!this.opts.xs;var e=this.xhr=new h(t),r=this;try{e.open(this.method,this.uri,this.async);try{if(this.opts.extraHeaders)for(var o in e.setDisableHeaderCheck&&e.setDisableHeaderCheck(!0),this.opts.extraHeaders)this.opts.extraHeaders.hasOwnProperty(o)&&e.setRequestHeader(o,this.opts.extraHeaders[o])}catch(t){}if(\"POST\"===this.method)try{e.setRequestHeader(\"Content-type\",\"text/plain;charset=UTF-8\")}catch(t){}try{e.setRequestHeader(\"Accept\",\"*/*\")}catch(t){}\"withCredentials\"in e&&(e.withCredentials=this.opts.withCredentials),this.opts.requestTimeout&&(e.timeout=this.opts.requestTimeout),this.hasXDR()?(e.onload=function(){r.onLoad()},e.onerror=function(){r.onError(e.responseText)}):e.onreadystatechange=function(){4===e.readyState&&(200===e.status||1223===e.status?r.onLoad():setTimeout((function(){r.onError(\"number\"==typeof e.status?e.status:0)}),0))},e.send(this.data)}catch(t){return void setTimeout((function(){r.onError(t)}),0)}\"undefined\"!=typeof document&&(this.index=n.requestsCount++,n.requests[this.index]=this)}},{key:\"onSuccess\",value:function(){this.emit(\"success\"),this.cleanup()}},{key:\"onData\",value:function(t){this.emit(\"data\",t),this.onSuccess()}},{key:\"onError\",value:function(t){this.emit(\"error\",t),this.cleanup(!0)}},{key:\"cleanup\",value:function(t){if(void 0!==this.xhr&&null!==this.xhr){if(this.hasXDR()?this.xhr.onload=this.xhr.onerror=m:this.xhr.onreadystatechange=m,t)try{this.xhr.abort()}catch(t){}\"undefined\"!=typeof document&&delete n.requests[this.index],this.xhr=null}}},{key:\"onLoad\",value:function(){var t=this.xhr.responseText;null!==t&&this.onData(t)}},{key:\"hasXDR\",value:function(){return\"undefined\"!=typeof XDomainRequest&&!this.xs&&this.enablesXDR}},{key:\"abort\",value:function(){this.cleanup()}}]),n}(d);if(w.requestsCount=0,w.requests={},\"undefined\"!=typeof document)if(\"function\"==typeof attachEvent)attachEvent(\"onunload\",_);else if(\"function\"==typeof addEventListener){addEventListener(\"onpagehide\"in b?\"pagehide\":\"unload\",_,!1)}function _(){for(var t in w.requests)w.requests.hasOwnProperty(t)&&w.requests[t].abort()}t.exports=k,t.exports.Request=w},function(t,e,n){var r=n(11).PACKET_TYPES,o=\"function\"==typeof Blob||\"undefined\"!=typeof Blob&&\"[object BlobConstructor]\"===Object.prototype.toString.call(Blob),i=\"function\"==typeof ArrayBuffer,s=function(t,e){var n=new FileReader;return n.onload=function(){var t=n.result.split(\",\")[1];e(\"b\"+t)},n.readAsDataURL(t)};t.exports=function(t,e,n){var c,a=t.type,u=t.data;return o&&u instanceof Blob?e?n(u):s(u,n):i&&(u instanceof ArrayBuffer||(c=u,\"function\"==typeof ArrayBuffer.isView?ArrayBuffer.isView(c):c&&c.buffer instanceof ArrayBuffer))?e?n(u instanceof ArrayBuffer?u:u.buffer):s(new Blob([u]),n):n(r[a]+(u||\"\"))}},function(t,e,n){var r,o=n(11),i=o.PACKET_TYPES_REVERSE,s=o.ERROR_PACKET;\"function\"==typeof ArrayBuffer&&(r=n(25));var c=function(t,e){if(r){var n=r.decode(t);return a(n,e)}return{base64:!0,data:t}},a=function(t,e){switch(e){case\"blob\":return t instanceof ArrayBuffer?new Blob([t]):t;case\"arraybuffer\":default:return t}};t.exports=function(t,e){if(\"string\"!=typeof t)return{type:\"message\",data:a(t,e)};var n=t.charAt(0);return\"b\"===n?{type:\"message\",data:c(t.substring(1),e)}:i[n]?t.length>1?{type:i[n],data:t.substring(1)}:{type:i[n]}:s}},function(t,e){!function(t){\"use strict\";e.encode=function(e){var n,r=new Uint8Array(e),o=r.length,i=\"\";for(n=0;n<o;n+=3)i+=t[r[n]>>2],i+=t[(3&r[n])<<4|r[n+1]>>4],i+=t[(15&r[n+1])<<2|r[n+2]>>6],i+=t[63&r[n+2]];return o%3==2?i=i.substring(0,i.length-1)+\"=\":o%3==1&&(i=i.substring(0,i.length-2)+\"==\"),i},e.decode=function(e){var n,r,o,i,s,c=.75*e.length,a=e.length,u=0;\"=\"===e[e.length-1]&&(c--,\"=\"===e[e.length-2]&&c--);var f=new ArrayBuffer(c),p=new Uint8Array(f);for(n=0;n<a;n+=4)r=t.indexOf(e[n]),o=t.indexOf(e[n+1]),i=t.indexOf(e[n+2]),s=t.indexOf(e[n+3]),p[u++]=r<<2|o>>4,p[u++]=(15&o)<<4|i>>2,p[u++]=(3&i)<<6|63&s;return f}}(\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\")},function(t,e,n){function r(t){return(r=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}function o(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}function i(t,e,n){return(i=\"undefined\"!=typeof Reflect&&Reflect.get?Reflect.get:function(t,e,n){var r=function(t,e){for(;!Object.prototype.hasOwnProperty.call(t,e)&&null!==(t=f(t)););return t}(t,e);if(r){var o=Object.getOwnPropertyDescriptor(r,e);return o.get?o.get.call(n):o.value}})(t,e,n||t)}function s(t,e){return(s=Object.setPrototypeOf||function(t,e){return t.__proto__=e,t})(t,e)}function c(t){var e=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(t){return!1}}();return function(){var n,r=f(t);if(e){var o=f(this).constructor;n=Reflect.construct(r,arguments,o)}else n=r.apply(this,arguments);return a(this,n)}}function a(t,e){return!e||\"object\"!==r(e)&&\"function\"!=typeof e?u(t):e}function u(t){if(void 0===t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return t}function f(t){return(f=Object.setPrototypeOf?Object.getPrototypeOf:function(t){return t.__proto__||Object.getPrototypeOf(t)})(t)}var p,l=n(10),h=n(2),y=/\\n/g,d=/\\\\n/g,v=function(t){!function(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function\");t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),e&&s(t,e)}(l,t);var e,n,r,a=c(l);function l(t){var e;!function(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}(this,l),(e=a.call(this,t)).query=e.query||{},p||(p=h.___eio=h.___eio||[]),e.index=p.length;var n=u(e);return p.push((function(t){n.onData(t)})),e.query.j=e.index,e}return e=l,(n=[{key:\"doClose\",value:function(){this.script&&(this.script.onerror=function(){},this.script.parentNode.removeChild(this.script),this.script=null),this.form&&(this.form.parentNode.removeChild(this.form),this.form=null,this.iframe=null),i(f(l.prototype),\"doClose\",this).call(this)}},{key:\"doPoll\",value:function(){var t=this,e=document.createElement(\"script\");this.script&&(this.script.parentNode.removeChild(this.script),this.script=null),e.async=!0,e.src=this.uri(),e.onerror=function(e){t.onError(\"jsonp poll error\",e)};var n=document.getElementsByTagName(\"script\")[0];n?n.parentNode.insertBefore(e,n):(document.head||document.body).appendChild(e),this.script=e,\"undefined\"!=typeof navigator&&/gecko/i.test(navigator.userAgent)&&setTimeout((function(){var t=document.createElement(\"iframe\");document.body.appendChild(t),document.body.removeChild(t)}),100)}},{key:\"doWrite\",value:function(t,e){var n,r=this;if(!this.form){var o=document.createElement(\"form\"),i=document.createElement(\"textarea\"),s=this.iframeId=\"eio_iframe_\"+this.index;o.className=\"socketio\",o.style.position=\"absolute\",o.style.top=\"-1000px\",o.style.left=\"-1000px\",o.target=s,o.method=\"POST\",o.setAttribute(\"accept-charset\",\"utf-8\"),i.name=\"d\",o.appendChild(i),document.body.appendChild(o),this.form=o,this.area=i}function c(){a(),e()}function a(){if(r.iframe)try{r.form.removeChild(r.iframe)}catch(t){r.onError(\"jsonp polling iframe removal error\",t)}try{var t='<iframe src=\"javascript:0\" name=\"'+r.iframeId+'\">';n=document.createElement(t)}catch(t){(n=document.createElement(\"iframe\")).name=r.iframeId,n.src=\"javascript:0\"}n.id=r.iframeId,r.form.appendChild(n),r.iframe=n}this.form.action=this.uri(),a(),t=t.replace(d,\"\\\\\\n\"),this.area.value=t.replace(y,\"\\\\n\");try{this.form.submit()}catch(t){}this.iframe.attachEvent?this.iframe.onreadystatechange=function(){\"complete\"===r.iframe.readyState&&c()}:this.iframe.onload=c}},{key:\"supportsBinary\",get:function(){return!1}}])&&o(e.prototype,n),r&&o(e,r),l}(l);t.exports=v},function(t,e,n){function r(t){return(r=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}function o(t,e){for(var n=0;n<e.length;n++){var r=e[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(t,r.key,r)}}function i(t,e){return(i=Object.setPrototypeOf||function(t,e){return t.__proto__=e,t})(t,e)}function s(t){var e=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(t){return!1}}();return function(){var n,r=a(t);if(e){var o=a(this).constructor;n=Reflect.construct(r,arguments,o)}else n=r.apply(this,arguments);return c(this,n)}}function c(t,e){return!e||\"object\"!==r(e)&&\"function\"!=typeof e?function(t){if(void 0===t)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return t}(t):e}function a(t){return(a=Object.setPrototypeOf?Object.getPrototypeOf:function(t){return t.__proto__||Object.getPrototypeOf(t)})(t)}var u=n(3),f=n(1),p=n(4),l=n(12),h=n(13).pick,y=n(28),d=y.WebSocket,v=y.usingBrowserWebSocket,b=y.defaultBinaryType,m=\"undefined\"!=typeof navigator&&\"string\"==typeof navigator.product&&\"reactnative\"===navigator.product.toLowerCase(),g=function(t){!function(t,e){if(\"function\"!=typeof e&&null!==e)throw new TypeError(\"Super expression must either be null or a function\");t.prototype=Object.create(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),e&&i(t,e)}(a,t);var e,n,r,c=s(a);function a(t){var e;return function(t,e){if(!(t instanceof e))throw new TypeError(\"Cannot call a class as a function\")}(this,a),(e=c.call(this,t)).supportsBinary=!t.forceBase64,e}return e=a,(n=[{key:\"doOpen\",value:function(){if(this.check()){var t=this.uri(),e=this.opts.protocols,n=m?{}:h(this.opts,\"agent\",\"perMessageDeflate\",\"pfx\",\"key\",\"passphrase\",\"cert\",\"ca\",\"ciphers\",\"rejectUnauthorized\",\"localAddress\",\"protocolVersion\",\"origin\",\"maxPayload\",\"family\",\"checkServerIdentity\");this.opts.extraHeaders&&(n.headers=this.opts.extraHeaders);try{this.ws=v&&!m?e?new d(t,e):new d(t):new d(t,e,n)}catch(t){return this.emit(\"error\",t)}this.ws.binaryType=this.socket.binaryType||b,this.addEventListeners()}}},{key:\"addEventListeners\",value:function(){var t=this;this.ws.onopen=function(){t.onOpen()},this.ws.onclose=function(){t.onClose()},this.ws.onmessage=function(e){t.onData(e.data)},this.ws.onerror=function(e){t.onError(\"websocket error\",e)}}},{key:\"write\",value:function(t){var e=this;this.writable=!1;for(var n=t.length,r=0,o=n;r<o;r++)!function(t){f.encodePacket(t,e.supportsBinary,(function(r){var o={};v||(t.options&&(o.compress=t.options.compress),e.opts.perMessageDeflate&&(\"string\"==typeof r?Buffer.byteLength(r):r.length)<e.opts.perMessageDeflate.threshold&&(o.compress=!1));try{v?e.ws.send(r):e.ws.send(r,o)}catch(t){}--n||(e.emit(\"flush\"),setTimeout((function(){e.writable=!0,e.emit(\"drain\")}),0))}))}(t[r])}},{key:\"onClose\",value:function(){u.prototype.onClose.call(this)}},{key:\"doClose\",value:function(){void 0!==this.ws&&(this.ws.close(),this.ws=null)}},{key:\"uri\",value:function(){var t=this.query||{},e=this.opts.secure?\"wss\":\"ws\",n=\"\";return this.opts.port&&(\"wss\"===e&&443!==Number(this.opts.port)||\"ws\"===e&&80!==Number(this.opts.port))&&(n=\":\"+this.opts.port),this.opts.timestampRequests&&(t[this.opts.timestampParam]=l()),this.supportsBinary||(t.b64=1),(t=p.encode(t)).length&&(t=\"?\"+t),e+\"://\"+(-1!==this.opts.hostname.indexOf(\":\")?\"[\"+this.opts.hostname+\"]\":this.opts.hostname)+n+this.opts.path+t}},{key:\"check\",value:function(){return!(!d||\"__initialize\"in d&&this.name===a.prototype.name)}},{key:\"name\",get:function(){return\"websocket\"}}])&&o(e.prototype,n),r&&o(e,r),a}(u);t.exports=g},function(t,e,n){var r=n(2);t.exports={WebSocket:r.WebSocket||r.MozWebSocket,usingBrowserWebSocket:!0,defaultBinaryType:\"arraybuffer\"}},function(t,e,n){\"use strict\";function r(t){return(r=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(t){return typeof t}:function(t){return t&&\"function\"==typeof Symbol&&t.constructor===Symbol&&t!==Symbol.prototype?\"symbol\":typeof t})(t)}Object.defineProperty(e,\"__esModule\",{value:!0}),e.reconstructPacket=e.deconstructPacket=void 0;var o=n(15);e.deconstructPacket=function(t){var e=[],n=t.data,i=t;return i.data=function t(e,n){if(!e)return e;if(o.isBinary(e)){var i={_placeholder:!0,num:n.length};return n.push(e),i}if(Array.isArray(e)){for(var s=new Array(e.length),c=0;c<e.length;c++)s[c]=t(e[c],n);return s}if(\"object\"===r(e)&&!(e instanceof Date)){var a={};for(var u in e)e.hasOwnProperty(u)&&(a[u]=t(e[u],n));return a}return e}(n,e),i.attachments=e.length,{packet:i,buffers:e}},e.reconstructPacket=function(t,e){return t.data=function t(e,n){if(!e)return e;if(e&&e._placeholder)return n[e.num];if(Array.isArray(e))for(var o=0;o<e.length;o++)e[o]=t(e[o],n);else if(\"object\"===r(e))for(var i in e)e.hasOwnProperty(i)&&(e[i]=t(e[i],n));return e}(t.data,e),t.attachments=void 0,t}},function(t,e){function n(t){t=t||{},this.ms=t.min||100,this.max=t.max||1e4,this.factor=t.factor||2,this.jitter=t.jitter>0&&t.jitter<=1?t.jitter:0,this.attempts=0}t.exports=n,n.prototype.duration=function(){var t=this.ms*Math.pow(this.factor,this.attempts++);if(this.jitter){var e=Math.random(),n=Math.floor(e*this.jitter*t);t=0==(1&Math.floor(10*e))?t-n:t+n}return 0|Math.min(t,this.max)},n.prototype.reset=function(){this.attempts=0},n.prototype.setMin=function(t){this.ms=t},n.prototype.setMax=function(t){this.max=t},n.prototype.setJitter=function(t){this.jitter=t}}])}));\n+//# sourceMappingURL=socket.io.min.js.map\n\\ No newline at end of file\ndiff --git a/changedetectionio/static/styles/scss/parts/_watch_table.scss b/changedetectionio/static/styles/scss/parts/_watch_table.scss\nindex bce774c0e03..b6f0ff6bd0e 100644\n--- a/changedetectionio/static/styles/scss/parts/_watch_table.scss\n+++ b/changedetectionio/static/styles/scss/parts/_watch_table.scss\n@@ -39,10 +39,13 @@\n     }\n   }\n \n-  .title-col a[target=\"_blank\"]::after,\n-  .current-diff-url::after {\n-    content: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAAQElEQVR42qXKwQkAIAxDUUdxtO6/RBQkQZvSi8I/pL4BoGw/XPkh4XigPmsUgh0626AjRsgxHTkUThsG2T/sIlzdTsp52kSS1wAAAABJRU5ErkJggg==);\n+  .title-col a[target=\"_blank\"] i[data-feather],\n+  .current-diff-url i[data-feather] {\n+    width: 12px;\n+    height: 12px;\n+    stroke: #666;\n     margin: 0 3px 0 5px;\n+    vertical-align: middle;\n   }\n \n \n@@ -114,5 +117,18 @@\n       display: block !important;\n     }\n   }\n+\n+  tr.single-history {\n+    a.preview-link {\n+      display: inline-block !important;\n+    }\n+  }\n+  tr.multiple-history {\n+    a.history-link {\n+      display: inline-block !important;\n+    }\n+  }\n }\n \n+\n+\ndiff --git a/changedetectionio/static/styles/scss/styles.scss b/changedetectionio/static/styles/scss/styles.scss\nindex 07c4fc92915..514d825c212 100644\n--- a/changedetectionio/static/styles/scss/styles.scss\n+++ b/changedetectionio/static/styles/scss/styles.scss\n@@ -1083,6 +1083,9 @@ ul {\n     /* some space if they wrap the page */\n     margin-bottom: 3px;\n     margin-top: 3px;\n+    /* vertically center icon and text */\n+    display: inline-flex;\n+    align-items: center;\n   }\n }\n \ndiff --git a/changedetectionio/static/styles/styles.css b/changedetectionio/static/styles/styles.css\nindex ccdb0db8173..7699ea28998 100644\n--- a/changedetectionio/static/styles/styles.css\n+++ b/changedetectionio/static/styles/styles.css\n@@ -545,10 +545,13 @@ body.preview-text-enabled {\n         font-weight: bolder; }\n       .watch-table th a.inactive .arrow {\n         display: none; }\n-  .watch-table .title-col a[target=\"_blank\"]::after,\n-  .watch-table .current-diff-url::after {\n-    content: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAAQElEQVR42qXKwQkAIAxDUUdxtO6/RBQkQZvSi8I/pL4BoGw/XPkh4XigPmsUgh0626AjRsgxHTkUThsG2T/sIlzdTsp52kSS1wAAAABJRU5ErkJggg==);\n-    margin: 0 3px 0 5px; }\n+  .watch-table .title-col a[target=\"_blank\"] i[data-feather],\n+  .watch-table .current-diff-url i[data-feather] {\n+    width: 12px;\n+    height: 12px;\n+    stroke: #666;\n+    margin: 0 3px 0 5px;\n+    vertical-align: middle; }\n   .watch-table tr.checking-now td:first-child {\n     position: relative; }\n   .watch-table tr.checking-now td:first-child::before {\n@@ -579,6 +582,10 @@ body.preview-text-enabled {\n     color: var(--color-watch-table-error); }\n     .watch-table tr.has-error .error-text {\n       display: block !important; }\n+  .watch-table tr.single-history a.preview-link {\n+    display: inline-block !important; }\n+  .watch-table tr.multiple-history a.history-link {\n+    display: inline-block !important; }\n \n ul#conditions_match_logic {\n   list-style: none; }\n@@ -1457,7 +1464,10 @@ ul {\n   #checkbox-operations button {\n     /* some space if they wrap the page */\n     margin-bottom: 3px;\n-    margin-top: 3px; }\n+    margin-top: 3px;\n+    /* vertically center icon and text */\n+    display: inline-flex;\n+    align-items: center; }\n \n .checkbox-uuid > * {\n   vertical-align: middle; }\ndiff --git a/changedetectionio/store.py b/changedetectionio/store.py\nindex f200bef3111..ed5b4dca99e 100644\n--- a/changedetectionio/store.py\n+++ b/changedetectionio/store.py\n@@ -238,6 +238,7 @@ def delete(self, uuid):\n         with self.lock:\n             if uuid == 'all':\n                 self.__data['watching'] = {}\n+                time.sleep(1) # Mainly used for testing to allow all items to flush before running next test\n \n                 # GitHub #30 also delete history records\n                 for uuid in self.data['watching']:\n@@ -407,7 +408,12 @@ def sync_to_json(self):\n                 # This is a fairly basic strategy to deal with the case that the file is corrupted,\n                 # system was out of memory, out of RAM etc\n                 with open(self.json_store_path+\".tmp\", 'w') as json_file:\n-                    json.dump(data, json_file, indent=4)\n+                    # Use compact JSON in production for better performance\n+                    debug_mode = os.environ.get('CHANGEDETECTION_DEBUG', 'false').lower() == 'true'\n+                    if debug_mode:\n+                        json.dump(data, json_file, indent=4)\n+                    else:\n+                        json.dump(data, json_file, separators=(',', ':'))\n                 os.replace(self.json_store_path+\".tmp\", self.json_store_path)\n             except Exception as e:\n                 logger.error(f\"Error writing JSON!! (Main JSON file save was skipped) : {str(e)}\")\ndiff --git a/changedetectionio/templates/base.html b/changedetectionio/templates/base.html\nindex ee6410c6b65..e564df6aa47 100644\n--- a/changedetectionio/templates/base.html\n+++ b/changedetectionio/templates/base.html\n@@ -31,11 +31,13 @@\n         const socketio_url=\"{{ get_socketio_path() }}/socket.io\";\n         const is_authenticated = {% if current_user.is_authenticated or not has_password %}true{% else %}false{% endif %};\n     </script>\n+    <script src=\"https://unpkg.com/feather-icons\"></script>\n     <script src=\"{{url_for('static_content', group='js', filename='jquery-3.6.0.min.js')}}\"></script>\n     <script src=\"{{url_for('static_content', group='js', filename='csrf.js')}}\" defer></script>\n-    <script src=\"{{url_for('static_content', group='js', filename='socket.io.min.js')}}\" integrity=\"sha384-c79GN5VsunZvi+Q/WObgk2in0CbZsHnjEqvFxC5DxHn9lTfNce2WW6h2pH6u/kF+\" crossorigin=\"anonymous\"></script>\n+    {% if socket_io_enabled %}\n+    <script src=\"{{url_for('static_content', group='js', filename='socket.io.min.js')}}\"></script>\n     <script src=\"{{url_for('static_content', group='js', filename='realtime.js')}}\" defer></script>\n-    <script src=\"{{url_for('static_content', group='js', filename='timeago-init.js')}}\" defer></script>\n+    {% endif %}\n   </head>\n \n   <body class=\"\">\ndiff --git a/changedetectionio/update_worker.py b/changedetectionio/update_worker.py\ndeleted file mode 100644\nindex 53d2d79835c..00000000000\n--- a/changedetectionio/update_worker.py\n+++ /dev/null\n@@ -1,608 +0,0 @@\n-from .processors.exceptions import ProcessorException\n-import changedetectionio.content_fetchers.exceptions as content_fetchers_exceptions\n-from changedetectionio.processors.text_json_diff.processor import FilterNotFoundInResponse\n-from changedetectionio import html_tools\n-from changedetectionio.flask_app import watch_check_update\n-\n-import importlib\n-import os\n-import queue\n-import threading\n-import time\n-\n-# A single update worker\n-#\n-# Requests for checking on a single site(watch) from a queue of watches\n-# (another process inserts watches into the queue that are time-ready for checking)\n-\n-from loguru import logger\n-\n-class update_worker(threading.Thread):\n-    current_uuid = None\n-\n-    def __init__(self, q, notification_q, app, datastore, *args, **kwargs):\n-        self.q = q\n-        self.app = app\n-        self.notification_q = notification_q\n-        self.datastore = datastore\n-        super().__init__(*args, **kwargs)\n-\n-    def queue_notification_for_watch(self, notification_q, n_object, watch):\n-        from changedetectionio import diff\n-        from changedetectionio.notification import default_notification_format_for_watch\n-\n-        dates = []\n-        trigger_text = ''\n-\n-        now = time.time()\n-\n-        if watch:\n-            watch_history = watch.history\n-            dates = list(watch_history.keys())\n-            trigger_text = watch.get('trigger_text', [])\n-\n-        # Add text that was triggered\n-        if len(dates):\n-            snapshot_contents = watch.get_history_snapshot(dates[-1])\n-        else:\n-            snapshot_contents = \"No snapshot/history available, the watch should fetch atleast once.\"\n-\n-        # If we ended up here with \"System default\"\n-        if n_object.get('notification_format') == default_notification_format_for_watch:\n-            n_object['notification_format'] = self.datastore.data['settings']['application'].get('notification_format')\n-\n-        html_colour_enable = False\n-        # HTML needs linebreak, but MarkDown and Text can use a linefeed\n-        if n_object.get('notification_format') == 'HTML':\n-            line_feed_sep = \"<br>\"\n-            # Snapshot will be plaintext on the disk, convert to some kind of HTML\n-            snapshot_contents = snapshot_contents.replace('\\n', line_feed_sep)\n-        elif n_object.get('notification_format') == 'HTML Color':\n-            line_feed_sep = \"<br>\"\n-            # Snapshot will be plaintext on the disk, convert to some kind of HTML\n-            snapshot_contents = snapshot_contents.replace('\\n', line_feed_sep)\n-            html_colour_enable = True\n-        else:\n-            line_feed_sep = \"\\n\"\n-\n-        triggered_text = ''\n-        if len(trigger_text):\n-            from . import html_tools\n-            triggered_text = html_tools.get_triggered_text(content=snapshot_contents, trigger_text=trigger_text)\n-            if triggered_text:\n-                triggered_text = line_feed_sep.join(triggered_text)\n-\n-        # Could be called as a 'test notification' with only 1 snapshot available\n-        prev_snapshot = \"Example text: example test\\nExample text: change detection is cool\\nExample text: some more examples\\n\"\n-        current_snapshot = \"Example text: example test\\nExample text: change detection is fantastic\\nExample text: even more examples\\nExample text: a lot more examples\"\n-\n-        if len(dates) > 1:\n-            prev_snapshot = watch.get_history_snapshot(dates[-2])\n-            current_snapshot = watch.get_history_snapshot(dates[-1])\n-\n-        n_object.update({\n-            'current_snapshot': snapshot_contents,\n-            'diff': diff.render_diff(prev_snapshot, current_snapshot, line_feed_sep=line_feed_sep, html_colour=html_colour_enable),\n-            'diff_added': diff.render_diff(prev_snapshot, current_snapshot, include_removed=False, line_feed_sep=line_feed_sep),\n-            'diff_full': diff.render_diff(prev_snapshot, current_snapshot, include_equal=True, line_feed_sep=line_feed_sep, html_colour=html_colour_enable),\n-            'diff_patch': diff.render_diff(prev_snapshot, current_snapshot, line_feed_sep=line_feed_sep, patch_format=True),\n-            'diff_removed': diff.render_diff(prev_snapshot, current_snapshot, include_added=False, line_feed_sep=line_feed_sep),\n-            'notification_timestamp': now,\n-            'screenshot': watch.get_screenshot() if watch and watch.get('notification_screenshot') else None,\n-            'triggered_text': triggered_text,\n-            'uuid': watch.get('uuid') if watch else None,\n-            'watch_url': watch.get('url') if watch else None,\n-        })\n-\n-        if watch:\n-            n_object.update(watch.extra_notification_token_values())\n-\n-        logger.trace(f\"Main rendered notification placeholders (diff_added etc) calculated in {time.time()-now:.3f}s\")\n-        logger.debug(\"Queued notification for sending\")\n-        notification_q.put(n_object)\n-\n-    # Prefer - Individual watch settings > Tag settings >  Global settings (in that order)\n-    def _check_cascading_vars(self, var_name, watch):\n-\n-        from changedetectionio.notification import (\n-            default_notification_format_for_watch,\n-            default_notification_body,\n-            default_notification_title\n-        )\n-\n-        # Would be better if this was some kind of Object where Watch can reference the parent datastore etc\n-        v = watch.get(var_name)\n-        if v and not watch.get('notification_muted'):\n-            if var_name == 'notification_format' and v == default_notification_format_for_watch:\n-                return self.datastore.data['settings']['application'].get('notification_format')\n-\n-            return v\n-\n-        tags = self.datastore.get_all_tags_for_watch(uuid=watch.get('uuid'))\n-        if tags:\n-            for tag_uuid, tag in tags.items():\n-                v = tag.get(var_name)\n-                if v and not tag.get('notification_muted'):\n-                    return v\n-\n-        if self.datastore.data['settings']['application'].get(var_name):\n-            return self.datastore.data['settings']['application'].get(var_name)\n-\n-        # Otherwise could be defaults\n-        if var_name == 'notification_format':\n-            return default_notification_format_for_watch\n-        if var_name == 'notification_body':\n-            return default_notification_body\n-        if var_name == 'notification_title':\n-            return default_notification_title\n-\n-        return None\n-\n-    def send_content_changed_notification(self, watch_uuid):\n-\n-        n_object = {}\n-        watch = self.datastore.data['watching'].get(watch_uuid)\n-        if not watch:\n-            return\n-\n-        watch_history = watch.history\n-        dates = list(watch_history.keys())\n-        # Theoretically it's possible that this could be just 1 long,\n-        # - In the case that the timestamp key was not unique\n-        if len(dates) == 1:\n-            raise ValueError(\n-                \"History index had 2 or more, but only 1 date loaded, timestamps were not unique? maybe two of the same timestamps got written, needs more delay?\"\n-            )\n-\n-        # Should be a better parent getter in the model object\n-\n-        # Prefer - Individual watch settings > Tag settings >  Global settings (in that order)\n-        n_object['notification_urls'] = self._check_cascading_vars('notification_urls', watch)\n-        n_object['notification_title'] = self._check_cascading_vars('notification_title', watch)\n-        n_object['notification_body'] = self._check_cascading_vars('notification_body', watch)\n-        n_object['notification_format'] = self._check_cascading_vars('notification_format', watch)\n-\n-        # (Individual watch) Only prepare to notify if the rules above matched\n-        queued = False\n-        if n_object and n_object.get('notification_urls'):\n-            queued = True\n-\n-            count = watch.get('notification_alert_count', 0) + 1\n-            self.datastore.update_watch(uuid=watch_uuid, update_obj={'notification_alert_count': count})\n-\n-            self.queue_notification_for_watch(notification_q=self.notification_q, n_object=n_object, watch=watch)\n-\n-        return queued\n-\n-\n-    def send_filter_failure_notification(self, watch_uuid):\n-\n-        threshold = self.datastore.data['settings']['application'].get('filter_failure_notification_threshold_attempts')\n-        watch = self.datastore.data['watching'].get(watch_uuid)\n-        if not watch:\n-            return\n-\n-        n_object = {'notification_title': 'Changedetection.io - Alert - CSS/xPath filter was not present in the page',\n-                    'notification_body': \"Your configured CSS/xPath filters of '{}' for {{{{watch_url}}}} did not appear on the page after {} attempts, did the page change layout?\\n\\nLink: {{{{base_url}}}}/edit/{{{{watch_uuid}}}}\\n\\nThanks - Your omniscient changedetection.io installation :)\\n\".format(\n-                        \", \".join(watch['include_filters']),\n-                        threshold),\n-                    'notification_format': 'text'}\n-\n-        if len(watch['notification_urls']):\n-            n_object['notification_urls'] = watch['notification_urls']\n-\n-        elif len(self.datastore.data['settings']['application']['notification_urls']):\n-            n_object['notification_urls'] = self.datastore.data['settings']['application']['notification_urls']\n-\n-        # Only prepare to notify if the rules above matched\n-        if 'notification_urls' in n_object:\n-            n_object.update({\n-                'watch_url': watch['url'],\n-                'uuid': watch_uuid,\n-                'screenshot': None\n-            })\n-            self.notification_q.put(n_object)\n-            logger.debug(f\"Sent filter not found notification for {watch_uuid}\")\n-        else:\n-            logger.debug(f\"NOT sending filter not found notification for {watch_uuid} - no notification URLs\")\n-\n-    def send_step_failure_notification(self, watch_uuid, step_n):\n-        watch = self.datastore.data['watching'].get(watch_uuid, False)\n-        if not watch:\n-            return\n-        threshold = self.datastore.data['settings']['application'].get('filter_failure_notification_threshold_attempts')\n-        n_object = {'notification_title': \"Changedetection.io - Alert - Browser step at position {} could not be run\".format(step_n+1),\n-                    'notification_body': \"Your configured browser step at position {} for {{{{watch_url}}}} \"\n-                                         \"did not appear on the page after {} attempts, did the page change layout? \"\n-                                         \"Does it need a delay added?\\n\\nLink: {{{{base_url}}}}/edit/{{{{watch_uuid}}}}\\n\\n\"\n-                                         \"Thanks - Your omniscient changedetection.io installation :)\\n\".format(step_n+1, threshold),\n-                    'notification_format': 'text'}\n-\n-        if len(watch['notification_urls']):\n-            n_object['notification_urls'] = watch['notification_urls']\n-\n-        elif len(self.datastore.data['settings']['application']['notification_urls']):\n-            n_object['notification_urls'] = self.datastore.data['settings']['application']['notification_urls']\n-\n-        # Only prepare to notify if the rules above matched\n-        if 'notification_urls' in n_object:\n-            n_object.update({\n-                'watch_url': watch['url'],\n-                'uuid': watch_uuid\n-            })\n-            self.notification_q.put(n_object)\n-            logger.error(f\"Sent step not found notification for {watch_uuid}\")\n-\n-\n-    def cleanup_error_artifacts(self, uuid):\n-        # All went fine, remove error artifacts\n-        cleanup_files = [\"last-error-screenshot.png\", \"last-error.txt\"]\n-        for f in cleanup_files:\n-            full_path = os.path.join(self.datastore.datastore_path, uuid, f)\n-            if os.path.isfile(full_path):\n-                os.unlink(full_path)\n-\n-    def run(self):\n-\n-        while not self.app.config.exit.is_set():\n-            update_handler = None\n-            watch = None\n-\n-            try:\n-                queued_item_data = self.q.get(block=False)\n-            except queue.Empty:\n-                pass\n-            else:\n-                uuid = queued_item_data.item.get('uuid')\n-                fetch_start_time = round(time.time())  # Also used for a unique history key for now\n-                self.current_uuid = uuid\n-                if uuid in list(self.datastore.data['watching'].keys()) and self.datastore.data['watching'][uuid].get('url'):\n-                    changed_detected = False\n-                    contents = b''\n-                    process_changedetection_results = True\n-                    update_obj = {}\n-\n-\n-                    # Clear last errors (move to preflight func?)\n-                    self.datastore.data['watching'][uuid]['browser_steps_last_error_step'] = None\n-                    self.datastore.data['watching'][uuid]['last_checked'] = fetch_start_time\n-\n-                    watch = self.datastore.data['watching'].get(uuid)\n-\n-                    logger.info(f\"Processing watch UUID {uuid} Priority {queued_item_data.priority} URL {watch['url']}\")\n-\n-                    try:\n-                        watch_check_update.send(watch_uuid=uuid)\n-\n-                        # Processor is what we are using for detecting the \"Change\"\n-                        processor = watch.get('processor', 'text_json_diff')\n-\n-                        # Init a new 'difference_detection_processor', first look in processors\n-                        processor_module_name = f\"changedetectionio.processors.{processor}.processor\"\n-                        try:\n-                            processor_module = importlib.import_module(processor_module_name)\n-                        except ModuleNotFoundError as e:\n-                            print(f\"Processor module '{processor}' not found.\")\n-                            raise e\n-\n-                        update_handler = processor_module.perform_site_check(datastore=self.datastore,\n-                                                                             watch_uuid=uuid\n-                                                                             )\n-\n-                        update_handler.call_browser()\n-\n-                        changed_detected, update_obj, contents = update_handler.run_changedetection(watch=watch)\n-\n-                        # Re #342\n-                        # In Python 3, all strings are sequences of Unicode characters. There is a bytes type that holds raw bytes.\n-                        # We then convert/.decode('utf-8') for the notification etc\n-#                        if not isinstance(contents, (bytes, bytearray)):\n-#                            raise Exception(\"Error - returned data from the fetch handler SHOULD be bytes\")\n-                    except PermissionError as e:\n-                        logger.critical(f\"File permission error updating file, watch: {uuid}\")\n-                        logger.critical(str(e))\n-                        process_changedetection_results = False\n-\n-                    # A generic other-exception thrown by processors\n-                    except ProcessorException as e:\n-                        if e.screenshot:\n-                            watch.save_screenshot(screenshot=e.screenshot)\n-                        if e.xpath_data:\n-                            watch.save_xpath_data(data=e.xpath_data)\n-                        self.datastore.update_watch(uuid=uuid, update_obj={'last_error': e.message})\n-                        process_changedetection_results = False\n-\n-                    except content_fetchers_exceptions.ReplyWithContentButNoText as e:\n-                        # Totally fine, it's by choice - just continue on, nothing more to care about\n-                        # Page had elements/content but no renderable text\n-                        # Backend (not filters) gave zero output\n-                        extra_help = \"\"\n-                        if e.has_filters:\n-                            # Maybe it contains an image? offer a more helpful link\n-                            has_img = html_tools.include_filters(include_filters='img',\n-                                                                 html_content=e.html_content)\n-                            if has_img:\n-                                extra_help = \", it's possible that the filters you have give an empty result or contain only an image.\"\n-                            else:\n-                                extra_help = \", it's possible that the filters were found, but contained no usable text.\"\n-\n-                        self.datastore.update_watch(uuid=uuid, update_obj={\n-                            'last_error': f\"Got HTML content but no text found (With {e.status_code} reply code){extra_help}\"\n-                        })\n-\n-                        if e.screenshot:\n-                            watch.save_screenshot(screenshot=e.screenshot, as_error=True)\n-\n-                        if e.xpath_data:\n-                            watch.save_xpath_data(data=e.xpath_data)\n-                            \n-                        process_changedetection_results = False\n-\n-                    except content_fetchers_exceptions.Non200ErrorCodeReceived as e:\n-                        if e.status_code == 403:\n-                            err_text = \"Error - 403 (Access denied) received\"\n-                        elif e.status_code == 404:\n-                            err_text = \"Error - 404 (Page not found) received\"\n-                        elif e.status_code == 407:\n-                            err_text = \"Error - 407 (Proxy authentication required) received, did you need a username and password for the proxy?\"\n-                        elif e.status_code == 500:\n-                            err_text = \"Error - 500 (Internal server error) received from the web site\"\n-                        else:\n-                            extra = ' (Access denied or blocked)' if str(e.status_code).startswith('4') else ''\n-                            err_text = f\"Error - Request returned a HTTP error code {e.status_code}{extra}\"\n-\n-                        if e.screenshot:\n-                            watch.save_screenshot(screenshot=e.screenshot, as_error=True)\n-                        if e.xpath_data:\n-                            watch.save_xpath_data(data=e.xpath_data, as_error=True)\n-                        if e.page_text:\n-                            watch.save_error_text(contents=e.page_text)\n-\n-                        self.datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text})\n-                        process_changedetection_results = False\n-\n-                    except FilterNotFoundInResponse as e:\n-                        if not self.datastore.data['watching'].get(uuid):\n-                            continue\n-\n-                        err_text = \"Warning, no filters were found, no change detection ran - Did the page change layout? update your Visual Filter if necessary.\"\n-                        self.datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text})\n-\n-                        # Filter wasnt found, but we should still update the visual selector so that they can have a chance to set it up again\n-                        if e.screenshot:\n-                            watch.save_screenshot(screenshot=e.screenshot)\n-\n-                        if e.xpath_data:\n-                            watch.save_xpath_data(data=e.xpath_data)\n-\n-                        # Only when enabled, send the notification\n-                        if watch.get('filter_failure_notification_send', False):\n-                            c = watch.get('consecutive_filter_failures', 0)\n-                            c += 1\n-                            # Send notification if we reached the threshold?\n-                            threshold = self.datastore.data['settings']['application'].get('filter_failure_notification_threshold_attempts', 0)\n-                            logger.debug(f\"Filter for {uuid} not found, consecutive_filter_failures: {c} of threshold {threshold}\")\n-                            if c >= threshold:\n-                                if not watch.get('notification_muted'):\n-                                    logger.debug(f\"Sending filter failed notification for {uuid}\")\n-                                    self.send_filter_failure_notification(uuid)\n-                                c = 0\n-                                logger.debug(f\"Reset filter failure count back to zero\")\n-\n-                            self.datastore.update_watch(uuid=uuid, update_obj={'consecutive_filter_failures': c})\n-                        else:\n-                            logger.trace(f\"{uuid} - filter_failure_notification_send not enabled, skipping\")\n-\n-\n-                        process_changedetection_results = False\n-\n-                    except content_fetchers_exceptions.checksumFromPreviousCheckWasTheSame as e:\n-                        # Yes fine, so nothing todo, don't continue to process.\n-                        process_changedetection_results = False\n-                        changed_detected = False\n-                    except content_fetchers_exceptions.BrowserConnectError as e:\n-                        self.datastore.update_watch(uuid=uuid,\n-                                                    update_obj={'last_error': e.msg\n-                                                                }\n-                                                    )\n-                        process_changedetection_results = False\n-                    except content_fetchers_exceptions.BrowserFetchTimedOut as e:\n-                        self.datastore.update_watch(uuid=uuid,\n-                                                    update_obj={'last_error': e.msg\n-                                                                }\n-                                                    )\n-                        process_changedetection_results = False\n-                    except content_fetchers_exceptions.BrowserStepsStepException as e:\n-\n-                        if not self.datastore.data['watching'].get(uuid):\n-                            continue\n-\n-                        error_step = e.step_n + 1\n-                        from playwright._impl._errors import TimeoutError, Error\n-\n-                        # Generally enough info for TimeoutError (couldnt locate the element after default seconds)\n-                        err_text = f\"Browser step at position {error_step} could not run, check the watch, add a delay if necessary, view Browser Steps to see screenshot at that step.\"\n-\n-                        if e.original_e.name == \"TimeoutError\":\n-                            # Just the first line is enough, the rest is the stack trace\n-                            err_text += \" Could not find the target.\"\n-                        else:\n-                            # Other Error, more info is good.\n-                            err_text += \" \" + str(e.original_e).splitlines()[0]\n-\n-                        logger.debug(f\"BrowserSteps exception at step {error_step} {str(e.original_e)}\")\n-\n-                        self.datastore.update_watch(uuid=uuid,\n-                                                    update_obj={'last_error': err_text,\n-                                                                'browser_steps_last_error_step': error_step\n-                                                                }\n-                                                    )\n-\n-                        if watch.get('filter_failure_notification_send', False):\n-                            c = watch.get('consecutive_filter_failures', 0)\n-                            c += 1\n-                            # Send notification if we reached the threshold?\n-                            threshold = self.datastore.data['settings']['application'].get('filter_failure_notification_threshold_attempts',\n-                                                                                           0)\n-                            logger.error(f\"Step for {uuid} not found, consecutive_filter_failures: {c}\")\n-                            if threshold > 0 and c >= threshold:\n-                                if not watch.get('notification_muted'):\n-                                    self.send_step_failure_notification(watch_uuid=uuid, step_n=e.step_n)\n-                                c = 0\n-\n-                            self.datastore.update_watch(uuid=uuid, update_obj={'consecutive_filter_failures': c})\n-\n-                        process_changedetection_results = False\n-\n-                    except content_fetchers_exceptions.EmptyReply as e:\n-                        # Some kind of custom to-str handler in the exception handler that does this?\n-                        err_text = \"EmptyReply - try increasing 'Wait seconds before extracting text', Status Code {}\".format(e.status_code)\n-                        self.datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text,\n-                                                                           'last_check_status': e.status_code})\n-                        process_changedetection_results = False\n-                    except content_fetchers_exceptions.ScreenshotUnavailable as e:\n-                        err_text = \"Screenshot unavailable, page did not render fully in the expected time or page was too long - try increasing 'Wait seconds before extracting text'\"\n-                        self.datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text,\n-                                                                           'last_check_status': e.status_code})\n-                        process_changedetection_results = False\n-                    except content_fetchers_exceptions.JSActionExceptions as e:\n-                        err_text = \"Error running JS Actions - Page request - \"+e.message\n-                        if e.screenshot:\n-                            watch.save_screenshot(screenshot=e.screenshot, as_error=True)\n-                        self.datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text,\n-                                                                           'last_check_status': e.status_code})\n-                        process_changedetection_results = False\n-                    except content_fetchers_exceptions.PageUnloadable as e:\n-                        err_text = \"Page request from server didnt respond correctly\"\n-                        if e.message:\n-                            err_text = \"{} - {}\".format(err_text, e.message)\n-\n-                        if e.screenshot:\n-                            watch.save_screenshot(screenshot=e.screenshot, as_error=True)\n-\n-                        self.datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text,\n-                                                                           'last_check_status': e.status_code,\n-                                                                           'has_ldjson_price_data': None})\n-                        process_changedetection_results = False\n-                    except content_fetchers_exceptions.BrowserStepsInUnsupportedFetcher as e:\n-                        err_text = \"This watch has Browser Steps configured and so it cannot run with the 'Basic fast Plaintext/HTTP Client', either remove the Browser Steps or select a Chrome fetcher.\"\n-                        self.datastore.update_watch(uuid=uuid, update_obj={'last_error': err_text})\n-                        process_changedetection_results = False\n-                        logger.error(f\"Exception (BrowserStepsInUnsupportedFetcher) reached processing watch UUID: {uuid}\")\n-\n-                    except Exception as e:\n-                        logger.error(f\"Exception reached processing watch UUID: {uuid}\")\n-                        logger.error(str(e))\n-                        self.datastore.update_watch(uuid=uuid, update_obj={'last_error': \"Exception: \" + str(e)})\n-                        # Other serious error\n-                        process_changedetection_results = False\n-\n-                    else:\n-                        # Crash protection, the watch entry could have been removed by this point (during a slow chrome fetch etc)\n-                        if not self.datastore.data['watching'].get(uuid):\n-                            continue\n-\n-                        update_obj['content-type'] = update_handler.fetcher.get_all_headers().get('content-type', '').lower()\n-\n-                        # Mark that we never had any failures\n-                        if not watch.get('ignore_status_codes'):\n-                            update_obj['consecutive_filter_failures'] = 0\n-\n-                        # Everything ran OK, clean off any previous error\n-                        update_obj['last_error'] = False\n-\n-                        self.cleanup_error_artifacts(uuid)\n-\n-                    if not self.datastore.data['watching'].get(uuid):\n-                        continue\n-\n-                    # Different exceptions mean that we may or may not want to bump the snapshot, trigger notifications etc\n-                    if process_changedetection_results:\n-\n-                        # Extract <title> as title if possible/requested.\n-                        if self.datastore.data['settings']['application'].get('extract_title_as_title') or watch['extract_title_as_title']:\n-                            if not watch['title'] or not len(watch['title']):\n-                                try:\n-                                    update_obj['title'] = html_tools.extract_element(find='title', html_content=update_handler.fetcher.content)\n-                                    logger.info(f\"UUID: {uuid} Extract <title> updated title to '{update_obj['title']}\")\n-                                except Exception as e:\n-                                    logger.warning(f\"UUID: {uuid} Extract <title> as watch title was enabled, but couldn't find a <title>.\")\n-\n-                        try:\n-                            self.datastore.update_watch(uuid=uuid, update_obj=update_obj)\n-\n-\n-                            # Also save the snapshot on the first time checked, \"last checked\" will always be updated, so we just check history length.\n-                            if changed_detected or not watch.history_n:\n-\n-                                if update_handler.screenshot:\n-                                    watch.save_screenshot(screenshot=update_handler.screenshot)\n-\n-                                if update_handler.xpath_data:\n-                                    watch.save_xpath_data(data=update_handler.xpath_data)\n-\n-                                # Small hack so that we sleep just enough to allow 1 second  between history snapshots\n-                                # this is because history.txt indexes/keys snapshots by epoch seconds and we dont want dupe keys\n-                                # @also - the keys are one per second at the most (for now)\n-                                if watch.newest_history_key and int(fetch_start_time) == int(watch.newest_history_key):\n-                                    logger.warning(\n-                                        f\"Timestamp {fetch_start_time} already exists, waiting 1 seconds so we have a unique key in history.txt\")\n-                                    fetch_start_time += 1\n-                                    time.sleep(1)\n-\n-                                watch.save_history_text(contents=contents,\n-                                                        timestamp=int(fetch_start_time),\n-                                                        snapshot_id=update_obj.get('previous_md5', 'none'))\n-\n-\n-                                empty_pages_are_a_change = self.datastore.data['settings']['application'].get('empty_pages_are_a_change', False)\n-                                if update_handler.fetcher.content or (not update_handler.fetcher.content and empty_pages_are_a_change):\n-                                    # attribute .last_changed is then based on this data\n-                                    watch.save_last_fetched_html(contents=update_handler.fetcher.content, timestamp=int(fetch_start_time))\n-\n-                                # Notifications should only trigger on the second time (first time, we gather the initial snapshot)\n-                                if watch.history_n >= 2:\n-                                    logger.info(f\"Change detected in UUID {uuid} - {watch['url']}\")\n-                                    if not watch.get('notification_muted'):\n-                                        # @todo only run this if notifications exist\n-                                        self.send_content_changed_notification(watch_uuid=uuid)\n-\n-                        except Exception as e:\n-                            # Catch everything possible here, so that if a worker crashes, we don't lose it until restart!\n-                            logger.critical(\"!!!! Exception in update_worker while processing process_changedetection_results !!!\")\n-                            logger.critical(str(e))\n-                            self.datastore.update_watch(uuid=uuid, update_obj={'last_error': str(e)})\n-\n-\n-                    # Always record that we atleast tried\n-                    count = watch.get('check_count', 0) + 1\n-\n-                    # Record the 'server' header reply, can be used for actions in the future like cloudflare/akamai workarounds\n-                    try:\n-                        server_header = update_handler.fetcher.headers.get('server', '').strip().lower()[:255]\n-                        self.datastore.update_watch(uuid=uuid,\n-                                                    update_obj={'remote_server_reply': server_header}\n-                                                    )\n-                    except Exception as e:\n-                        pass\n-\n-                    self.datastore.update_watch(uuid=uuid, update_obj={'fetch_time': round(time.time() - fetch_start_time, 3),\n-                                                                       'check_count': count\n-                                                                       })\n-\n-                self.current_uuid = None  # Done\n-                self.q.task_done()\n-\n-                # Send signal for watch check completion with the watch data\n-                if watch:\n-                    logger.info(f\"Sending watch_check_update signal for UUID {watch['uuid']}\")\n-                    watch_check_update.send(watch_uuid=watch['uuid'])\n-\n-                update_handler = None\n-                logger.debug(f\"Watch {uuid} done in {time.time()-fetch_start_time:.2f}s\")\n-\n-\n-                # Give the CPU time to interrupt\n-                time.sleep(0.1)\n-\n-            self.app.config.exit.wait(1)\ndiff --git a/changedetectionio/worker_handler.py b/changedetectionio/worker_handler.py\nnew file mode 100644\nindex 00000000000..953d23544c1\n--- /dev/null\n+++ b/changedetectionio/worker_handler.py\n@@ -0,0 +1,395 @@\n+\"\"\"\n+Worker management module for changedetection.io\n+\n+Handles asynchronous workers for dynamic worker scaling.\n+Sync worker support has been removed in favor of async-only architecture.\n+\"\"\"\n+\n+import asyncio\n+import os\n+import threading\n+import time\n+from loguru import logger\n+\n+# Global worker state\n+running_async_tasks = []\n+async_loop = None\n+async_loop_thread = None\n+\n+# Track currently processing UUIDs for async workers\n+currently_processing_uuids = set()\n+\n+# Configuration - async workers only\n+USE_ASYNC_WORKERS = True\n+\n+\n+def start_async_event_loop():\n+    \"\"\"Start a dedicated event loop for async workers in a separate thread\"\"\"\n+    global async_loop\n+    logger.info(\"Starting async event loop for workers\")\n+    \n+    try:\n+        # Create a new event loop for this thread\n+        async_loop = asyncio.new_event_loop()\n+        # Set it as the event loop for this thread\n+        asyncio.set_event_loop(async_loop)\n+        \n+        logger.debug(f\"Event loop created and set: {async_loop}\")\n+        \n+        # Run the event loop forever\n+        async_loop.run_forever()\n+    except Exception as e:\n+        logger.error(f\"Async event loop error: {e}\")\n+    finally:\n+        # Clean up\n+        if async_loop and not async_loop.is_closed():\n+            async_loop.close()\n+        async_loop = None\n+        logger.info(\"Async event loop stopped\")\n+\n+\n+def start_async_workers(n_workers, update_q, notification_q, app, datastore):\n+    \"\"\"Start the async worker management system\"\"\"\n+    global async_loop_thread, async_loop, running_async_tasks, currently_processing_uuids\n+    \n+    # Clear any stale UUID tracking state\n+    currently_processing_uuids.clear()\n+    \n+    # Start the event loop in a separate thread\n+    async_loop_thread = threading.Thread(target=start_async_event_loop, daemon=True)\n+    async_loop_thread.start()\n+    \n+    # Wait for the loop to be available (with timeout for safety)\n+    max_wait_time = 5.0\n+    wait_start = time.time()\n+    while async_loop is None and (time.time() - wait_start) < max_wait_time:\n+        time.sleep(0.1)\n+    \n+    if async_loop is None:\n+        logger.error(\"Failed to start async event loop within timeout\")\n+        return\n+    \n+    # Additional brief wait to ensure loop is running\n+    time.sleep(0.2)\n+    \n+    # Start async workers\n+    logger.info(f\"Starting {n_workers} async workers\")\n+    for i in range(n_workers):\n+        try:\n+            # Use a factory function to create named worker coroutines\n+            def create_named_worker(worker_id):\n+                async def named_worker():\n+                    task = asyncio.current_task()\n+                    if task:\n+                        task.set_name(f\"async-worker-{worker_id}\")\n+                    return await start_single_async_worker(worker_id, update_q, notification_q, app, datastore)\n+                return named_worker()\n+            \n+            task_future = asyncio.run_coroutine_threadsafe(create_named_worker(i), async_loop)\n+            running_async_tasks.append(task_future)\n+        except RuntimeError as e:\n+            logger.error(f\"Failed to start async worker {i}: {e}\")\n+            continue\n+\n+\n+async def start_single_async_worker(worker_id, update_q, notification_q, app, datastore):\n+    \"\"\"Start a single async worker with auto-restart capability\"\"\"\n+    from changedetectionio.async_update_worker import async_update_worker\n+    \n+    # Check if we're in pytest environment - if so, be more gentle with logging\n+    import os\n+    in_pytest = \"pytest\" in os.sys.modules or \"PYTEST_CURRENT_TEST\" in os.environ\n+    \n+    while not app.config.exit.is_set():\n+        try:\n+            if not in_pytest:\n+                logger.info(f\"Starting async worker {worker_id}\")\n+            await async_update_worker(worker_id, update_q, notification_q, app, datastore)\n+            # If we reach here, worker exited cleanly\n+            if not in_pytest:\n+                logger.info(f\"Async worker {worker_id} exited cleanly\")\n+            break\n+        except asyncio.CancelledError:\n+            # Task was cancelled (normal shutdown)\n+            if not in_pytest:\n+                logger.info(f\"Async worker {worker_id} cancelled\")\n+            break\n+        except Exception as e:\n+            logger.error(f\"Async worker {worker_id} crashed: {e}\")\n+            if not in_pytest:\n+                logger.info(f\"Restarting async worker {worker_id} in 5 seconds...\")\n+            await asyncio.sleep(5)\n+    \n+    if not in_pytest:\n+        logger.info(f\"Async worker {worker_id} shutdown complete\")\n+\n+\n+def start_workers(n_workers, update_q, notification_q, app, datastore):\n+    \"\"\"Start async workers - sync workers are deprecated\"\"\"\n+    start_async_workers(n_workers, update_q, notification_q, app, datastore)\n+\n+\n+def add_worker(update_q, notification_q, app, datastore):\n+    \"\"\"Add a new async worker (for dynamic scaling)\"\"\"\n+    global running_async_tasks\n+    \n+    if not async_loop:\n+        logger.error(\"Async loop not running, cannot add worker\")\n+        return False\n+        \n+    worker_id = len(running_async_tasks)\n+    logger.info(f\"Adding async worker {worker_id}\")\n+    \n+    task_future = asyncio.run_coroutine_threadsafe(\n+        start_single_async_worker(worker_id, update_q, notification_q, app, datastore), async_loop\n+    )\n+    running_async_tasks.append(task_future)\n+    return True\n+\n+\n+def remove_worker():\n+    \"\"\"Remove an async worker (for dynamic scaling)\"\"\"\n+    global running_async_tasks\n+    \n+    if not running_async_tasks:\n+        return False\n+        \n+    # Cancel the last worker\n+    task_future = running_async_tasks.pop()\n+    task_future.cancel()\n+    logger.info(f\"Removed async worker, {len(running_async_tasks)} workers remaining\")\n+    return True\n+\n+\n+def get_worker_count():\n+    \"\"\"Get current number of async workers\"\"\"\n+    return len(running_async_tasks)\n+\n+\n+def get_running_uuids():\n+    \"\"\"Get list of UUIDs currently being processed by async workers\"\"\"\n+    return list(currently_processing_uuids)\n+\n+\n+def set_uuid_processing(uuid, processing=True):\n+    \"\"\"Mark a UUID as being processed or completed\"\"\"\n+    global currently_processing_uuids\n+    if processing:\n+        currently_processing_uuids.add(uuid)\n+        logger.debug(f\"Started processing UUID: {uuid}\")\n+    else:\n+        currently_processing_uuids.discard(uuid)\n+        logger.debug(f\"Finished processing UUID: {uuid}\")\n+\n+\n+def is_watch_running(watch_uuid):\n+    \"\"\"Check if a specific watch is currently being processed\"\"\"\n+    return watch_uuid in get_running_uuids()\n+\n+\n+def queue_item_async_safe(update_q, item):\n+    \"\"\"Queue an item for async queue processing\"\"\"\n+    if async_loop and not async_loop.is_closed():\n+        try:\n+            # For async queue, schedule the put operation\n+            asyncio.run_coroutine_threadsafe(update_q.put(item), async_loop)\n+        except RuntimeError as e:\n+            logger.error(f\"Failed to queue item: {e}\")\n+    else:\n+        logger.error(\"Async loop not available or closed for queueing item\")\n+\n+\n+def shutdown_workers():\n+    \"\"\"Shutdown all async workers fast and aggressively\"\"\"\n+    global async_loop, async_loop_thread, running_async_tasks\n+    \n+    # Check if we're in pytest environment - if so, be more gentle with logging\n+    import os\n+    in_pytest = \"pytest\" in os.sys.modules or \"PYTEST_CURRENT_TEST\" in os.environ\n+    \n+    if not in_pytest:\n+        logger.info(\"Fast shutdown of async workers initiated...\")\n+    \n+    # Cancel all async tasks immediately\n+    for task_future in running_async_tasks:\n+        if not task_future.done():\n+            task_future.cancel()\n+    \n+    # Stop the async event loop immediately\n+    if async_loop and not async_loop.is_closed():\n+        try:\n+            async_loop.call_soon_threadsafe(async_loop.stop)\n+        except RuntimeError:\n+            # Loop might already be stopped\n+            pass\n+        \n+    running_async_tasks.clear()\n+    async_loop = None\n+        \n+    # Give async thread minimal time to finish, then continue\n+    if async_loop_thread and async_loop_thread.is_alive():\n+        async_loop_thread.join(timeout=1.0)  # Only 1 second timeout\n+        if async_loop_thread.is_alive() and not in_pytest:\n+            logger.info(\"Async thread still running after timeout - continuing with shutdown\")\n+        async_loop_thread = None\n+    \n+    if not in_pytest:\n+        logger.info(\"Async workers fast shutdown complete\")\n+\n+\n+\n+\n+def adjust_async_worker_count(new_count, update_q=None, notification_q=None, app=None, datastore=None):\n+    \"\"\"\n+    Dynamically adjust the number of async workers.\n+    \n+    Args:\n+        new_count: Target number of workers\n+        update_q, notification_q, app, datastore: Required for adding new workers\n+    \n+    Returns:\n+        dict: Status of the adjustment operation\n+    \"\"\"\n+    global running_async_tasks\n+    \n+    current_count = get_worker_count()\n+    \n+    if new_count == current_count:\n+        return {\n+            'status': 'no_change',\n+            'message': f'Worker count already at {current_count}',\n+            'current_count': current_count\n+        }\n+    \n+    if new_count > current_count:\n+        # Add workers\n+        workers_to_add = new_count - current_count\n+        logger.info(f\"Adding {workers_to_add} async workers (from {current_count} to {new_count})\")\n+        \n+        if not all([update_q, notification_q, app, datastore]):\n+            return {\n+                'status': 'error',\n+                'message': 'Missing required parameters to add workers',\n+                'current_count': current_count\n+            }\n+        \n+        for i in range(workers_to_add):\n+            worker_id = len(running_async_tasks)\n+            task_future = asyncio.run_coroutine_threadsafe(\n+                start_single_async_worker(worker_id, update_q, notification_q, app, datastore), \n+                async_loop\n+            )\n+            running_async_tasks.append(task_future)\n+        \n+        return {\n+            'status': 'success',\n+            'message': f'Added {workers_to_add} workers',\n+            'previous_count': current_count,\n+            'current_count': new_count\n+        }\n+        \n+    else:\n+        # Remove workers\n+        workers_to_remove = current_count - new_count\n+        logger.info(f\"Removing {workers_to_remove} async workers (from {current_count} to {new_count})\")\n+        \n+        removed_count = 0\n+        for _ in range(workers_to_remove):\n+            if running_async_tasks:\n+                task_future = running_async_tasks.pop()\n+                task_future.cancel()\n+                # Wait for the task to actually stop\n+                try:\n+                    task_future.result(timeout=5)  # 5 second timeout\n+                except Exception:\n+                    pass  # Task was cancelled, which is expected\n+                removed_count += 1\n+        \n+        return {\n+            'status': 'success',\n+            'message': f'Removed {removed_count} workers',\n+            'previous_count': current_count,\n+            'current_count': current_count - removed_count\n+        }\n+\n+\n+def get_worker_status():\n+    \"\"\"Get status information about async workers\"\"\"\n+    return {\n+        'worker_type': 'async',\n+        'worker_count': get_worker_count(),\n+        'running_uuids': get_running_uuids(),\n+        'async_loop_running': async_loop is not None,\n+    }\n+\n+\n+def check_worker_health(expected_count, update_q=None, notification_q=None, app=None, datastore=None):\n+    \"\"\"\n+    Check if the expected number of async workers are running and restart any missing ones.\n+    \n+    Args:\n+        expected_count: Expected number of workers\n+        update_q, notification_q, app, datastore: Required for restarting workers\n+    \n+    Returns:\n+        dict: Health check results\n+    \"\"\"\n+    global running_async_tasks\n+    \n+    current_count = get_worker_count()\n+    \n+    if current_count == expected_count:\n+        return {\n+            'status': 'healthy',\n+            'expected_count': expected_count,\n+            'actual_count': current_count,\n+            'message': f'All {expected_count} async workers running'\n+        }\n+    \n+    # Check for crashed async workers\n+    dead_workers = []\n+    alive_count = 0\n+    \n+    for i, task_future in enumerate(running_async_tasks[:]):\n+        if task_future.done():\n+            try:\n+                result = task_future.result()\n+                dead_workers.append(i)\n+                logger.warning(f\"Async worker {i} completed unexpectedly\")\n+            except Exception as e:\n+                dead_workers.append(i)\n+                logger.error(f\"Async worker {i} crashed: {e}\")\n+        else:\n+            alive_count += 1\n+    \n+    # Remove dead workers from tracking\n+    for i in reversed(dead_workers):\n+        if i < len(running_async_tasks):\n+            running_async_tasks.pop(i)\n+    \n+    missing_workers = expected_count - alive_count\n+    restarted_count = 0\n+    \n+    if missing_workers > 0 and all([update_q, notification_q, app, datastore]):\n+        logger.info(f\"Restarting {missing_workers} crashed async workers\")\n+        \n+        for i in range(missing_workers):\n+            worker_id = alive_count + i\n+            try:\n+                task_future = asyncio.run_coroutine_threadsafe(\n+                    start_single_async_worker(worker_id, update_q, notification_q, app, datastore), \n+                    async_loop\n+                )\n+                running_async_tasks.append(task_future)\n+                restarted_count += 1\n+            except Exception as e:\n+                logger.error(f\"Failed to restart worker {worker_id}: {e}\")\n+    \n+    return {\n+        'status': 'repaired' if restarted_count > 0 else 'degraded',\n+        'expected_count': expected_count,\n+        'actual_count': alive_count,\n+        'dead_workers': len(dead_workers),\n+        'restarted_workers': restarted_count,\n+        'message': f'Found {len(dead_workers)} dead workers, restarted {restarted_count}'\n+    }\n\\ No newline at end of file\ndiff --git a/requirements.txt b/requirements.txt\nindex 79c113beef7..2e90ccc37d9 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,4 +1,4 @@\n-eventlet>=0.38.0\n+# eventlet>=0.38.0  # Removed - replaced with threading mode for better Python 3.12+ compatibility\n feedgen~=0.9\n flask-compress\n # 0.6.3 included compatibility fix for werkzeug 3.x (2.x had deprecation of url handlers)\n@@ -9,9 +9,9 @@ flask_restful\n flask_cors # For the Chrome extension to operate\n flask_wtf~=1.2\n flask~=2.3\n-flask-socketio>=5.5.1\n-python-socketio>=5.13.0\n-python-engineio>=4.12.0\n+flask-socketio~=5.5.1\n+python-socketio~=5.13.0\n+python-engineio~=4.12.0\n inscriptis~=2.2\n pytz\n timeago~=1.0\n@@ -24,13 +24,16 @@ brotli~=1.0\n requests[socks]\n requests-file\n \n-urllib3==1.26.19\n+# urllib3==1.26.19  # Unpinned - let requests decide compatible version\n+# If specific version needed for security, use urllib3>=1.26.19,<3.0\n chardet>2.3.0\n \n wtforms~=3.0\n jsonpath-ng~=1.5.3\n \n-dnspython==2.6.1 # related to eventlet fixes\n+# dnspython - Used by paho-mqtt for MQTT broker resolution  \n+# Version pin removed since eventlet (which required the specific 2.6.1 pin) has been eliminated\n+# paho-mqtt will install compatible dnspython version automatically\n \n # jq not available on Windows so must be installed manually\n \n@@ -53,7 +56,8 @@ beautifulsoup4>=4.0.0\n #         https://bugs.launchpad.net/lxml/+bug/2059910/comments/16\n lxml >=4.8.0,<6,!=5.2.0,!=5.2.1\n \n-# XPath 2.0-3.1 support - 4.2.0 broke something?\n+# XPath 2.0-3.1 support - 4.2.0 had issues, 4.1.5 stable\n+# Consider updating to latest stable version periodically\n elementpath==4.1.5\n \n selenium~=4.31.0\n@@ -98,7 +102,9 @@ levenshtein\n # Needed for > 3.10, https://github.com/microsoft/playwright-python/issues/2096\n greenlet >= 3.0.3\n \n-# Used for realtime socketio mode (so its a different driver to eventlet/threading not to interfere with playwright)\n+# Optional: Used for high-concurrency SocketIO mode (via SOCKETIO_MODE=gevent)\n+# Note: gevent has cross-platform limitations (Windows 1024 socket limit, macOS ARM build issues)\n+# Default SOCKETIO_MODE=threading is recommended for better compatibility\n gevent\n \n # Pinned or it causes problems with flask_expects_json which seems unmaintained\n", "test_patch": "diff --git a/.github/workflows/test-stack-reusable-workflow.yml b/.github/workflows/test-stack-reusable-workflow.yml\nindex 8f3b9301bbe..af3d0fe2ec0 100644\n--- a/.github/workflows/test-stack-reusable-workflow.yml\n+++ b/.github/workflows/test-stack-reusable-workflow.yml\n@@ -86,10 +86,10 @@ jobs:\n         run: |\n           # Playwright via Sockpuppetbrowser fetch\n           # tests/visualselector/test_fetch_data.py will do browser steps  \n-          docker run --rm -e \"FLASK_SERVER_NAME=cdio\" -e \"PLAYWRIGHT_DRIVER_URL=ws://sockpuppetbrowser:3000\" --network changedet-network --hostname=cdio test-changedetectionio  bash -c 'cd changedetectionio;pytest --live-server-host=0.0.0.0 --live-server-port=5004 tests/fetchers/test_content.py'\n-          docker run --rm -e \"FLASK_SERVER_NAME=cdio\" -e \"PLAYWRIGHT_DRIVER_URL=ws://sockpuppetbrowser:3000\" --network changedet-network --hostname=cdio test-changedetectionio  bash -c 'cd changedetectionio;pytest --live-server-host=0.0.0.0 --live-server-port=5004 tests/test_errorhandling.py'\n-          docker run --rm -e \"FLASK_SERVER_NAME=cdio\" -e \"PLAYWRIGHT_DRIVER_URL=ws://sockpuppetbrowser:3000\" --network changedet-network --hostname=cdio test-changedetectionio  bash -c 'cd changedetectionio;pytest --live-server-host=0.0.0.0 --live-server-port=5004 tests/visualselector/test_fetch_data.py'\n-          docker run --rm -e \"FLASK_SERVER_NAME=cdio\" -e \"PLAYWRIGHT_DRIVER_URL=ws://sockpuppetbrowser:3000\" --network changedet-network --hostname=cdio test-changedetectionio  bash -c 'cd changedetectionio;pytest --live-server-host=0.0.0.0 --live-server-port=5004 tests/fetchers/test_custom_js_before_content.py'\n+          docker run --rm -e \"FLASK_SERVER_NAME=cdio\" -e \"PLAYWRIGHT_DRIVER_URL=ws://sockpuppetbrowser:3000\" --network changedet-network --hostname=cdio test-changedetectionio  bash -c 'cd changedetectionio;pytest  -vv --capture=tee-sys --showlocals --tb=long --live-server-host=0.0.0.0 --live-server-port=5004 tests/fetchers/test_content.py'\n+          docker run --rm -e \"FLASK_SERVER_NAME=cdio\" -e \"PLAYWRIGHT_DRIVER_URL=ws://sockpuppetbrowser:3000\" --network changedet-network --hostname=cdio test-changedetectionio  bash -c 'cd changedetectionio;pytest  -vv --capture=tee-sys --showlocals --tb=long --live-server-host=0.0.0.0 --live-server-port=5004 tests/test_errorhandling.py'\n+          docker run --rm -e \"FLASK_SERVER_NAME=cdio\" -e \"PLAYWRIGHT_DRIVER_URL=ws://sockpuppetbrowser:3000\" --network changedet-network --hostname=cdio test-changedetectionio  bash -c 'cd changedetectionio;pytest  -vv --capture=tee-sys --showlocals --tb=long --live-server-host=0.0.0.0 --live-server-port=5004 tests/visualselector/test_fetch_data.py'\n+          docker run --rm -e \"FLASK_SERVER_NAME=cdio\" -e \"PLAYWRIGHT_DRIVER_URL=ws://sockpuppetbrowser:3000\" --network changedet-network --hostname=cdio test-changedetectionio  bash -c 'cd changedetectionio;pytest  -vv --capture=tee-sys --showlocals --tb=long --live-server-host=0.0.0.0 --live-server-port=5004 tests/fetchers/test_custom_js_before_content.py'\n \n \n       - name: Playwright and SocketPuppetBrowser - Headers and requests\ndiff --git a/changedetectionio/tests/conftest.py b/changedetectionio/tests/conftest.py\nindex c1195bcb6b1..3e7069caaca 100644\n--- a/changedetectionio/tests/conftest.py\n+++ b/changedetectionio/tests/conftest.py\n@@ -10,6 +10,8 @@\n import sys\n from loguru import logger\n \n+from changedetectionio.tests.util import live_server_setup, new_live_server_setup\n+\n # https://github.com/pallets/flask/blob/1.1.2/examples/tutorial/tests/test_auth.py\n # Much better boilerplate than the docs\n # https://www.python-boilerplate.com/py3+flask+pytest/\n@@ -70,6 +72,22 @@ def cleanup(datastore_path):\n             if os.path.isfile(f):\n                 os.unlink(f)\n \n+@pytest.fixture(scope='function', autouse=True)\n+def prepare_test_function(live_server):\n+\n+    routes = [rule.rule for rule in live_server.app.url_map.iter_rules()]\n+    if '/test-random-content-endpoint' not in routes:\n+        logger.debug(\"Setting up test URL routes\")\n+        new_live_server_setup(live_server)\n+\n+\n+    yield\n+    # Then cleanup/shutdown\n+    live_server.app.config['DATASTORE'].data['watching']={}\n+    time.sleep(0.3)\n+    live_server.app.config['DATASTORE'].data['watching']={}\n+\n+\n @pytest.fixture(scope='session')\n def app(request):\n     \"\"\"Create application for the tests.\"\"\"\n@@ -106,8 +124,33 @@ def app(request):\n     app.config['STOP_THREADS'] = True\n \n     def teardown():\n+        # Stop all threads and services\n         datastore.stop_thread = True\n         app.config.exit.set()\n+        \n+        # Shutdown workers gracefully before loguru cleanup\n+        try:\n+            from changedetectionio import worker_handler\n+            worker_handler.shutdown_workers()\n+        except Exception:\n+            pass\n+            \n+        # Stop socket server threads\n+        try:\n+            from changedetectionio.flask_app import socketio_server\n+            if socketio_server and hasattr(socketio_server, 'shutdown'):\n+                socketio_server.shutdown()\n+        except Exception:\n+            pass\n+        \n+        # Give threads a moment to finish their shutdown\n+        import time\n+        time.sleep(0.1)\n+        \n+        # Remove all loguru handlers to prevent \"closed file\" errors\n+        logger.remove()\n+        \n+        # Cleanup files\n         cleanup(app_config['datastore_path'])\n \n        \ndiff --git a/changedetectionio/tests/custom_browser_url/test_custom_browser_url.py b/changedetectionio/tests/custom_browser_url/test_custom_browser_url.py\nindex efc6e127dbb..6ec4205ee96 100644\n--- a/changedetectionio/tests/custom_browser_url/test_custom_browser_url.py\n+++ b/changedetectionio/tests/custom_browser_url/test_custom_browser_url.py\n@@ -78,12 +78,12 @@ def do_test(client, live_server, make_test_use_extra_browser=False):\n \n # Requires playwright to be installed\n def test_request_via_custom_browser_url(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     # We do this so we can grep the logs of the custom container and see if the request actually went through that container\n     do_test(client, live_server, make_test_use_extra_browser=True)\n \n \n def test_request_not_via_custom_browser_url(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     # We do this so we can grep the logs of the custom container and see if the request actually went through that container\n     do_test(client, live_server, make_test_use_extra_browser=False)\ndiff --git a/changedetectionio/tests/fetchers/test_content.py b/changedetectionio/tests/fetchers/test_content.py\nindex dc02f50c760..e09781c5553 100644\n--- a/changedetectionio/tests/fetchers/test_content.py\n+++ b/changedetectionio/tests/fetchers/test_content.py\n@@ -7,7 +7,7 @@\n \n # Requires playwright to be installed\n def test_fetch_webdriver_content(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     #####################\n     res = client.post(\ndiff --git a/changedetectionio/tests/fetchers/test_custom_js_before_content.py b/changedetectionio/tests/fetchers/test_custom_js_before_content.py\nindex e145a79ea1f..cb4d6286ca0 100644\n--- a/changedetectionio/tests/fetchers/test_custom_js_before_content.py\n+++ b/changedetectionio/tests/fetchers/test_custom_js_before_content.py\n@@ -5,7 +5,7 @@\n \n def test_execute_custom_js(client, live_server, measure_memory_usage):\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     assert os.getenv('PLAYWRIGHT_DRIVER_URL'), \"Needs PLAYWRIGHT_DRIVER_URL set for this test\"\n \n     test_url = url_for('test_interactive_html_endpoint', _external=True)\ndiff --git a/changedetectionio/tests/proxy_list/test_multiple_proxy.py b/changedetectionio/tests/proxy_list/test_multiple_proxy.py\nindex f1818e3a338..cc0da45f4b7 100644\n--- a/changedetectionio/tests/proxy_list/test_multiple_proxy.py\n+++ b/changedetectionio/tests/proxy_list/test_multiple_proxy.py\n@@ -6,7 +6,7 @@\n \n \n def test_preferred_proxy(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     url = \"http://chosen.changedetection.io\"\n \n \ndiff --git a/changedetectionio/tests/proxy_list/test_noproxy.py b/changedetectionio/tests/proxy_list/test_noproxy.py\nindex ffae929b679..fdd9aa35421 100644\n--- a/changedetectionio/tests/proxy_list/test_noproxy.py\n+++ b/changedetectionio/tests/proxy_list/test_noproxy.py\n@@ -6,7 +6,7 @@\n \n \n def test_noproxy_option(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     # Run by run_proxy_tests.sh\n     # Call this URL then scan the containers that it never went through them\n     url = \"http://noproxy.changedetection.io\"\ndiff --git a/changedetectionio/tests/proxy_list/test_proxy.py b/changedetectionio/tests/proxy_list/test_proxy.py\nindex 726d0c82ba1..bda17d1b6a5 100644\n--- a/changedetectionio/tests/proxy_list/test_proxy.py\n+++ b/changedetectionio/tests/proxy_list/test_proxy.py\n@@ -6,7 +6,7 @@\n \n # just make a request, we will grep in the docker logs to see it actually got called\n def test_check_basic_change_detection_functionality(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     res = client.post(\n         url_for(\"imports.import_page\"),\n         # Because a URL wont show in squid/proxy logs due it being SSLed\ndiff --git a/changedetectionio/tests/proxy_list/test_proxy_noconnect.py b/changedetectionio/tests/proxy_list/test_proxy_noconnect.py\nindex 72f3e512c74..31edaadb23d 100644\n--- a/changedetectionio/tests/proxy_list/test_proxy_noconnect.py\n+++ b/changedetectionio/tests/proxy_list/test_proxy_noconnect.py\n@@ -13,7 +13,7 @@\n # WEBDRIVER_URL=http://127.0.0.1:4444/wd/hub pytest tests/proxy_list/test_proxy_noconnect.py\n \n def test_proxy_noconnect_custom(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     # Goto settings, add our custom one\n     res = client.post(\ndiff --git a/changedetectionio/tests/proxy_list/test_select_custom_proxy.py b/changedetectionio/tests/proxy_list/test_select_custom_proxy.py\nindex e35c3718197..ab831358754 100644\n--- a/changedetectionio/tests/proxy_list/test_select_custom_proxy.py\n+++ b/changedetectionio/tests/proxy_list/test_select_custom_proxy.py\n@@ -7,7 +7,7 @@\n \n # just make a request, we will grep in the docker logs to see it actually got called\n def test_select_custom(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     # Goto settings, add our custom one\n     res = client.post(\ndiff --git a/changedetectionio/tests/proxy_socks5/test_socks5_proxy.py b/changedetectionio/tests/proxy_socks5/test_socks5_proxy.py\nindex 3d0271a8478..f2595bd013a 100644\n--- a/changedetectionio/tests/proxy_socks5/test_socks5_proxy.py\n+++ b/changedetectionio/tests/proxy_socks5/test_socks5_proxy.py\n@@ -20,7 +20,7 @@ def set_response():\n     time.sleep(1)\n \n def test_socks5(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     set_response()\n \n     # Setup a proxy\ndiff --git a/changedetectionio/tests/proxy_socks5/test_socks5_proxy_sources.py b/changedetectionio/tests/proxy_socks5/test_socks5_proxy_sources.py\nindex 040248431ef..3805c88d128 100644\n--- a/changedetectionio/tests/proxy_socks5/test_socks5_proxy_sources.py\n+++ b/changedetectionio/tests/proxy_socks5/test_socks5_proxy_sources.py\n@@ -21,7 +21,7 @@ def set_response():\n # should be proxies.json mounted from run_proxy_tests.sh already\n # -v `pwd`/tests/proxy_socks5/proxies.json-example:/app/changedetectionio/test-datastore/proxies.json\n def test_socks5_from_proxiesjson_file(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     set_response()\n     # Because the socks server should connect back to us\n     test_url = url_for('test_endpoint', _external=True) + f\"?socks-test-tag={os.getenv('SOCKSTEST', '')}\"\ndiff --git a/changedetectionio/tests/restock/test_restock.py b/changedetectionio/tests/restock/test_restock.py\nindex 1d1accec77a..ecee00fabc3 100644\n--- a/changedetectionio/tests/restock/test_restock.py\n+++ b/changedetectionio/tests/restock/test_restock.py\n@@ -54,7 +54,7 @@ def test_restock_detection(client, live_server, measure_memory_usage):\n \n     set_original_response()\n     #assert os.getenv('PLAYWRIGHT_DRIVER_URL'), \"Needs PLAYWRIGHT_DRIVER_URL set for this test\"\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     #####################\n     notification_url = url_for('test_notification_endpoint', _external=True).replace('http://localhost', 'http://changedet').replace('http', 'json')\n \ndiff --git a/changedetectionio/tests/smtp/test_notification_smtp.py b/changedetectionio/tests/smtp/test_notification_smtp.py\nindex ea17a2b57d9..a400901c4cb 100644\n--- a/changedetectionio/tests/smtp/test_notification_smtp.py\n+++ b/changedetectionio/tests/smtp/test_notification_smtp.py\n@@ -20,8 +20,7 @@\n     valid_notification_formats,\n )\n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n def get_last_message_from_smtp_server():\n     import socket\n@@ -40,7 +39,7 @@ def get_last_message_from_smtp_server():\n # Requires running the test SMTP server\n \n def test_check_notification_email_formats_default_HTML(client, live_server, measure_memory_usage):\n-    # live_server_setup(live_server)\n+    ##  live_server_setup(live_server) # Setup on conftest per function\n     set_original_response()\n \n     notification_url = f'mailto://changedetection@{smtp_test_server}:11025/?to=fff@home.com'\n@@ -91,7 +90,7 @@ def test_check_notification_email_formats_default_HTML(client, live_server, meas\n \n \n def test_check_notification_email_formats_default_Text_override_HTML(client, live_server, measure_memory_usage):\n-    # live_server_setup(live_server)\n+    ##  live_server_setup(live_server) # Setup on conftest per function\n \n     # HTML problems? see this\n     # https://github.com/caronc/apprise/issues/633\ndiff --git a/changedetectionio/tests/test_access_control.py b/changedetectionio/tests/test_access_control.py\nindex b35de26842b..a72e64a9a0a 100644\n--- a/changedetectionio/tests/test_access_control.py\n+++ b/changedetectionio/tests/test_access_control.py\n@@ -4,7 +4,7 @@\n \n def test_check_access_control(app, client, live_server):\n     # Still doesnt work, but this is closer.\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     with app.test_client(use_cookies=True) as c:\n         # Check we don't have any password protection enabled yet.\ndiff --git a/changedetectionio/tests/test_add_replace_remove_filter.py b/changedetectionio/tests/test_add_replace_remove_filter.py\nindex 62074e42405..3ca5284e47f 100644\n--- a/changedetectionio/tests/test_add_replace_remove_filter.py\n+++ b/changedetectionio/tests/test_add_replace_remove_filter.py\n@@ -4,7 +4,7 @@\n \n from flask import url_for\n from .util import live_server_setup, wait_for_all_checks, wait_for_notification_endpoint_output\n-\n+import time\n \n def set_original(excluding=None, add_line=None):\n     test_return_data = \"\"\"<html>\n@@ -35,11 +35,11 @@ def set_original(excluding=None, add_line=None):\n     with open(\"test-datastore/endpoint-content.txt\", \"w\") as f:\n         f.write(test_return_data)\n \n-def test_setup(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server, measure_memory_usage):\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n def test_check_removed_line_contains_trigger(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+\n     # Give the endpoint time to spin up\n     set_original()\n     # Add our URL to the import page\n@@ -72,6 +72,7 @@ def test_check_removed_line_contains_trigger(client, live_server, measure_memory\n     res = client.get(url_for(\"ui.form_watch_checknow\"), follow_redirects=True)\n     assert b'Queued 1 watch for rechecking.' in res.data\n     wait_for_all_checks(client)\n+    time.sleep(0.5)\n     res = client.get(url_for(\"watchlist.index\"))\n     assert b'unviewed' not in res.data\n \n@@ -84,12 +85,17 @@ def test_check_removed_line_contains_trigger(client, live_server, measure_memory\n     res = client.get(url_for(\"watchlist.index\"))\n     assert b'unviewed' in res.data\n \n+    time.sleep(1)\n \n     # Now add it back, and we should not get a trigger\n     client.get(url_for(\"ui.mark_all_viewed\"), follow_redirects=True)\n+    time.sleep(0.2)\n+\n+    time.sleep(1)\n     set_original(excluding=None)\n     client.get(url_for(\"ui.form_watch_checknow\"), follow_redirects=True)\n     wait_for_all_checks(client)\n+    time.sleep(1)\n     res = client.get(url_for(\"watchlist.index\"))\n     assert b'unviewed' not in res.data\n \n@@ -105,7 +111,10 @@ def test_check_removed_line_contains_trigger(client, live_server, measure_memory\n \n \n def test_check_add_line_contains_trigger(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n+    res = client.get(url_for(\"ui.form_delete\", uuid=\"all\"), follow_redirects=True)\n+    assert b'Deleted' in res.data\n+    time.sleep(1)\n \n     # Give the endpoint time to spin up\n     test_notification_url = url_for('test_notification_endpoint', _external=True).replace('http://', 'post://') + \"?xxx={{ watch_url }}\"\ndiff --git a/changedetectionio/tests/test_api.py b/changedetectionio/tests/test_api.py\nindex 7703f42ca27..2cd87e5bc15 100644\n--- a/changedetectionio/tests/test_api.py\n+++ b/changedetectionio/tests/test_api.py\n@@ -52,12 +52,12 @@ def is_valid_uuid(val):\n         return False\n \n \n-def test_setup(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server, measure_memory_usage):\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n \n def test_api_simple(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     api_key = live_server.app.config['DATASTORE'].data['settings']['application'].get('api_access_token')\n \n@@ -108,7 +108,7 @@ def test_api_simple(client, live_server, measure_memory_usage):\n         headers={'x-api-key': api_key}\n     )\n     assert len(res.json) == 0\n-\n+    time.sleep(1)\n     wait_for_all_checks(client)\n \n     set_modified_response()\n@@ -119,6 +119,7 @@ def test_api_simple(client, live_server, measure_memory_usage):\n     )\n     wait_for_all_checks(client)\n \n+    time.sleep(1)\n     # Did the recheck fire?\n     res = client.get(\n         url_for(\"createwatch\"),\n@@ -291,7 +292,7 @@ def test_access_denied(client, live_server, measure_memory_usage):\n \n def test_api_watch_PUT_update(client, live_server, measure_memory_usage):\n \n-    #live_server_setup(live_server)\n+    \n     api_key = live_server.app.config['DATASTORE'].data['settings']['application'].get('api_access_token')\n \n     # Create a watch\n@@ -371,7 +372,7 @@ def test_api_watch_PUT_update(client, live_server, measure_memory_usage):\n \n \n def test_api_import(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     api_key = live_server.app.config['DATASTORE'].data['settings']['application'].get('api_access_token')\n \n     res = client.post(\n@@ -393,7 +394,7 @@ def test_api_import(client, live_server, measure_memory_usage):\n \n def test_api_conflict_UI_password(client, live_server, measure_memory_usage):\n \n-    #live_server_setup(live_server)\n+    \n     api_key = live_server.app.config['DATASTORE'].data['settings']['application'].get('api_access_token')\n \n     # Enable password check and diff page access bypass\ndiff --git a/changedetectionio/tests/test_api_notifications.py b/changedetectionio/tests/test_api_notifications.py\nindex 9a030e66b71..d8bad0aa93c 100644\n--- a/changedetectionio/tests/test_api_notifications.py\n+++ b/changedetectionio/tests/test_api_notifications.py\n@@ -5,7 +5,7 @@\n import json\n \n def test_api_notifications_crud(client, live_server):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     api_key = live_server.app.config['DATASTORE'].data['settings']['application'].get('api_access_token')\n \n     # Confirm notifications are initially empty\ndiff --git a/changedetectionio/tests/test_api_search.py b/changedetectionio/tests/test_api_search.py\nindex 3369905ef8f..7f7dd6a389e 100644\n--- a/changedetectionio/tests/test_api_search.py\n+++ b/changedetectionio/tests/test_api_search.py\n@@ -7,7 +7,7 @@\n \n \n def test_api_search(client, live_server):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     api_key = live_server.app.config['DATASTORE'].data['settings']['application'].get('api_access_token')\n \n     watch_data = {}\ndiff --git a/changedetectionio/tests/test_api_tags.py b/changedetectionio/tests/test_api_tags.py\nindex 55131d6dbd5..831d052e8be 100644\n--- a/changedetectionio/tests/test_api_tags.py\n+++ b/changedetectionio/tests/test_api_tags.py\n@@ -5,7 +5,7 @@\n import json\n \n def test_api_tags_listing(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     api_key = live_server.app.config['DATASTORE'].data['settings']['application'].get('api_access_token')\n     tag_title = 'Test Tag'\n \ndiff --git a/changedetectionio/tests/test_auth.py b/changedetectionio/tests/test_auth.py\nindex b3065fc0e3d..a9859961971 100644\n--- a/changedetectionio/tests/test_auth.py\n+++ b/changedetectionio/tests/test_auth.py\n@@ -6,7 +6,7 @@\n \n # test pages with http://username@password:foobar.com/ work\n def test_basic_auth(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n \n     # This page will echo back any auth info\ndiff --git a/changedetectionio/tests/test_automatic_follow_ldjson_price.py b/changedetectionio/tests/test_automatic_follow_ldjson_price.py\nindex f1908053aef..c730286c908 100644\n--- a/changedetectionio/tests/test_automatic_follow_ldjson_price.py\n+++ b/changedetectionio/tests/test_automatic_follow_ldjson_price.py\n@@ -76,12 +76,12 @@ def set_response_without_ldjson():\n         f.write(test_return_data)\n     return None\n \n-def test_setup(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server, measure_memory_usage):\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n # actually only really used by the distll.io importer, but could be handy too\n def test_check_ldjson_price_autodetect(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     set_response_with_ldjson()\n \n     # Add our URL to the import page\n@@ -164,7 +164,7 @@ def _test_runner_check_bad_format_ignored(live_server, client, has_ldjson_price_\n \n \n def test_bad_ldjson_is_correctly_ignored(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     test_return_data = \"\"\"\n             <html>\n             <head>\ndiff --git a/changedetectionio/tests/test_backend.py b/changedetectionio/tests/test_backend.py\nindex de0c169e4ab..2cbbc530a13 100644\n--- a/changedetectionio/tests/test_backend.py\n+++ b/changedetectionio/tests/test_backend.py\n@@ -18,7 +18,7 @@ def test_inscriptus():\n \n def test_check_basic_change_detection_functionality(client, live_server, measure_memory_usage):\n     set_original_response()\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     # Add our URL to the import page\n     res = client.post(\n@@ -143,6 +143,7 @@ def test_check_basic_change_detection_functionality(client, live_server, measure\n \n     # hit the mark all viewed link\n     res = client.get(url_for(\"ui.mark_all_viewed\"), follow_redirects=True)\n+    time.sleep(0.2)\n \n     assert b'class=\"has-unviewed' not in res.data\n     assert b'unviewed' not in res.data\ndiff --git a/changedetectionio/tests/test_backup.py b/changedetectionio/tests/test_backup.py\nindex 16366e9117d..ca1a0c6f693 100644\n--- a/changedetectionio/tests/test_backup.py\n+++ b/changedetectionio/tests/test_backup.py\n@@ -9,7 +9,7 @@\n \n \n def test_backup(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     set_original_response()\n \ndiff --git a/changedetectionio/tests/test_basic_socketio.py b/changedetectionio/tests/test_basic_socketio.py\nindex 82a31e2220a..1c48e758bad 100644\n--- a/changedetectionio/tests/test_basic_socketio.py\n+++ b/changedetectionio/tests/test_basic_socketio.py\n@@ -110,7 +110,7 @@ def run_socketio_watch_update_test(client, live_server, password_mode=\"\"):\n \n def test_everything(live_server, client):\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     run_socketio_watch_update_test(password_mode=\"\", live_server=live_server, client=client)\n \ndiff --git a/changedetectionio/tests/test_block_while_text_present.py b/changedetectionio/tests/test_block_while_text_present.py\nindex 473f5645f5f..6ab36855fea 100644\n--- a/changedetectionio/tests/test_block_while_text_present.py\n+++ b/changedetectionio/tests/test_block_while_text_present.py\n@@ -62,7 +62,7 @@ def set_modified_response_minus_block_text():\n \n def test_check_block_changedetection_text_NOT_present(client, live_server, measure_memory_usage):\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     # Use a mix of case in ZzZ to prove it works case-insensitive.\n     ignore_text = \"out of stoCk\\r\\nfoobar\"\n     set_original_ignore_response()\ndiff --git a/changedetectionio/tests/test_clone.py b/changedetectionio/tests/test_clone.py\nindex fd43384ae6b..aeb3b4f2a62 100644\n--- a/changedetectionio/tests/test_clone.py\n+++ b/changedetectionio/tests/test_clone.py\n@@ -7,7 +7,7 @@\n \n def test_clone_functionality(client, live_server, measure_memory_usage):\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     with open(\"test-datastore/endpoint-content.txt\", \"w\") as f:\n         f.write(\"<html><body>Some content</body></html>\")\n \ndiff --git a/changedetectionio/tests/test_conditions.py b/changedetectionio/tests/test_conditions.py\nindex 14dde02450c..9c6fae45d5b 100644\n--- a/changedetectionio/tests/test_conditions.py\n+++ b/changedetectionio/tests/test_conditions.py\n@@ -45,15 +45,15 @@ def set_number_out_of_range_response(number=\"150\"):\n         f.write(test_return_data)\n \n \n-def test_setup(client, live_server):\n+# def test_setup(client, live_server):\n     \"\"\"Test that both text and number conditions work together with AND logic.\"\"\"\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n def test_conditions_with_text_and_number(client, live_server):\n     \"\"\"Test that both text and number conditions work together with AND logic.\"\"\"\n     \n     set_original_response(\"50\")\n-    #live_server_setup(live_server)\n+    \n \n     test_url = url_for('test_endpoint', _external=True)\n \n@@ -110,6 +110,8 @@ def test_conditions_with_text_and_number(client, live_server):\n \n     wait_for_all_checks(client)\n     client.get(url_for(\"ui.mark_all_viewed\"), follow_redirects=True)\n+    time.sleep(0.2)\n+\n     wait_for_all_checks(client)\n \n     # Case 1\n@@ -126,6 +128,8 @@ def test_conditions_with_text_and_number(client, live_server):\n     # Case 2: Change with one condition violated\n     # Number out of range (150) but contains '5'\n     client.get(url_for(\"ui.mark_all_viewed\"), follow_redirects=True)\n+    time.sleep(0.2)\n+\n     set_number_out_of_range_response(\"150.5\")\n \n \n@@ -206,7 +210,7 @@ def test_condition_validate_rule_row(client, live_server):\n \n # If there was only a change in the whitespacing, then we shouldnt have a change detected\n def test_wordcount_conditions_plugin(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     test_return_data = \"\"\"<html>\n        <body>\n@@ -249,7 +253,7 @@ def test_wordcount_conditions_plugin(client, live_server, measure_memory_usage):\n \n # If there was only a change in the whitespacing, then we shouldnt have a change detected\n def test_lev_conditions_plugin(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     with open(\"test-datastore/endpoint-content.txt\", \"w\") as f:\n         f.write(\"\"\"<html>\ndiff --git a/changedetectionio/tests/test_css_selector.py b/changedetectionio/tests/test_css_selector.py\nindex 545d97f09d6..bad181c251b 100644\n--- a/changedetectionio/tests/test_css_selector.py\n+++ b/changedetectionio/tests/test_css_selector.py\n@@ -6,8 +6,7 @@\n \n from ..html_tools import *\n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n def set_original_response():\n     test_return_data = \"\"\"<html>\n@@ -125,7 +124,7 @@ def test_check_markup_include_filters_restriction(client, live_server, measure_m\n \n # Tests the whole stack works with the CSS Filter\n def test_check_multiple_filters(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     include_filters = \"#blob-a\\r\\nxpath://*[contains(@id,'blob-b')]\"\n \n     with open(\"test-datastore/endpoint-content.txt\", \"w\") as f:\n@@ -177,7 +176,7 @@ def test_check_multiple_filters(client, live_server, measure_memory_usage):\n # Mainly used when the filter contains just an IMG, this can happen when someone selects an image in the visual-selector\n # Tests fetcher can throw a \"ReplyWithContentButNoText\" exception after applying filter and extracting text\n def test_filter_is_empty_help_suggestion(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     include_filters = \"#blob-a\"\n \ndiff --git a/changedetectionio/tests/test_element_removal.py b/changedetectionio/tests/test_element_removal.py\nindex b4a31079279..36643b7137c 100644\n--- a/changedetectionio/tests/test_element_removal.py\n+++ b/changedetectionio/tests/test_element_removal.py\n@@ -8,8 +8,7 @@\n from .util import live_server_setup, wait_for_all_checks\n \n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n def set_response_with_multiple_index():\n     data= \"\"\"<!DOCTYPE html>\n@@ -148,7 +147,7 @@ def test_element_removal_output():\n \n \n def test_element_removal_full(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     set_original_response()\n \n@@ -209,7 +208,7 @@ def test_element_removal_full(client, live_server, measure_memory_usage):\n \n # Re #2752\n def test_element_removal_nth_offset_no_shift(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     set_response_with_multiple_index()\n     subtractive_selectors_data = [\"\"\"\ndiff --git a/changedetectionio/tests/test_encoding.py b/changedetectionio/tests/test_encoding.py\nindex d70dc56a5c7..722d1e6b81b 100644\n--- a/changedetectionio/tests/test_encoding.py\n+++ b/changedetectionio/tests/test_encoding.py\n@@ -7,8 +7,7 @@\n import pytest\n \n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n \n def set_html_response():\ndiff --git a/changedetectionio/tests/test_errorhandling.py b/changedetectionio/tests/test_errorhandling.py\nindex 0717f611b01..27b9a3188bd 100644\n--- a/changedetectionio/tests/test_errorhandling.py\n+++ b/changedetectionio/tests/test_errorhandling.py\n@@ -5,10 +5,7 @@\n from flask import url_for\n from .util import live_server_setup, wait_for_all_checks\n \n-from ..html_tools import *\n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n \n \n def _runner_test_http_errors(client, live_server, http_code, expected_text):\n@@ -79,7 +76,14 @@ def test_DNS_errors(client, live_server, measure_memory_usage):\n     wait_for_all_checks(client)\n \n     res = client.get(url_for(\"watchlist.index\"))\n-    found_name_resolution_error = b\"Temporary failure in name resolution\" in res.data or b\"Name or service not known\" in res.data\n+    found_name_resolution_error = (\n+        b\"No address found\" in res.data or\n+        b\"Name or service not known\" in res.data or\n+        b\"nodename nor servname provided\" in res.data or\n+        b\"Temporary failure in name resolution\" in res.data or\n+        b\"Failed to establish a new connection\" in res.data or\n+        b\"Connection error occurred\" in res.data\n+    )\n     assert found_name_resolution_error\n     # Should always record that we tried\n     assert bytes(\"just now\".encode('utf-8')) in res.data\n@@ -88,7 +92,7 @@ def test_DNS_errors(client, live_server, measure_memory_usage):\n \n # Re 1513\n def test_low_level_errors_clear_correctly(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     # Give the endpoint time to spin up\n     time.sleep(1)\n \n@@ -108,7 +112,14 @@ def test_low_level_errors_clear_correctly(client, live_server, measure_memory_us\n \n     # We should see the DNS error\n     res = client.get(url_for(\"watchlist.index\"))\n-    found_name_resolution_error = b\"Temporary failure in name resolution\" in res.data or b\"Name or service not known\" in res.data\n+    found_name_resolution_error = (\n+        b\"No address found\" in res.data or\n+        b\"Name or service not known\" in res.data or\n+        b\"nodename nor servname provided\" in res.data or\n+        b\"Temporary failure in name resolution\" in res.data or\n+        b\"Failed to establish a new connection\" in res.data or\n+        b\"Connection error occurred\" in res.data\n+    )\n     assert found_name_resolution_error\n \n     # Update with what should work\n@@ -123,7 +134,14 @@ def test_low_level_errors_clear_correctly(client, live_server, measure_memory_us\n     # Now the error should be gone\n     wait_for_all_checks(client)\n     res = client.get(url_for(\"watchlist.index\"))\n-    found_name_resolution_error = b\"Temporary failure in name resolution\" in res.data or b\"Name or service not known\" in res.data\n+    found_name_resolution_error = (\n+        b\"No address found\" in res.data or\n+        b\"Name or service not known\" in res.data or\n+        b\"nodename nor servname provided\" in res.data or\n+        b\"Temporary failure in name resolution\" in res.data or\n+        b\"Failed to establish a new connection\" in res.data or\n+        b\"Connection error occurred\" in res.data\n+    )\n     assert not found_name_resolution_error\n \n     res = client.get(url_for(\"ui.form_delete\", uuid=\"all\"), follow_redirects=True)\ndiff --git a/changedetectionio/tests/test_extract_csv.py b/changedetectionio/tests/test_extract_csv.py\nindex e7073638fa4..e70c41b26bc 100644\n--- a/changedetectionio/tests/test_extract_csv.py\n+++ b/changedetectionio/tests/test_extract_csv.py\n@@ -14,7 +14,7 @@ def test_check_extract_text_from_diff(client, live_server, measure_memory_usage)\n     with open(\"test-datastore/endpoint-content.txt\", \"w\") as f:\n         f.write(\"Now it's {} seconds since epoch, time flies!\".format(str(time.time())))\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     # Add our URL to the import page\n     res = client.post(\ndiff --git a/changedetectionio/tests/test_extract_regex.py b/changedetectionio/tests/test_extract_regex.py\nindex 68155ff1631..3b270d3fa91 100644\n--- a/changedetectionio/tests/test_extract_regex.py\n+++ b/changedetectionio/tests/test_extract_regex.py\n@@ -67,11 +67,11 @@ def set_multiline_response():\n     return None\n \n \n-def test_setup(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server, measure_memory_usage):\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n def test_check_filter_multiline(client, live_server, measure_memory_usage):\n-   # live_server_setup(live_server)\n+   ##  live_server_setup(live_server) # Setup on conftest per function\n     set_multiline_response()\n \n     # Add our URL to the import page\n@@ -206,7 +206,7 @@ def test_check_filter_and_regex_extract(client, live_server, measure_memory_usag\n \n def test_regex_error_handling(client, live_server, measure_memory_usage):\n \n-    #live_server_setup(live_server)\n+    \n \n     # Add our URL to the import page\n     test_url = url_for('test_endpoint', _external=True)\ndiff --git a/changedetectionio/tests/test_filter_exist_changes.py b/changedetectionio/tests/test_filter_exist_changes.py\nindex 9b2f9350796..c7841bad73d 100644\n--- a/changedetectionio/tests/test_filter_exist_changes.py\n+++ b/changedetectionio/tests/test_filter_exist_changes.py\n@@ -46,7 +46,7 @@ def test_filter_doesnt_exist_then_exists_should_get_notification(client, live_se\n #  And the page has that filter available\n #  Then I should get a notification\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     # Give the endpoint time to spin up\n     time.sleep(1)\ndiff --git a/changedetectionio/tests/test_filter_failure_notification.py b/changedetectionio/tests/test_filter_failure_notification.py\nindex fcb13d88a75..7023fb55acc 100644\n--- a/changedetectionio/tests/test_filter_failure_notification.py\n+++ b/changedetectionio/tests/test_filter_failure_notification.py\n@@ -163,15 +163,14 @@ def run_filter_test(client, live_server, content_filter):\n     os.unlink(\"test-datastore/notification.txt\")\n \n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n def test_check_include_filters_failure_notification(client, live_server, measure_memory_usage):\n-#    live_server_setup(live_server)\n+#   #  live_server_setup(live_server) # Setup on conftest per function\n     run_filter_test(client, live_server,'#nope-doesnt-exist')\n \n def test_check_xpath_filter_failure_notification(client, live_server, measure_memory_usage):\n-#    live_server_setup(live_server)\n+#   #  live_server_setup(live_server) # Setup on conftest per function\n     run_filter_test(client, live_server, '//*[@id=\"nope-doesnt-exist\"]')\n \n # Test that notification is never sent\ndiff --git a/changedetectionio/tests/test_group.py b/changedetectionio/tests/test_group.py\nindex ae294d40752..e166a8daa5c 100644\n--- a/changedetectionio/tests/test_group.py\n+++ b/changedetectionio/tests/test_group.py\n@@ -6,8 +6,8 @@\n import os\n \n \n-def test_setup(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server, measure_memory_usage):\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n def set_original_response():\n     test_return_data = \"\"\"<html>\n@@ -40,7 +40,7 @@ def set_modified_response():\n     return None\n \n def test_setup_group_tag(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     set_original_response()\n \n     # Add a tag with some config, import a tag and it should roughly work\n@@ -131,7 +131,7 @@ def test_setup_group_tag(client, live_server, measure_memory_usage):\n     assert b'Deleted' in res.data\n \n def test_tag_import_singular(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     test_url = url_for('test_endpoint', _external=True)\n     res = client.post(\n@@ -151,7 +151,7 @@ def test_tag_import_singular(client, live_server, measure_memory_usage):\n     assert b'Deleted' in res.data\n \n def test_tag_add_in_ui(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n #\n     res = client.post(\n         url_for(\"tags.form_tag_add\"),\n@@ -168,7 +168,7 @@ def test_tag_add_in_ui(client, live_server, measure_memory_usage):\n     assert b'Deleted' in res.data\n \n def test_group_tag_notification(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     set_original_response()\n \n     test_url = url_for('test_endpoint', _external=True)\n@@ -236,7 +236,7 @@ def test_group_tag_notification(client, live_server, measure_memory_usage):\n     assert b'Deleted' in res.data\n \n def test_limit_tag_ui(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     test_url = url_for('test_endpoint', _external=True)\n     urls=[]\n@@ -275,7 +275,7 @@ def test_limit_tag_ui(client, live_server, measure_memory_usage):\n     assert b'All tags deleted' in res.data\n \n def test_clone_tag_on_import(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     test_url = url_for('test_endpoint', _external=True)\n     res = client.post(\n         url_for(\"imports.import_page\"),\n@@ -301,7 +301,7 @@ def test_clone_tag_on_import(client, live_server, measure_memory_usage):\n     assert b'Deleted' in res.data\n \n def test_clone_tag_on_quickwatchform_add(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     test_url = url_for('test_endpoint', _external=True)\n \ndiff --git a/changedetectionio/tests/test_history_consistency.py b/changedetectionio/tests/test_history_consistency.py\nindex 1558c275d6c..fbcfba4d140 100644\n--- a/changedetectionio/tests/test_history_consistency.py\n+++ b/changedetectionio/tests/test_history_consistency.py\n@@ -9,7 +9,7 @@\n from urllib.parse import urlparse, parse_qs\n \n def test_consistent_history(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     workers = int(os.getenv(\"FETCH_WORKERS\", 10))\n     r = range(1, 10+workers)\n \ndiff --git a/changedetectionio/tests/test_ignore.py b/changedetectionio/tests/test_ignore.py\nindex 1a88c0b73b3..985e58b8bed 100644\n--- a/changedetectionio/tests/test_ignore.py\n+++ b/changedetectionio/tests/test_ignore.py\n@@ -24,7 +24,7 @@ def set_original_ignore_response():\n \n \n def test_ignore(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     set_original_ignore_response()\n     test_url = url_for('test_endpoint', _external=True)\n     res = client.post(\ndiff --git a/changedetectionio/tests/test_ignore_regex_text.py b/changedetectionio/tests/test_ignore_regex_text.py\nindex dca89817f75..847a3e745cd 100644\n--- a/changedetectionio/tests/test_ignore_regex_text.py\n+++ b/changedetectionio/tests/test_ignore_regex_text.py\n@@ -3,8 +3,7 @@\n from . util import live_server_setup\n from changedetectionio import html_tools\n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n # Unit test of the stripper\n # Always we are dealing in utf-8\ndiff --git a/changedetectionio/tests/test_ignore_text.py b/changedetectionio/tests/test_ignore_text.py\nindex 7864e08e4dd..19fe230307b 100644\n--- a/changedetectionio/tests/test_ignore_text.py\n+++ b/changedetectionio/tests/test_ignore_text.py\n@@ -5,8 +5,7 @@\n from .util import live_server_setup, wait_for_all_checks\n from changedetectionio import html_tools\n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n # Unit test of the stripper\n # Always we are dealing in utf-8\n@@ -256,9 +255,9 @@ def _run_test_global_ignore(client, as_source=False, extra_ignore=\"\"):\n     assert b'Deleted' in res.data\n \n def test_check_global_ignore_text_functionality(client, live_server):\n-    #live_server_setup(live_server)\n+    \n     _run_test_global_ignore(client, as_source=False)\n \n def test_check_global_ignore_text_functionality_as_source(client, live_server):\n-    #live_server_setup(live_server)\n+    \n     _run_test_global_ignore(client, as_source=True, extra_ignore='/\\?v=\\d/')\ndiff --git a/changedetectionio/tests/test_ignorehyperlinks.py b/changedetectionio/tests/test_ignorehyperlinks.py\nindex 34b43a1f480..5df8f9ae44f 100644\n--- a/changedetectionio/tests/test_ignorehyperlinks.py\n+++ b/changedetectionio/tests/test_ignorehyperlinks.py\n@@ -6,8 +6,7 @@\n from .util import live_server_setup, wait_for_all_checks\n \n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n def set_original_ignore_response():\n     test_return_data = \"\"\"<html>\ndiff --git a/changedetectionio/tests/test_ignorestatuscode.py b/changedetectionio/tests/test_ignorestatuscode.py\nindex cac971bec6d..a28e8996a07 100644\n--- a/changedetectionio/tests/test_ignorestatuscode.py\n+++ b/changedetectionio/tests/test_ignorestatuscode.py\n@@ -5,8 +5,7 @@\n from .util import live_server_setup, wait_for_all_checks\n \n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n \n def set_original_response():\ndiff --git a/changedetectionio/tests/test_ignorewhitespace.py b/changedetectionio/tests/test_ignorewhitespace.py\nindex fe97d6cadfe..93fa94b2e90 100644\n--- a/changedetectionio/tests/test_ignorewhitespace.py\n+++ b/changedetectionio/tests/test_ignorewhitespace.py\n@@ -4,8 +4,7 @@\n from flask import url_for\n from . util import live_server_setup\n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n \n # Should be the same as set_original_ignore_response() but with a little more whitespacing\ndiff --git a/changedetectionio/tests/test_import.py b/changedetectionio/tests/test_import.py\nindex 26cc6888e86..899ff1bac26 100644\n--- a/changedetectionio/tests/test_import.py\n+++ b/changedetectionio/tests/test_import.py\n@@ -8,8 +8,8 @@\n from .util import live_server_setup, wait_for_all_checks\n \n \n-def test_setup(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server, measure_memory_usage):\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n def test_import(client, live_server, measure_memory_usage):\n     # Give the endpoint time to spin up\n@@ -126,7 +126,7 @@ def test_import_distillio(client, live_server, measure_memory_usage):\n def test_import_custom_xlsx(client, live_server, measure_memory_usage):\n     \"\"\"Test can upload a excel spreadsheet and the watches are created correctly\"\"\"\n \n-    #live_server_setup(live_server)\n+    \n \n     dirname = os.path.dirname(__file__)\n     filename = os.path.join(dirname, 'import/spreadsheet.xlsx')\n@@ -175,7 +175,7 @@ def test_import_custom_xlsx(client, live_server, measure_memory_usage):\n def test_import_watchete_xlsx(client, live_server, measure_memory_usage):\n     \"\"\"Test can upload a excel spreadsheet and the watches are created correctly\"\"\"\n \n-    #live_server_setup(live_server)\n+    \n     dirname = os.path.dirname(__file__)\n     filename = os.path.join(dirname, 'import/spreadsheet.xlsx')\n     with open(filename, 'rb') as f:\ndiff --git a/changedetectionio/tests/test_jinja2.py b/changedetectionio/tests/test_jinja2.py\nindex ca06b46710c..71152943f17 100644\n--- a/changedetectionio/tests/test_jinja2.py\n+++ b/changedetectionio/tests/test_jinja2.py\n@@ -5,12 +5,12 @@\n from .util import live_server_setup, wait_for_all_checks\n \n \n-def test_setup(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server, measure_memory_usage):\n+   # #  live_server_setup(live_server) # Setup on conftest per function\n \n # If there was only a change in the whitespacing, then we shouldnt have a change detected\n def test_jinja2_in_url_query(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     # Add our URL to the import page\n     test_url = url_for('test_return_query', _external=True)\n@@ -35,7 +35,7 @@ def test_jinja2_in_url_query(client, live_server, measure_memory_usage):\n \n # https://techtonics.medium.com/secure-templating-with-jinja2-understanding-ssti-and-jinja2-sandbox-environment-b956edd60456\n def test_jinja2_security_url_query(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     # Add our URL to the import page\n     test_url = url_for('test_return_query', _external=True)\ndiff --git a/changedetectionio/tests/test_jsonpath_jq_selector.py b/changedetectionio/tests/test_jsonpath_jq_selector.py\nindex 7f22ea039c0..380e6dea9e0 100644\n--- a/changedetectionio/tests/test_jsonpath_jq_selector.py\n+++ b/changedetectionio/tests/test_jsonpath_jq_selector.py\n@@ -12,8 +12,7 @@\n except ModuleNotFoundError:\n     jq_support = False\n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n def test_unittest_inline_html_extract():\n     # So lets pretend that the JSON we want is inside some HTML\ndiff --git a/changedetectionio/tests/test_live_preview.py b/changedetectionio/tests/test_live_preview.py\nindex f8997692a4f..088e695f216 100644\n--- a/changedetectionio/tests/test_live_preview.py\n+++ b/changedetectionio/tests/test_live_preview.py\n@@ -19,7 +19,7 @@ def set_response():\n         f.write(data)\n \n def test_content_filter_live_preview(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     set_response()\n \n     test_url = url_for('test_endpoint', _external=True)\ndiff --git a/changedetectionio/tests/test_nonrenderable_pages.py b/changedetectionio/tests/test_nonrenderable_pages.py\nindex df2bef00e91..2b3bd30553a 100644\n--- a/changedetectionio/tests/test_nonrenderable_pages.py\n+++ b/changedetectionio/tests/test_nonrenderable_pages.py\n@@ -27,7 +27,7 @@ def set_zero_byte_response():\n \n def test_check_basic_change_detection_functionality(client, live_server, measure_memory_usage):\n     set_original_response()\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     # Add our URL to the import page\n     res = client.post(\n@@ -96,6 +96,8 @@ def test_check_basic_change_detection_functionality(client, live_server, measure\n     res = client.get(url_for(\"watchlist.index\"))\n     assert b'unviewed' in res.data\n     client.get(url_for(\"ui.mark_all_viewed\"), follow_redirects=True)\n+    time.sleep(0.2)\n+\n \n     # A totally zero byte (#2528) response should also not trigger an error\n     set_zero_byte_response()\ndiff --git a/changedetectionio/tests/test_notification.py b/changedetectionio/tests/test_notification.py\nindex 1d4a19847e2..8640d2ccaf3 100644\n--- a/changedetectionio/tests/test_notification.py\n+++ b/changedetectionio/tests/test_notification.py\n@@ -5,8 +5,7 @@\n from flask import url_for\n from loguru import logger\n \n-from .util import set_original_response, set_modified_response, set_more_modified_response, live_server_setup, wait_for_all_checks, \\\n-    set_longer_modified_response, get_index\n+from .util import set_original_response, set_modified_response, set_more_modified_response, live_server_setup, wait_for_all_checks\n from . util import  extract_UUID_from_client\n import logging\n import base64\n@@ -18,13 +17,12 @@\n     valid_notification_formats,\n )\n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n # Hard to just add more live server URLs when one test is already running (I think)\n # So we add our test here (was in a different file)\n def test_check_notification(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     set_original_response()\n \n     # Re 360 - new install should have defaults set\n@@ -286,7 +284,7 @@ def test_notification_validation(client, live_server, measure_memory_usage):\n \n \n def test_notification_custom_endpoint_and_jinja2(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     # test_endpoint - that sends the contents of a file\n     # test_notification_endpoint - that takes a POST and writes it to file (test-datastore/notification.txt)\n@@ -331,7 +329,7 @@ def test_notification_custom_endpoint_and_jinja2(client, live_server, measure_me\n \n \n     # Check no errors were recorded, because we asked for 204 which is slightly uncommon but is still OK\n-    res = get_index(client)\n+    res = client.get(url_for(\"watchlist.index\"))\n     assert b'notification-error' not in res.data\n \n     with open(\"test-datastore/notification.txt\", 'r') as f:\n@@ -372,7 +370,7 @@ def test_notification_custom_endpoint_and_jinja2(client, live_server, measure_me\n #2510\n def test_global_send_test_notification(client, live_server, measure_memory_usage):\n \n-    #live_server_setup(live_server)\n+    \n     set_original_response()\n     if os.path.isfile(\"test-datastore/notification.txt\"):\n         os.unlink(\"test-datastore/notification.txt\") \\\n@@ -517,7 +515,7 @@ def _test_color_notifications(client, notification_body_token):\n \n def test_html_color_notifications(client, live_server, measure_memory_usage):\n \n-    #live_server_setup(live_server)\n+    \n     _test_color_notifications(client, '{{diff}}')\n     _test_color_notifications(client, '{{diff_full}}')\n     \n\\ No newline at end of file\ndiff --git a/changedetectionio/tests/test_notification_errors.py b/changedetectionio/tests/test_notification_errors.py\nindex 28f503b319a..9b4ac7706d6 100644\n--- a/changedetectionio/tests/test_notification_errors.py\n+++ b/changedetectionio/tests/test_notification_errors.py\n@@ -6,7 +6,7 @@\n \n def test_check_notification_error_handling(client, live_server, measure_memory_usage):\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     set_original_response()\n \n     # Set a URL and fetch it, then set a notification URL which is going to give errors\n@@ -60,7 +60,15 @@ def test_check_notification_error_handling(client, live_server, measure_memory_u\n     # The error should show in the notification logs\n     res = client.get(\n         url_for(\"settings.notification_logs\"))\n-    found_name_resolution_error = b\"Temporary failure in name resolution\" in res.data or b\"Name or service not known\" in res.data\n+    # Check for various DNS/connection error patterns that may appear in different environments\n+    found_name_resolution_error = (\n+        b\"No address found\" in res.data or \n+        b\"Name or service not known\" in res.data or\n+        b\"nodename nor servname provided\" in res.data or\n+        b\"Temporary failure in name resolution\" in res.data or\n+        b\"Failed to establish a new connection\" in res.data or\n+        b\"Connection error occurred\" in res.data\n+    )\n     assert found_name_resolution_error\n \n     # And the working one, which is after the 'broken' one should still have fired\ndiff --git a/changedetectionio/tests/test_obfuscations.py b/changedetectionio/tests/test_obfuscations.py\nindex 055e2dc897b..9004f0fd3e8 100644\n--- a/changedetectionio/tests/test_obfuscations.py\n+++ b/changedetectionio/tests/test_obfuscations.py\n@@ -20,7 +20,7 @@ def set_original_ignore_response():\n \n def test_obfuscations(client, live_server, measure_memory_usage):\n     set_original_ignore_response()\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     time.sleep(1)\n     # Add our URL to the import page\n     test_url = url_for('test_endpoint', _external=True)\ndiff --git a/changedetectionio/tests/test_pdf.py b/changedetectionio/tests/test_pdf.py\nindex 7dc32b5b9f9..5a8080b2672 100644\n--- a/changedetectionio/tests/test_pdf.py\n+++ b/changedetectionio/tests/test_pdf.py\n@@ -10,7 +10,7 @@ def test_fetch_pdf(client, live_server, measure_memory_usage):\n     import shutil\n     shutil.copy(\"tests/test.pdf\", \"test-datastore/endpoint-test.pdf\")\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     test_url = url_for('test_pdf_endpoint', _external=True)\n     # Add our URL to the import page\n     res = client.post(\ndiff --git a/changedetectionio/tests/test_preview_endpoints.py b/changedetectionio/tests/test_preview_endpoints.py\nindex 3cb23b7c448..ada52ed44ec 100644\n--- a/changedetectionio/tests/test_preview_endpoints.py\n+++ b/changedetectionio/tests/test_preview_endpoints.py\n@@ -10,7 +10,7 @@ def test_fetch_pdf(client, live_server, measure_memory_usage):\n     import shutil\n     shutil.copy(\"tests/test.pdf\", \"test-datastore/endpoint-test.pdf\")\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     test_url = url_for('test_pdf_endpoint', _external=True)\n     # Add our URL to the import page\n     res = client.post(\ndiff --git a/changedetectionio/tests/test_request.py b/changedetectionio/tests/test_request.py\nindex 142984c261d..85b00633831 100644\n--- a/changedetectionio/tests/test_request.py\n+++ b/changedetectionio/tests/test_request.py\n@@ -4,8 +4,7 @@\n from flask import url_for\n from . util import set_original_response, set_modified_response, live_server_setup, wait_for_all_checks, extract_UUID_from_client\n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n # Hard to just add more live server URLs when one test is already running (I think)\n # So we add our test here (was in a different file)\n@@ -154,7 +153,7 @@ def test_body_in_request(client, live_server, measure_memory_usage):\n         follow_redirects=True\n     )\n     assert b\"1 Imported\" in res.data\n-\n+    wait_for_all_checks(client)\n     watches_with_body = 0\n     with open('test-datastore/url-watches.json') as f:\n         app_struct = json.load(f)\n@@ -258,7 +257,7 @@ def test_method_in_request(client, live_server, measure_memory_usage):\n \n # Re #2408 - user-agent override test, also should handle case-insensitive header deduplication\n def test_ua_global_override(client, live_server, measure_memory_usage):\n-    # live_server_setup(live_server)\n+    ##  live_server_setup(live_server) # Setup on conftest per function\n     test_url = url_for('test_headers', _external=True)\n \n     res = client.post(\n@@ -313,7 +312,7 @@ def test_ua_global_override(client, live_server, measure_memory_usage):\n     assert b'Deleted' in res.data\n \n def test_headers_textfile_in_request(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     # Add our URL to the import page\n \n     webdriver_ua = \"Hello fancy webdriver UA 1.0\"\n@@ -426,7 +425,7 @@ def test_headers_textfile_in_request(client, live_server, measure_memory_usage):\n     assert b'Deleted' in res.data\n \n def test_headers_validation(client, live_server):\n-    #live_server_setup(live_server)\n+    \n \n     test_url = url_for('test_headers', _external=True)\n     res = client.post(\ndiff --git a/changedetectionio/tests/test_restock_itemprop.py b/changedetectionio/tests/test_restock_itemprop.py\nindex 73454cd84a1..0627f59731a 100644\n--- a/changedetectionio/tests/test_restock_itemprop.py\n+++ b/changedetectionio/tests/test_restock_itemprop.py\n@@ -44,13 +44,13 @@ def set_original_response(props_markup='', price=\"121.95\"):\n \n \n \n-def test_setup(client, live_server):\n+# def test_setup(client, live_server):\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n def test_restock_itemprop_basic(client, live_server):\n \n-    #live_server_setup(live_server)\n+    \n \n     test_url = url_for('test_endpoint', _external=True)\n \n@@ -89,7 +89,7 @@ def test_restock_itemprop_basic(client, live_server):\n         assert b'Deleted' in res.data\n \n def test_itemprop_price_change(client, live_server):\n-    #live_server_setup(live_server)\n+    \n \n     # Out of the box 'Follow price changes' should be ON\n     test_url = url_for('test_endpoint', _external=True)\n@@ -114,6 +114,8 @@ def test_itemprop_price_change(client, live_server):\n     assert b'180.45' in res.data\n     assert b'unviewed' in res.data\n     client.get(url_for(\"ui.mark_all_viewed\"), follow_redirects=True)\n+    time.sleep(0.2)\n+\n \n     # turning off price change trigger, but it should show the new price, with no change notification\n     set_original_response(props_markup=instock_props[0], price='120.45')\n@@ -214,7 +216,7 @@ def _run_test_minmax_limit(client, extra_watch_edit_form):\n \n \n def test_restock_itemprop_minmax(client, live_server):\n-    #live_server_setup(live_server)\n+    \n     extras = {\n         \"restock_settings-follow_price_changes\": \"y\",\n         \"restock_settings-price_change_min\": 900.0,\n@@ -223,7 +225,7 @@ def test_restock_itemprop_minmax(client, live_server):\n     _run_test_minmax_limit(client, extra_watch_edit_form=extras)\n \n def test_restock_itemprop_with_tag(client, live_server):\n-    #live_server_setup(live_server)\n+    \n \n     res = client.post(\n         url_for(\"tags.form_tag_add\"),\n@@ -252,7 +254,7 @@ def test_restock_itemprop_with_tag(client, live_server):\n \n \n def test_itemprop_percent_threshold(client, live_server):\n-    #live_server_setup(live_server)\n+    \n \n     res = client.get(url_for(\"ui.form_delete\", uuid=\"all\"), follow_redirects=True)\n     assert b'Deleted' in res.data\n@@ -319,7 +321,7 @@ def test_itemprop_percent_threshold(client, live_server):\n \n \n def test_change_with_notification_values(client, live_server):\n-    #live_server_setup(live_server)\n+    \n \n     if os.path.isfile(\"test-datastore/notification.txt\"):\n         os.unlink(\"test-datastore/notification.txt\")\n@@ -387,7 +389,7 @@ def test_change_with_notification_values(client, live_server):\n \n \n def test_data_sanity(client, live_server):\n-    #live_server_setup(live_server)\n+    \n \n     res = client.get(url_for(\"ui.form_delete\", uuid=\"all\"), follow_redirects=True)\n     assert b'Deleted' in res.data\n@@ -437,7 +439,7 @@ def test_data_sanity(client, live_server):\n # All examples should give a prive of 666.66\n def test_special_prop_examples(client, live_server):\n     import glob\n-    #live_server_setup(live_server)\n+    \n \n     test_url = url_for('test_endpoint', _external=True)\n     check_path = os.path.join(os.path.dirname(__file__), \"itemprop_test_examples\", \"*.txt\")\ndiff --git a/changedetectionio/tests/test_rss.py b/changedetectionio/tests/test_rss.py\nindex 5701f690589..847c9fade4a 100644\n--- a/changedetectionio/tests/test_rss.py\n+++ b/changedetectionio/tests/test_rss.py\n@@ -65,11 +65,11 @@ def set_html_content(content):\n     with open(\"test-datastore/endpoint-content.txt\", \"wb\") as f:\n         f.write(test_return_data.encode('utf-8'))\n \n-def test_setup(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server, measure_memory_usage):\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n def test_rss_and_token(client, live_server, measure_memory_usage):\n-    #    live_server_setup(live_server)\n+    #   #  live_server_setup(live_server) # Setup on conftest per function\n \n     set_original_response()\n     rss_token = extract_rss_token_from_UI(client)\n@@ -107,7 +107,7 @@ def test_rss_and_token(client, live_server, measure_memory_usage):\n     client.get(url_for(\"ui.form_delete\", uuid=\"all\"), follow_redirects=True)\n \n def test_basic_cdata_rss_markup(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     set_original_cdata_xml()\n \n@@ -135,7 +135,7 @@ def test_basic_cdata_rss_markup(client, live_server, measure_memory_usage):\n     res = client.get(url_for(\"ui.form_delete\", uuid=\"all\"), follow_redirects=True)\n \n def test_rss_xpath_filtering(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     set_original_cdata_xml()\n \n@@ -191,7 +191,7 @@ def test_rss_bad_chars_breaking(client, live_server):\n \n     Otherwise feedgen should support regular unicode\n     \"\"\"\n-    #live_server_setup(live_server)\n+    \n \n     with open(\"test-datastore/endpoint-content.txt\", \"w\") as f:\n         ten_kb_string = \"A\" * 10_000\ndiff --git a/changedetectionio/tests/test_scheduler.py b/changedetectionio/tests/test_scheduler.py\nindex caacc3ad9fc..51610d60ed9 100644\n--- a/changedetectionio/tests/test_scheduler.py\n+++ b/changedetectionio/tests/test_scheduler.py\n@@ -6,11 +6,11 @@\n from flask import url_for\n from .util import  live_server_setup, wait_for_all_checks, extract_UUID_from_client\n \n-def test_setup(client, live_server):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server):\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n def test_check_basic_scheduler_functionality(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     days = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n     test_url = url_for('test_random_content_endpoint', _external=True)\n \n@@ -92,7 +92,7 @@ def test_check_basic_scheduler_functionality(client, live_server, measure_memory\n \n \n def test_check_basic_global_scheduler_functionality(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     days = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n     test_url = url_for('test_random_content_endpoint', _external=True)\n \ndiff --git a/changedetectionio/tests/test_search.py b/changedetectionio/tests/test_search.py\nindex 1668eaab67a..eb6f0ee9f70 100644\n--- a/changedetectionio/tests/test_search.py\n+++ b/changedetectionio/tests/test_search.py\n@@ -2,11 +2,10 @@\n from .util import set_original_response, set_modified_response, live_server_setup\n import time\n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n def test_basic_search(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     urls = ['https://localhost:12300?first-result=1',\n             'https://localhost:5000?second-result=1'\n@@ -39,7 +38,7 @@ def test_basic_search(client, live_server, measure_memory_usage):\n \n \n def test_search_in_tag_limit(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     urls = ['https://localhost:12300?first-result=1 tag-one',\n             'https://localhost:5000?second-result=1 tag-two'\ndiff --git a/changedetectionio/tests/test_security.py b/changedetectionio/tests/test_security.py\nindex 00902bfc4f3..495e12a83d3 100644\n--- a/changedetectionio/tests/test_security.py\n+++ b/changedetectionio/tests/test_security.py\n@@ -5,11 +5,11 @@\n from .. import strtobool\n \n \n-def test_setup(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server, measure_memory_usage):\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n def test_bad_access(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     res = client.post(\n         url_for(\"imports.import_page\"),\n         data={\"urls\": 'https://localhost'},\n@@ -89,7 +89,7 @@ def _runner_test_various_file_slash(client, file_uri):\n     assert b'Deleted' in res.data\n \n def test_file_slash_access(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     # file: is NOT permitted by default, so it will be caught by ALLOW_FILE_URI check\n \n@@ -99,7 +99,7 @@ def test_file_slash_access(client, live_server, measure_memory_usage):\n     _runner_test_various_file_slash(client, file_uri=f\"file:{test_file_path}\") # CVE-2024-56509\n \n def test_xss(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n     from changedetectionio.notification import (\n         default_notification_format\n     )\ndiff --git a/changedetectionio/tests/test_share_watch.py b/changedetectionio/tests/test_share_watch.py\nindex 004563174b1..09e3f35d34b 100644\n--- a/changedetectionio/tests/test_share_watch.py\n+++ b/changedetectionio/tests/test_share_watch.py\n@@ -11,7 +11,7 @@\n \n def test_share_watch(client, live_server, measure_memory_usage):\n     set_original_response()\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     test_url = url_for('test_endpoint', _external=True)\n     include_filters = \".nice-filter\"\ndiff --git a/changedetectionio/tests/test_source.py b/changedetectionio/tests/test_source.py\nindex 809b1909a29..992314c1f24 100644\n--- a/changedetectionio/tests/test_source.py\n+++ b/changedetectionio/tests/test_source.py\n@@ -7,8 +7,7 @@\n \n sleep_time_for_fetch_thread = 3\n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n def test_check_basic_change_detection_functionality_source(client, live_server, measure_memory_usage):\n     set_original_response()\ndiff --git a/changedetectionio/tests/test_trigger.py b/changedetectionio/tests/test_trigger.py\nindex dc18c1a5a4d..0df5ec3b55d 100644\n--- a/changedetectionio/tests/test_trigger.py\n+++ b/changedetectionio/tests/test_trigger.py\n@@ -57,7 +57,7 @@ def set_modified_with_trigger_text_response():\n \n def test_trigger_functionality(client, live_server, measure_memory_usage):\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     trigger_text = \"Add to cart\"\n     set_original_ignore_response()\ndiff --git a/changedetectionio/tests/test_trigger_regex.py b/changedetectionio/tests/test_trigger_regex.py\nindex 62c6acb7b36..25253f216f6 100644\n--- a/changedetectionio/tests/test_trigger_regex.py\n+++ b/changedetectionio/tests/test_trigger_regex.py\n@@ -24,7 +24,7 @@ def set_original_ignore_response():\n \n def test_trigger_regex_functionality(client, live_server, measure_memory_usage):\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     set_original_ignore_response()\n \ndiff --git a/changedetectionio/tests/test_trigger_regex_with_filter.py b/changedetectionio/tests/test_trigger_regex_with_filter.py\nindex bf69da9d830..a78b8fc4647 100644\n--- a/changedetectionio/tests/test_trigger_regex_with_filter.py\n+++ b/changedetectionio/tests/test_trigger_regex_with_filter.py\n@@ -24,7 +24,7 @@ def set_original_ignore_response():\n \n def test_trigger_regex_functionality_with_filter(client, live_server, measure_memory_usage):\n \n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n     sleep_time_for_fetch_thread = 3\n \n     set_original_ignore_response()\ndiff --git a/changedetectionio/tests/test_ui.py b/changedetectionio/tests/test_ui.py\nindex 743b70b5bf1..aec1ff601e7 100644\n--- a/changedetectionio/tests/test_ui.py\n+++ b/changedetectionio/tests/test_ui.py\n@@ -6,7 +6,7 @@\n def test_checkbox_open_diff_in_new_tab(client, live_server):\n     \n     set_original_response()\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     # Add our URL to the import page\n     res = client.post(\ndiff --git a/changedetectionio/tests/test_unique_lines.py b/changedetectionio/tests/test_unique_lines.py\nindex f3f70dc3708..b4829e62e49 100644\n--- a/changedetectionio/tests/test_unique_lines.py\n+++ b/changedetectionio/tests/test_unique_lines.py\n@@ -68,11 +68,11 @@ def set_modified_with_trigger_text_response():\n     with open(\"test-datastore/endpoint-content.txt\", \"w\") as f:\n         f.write(test_return_data)\n \n-def test_setup(client, live_server, measure_memory_usage):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server, measure_memory_usage):\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n def test_unique_lines_functionality(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n \n     set_original_ignore_response()\n@@ -121,7 +121,7 @@ def test_unique_lines_functionality(client, live_server, measure_memory_usage):\n     assert b'Deleted' in res.data\n \n def test_sort_lines_functionality(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     set_modified_swapped_lines_with_extra_text_for_sorting()\n \n@@ -171,7 +171,7 @@ def test_sort_lines_functionality(client, live_server, measure_memory_usage):\n \n \n def test_extra_filters(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     set_original_ignore_response()\n \ndiff --git a/changedetectionio/tests/test_watch_fields_storage.py b/changedetectionio/tests/test_watch_fields_storage.py\nindex 8765a51f133..506722fe2a8 100644\n--- a/changedetectionio/tests/test_watch_fields_storage.py\n+++ b/changedetectionio/tests/test_watch_fields_storage.py\n@@ -6,7 +6,7 @@\n \n def test_check_watch_field_storage(client, live_server, measure_memory_usage):\n     set_original_response()\n-    live_server_setup(live_server)\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n     test_url = \"http://somerandomsitewewatch.com\"\n \ndiff --git a/changedetectionio/tests/test_xpath_selector.py b/changedetectionio/tests/test_xpath_selector.py\nindex b3d0350ebac..fbdf201c6dd 100644\n--- a/changedetectionio/tests/test_xpath_selector.py\n+++ b/changedetectionio/tests/test_xpath_selector.py\n@@ -7,8 +7,7 @@\n from ..html_tools import *\n \n \n-def test_setup(live_server):\n-    live_server_setup(live_server)\n+\n \n \n def set_original_response():\n@@ -256,7 +255,7 @@ def test_xpath23_prefix_validation(client, live_server, measure_memory_usage):\n     assert b'Deleted' in res.data\n \n def test_xpath1_lxml(client, live_server, measure_memory_usage):\n-    #live_server_setup(live_server)\n+    \n \n     d = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n     <rss xmlns:taxo=\"http://purl.org/rss/1.0/modules/taxonomy/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:itunes=\"http://www.itunes.com/dtds/podcast-1.0.dtd\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" version=\"2.0\">\n@@ -380,7 +379,7 @@ def test_check_with_prefix_include_filters(client, live_server, measure_memory_u\n \n def test_various_rules(client, live_server, measure_memory_usage):\n     # Just check these don't error\n-    # live_server_setup(live_server)\n+    ##  live_server_setup(live_server) # Setup on conftest per function\n     with open(\"test-datastore/endpoint-content.txt\", \"w\") as f:\n         f.write(\"\"\"<html>\n        <body>\ndiff --git a/changedetectionio/tests/util.py b/changedetectionio/tests/util.py\nindex effbadfaa65..6261b7b22a3 100644\n--- a/changedetectionio/tests/util.py\n+++ b/changedetectionio/tests/util.py\n@@ -1,4 +1,5 @@\n #!/usr/bin/env python3\n+from operator import truediv\n \n from flask import make_response, request\n from flask import url_for\n@@ -129,52 +130,46 @@ def extract_UUID_from_client(client):\n \n def wait_for_all_checks(client=None):\n     \"\"\"\n-    Waits until the queue is empty and remains empty for at least `required_empty_duration` seconds,\n-    and also ensures no running threads have `current_uuid` set.\n-    Retries for up to `max_attempts` times, sleeping `wait_between_attempts` seconds between checks.\n+    Waits until the queue is empty and workers are idle.\n+    Much faster than the original with adaptive timing.\n     \"\"\"\n-    from changedetectionio.flask_app import update_q as global_update_q, running_update_threads\n-\n-    # Configuration\n-    attempt = 0\n-    i=0\n-    max_attempts = 60\n-    required_empty_duration = 0.2\n+    from changedetectionio.flask_app import update_q as global_update_q\n+    from changedetectionio import worker_handler\n \n     logger = logging.getLogger()\n-    time.sleep(1.2)\n-\n     empty_since = None\n+    attempt = 0\n+    max_attempts = 150  # Still reasonable upper bound\n \n     while attempt < max_attempts:\n-        q_length = global_update_q.qsize()\n-\n-        # Check if any threads are still processing\n-        time.sleep(1.2)\n-        any_threads_busy = any(t.current_uuid for t in running_update_threads)\n+        # Start with fast checks, slow down if needed\n+        if attempt < 10:\n+            time.sleep(0.1)  # Very fast initial checks\n+        elif attempt < 30:\n+            time.sleep(0.3)  # Medium speed\n+        else:\n+            time.sleep(0.8)  # Slower for persistent issues\n \n+        q_length = global_update_q.qsize()\n+        running_uuids = worker_handler.get_running_uuids()\n+        any_workers_busy = len(running_uuids) > 0\n \n-        if q_length == 0 and not any_threads_busy:\n+        if q_length == 0 and not any_workers_busy:\n             if empty_since is None:\n                 empty_since = time.time()\n-                logger.info(f\"Queue empty and no active threads at attempt {attempt}, starting empty timer...\")\n-            elif time.time() - empty_since >= required_empty_duration:\n-                logger.info(f\"Queue has been empty and threads idle for {required_empty_duration} seconds. Done waiting.\")\n+            elif time.time() - empty_since >= 0.15:  # Shorter wait\n                 break\n-            else:\n-                logger.info(f\"Still waiting: queue empty and no active threads, but not yet {required_empty_duration} seconds...\")\n         else:\n-            if q_length != 0:\n-                logger.info(f\"Queue not empty (size={q_length}), resetting timer.\")\n-            if any_threads_busy:\n-                busy_threads = [t.name for t in running_update_threads if t.current_uuid]\n-                logger.info(f\"Threads still busy: {busy_threads}, resetting timer.\")\n             empty_since = None\n+        \n         attempt += 1\n+        time.sleep(0.3)\n \n-    time.sleep(1)\n+# Replaced by new_live_server_setup and calling per function scope in conftest.py\n+def  live_server_setup(live_server):\n+    return True\n \n-def live_server_setup(live_server):\n+def new_live_server_setup(live_server):\n \n     @live_server.app.route('/test-random-content-endpoint')\n     def test_random_content_endpoint():\n@@ -328,20 +323,3 @@ def test_interactive_html_endpoint():\n \n     live_server.start()\n \n-\n-\n-def get_index(client):\n-    import inspect\n-    # Get the caller's frame (parent function)\n-    frame = inspect.currentframe()\n-    caller_frame = frame.f_back  # Go back to the caller's frame\n-    caller_name = caller_frame.f_code.co_name\n-    caller_line = caller_frame.f_lineno\n-\n-    print(f\"Called by: {caller_name}, Line: {caller_line}\")\n-\n-    res = client.get(url_for(\"watchlist.index\"))\n-    with open(f\"test-datastore/index-{caller_name}-{caller_line}.html\", 'wb') as f:\n-        f.write(res.data)\n-\n-    return res\ndiff --git a/changedetectionio/tests/visualselector/test_fetch_data.py b/changedetectionio/tests/visualselector/test_fetch_data.py\nindex e1c76b79889..c476f7db256 100644\n--- a/changedetectionio/tests/visualselector/test_fetch_data.py\n+++ b/changedetectionio/tests/visualselector/test_fetch_data.py\n@@ -2,16 +2,14 @@\n \n import os\n from flask import url_for\n-from ..util import live_server_setup, wait_for_all_checks, get_index\n+from ..util import live_server_setup, wait_for_all_checks\n \n-def test_setup(client, live_server):\n-    live_server_setup(live_server)\n+# def test_setup(client, live_server):\n+   #  live_server_setup(live_server) # Setup on conftest per function\n \n \n # Add a site in paused mode, add an invalid filter, we should still have visual selector data ready\n def test_visual_selector_content_ready(client, live_server, measure_memory_usage):\n-    live_server.stop()\n-    live_server.start()\n \n     import os\n     import json\n@@ -89,9 +87,6 @@ def test_visual_selector_content_ready(client, live_server, measure_memory_usage\n \n def test_basic_browserstep(client, live_server, measure_memory_usage):\n \n-    live_server.stop()\n-    live_server.start()\n-\n     assert os.getenv('PLAYWRIGHT_DRIVER_URL'), \"Needs PLAYWRIGHT_DRIVER_URL set for this test\"\n \n     test_url = url_for('test_interactive_html_endpoint', _external=True)\n@@ -144,12 +139,9 @@ def test_basic_browserstep(client, live_server, measure_memory_usage):\n \n     assert b\"testheader: yes\" in res.data\n     assert b\"user-agent: mycustomagent\" in res.data\n-    live_server.stop()\n \n def test_non_200_errors_report_browsersteps(client, live_server):\n \n-    live_server.stop()\n-    live_server.start()\n \n     four_o_four_url =  url_for('test_endpoint', status_code=404, _external=True)\n     four_o_four_url = four_o_four_url.replace('localhost.localdomain', 'cdio')\n@@ -183,7 +175,7 @@ def test_non_200_errors_report_browsersteps(client, live_server):\n \n     wait_for_all_checks(client)\n \n-    res = get_index(client)\n+    res = client.get(url_for(\"watchlist.index\"))\n \n     assert b'Error - 404' in res.data\n \n", "problem_statement": "Websocket support very slow loading\n**Describe the bug**\nAfter adding websocket support, loading is very tedious and takes a very long time to loa\n\n**Version**\nv0.49.18\n\n**How did you install?**\n\nDocker\n\n**To Reproduce**\n\nSteps to reproduce the behavior:\nnormal opening and using changedetection\n\n**Expected behavior**\nloading speed as lower than version 0.49.18\n\n**Desktop (please complete the following information):**\n - OS: Windows 11\n - Browser Chrome\n - Version 137.0.7151.40\n\n**Additional context**\nChecking websites and sending notifications works fine, but the website interface is unusable.\n\n`              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/engineio/socket.py\", line 92, in handle_get_request\n    return getattr(self, '_upgrade_' + transport)(environ,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/engineio/socket.py\", line 151, in _upgrade_websocket\n    return ws(environ, start_response)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/engineio/async_drivers/_websocket_wsgi.py\", line 15, in __call__\n    ret = self.app(self)\n          ^^^^^^^^^^^^^^\n  File \"/usr/local/engineio/socket.py\", line 180, in _websocket_handler\n    ws.send(packet.Packet(packet.PONG, data='probe').encode())\n  File \"/usr/local/engineio/async_drivers/_websocket_wsgi.py\", line 28, in send\n    raise OSError()\nOSError\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /socket.io/?EIO=4&transport=polling&t=PSLpItR&sid=ODkDVg6DZTRA8nlJAAAQ HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"POST /socket.io/?EIO=4&transport=polling&t=PSLpItj&sid=ODkDVg6DZTRA8nlJAAAQ HTTP/1.1\" 200 -\n2025-05-28 07:31:44.042 | INFO     | changedetectionio.realtime.socket_server:handle_disconnect:237 - Socket.IO: Client disconnected\n172.18.0.1 - - [28/May/2025 07:31:44] \"POST /socket.io/?EIO=4&transport=polling&t=PSLpLHK&sid=ODkDVg6DZTRA8nlJAAAQ HTTP/1.1\" 200 -\n2025-05-28 07:31:44.061 | DEBUG    | changedetectionio.model.Watch:history:199 - Reading watch history index for 8cdd6abb-5f38-4129-b4ee-a85f808e3f91\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET / HTTP/1.1\" 200 -\n2025-05-28 07:31:44.117 | DEBUG    | changedetectionio.model.Watch:history:199 - Reading watch history index for 8cdd6abb-5f38-4129-b4ee-a85f808e3f91\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET / HTTP/1.1\" 200 -\nThe client is using an unsupported version of the Socket.IO or Engine.IO protocols (further occurrences of this error will be logged with level INFO)\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /socket.io/ HTTP/1.1\" 400 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET / HTTP/1.1\" 302 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /login?next=/ HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/styles/pure-min.css HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/styles/styles.css?v=0.49.18 HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/js/csrf.js HTTP/1.1\" 304 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/js/jquery-3.6.0.min.js HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/js/socket.io.min.js HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/js/timeago-init.js HTTP/1.1\" 404 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/js/realtime.js HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/js/toggle-theme.js HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/images/google-chrome-icon.png HTTP/1.1\" 304 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/js/socket.io.min.js.map HTTP/1.1\" 404 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /socket.io/?EIO=4&transport=polling&t=PSLpU4J HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/favicons/apple-touch-icon.png HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /static/favicons/favicon-16x16.png HTTP/1.1\" 200 -\n2025-05-28 07:31:44.429 | DEBUG    | changedetectionio.realtime.socket_server:handle_connect:228 - Socket.IO: Sent initial queue size 0 to new client\n2025-05-28 07:31:44.429 | INFO     | changedetectionio.realtime.socket_server:handle_connect:232 - Socket.IO: Client connected\n172.18.0.1 - - [28/May/2025 07:31:44] \"POST /socket.io/?EIO=4&transport=polling&t=PSLpU57&sid=Es6kD0Y2kb5ZskEUAAAS HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:31:44] \"GET /socket.io/?EIO=4&transport=polling&t=PSLpU58&sid=Es6kD0Y2kb5ZskEUAAAS HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:32:30] \"GET /socket.io/?EIO=4&transport=websocket&sid=Es6kD0Y2kb5ZskEUAAAS HTTP/1.1\" 500 -\nError on request:\nTraceback (most recent call last):\n  File \"/usr/local/engineio/async_drivers/_websocket_wsgi.py\", line 26, in send\n    return self.ws.send(message)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/simple_websocket/ws.py\", line 78, in send\n    raise ConnectionClosed(self.close_reason, self.close_message)\nsimple_websocket.errors.ConnectionClosed: Connection closed: 1005 \nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/usr/local/werkzeug/serving.py\", line 370, in run_wsgi\n    execute(self.server.app)\n  File \"/usr/local/werkzeug/serving.py\", line 331, in execute\n    application_iter = app(environ, start_response)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/flask/app.py\", line 2213, in __call__\n    return self.wsgi_app(environ, start_response)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/flask_socketio/__init__.py\", line 42, in __call__\n    return super().__call__(environ, start_response)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/engineio/middleware.py\", line 63, in __call__\n    return self.engineio_app.handle_request(environ, start_response)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/socketio/server.py\", line 434, in handle_request\n    return self.eio.handle_request(environ, start_response)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/engineio/server.py\", line 286, in handle_request\n    packets = socket.handle_get_request(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/engineio/socket.py\", line 92, in handle_get_request\n    return getattr(self, '_upgrade_' + transport)(environ,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/engineio/socket.py\", line 151, in _upgrade_websocket\n    return ws(environ, start_response)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/engineio/async_drivers/_websocket_wsgi.py\", line 15, in __call__\n    ret = self.app(self)\n          ^^^^^^^^^^^^^^\n  File \"/usr/local/engineio/socket.py\", line 180, in _websocket_handler\n    ws.send(packet.Packet(packet.PONG, data='probe').encode())\n  File \"/usr/local/engineio/async_drivers/_websocket_wsgi.py\", line 28, in send\n    raise OSError()\nOSError\n172.18.0.1 - - [28/May/2025 07:32:30] \"GET /static/favicons/site.webmanifest HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:32:30] \"GET /socket.io/?EIO=4&transport=polling&t=PSLpU5n&sid=Es6kD0Y2kb5ZskEUAAAS HTTP/1.1\" 200 -\n2025-05-28 07:32:30.038 | INFO     | changedetectionio.realtime.socket_server:handle_disconnect:237 - Socket.IO: Client disconnected\n172.18.0.1 - - [28/May/2025 07:32:30] \"POST /socket.io/?EIO=4&transport=polling&t=PSLpfDL&sid=Es6kD0Y2kb5ZskEUAAAS HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:32:32] \"GET /socket.io/?EIO=4&transport=polling&t=PSLpfiT HTTP/1.1\" 200 -\n172.18.0.1 - - [28/May/2025 07:32:57] \"GET /socket.io/?EIO=4&transport=polling&t=PSLpfj3&sid=nwe6YHrMtdGEmTapAAAU HTTP/1.1\" 200 -\n2025-05-28 07:32:57.056 | DEBUG    | changedetectionio.realtime.socket_server:handle_connect:228 - Socket.IO: Sent initial queue size 0 to new client\n2025-05-28 07:32:57.056 | INFO     | changedetectionio.realtime.socket_server:handle_connect:232 - Socket.IO: Client connected\n172.18.0.1 - - [28/May/2025 07:32:57] \"POST /socket.io/?EIO=4&transport=polling&t=PSLpfj2&sid=nwe6YHrMtdGEmTapAAAU HTTP/1.1\" 200 -`\n\n![Image](https://github.com/user-attachments/assets/5201cc1a-d29b-40c1-a991-d0d1d25f722f)\n\n", "hints_text": "\"After adding websocket support, loading is very tedious and takes a very long time to loa\"\n\nCan you tell me, what is exactly is slow\n\nIs it the application frontend?\n\nOr is it the realtime connection?\n\nOr both?\n\nBasically both, but sometimes the first page (main page) loads right away, but the next click takes a spinning wheel and 10 seconds to load.\n\nMain page\n![Image](https://github.com/user-attachments/assets/25b49801-3593-44fe-8d98-01e3c1abe7db)\n\nclick on history some page:\n\n![Image](https://github.com/user-attachments/assets/85570e9b-a653-4e7a-a90d-36fa9c2cdfb3)\n@leroyloren can you shut the app down, then start it, but give me the first 100-200 lines of the startup log?\nsame problem here\n@bruno32600 please do NOT write just \"same problem here\", please be more helpful\n- operating system\n- how you installed\n- version you have\n- anything else that can help\ninstalled on debian11 , i did the update for docker with docker pull dgtlmoon/changedetection.io to go from v0.49.17 to v0.49.18I reinstalled v0.49.17, everything works fine again\n\n\n@bruno32600 thanks, I didnt know if it was just limited to windows or not\n> [@leroyloren](https://github.com/leroyloren) can you shut the app down, then start it, but give me the first 100-200 lines of the startup log?\n\nstop/start container\n\n*pages to check are filtered out from the log\n\n[log.txt](https://github.com/user-attachments/files/20492791/log.txt)\n\n", "all_hints_text": "\"After adding websocket support, loading is very tedious and takes a very long time to loa\"\n\nCan you tell me, what is exactly is slow\n\nIs it the application frontend?\n\nOr is it the realtime connection?\n\nOr both?\n\nBasically both, but sometimes the first page (main page) loads right away, but the next click takes a spinning wheel and 10 seconds to load.\n\nMain page\n![Image](https://github.com/user-attachments/assets/25b49801-3593-44fe-8d98-01e3c1abe7db)\n\nclick on history some page:\n\n![Image](https://github.com/user-attachments/assets/85570e9b-a653-4e7a-a90d-36fa9c2cdfb3)\n@leroyloren can you shut the app down, then start it, but give me the first 100-200 lines of the startup log?\nsame problem here\n@bruno32600 please do NOT write just \"same problem here\", please be more helpful\n- operating system\n- how you installed\n- version you have\n- anything else that can help\ninstalled on debian11 , i did the update for docker with docker pull dgtlmoon/changedetection.io to go from v0.49.17 to v0.49.18I reinstalled v0.49.17, everything works fine again\n\n\n@bruno32600 thanks, I didnt know if it was just limited to windows or not\n> [@leroyloren](https://github.com/leroyloren) can you shut the app down, then start it, but give me the first 100-200 lines of the startup log?\n\nstop/start container\n\n*pages to check are filtered out from the log\n\n[log.txt](https://github.com/user-attachments/files/20492791/log.txt)\nThis also starts to appear in the log:\n[_changedetection_logs.txt](https://github.com/user-attachments/files/20493039/_changedetection_logs.txt)\nvery slow loading on .18, probably same problem. \nubuntu desktop 25.04, had to downgrade from .18 to .17 to get normal speed, docker installation.\nok thank god i am not alone. I had migrated my Host to another device and I thought my migration was causing this lag and slowness. Its so bad, that you cant even log in at changedetection own GUI. It is getting stuck at logging in once you click enter a password, its failing.\n\nWhat i was able to analyze:\n\nI have my website https://change.furkan.it/ running behind cloudflare, so i first thought it may be an issue related with cloudflare. After switching temporarily back to without cloudflare, i still had the same issue accessing it via https://change.furkan.it, even tho it was not proxied through it.\n\nBUT, if i access it directly via the IP Address instead of the hostname, it works! \n\nTLDR:\n\nhttps://change.furkan.it      - Laggy, slow as hell, not even working.\n\nhttp://192.168.178.4:5000  - Works flawlessly.\n\n@dgtlmoon if you want any logs,  let me know.\n\nok one important update:\n\nhttps://change.furkan.it/     - Laggy, slow as hell, not even working.\n\n**http://89.245.5.6:5000/       - Works flawlessly!**\n\nhttp://192.168.178.4:5000/ - Works flawlessly.\n\nThis means now, that if the ChangeDetection GUI is being accessed with an external IP, doesnt matter if with Hostname or not, it is failing. But if accessed with the local internal IP, it works.\n\nWhy? I dont know, that is your job to find out @dgtlmoon :D \n\n(I dont care if i leak my hostname or IP, its no issue for me before somebody mentions it)\ni always access with local ip, 192.168.x.x.\n.18 extremely slow, .17 works fine for me.\nHi all, sorry about this, working on a fix now, its basically a 3 way problem\n\nSocketIO <-> eventlet/threads <-> playwright  :((\nHappens, gl fixing it!\nplaywright is such a piece of sh... its designed for web-testing not for automated page loading\nany update?\n@FurkanVG pretty sure you can check the progress by looking at the PR, i'm working on it when i should be seeing my family, enjoying summer etc\nhttps://github.com/dgtlmoon/changedetection.io/pull/3220\n> [@FurkanVG](https://github.com/FurkanVG) pretty sure you can check the progress by looking at the PR, i'm working on it when i should be seeing my family, enjoying summer etc\n\nsry im blind, ty\nas for the PR - it's more or less complete, theres one UI tweak and theres some problem with the tests, even tho the PR \"passes\" so i hope tomorrow or something\nplease try `0.50.2` and let me know as soon as possible if it helps\n> please try `0.50.2` and let me know as soon as possible if it helps\n\nfixed it for me, ty!\n> please try `0.50.2` and let me know as soon as possible if it helps\n\nonly briefly tested, but seems ok now.\nmany thanks.\n\n", "commit_urls": ["https://github.com/dgtlmoon/changedetection.io/commit/62653a46462d15d40a8cb24b200830e1a02ea2d6", "https://github.com/dgtlmoon/changedetection.io/commit/05ab0831ef0d37c2a212d12f8c7c5343716ac9a0", "https://github.com/dgtlmoon/changedetection.io/commit/46f78f01645c1a99bce99916ee473b04fc7b5392", "https://github.com/dgtlmoon/changedetection.io/commit/b74eaca83f3a551518244e91685d111233d36304", "https://github.com/dgtlmoon/changedetection.io/commit/9bc347158a5af7211f729975e8d3ad30a85e1a1e", "https://github.com/dgtlmoon/changedetection.io/commit/4c7395f203b8734be7b13dfae8b997e7e63cc456", "https://github.com/dgtlmoon/changedetection.io/commit/142f93cf886487c9c3a4514c4299ab9e030eb39a", "https://github.com/dgtlmoon/changedetection.io/commit/bf6bab6c05001a97a34bf598f571188742072036", "https://github.com/dgtlmoon/changedetection.io/commit/d40e017e29f90c68ae463d68f7520368b2070064", "https://github.com/dgtlmoon/changedetection.io/commit/b535339e94f03d3d8a456cfe3e1e890fd3d41ab8", "https://github.com/dgtlmoon/changedetection.io/commit/e891c2da42422759dc78fbc233531ae4e01de7a1", "https://github.com/dgtlmoon/changedetection.io/commit/6b68587bbf9802471614cbaa5d0cf3f7a8f44c61", "https://github.com/dgtlmoon/changedetection.io/commit/fb5e93691fd5c25633f1d363cb826715a8b2fc7d", "https://github.com/dgtlmoon/changedetection.io/commit/a52ae11062688eca21017a9230bc2cfb4be7d8ef", "https://github.com/dgtlmoon/changedetection.io/commit/34fbfa71134fd60f41f94f4d12d165863d8e89ee", "https://github.com/dgtlmoon/changedetection.io/commit/01742dd670497ceb87116c08fc176d6c04cdf8e3", "https://github.com/dgtlmoon/changedetection.io/commit/e5aba3b2f0ad11b4d95dc270754c64fa4305ab60", "https://github.com/dgtlmoon/changedetection.io/commit/e9d28b810af84998c45c1a45fad27ae60e0c36aa", "https://github.com/dgtlmoon/changedetection.io/commit/0d332dd519a113f80c51347bbcc899a95d497848", "https://github.com/dgtlmoon/changedetection.io/commit/40498a59b6af6e09c024a486d2df93351590aba3", "https://github.com/dgtlmoon/changedetection.io/commit/f7695f59d3c05cba4384f9074c8bb546067f32fb", "https://github.com/dgtlmoon/changedetection.io/commit/3d61ce8df79fd8dc337d268c7a8fb978636f2eff", "https://github.com/dgtlmoon/changedetection.io/commit/6c1ed57032041eb4d47570aa8f26c079f542e928", "https://github.com/dgtlmoon/changedetection.io/commit/337411c16ada7fb61205eba4c1751198cf479d5e", "https://github.com/dgtlmoon/changedetection.io/commit/5c0d151490ee34972d4e6dc04039a30776ac0407", "https://github.com/dgtlmoon/changedetection.io/commit/817afed17db83f9f45e373e2d0692c42d5dee5d6", "https://github.com/dgtlmoon/changedetection.io/commit/b4bfd23f987e7b3265c4e44150c114253b537cde", "https://github.com/dgtlmoon/changedetection.io/commit/6866956e67efbe30ff360c8955e7b161b37da030", "https://github.com/dgtlmoon/changedetection.io/commit/cd7dde44776b59a620b71281d6a3d3ae1abe8316", "https://github.com/dgtlmoon/changedetection.io/commit/821c0edff489ce2f6401e69d6c5b601dcd5f9cf4", "https://github.com/dgtlmoon/changedetection.io/commit/75e6fbd624311006cf7668bfdf317c1dea610d82", "https://github.com/dgtlmoon/changedetection.io/commit/6c3e88e26183bb314240b3a1d7829c06eeac7718", "https://github.com/dgtlmoon/changedetection.io/commit/03e751b57f9eea8e733120015672bbe6170cf561", "https://github.com/dgtlmoon/changedetection.io/commit/b90d03a78ecb666166fab8333071a3d433a918bb", "https://github.com/dgtlmoon/changedetection.io/commit/4352e8006c24ef86469e9b4d8f8e041b3de05c31", "https://github.com/dgtlmoon/changedetection.io/commit/a9cf6a4373fa70b394929cc026e4aebbdbe0b00a", "https://github.com/dgtlmoon/changedetection.io/commit/8f421a43efc4a75ea9c5f6c0e56cd0c004edfe72", "https://github.com/dgtlmoon/changedetection.io/commit/501aaf4b77256b0af33f8a67c3639c6839d92f2a", "https://github.com/dgtlmoon/changedetection.io/commit/1f710529a4597faca7dc17d48fa1c75eddc25d22", "https://github.com/dgtlmoon/changedetection.io/commit/cf01015601a9f75e05330ab932c93cb8c7cdb104", "https://github.com/dgtlmoon/changedetection.io/commit/6949a09aabdf1802e24a67607bfb1c6a915dd4ab", "https://github.com/dgtlmoon/changedetection.io/commit/5c7c5489290b6b933d7cb77c51dc1988a61b38d8", "https://github.com/dgtlmoon/changedetection.io/commit/40226dbad7307a08ae6b90f06766af82c5e4705c", "https://github.com/dgtlmoon/changedetection.io/commit/15e0a330fee19513f309ecb5109387641aea9143"], "created_at": "2025-05-28T08:26:43Z", "classification": "Efficiency"}
{"repo": "huggingface/datasets", "pull_number": 7614, "instance_id": "huggingface__datasets-7614", "issue_numbers": [4180], "base_commit": "1e53e0c52309fe9db49a7641c652d4905189567c", "patch": "diff --git a/docs/source/access.mdx b/docs/source/access.mdx\nindex 559168190f5..50b2fed19cb 100644\n--- a/docs/source/access.mdx\n+++ b/docs/source/access.mdx\n@@ -54,7 +54,7 @@ You can combine row and column name indexing to return a specific value at a pos\n 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'\n ```\n \n-But it is important to remember that indexing order matters, especially when working with large audio and image datasets. Indexing by the column name returns all the values in the column first, then loads the value at that position. For large datasets, it may be slower to index by the column name first.\n+Indexing order doesn't matter. Indexing by the column name first returns a [`Column`] object that you can index as usual with row indices as usual:\n \n ```py\n >>> import time\n@@ -69,7 +69,7 @@ Elapsed time: 0.0031 seconds\n >>> text = dataset[\"text\"][0]\n >>> end_time = time.time()\n >>> print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n-Elapsed time: 0.0094 seconds\n+Elapsed time: 0.0042 seconds\n ```\n \n ### Slicing\ndiff --git a/src/datasets/__init__.py b/src/datasets/__init__.py\nindex f50a9974843..91fe93ae9e0 100644\n--- a/src/datasets/__init__.py\n+++ b/src/datasets/__init__.py\n@@ -14,7 +14,7 @@\n \n __version__ = \"3.6.0.dev0\"\n \n-from .arrow_dataset import Dataset\n+from .arrow_dataset import Column, Dataset\n from .arrow_reader import ReadInstruction\n from .builder import ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n from .combine import concatenate_datasets, interleave_datasets\n@@ -30,7 +30,7 @@\n     get_dataset_infos,\n     get_dataset_split_names,\n )\n-from .iterable_dataset import IterableDataset\n+from .iterable_dataset import IterableColumn, IterableDataset\n from .load import load_dataset, load_dataset_builder, load_from_disk\n from .splits import (\n     NamedSplit,\ndiff --git a/src/datasets/arrow_dataset.py b/src/datasets/arrow_dataset.py\nindex 62be84f78cb..5c27a1ae0e4 100644\n--- a/src/datasets/arrow_dataset.py\n+++ b/src/datasets/arrow_dataset.py\n@@ -627,6 +627,48 @@ class NonExistentDatasetError(Exception):\n     pass\n \n \n+class Column(Sequence_):\n+    \"\"\"An iterable for a specific column of an [`Dataset`].\"\"\"\n+\n+    def __init__(self, source: Union[\"Dataset\", \"Column\"], column_name: str):\n+        self.source = source\n+        self.column_name = column_name\n+        if not isinstance(source.features, dict) or column_name not in source.features:\n+            raise ValueError(f\"Column '{column_name}' doesn't exist.\")\n+        self.features = source.features[column_name]\n+\n+    def __iter__(self) -> Iterator[Any]:\n+        if isinstance(self.source, Dataset):\n+            source = self.source._fast_select_column(self.column_name)\n+        for example in source:\n+            yield example[self.column_name]\n+\n+    def __getitem__(self, key: Union[int, str, list[int]]) -> Any:\n+        if isinstance(key, str):\n+            return Column(self, key)\n+        elif isinstance(self.source, Dataset):\n+            return self.source._fast_select_column(self.column_name)[key][self.column_name]\n+        elif isinstance(key, int):\n+            return self.source[key][self.column_name]\n+        else:\n+            return [item[self.column_name] for item in self.source[key]]\n+\n+    def __len__(self) -> int:\n+        return len(self.source)\n+\n+    def __repr__(self):\n+        return \"Column(\" + repr(list(self[:5])) + \")\"\n+\n+    def __str__(self):\n+        return \"Column(\" + str(list(self[:5])) + \")\"\n+\n+    def __eq__(self, value):\n+        if isinstance(value, Column):\n+            return list(self) == list(value)\n+        else:\n+            return value == list(self)\n+\n+\n class Dataset(DatasetInfoMixin, IndexableMixin, TensorflowDatasetMixin):\n     \"\"\"A Dataset backed by an Arrow table.\"\"\"\n \n@@ -2354,6 +2396,13 @@ def select_columns(self, column_names: Union[str, list[str]], new_fingerprint: O\n         dataset._fingerprint = new_fingerprint\n         return dataset\n \n+    @transmit_format\n+    def _fast_select_column(self, column_name: str) -> \"Dataset\":\n+        dataset = copy.copy(self)\n+        dataset._data = dataset._data.select([column_name])\n+        dataset._info = DatasetInfo(features=Features({column_name: self._info.features[column_name]}))\n+        return dataset\n+\n     def __len__(self):\n         \"\"\"Number of rows in the dataset.\n \n@@ -2776,6 +2825,9 @@ def __getitem__(self, key: str) -> list:  # noqa: F811\n \n     def __getitem__(self, key):  # noqa: F811\n         \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\n+        if isinstance(key, str):\n+            if self._format_type is None or self._format_type not in (\"arrow\", \"pandas\", \"polars\"):\n+                return Column(self, key)\n         return self._getitem(key)\n \n     def __getitems__(self, keys: list) -> list:\n", "test_patch": "diff --git a/tests/features/test_array_xd.py b/tests/features/test_array_xd.py\nindex 8a50823b996..8eb9e4e0242 100644\n--- a/tests/features/test_array_xd.py\n+++ b/tests/features/test_array_xd.py\n@@ -173,7 +173,7 @@ def get_dict_examples(self, shape_1, shape_2):\n         }\n \n     def _check_getitem_output_type(self, dataset, shape_1, shape_2, first_matrix):\n-        matrix_column = dataset[\"matrix\"]\n+        matrix_column = dataset[\"matrix\"][:]\n         self.assertIsInstance(matrix_column, list)\n         self.assertIsInstance(matrix_column[0], list)\n         self.assertIsInstance(matrix_column[0][0], list)\n@@ -192,7 +192,7 @@ def _check_getitem_output_type(self, dataset, shape_1, shape_2, first_matrix):\n         self.assertTupleEqual(np.array(matrix_field_of_first_two_examples).shape, (2, *shape_2))\n \n         with dataset.formatted_as(\"numpy\"):\n-            self.assertTupleEqual(dataset[\"matrix\"].shape, (2, *shape_2))\n+            self.assertTupleEqual(dataset[\"matrix\"][:].shape, (2, *shape_2))\n             self.assertEqual(dataset[0][\"matrix\"].shape, shape_2)\n             self.assertTupleEqual(dataset[:2][\"matrix\"].shape, (2, *shape_2))\n \ndiff --git a/tests/features/test_audio.py b/tests/features/test_audio.py\nindex 695e0502712..547c16e27e1 100644\n--- a/tests/features/test_audio.py\n+++ b/tests/features/test_audio.py\n@@ -4,7 +4,7 @@\n import pyarrow as pa\n import pytest\n \n-from datasets import Dataset, concatenate_datasets, load_dataset\n+from datasets import Column, Dataset, concatenate_datasets, load_dataset\n from datasets.features import Audio, Features, Sequence, Value\n \n from ..utils import (\n@@ -292,7 +292,7 @@ def test_dataset_with_audio_feature_with_none():\n     assert isinstance(batch[\"audio\"], list) and all(item is None for item in batch[\"audio\"])\n     column = dset[\"audio\"]\n     assert len(column) == 1\n-    assert isinstance(column, list) and all(item is None for item in column)\n+    assert isinstance(column, Column) and all(item is None for item in column)\n \n     # nested tests\n \ndiff --git a/tests/features/test_features.py b/tests/features/test_features.py\nindex 6234d7ede62..3e2d36cc77a 100644\n--- a/tests/features/test_features.py\n+++ b/tests/features/test_features.py\n@@ -9,7 +9,7 @@\n import pytest\n \n from datasets import Array2D\n-from datasets.arrow_dataset import Dataset\n+from datasets.arrow_dataset import Column, Dataset\n from datasets.features import Audio, ClassLabel, Features, Image, LargeList, Sequence, Value\n from datasets.features.features import (\n     _align_features,\n@@ -492,7 +492,7 @@ def test_dataset_feature_with_none(feature):\n     assert isinstance(batch[\"col\"], list) and all(item is None for item in batch[\"col\"])\n     column = dset[\"col\"]\n     assert len(column) == 1\n-    assert isinstance(column, list) and all(item is None for item in column)\n+    assert isinstance(column, Column) and all(item is None for item in column)\n \n     # nested tests\n \ndiff --git a/tests/features/test_image.py b/tests/features/test_image.py\nindex 50cff254f11..d639bf84ac5 100644\n--- a/tests/features/test_image.py\n+++ b/tests/features/test_image.py\n@@ -9,7 +9,7 @@\n import pyarrow as pa\n import pytest\n \n-from datasets import Dataset, Features, Image, Sequence, Value, concatenate_datasets, load_dataset\n+from datasets import Column, Dataset, Features, Image, Sequence, Value, concatenate_datasets, load_dataset\n from datasets.features.image import encode_np_array, image_to_bytes\n \n from ..utils import require_pil\n@@ -149,7 +149,7 @@ def test_dataset_with_image_feature(shared_datadir):\n     assert batch[\"image\"][0].mode == \"RGB\"\n     column = dset[\"image\"]\n     assert len(column) == 1\n-    assert isinstance(column, list) and all(isinstance(item, PIL.Image.Image) for item in column)\n+    assert isinstance(column, Column) and all(isinstance(item, PIL.Image.Image) for item in column)\n     assert os.path.samefile(column[0].filename, image_path)\n     assert column[0].format == \"JPEG\"\n     assert column[0].size == (640, 480)\n@@ -182,7 +182,7 @@ def test_dataset_with_image_feature_from_pil_image(infer_feature, shared_datadir\n     assert batch[\"image\"][0].mode == \"RGB\"\n     column = dset[\"image\"]\n     assert len(column) == 1\n-    assert isinstance(column, list) and all(isinstance(item, PIL.Image.Image) for item in column)\n+    assert isinstance(column, Column) and all(isinstance(item, PIL.Image.Image) for item in column)\n     assert os.path.samefile(column[0].filename, image_path)\n     assert column[0].format == \"JPEG\"\n     assert column[0].size == (640, 480)\n@@ -215,7 +215,7 @@ def test_dataset_with_image_feature_from_np_array():\n     assert batch[\"image\"][0].size == (640, 480)\n     column = dset[\"image\"]\n     assert len(column) == 1\n-    assert isinstance(column, list) and all(isinstance(item, PIL.Image.Image) for item in column)\n+    assert isinstance(column, Column) and all(isinstance(item, PIL.Image.Image) for item in column)\n     np.testing.assert_array_equal(np.array(column[0]), image_array)\n     assert column[0].filename == \"\"\n     assert column[0].format in [\"PNG\", \"TIFF\"]\n@@ -250,7 +250,7 @@ def test_dataset_with_image_feature_tar_jpg(tar_jpg_path):\n     assert batch[\"image\"][0].mode == \"RGB\"\n     column = dset[\"image\"]\n     assert len(column) == 1\n-    assert isinstance(column, list) and all(isinstance(item, PIL.Image.Image) for item in column)\n+    assert isinstance(column, Column) and all(isinstance(item, PIL.Image.Image) for item in column)\n     assert column[0].filename == \"\"\n     assert column[0].format == \"JPEG\"\n     assert column[0].size == (640, 480)\n@@ -271,7 +271,7 @@ def test_dataset_with_image_feature_with_none():\n     assert isinstance(batch[\"image\"], list) and all(item is None for item in batch[\"image\"])\n     column = dset[\"image\"]\n     assert len(column) == 1\n-    assert isinstance(column, list) and all(item is None for item in column)\n+    assert isinstance(column, Column) and all(item is None for item in column)\n \n     # nested tests\n \n@@ -527,8 +527,8 @@ def test_formatted_dataset_with_image_feature(shared_datadir):\n         assert batch[\"image\"].shape == (1, 480, 640, 3)\n         column = dset[\"image\"]\n         assert len(column) == 2\n-        assert isinstance(column, np.ndarray)\n-        assert column.shape == (2, 480, 640, 3)\n+        assert isinstance(column[:], np.ndarray)\n+        assert column[:].shape == (2, 480, 640, 3)\n \n     with dset.formatted_as(\"pandas\"):\n         item = dset[0]\ndiff --git a/tests/features/test_video.py b/tests/features/test_video.py\nindex a1008a67a48..b8185f955df 100644\n--- a/tests/features/test_video.py\n+++ b/tests/features/test_video.py\n@@ -1,6 +1,6 @@\n import pytest\n \n-from datasets import Dataset, Features, Video\n+from datasets import Column, Dataset, Features, Video\n \n from ..utils import require_torchvision\n \n@@ -53,7 +53,7 @@ def test_dataset_with_video_feature(shared_datadir):\n     assert isinstance(next(batch[\"video\"][0])[\"data\"], torch.Tensor)\n     column = dset[\"video\"]\n     assert len(column) == 1\n-    assert isinstance(column, list) and all(isinstance(item, VideoReader) for item in column)\n+    assert isinstance(column, Column) and all(isinstance(item, VideoReader) for item in column)\n     assert next(column[0])[\"data\"].shape == (3, 50, 66)\n     assert isinstance(next(column[0])[\"data\"], torch.Tensor)\n \ndiff --git a/tests/test_arrow_dataset.py b/tests/test_arrow_dataset.py\nindex 99d2ea74117..8e365462197 100644\n--- a/tests/test_arrow_dataset.py\n+++ b/tests/test_arrow_dataset.py\n@@ -399,9 +399,9 @@ def test_set_format_numpy_multiple_columns(self, in_memory):\n                 self.assertEqual(len(dset[0]), 1)\n                 self.assertIsInstance(dset[0][\"col_1\"], np.int64)\n                 self.assertEqual(dset[0][\"col_1\"].item(), 3)\n-                self.assertIsInstance(dset[\"col_1\"], np.ndarray)\n-                self.assertListEqual(list(dset[\"col_1\"].shape), [4])\n-                np.testing.assert_array_equal(dset[\"col_1\"], np.array([3, 2, 1, 0]))\n+                self.assertIsInstance(dset[\"col_1\"][:], np.ndarray)\n+                self.assertListEqual(list(dset[\"col_1\"][:].shape), [4])\n+                np.testing.assert_array_equal(dset[\"col_1\"][:], np.array([3, 2, 1, 0]))\n                 self.assertNotEqual(dset._fingerprint, fingerprint)\n \n                 dset.reset_format()\n@@ -409,8 +409,8 @@ def test_set_format_numpy_multiple_columns(self, in_memory):\n                     self.assertEqual(len(dset[0]), 1)\n                     self.assertIsInstance(dset[0][\"col_1\"], np.int64)\n                     self.assertEqual(dset[0][\"col_1\"].item(), 3)\n-                    self.assertIsInstance(dset[\"col_1\"], np.ndarray)\n-                    self.assertListEqual(list(dset[\"col_1\"].shape), [4])\n+                    self.assertIsInstance(dset[\"col_1\"][:], np.ndarray)\n+                    self.assertListEqual(list(dset[\"col_1\"][:].shape), [4])\n                     np.testing.assert_array_equal(dset[\"col_1\"], np.array([3, 2, 1, 0]))\n \n                 self.assertEqual(dset.format[\"type\"], None)\n@@ -438,7 +438,7 @@ def test_set_format_torch(self, in_memory):\n                 dset.set_format(type=\"torch\", columns=[\"col_1\"])\n                 self.assertEqual(len(dset[0]), 1)\n                 self.assertIsInstance(dset[0][\"col_1\"], torch.Tensor)\n-                self.assertIsInstance(dset[\"col_1\"], torch.Tensor)\n+                self.assertIsInstance(dset[\"col_1\"][:], torch.Tensor)\n                 self.assertListEqual(list(dset[0][\"col_1\"].shape), [])\n                 self.assertEqual(dset[0][\"col_1\"].item(), 3)\n \n@@ -450,13 +450,13 @@ def test_set_format_torch(self, in_memory):\n                 dset.set_format(type=\"torch\")\n                 self.assertEqual(len(dset[0]), 3)\n                 self.assertIsInstance(dset[0][\"col_1\"], torch.Tensor)\n-                self.assertIsInstance(dset[\"col_1\"], torch.Tensor)\n+                self.assertIsInstance(dset[\"col_1\"][:], torch.Tensor)\n                 self.assertListEqual(list(dset[0][\"col_1\"].shape), [])\n                 self.assertEqual(dset[0][\"col_1\"].item(), 3)\n                 self.assertIsInstance(dset[0][\"col_2\"], str)\n                 self.assertEqual(dset[0][\"col_2\"], \"a\")\n                 self.assertIsInstance(dset[0][\"col_3\"], torch.Tensor)\n-                self.assertIsInstance(dset[\"col_3\"], torch.Tensor)\n+                self.assertIsInstance(dset[\"col_3\"][:], torch.Tensor)\n                 self.assertListEqual(list(dset[0][\"col_3\"].shape), [])\n \n     @require_tf\n@@ -571,7 +571,7 @@ def test_class_encode_column(self, in_memory):\n                 with dset.class_encode_column(\"col_1\") as casted_dset:\n                     self.assertIsInstance(casted_dset.features[\"col_1\"], ClassLabel)\n                     self.assertListEqual(casted_dset.features[\"col_1\"].names, [\"0\", \"1\", \"2\", \"3\"])\n-                    self.assertListEqual(casted_dset[\"col_1\"], [3, 2, 1, 0])\n+                    self.assertListEqual(casted_dset[\"col_1\"][:], [3, 2, 1, 0])\n                     self.assertNotEqual(casted_dset._fingerprint, dset._fingerprint)\n                     self.assertNotEqual(casted_dset, dset)\n                     assert_arrow_metadata_are_synced_with_dataset_features(casted_dset)\n@@ -579,7 +579,7 @@ def test_class_encode_column(self, in_memory):\n                 with dset.class_encode_column(\"col_2\") as casted_dset:\n                     self.assertIsInstance(casted_dset.features[\"col_2\"], ClassLabel)\n                     self.assertListEqual(casted_dset.features[\"col_2\"].names, [\"a\", \"b\", \"c\", \"d\"])\n-                    self.assertListEqual(casted_dset[\"col_2\"], [0, 1, 2, 3])\n+                    self.assertListEqual(casted_dset[\"col_2\"][:], [0, 1, 2, 3])\n                     self.assertNotEqual(casted_dset._fingerprint, dset._fingerprint)\n                     self.assertNotEqual(casted_dset, dset)\n                     assert_arrow_metadata_are_synced_with_dataset_features(casted_dset)\n@@ -587,7 +587,7 @@ def test_class_encode_column(self, in_memory):\n                 with dset.class_encode_column(\"col_3\") as casted_dset:\n                     self.assertIsInstance(casted_dset.features[\"col_3\"], ClassLabel)\n                     self.assertListEqual(casted_dset.features[\"col_3\"].names, [\"False\", \"True\"])\n-                    self.assertListEqual(casted_dset[\"col_3\"], [0, 1, 0, 1])\n+                    self.assertListEqual(casted_dset[\"col_3\"][:], [0, 1, 0, 1])\n                     self.assertNotEqual(casted_dset._fingerprint, dset._fingerprint)\n                     self.assertNotEqual(casted_dset, dset)\n                     assert_arrow_metadata_are_synced_with_dataset_features(casted_dset)\n@@ -718,7 +718,7 @@ def test_concatenate(self, in_memory):\n             with concatenate_datasets([dset1, dset2, dset3]) as dset_concat:\n                 self.assertTupleEqual((len(dset1), len(dset2), len(dset3)), (3, 3, 2))\n                 self.assertEqual(len(dset_concat), len(dset1) + len(dset2) + len(dset3))\n-                self.assertListEqual(dset_concat[\"id\"], [0, 1, 2, 3, 4, 5, 6, 7])\n+                self.assertListEqual(dset_concat[\"id\"][:], [0, 1, 2, 3, 4, 5, 6, 7])\n                 self.assertEqual(len(dset_concat.cache_files), 0 if in_memory else 3)\n                 self.assertEqual(dset_concat.info.description, \"Dataset1\\n\\nDataset2\")\n             del dset1, dset2, dset3\n@@ -760,7 +760,7 @@ def test_concatenate_with_indices(self, in_memory):\n             with concatenate_datasets([dset3, dset2, dset1]) as dset_concat:\n                 self.assertTupleEqual((len(dset1), len(dset2), len(dset3)), (3, 3, 3))\n                 self.assertEqual(len(dset_concat), len(dset1) + len(dset2) + len(dset3))\n-                self.assertListEqual(dset_concat[\"id\"], [6, 7, 8, 5, 4, 3, 2, 1, 0])\n+                self.assertListEqual(dset_concat[\"id\"][:], [6, 7, 8, 5, 4, 3, 2, 1, 0])\n                 # in_memory = False:\n                 # 3 cache files for the dset_concat._data table\n                 # no cache file for the indices because it's in memory\n@@ -775,9 +775,9 @@ def test_concatenate_with_indices(self, in_memory):\n             with concatenate_datasets([dset1, dset2, dset3], axis=1) as dset_concat:\n                 self.assertTupleEqual((len(dset1), len(dset2), len(dset3)), (3, 3, 3))\n                 self.assertEqual(len(dset_concat), len(dset1))\n-                self.assertListEqual(dset_concat[\"id1\"], [2, 1, 0])\n-                self.assertListEqual(dset_concat[\"id2\"], [5, 4, 3])\n-                self.assertListEqual(dset_concat[\"id3\"], [6, 7, 8])\n+                self.assertListEqual(dset_concat[\"id1\"][:], [2, 1, 0])\n+                self.assertListEqual(dset_concat[\"id2\"][:], [5, 4, 3])\n+                self.assertListEqual(dset_concat[\"id3\"][:], [6, 7, 8])\n                 # in_memory = False:\n                 # 3 cache files for the dset_concat._data table\n                 # no cache file for the indices because it's None\n@@ -789,7 +789,7 @@ def test_concatenate_with_indices(self, in_memory):\n \n             with concatenate_datasets([dset1], axis=1) as dset_concat:\n                 self.assertEqual(len(dset_concat), len(dset1))\n-                self.assertListEqual(dset_concat[\"id1\"], [2, 1, 0])\n+                self.assertListEqual(dset_concat[\"id1\"][:], [2, 1, 0])\n                 # in_memory = False:\n                 # 1 cache file for the dset_concat._data table\n                 # no cache file for the indices because it's in memory\n@@ -820,7 +820,7 @@ def test_concatenate_with_indices_from_disk(self, in_memory):\n             with concatenate_datasets([dset3, dset2, dset1]) as dset_concat:\n                 self.assertTupleEqual((len(dset1), len(dset2), len(dset3)), (3, 3, 2))\n                 self.assertEqual(len(dset_concat), len(dset1) + len(dset2) + len(dset3))\n-                self.assertListEqual(dset_concat[\"id\"], [7, 6, 5, 4, 3, 2, 1, 0])\n+                self.assertListEqual(dset_concat[\"id\"][:], [7, 6, 5, 4, 3, 2, 1, 0])\n                 # in_memory = False:\n                 # 3 cache files for the dset_concat._data table, and 1 for the dset_concat._indices_table\n                 # There is only 1 for the indices tables (i1.arrow)\n@@ -876,7 +876,7 @@ def test_concatenate_pickle(self, in_memory):\n                 with pickle.loads(pickle.dumps(dset_concat)) as dset_concat:\n                     self.assertTupleEqual((len(dset1), len(dset2), len(dset3)), (3, 3, 2))\n                     self.assertEqual(len(dset_concat), len(dset1) + len(dset2) + len(dset3))\n-                    self.assertListEqual(dset_concat[\"id\"], [7, 6, 5, 4, 3, 2, 1, 0])\n+                    self.assertListEqual(dset_concat[\"id\"][:], [7, 6, 5, 4, 3, 2, 1, 0])\n                     # in_memory = True: 1 cache file for dset3\n                     # in_memory = False: 2 caches files for dset1 and dset2, and 1 cache file for i1.arrow\n                     self.assertEqual(len(dset_concat.cache_files), 1 if in_memory else 2 + 1)\n@@ -889,7 +889,7 @@ def test_repeat(self, in_memory):\n                 repeated_dset = dset.repeat(3)\n                 column_values_dict = {col: dset[col] for col in dset.column_names}\n                 for col, single_values in column_values_dict.items():\n-                    self.assertListEqual(repeated_dset[col], single_values * 3)\n+                    self.assertListEqual(repeated_dset[col][:], single_values[:] * 3)\n                 del repeated_dset\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -1060,7 +1060,7 @@ def test_map(self, in_memory):\n                         dset_test.features,\n                         Features({\"filename\": Value(\"string\"), \"name\": Value(\"string\"), \"id\": Value(\"int64\")}),\n                     )\n-                    self.assertListEqual(dset_test[\"id\"], list(range(30)))\n+                    self.assertListEqual(dset_test[\"id\"][:], list(range(30)))\n                     self.assertNotEqual(dset_test._fingerprint, fingerprint)\n                     assert_arrow_metadata_are_synced_with_dataset_features(dset_test)\n \n@@ -1085,7 +1085,7 @@ def test_map(self, in_memory):\n                         dset_test_with_indices.features,\n                         Features({\"filename\": Value(\"string\"), \"name\": Value(\"string\"), \"id\": Value(\"int64\")}),\n                     )\n-                    self.assertListEqual(dset_test_with_indices[\"id\"], list(range(30)))\n+                    self.assertListEqual(dset_test_with_indices[\"id\"][:], list(range(30)))\n                     assert_arrow_metadata_are_synced_with_dataset_features(dset_test_with_indices)\n \n         # interrupted\n@@ -1120,7 +1120,7 @@ def func(x, i):\n                         dset_test_with_indices.features,\n                         Features({\"filename\": Value(\"string\"), \"name\": Value(\"string\"), \"id\": Value(\"int64\")}),\n                     )\n-                    self.assertListEqual(dset_test_with_indices[\"id\"], list(range(30)))\n+                    self.assertListEqual(dset_test_with_indices[\"id\"][:], list(range(30)))\n                     assert_arrow_metadata_are_synced_with_dataset_features(dset_test_with_indices)\n \n         # formatted\n@@ -1130,8 +1130,8 @@ def func(x, i):\n                 with dset.map(lambda x: {\"col_1_plus_one\": x[\"col_1\"] + 1}) as dset_test:\n                     self.assertEqual(len(dset_test), 4)\n                     self.assertEqual(dset_test.format[\"type\"], \"numpy\")\n-                    self.assertIsInstance(dset_test[\"col_1\"], np.ndarray)\n-                    self.assertIsInstance(dset_test[\"col_1_plus_one\"], np.ndarray)\n+                    self.assertIsInstance(dset_test[\"col_1\"][:], np.ndarray)\n+                    self.assertIsInstance(dset_test[\"col_1_plus_one\"][:], np.ndarray)\n                     self.assertListEqual(sorted(dset_test[0].keys()), [\"col_1\", \"col_1_plus_one\"])\n                     self.assertListEqual(sorted(dset_test.column_names), [\"col_1\", \"col_1_plus_one\", \"col_2\", \"col_3\"])\n                     assert_arrow_metadata_are_synced_with_dataset_features(dset_test)\n@@ -1172,7 +1172,7 @@ def test_map_multiprocessing(self, in_memory):\n                     self.assertEqual(len(dset_test.cache_files), 0 if in_memory else 2)\n                     if not in_memory:\n                         self.assertIn(\"_of_00002.arrow\", dset_test.cache_files[0][\"filename\"])\n-                    self.assertListEqual(dset_test[\"id\"], list(range(30)))\n+                    self.assertListEqual(dset_test[\"id\"][:], list(range(30)))\n                     self.assertNotEqual(dset_test._fingerprint, fingerprint)\n                     assert_arrow_metadata_are_synced_with_dataset_features(dset_test)\n \n@@ -1188,7 +1188,7 @@ def test_map_multiprocessing(self, in_memory):\n                         Features({\"filename\": Value(\"string\"), \"id\": Value(\"int64\")}),\n                     )\n                     self.assertEqual(len(dset_test.cache_files), 0 if in_memory else 2)\n-                    self.assertListEqual(dset_test[\"id\"], list(range(2)))\n+                    self.assertListEqual(dset_test[\"id\"][:], list(range(2)))\n                     self.assertNotEqual(dset_test._fingerprint, fingerprint)\n                     assert_arrow_metadata_are_synced_with_dataset_features(dset_test)\n \n@@ -1203,7 +1203,7 @@ def test_map_multiprocessing(self, in_memory):\n                         Features({\"filename\": Value(\"string\"), \"id\": Value(\"int64\")}),\n                     )\n                     self.assertEqual(len(dset_test.cache_files), 0 if in_memory else 3)\n-                    self.assertListEqual(dset_test[\"id\"], list(range(30)))\n+                    self.assertListEqual(dset_test[\"id\"][:], list(range(30)))\n                     self.assertNotEqual(dset_test._fingerprint, fingerprint)\n                     assert_arrow_metadata_are_synced_with_dataset_features(dset_test)\n \n@@ -1218,7 +1218,7 @@ def test_map_multiprocessing(self, in_memory):\n                         Features({\"filename\": Value(\"string\"), \"rank\": Value(\"int64\")}),\n                     )\n                     self.assertEqual(len(dset_test.cache_files), 0 if in_memory else 3)\n-                    self.assertListEqual(dset_test[\"rank\"], [0] * 10 + [1] * 10 + [2] * 10)\n+                    self.assertListEqual(dset_test[\"rank\"][:], [0] * 10 + [1] * 10 + [2] * 10)\n                     self.assertNotEqual(dset_test._fingerprint, fingerprint)\n                     assert_arrow_metadata_are_synced_with_dataset_features(dset_test)\n \n@@ -1235,8 +1235,8 @@ def test_map_multiprocessing(self, in_memory):\n                         Features({\"filename\": Value(\"string\"), \"id\": Value(\"int64\"), \"rank\": Value(\"int64\")}),\n                     )\n                     self.assertEqual(len(dset_test.cache_files), 0 if in_memory else 3)\n-                    self.assertListEqual(dset_test[\"id\"], list(range(30)))\n-                    self.assertListEqual(dset_test[\"rank\"], [0] * 10 + [1] * 10 + [2] * 10)\n+                    self.assertListEqual(dset_test[\"id\"][:], list(range(30)))\n+                    self.assertListEqual(dset_test[\"rank\"][:], [0] * 10 + [1] * 10 + [2] * 10)\n                     self.assertNotEqual(dset_test._fingerprint, fingerprint)\n                     assert_arrow_metadata_are_synced_with_dataset_features(dset_test)\n \n@@ -1256,7 +1256,7 @@ def test_map_multiprocessing(self, in_memory):\n                         Features({\"filename\": Value(\"string\"), \"id\": Value(\"int64\")}),\n                     )\n                     self.assertEqual(len(dset_test.cache_files), 0 if in_memory else 2)\n-                    self.assertListEqual(dset_test[\"id\"], list(range(30)))\n+                    self.assertListEqual(dset_test[\"id\"][:], list(range(30)))\n                     self.assertNotEqual(dset_test._fingerprint, fingerprint)\n                     self.assertEqual(dset_test._fingerprint, new_fingerprint)\n                     assert_arrow_metadata_are_synced_with_dataset_features(dset_test)\n@@ -1275,7 +1275,7 @@ def test_map_multiprocessing(self, in_memory):\n                         Features({\"filename\": Value(\"string\"), \"id\": Value(\"int64\")}),\n                     )\n                     self.assertEqual(len(dset_test.cache_files), 0 if in_memory else 2)\n-                    self.assertListEqual(dset_test[\"id\"], list(range(30)))\n+                    self.assertListEqual(dset_test[\"id\"][:], list(range(30)))\n                     self.assertNotEqual(dset_test._fingerprint, fingerprint)\n                     assert_arrow_metadata_are_synced_with_dataset_features(dset_test)\n \n@@ -1923,9 +1923,9 @@ def test_filter_with_indices_mapping(self, in_memory):\n             dset = Dataset.from_dict({\"col\": [0, 1, 2]})\n             with self._to(in_memory, tmp_dir, dset) as dset:\n                 with dset.filter(lambda x: x[\"col\"] > 0) as dset:\n-                    self.assertListEqual(dset[\"col\"], [1, 2])\n+                    self.assertListEqual(dset[\"col\"][:], [1, 2])\n                     with dset.filter(lambda x: x[\"col\"] < 2) as dset:\n-                        self.assertListEqual(dset[\"col\"], [1])\n+                        self.assertListEqual(dset[\"col\"][:], [1])\n \n     def test_filter_empty(self, in_memory):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -1947,9 +1947,9 @@ def test_filter_batched(self, in_memory):\n             dset = Dataset.from_dict({\"col\": [0, 1, 2]})\n             with self._to(in_memory, tmp_dir, dset) as dset:\n                 with dset.filter(lambda x: [i > 0 for i in x[\"col\"]], batched=True) as dset:\n-                    self.assertListEqual(dset[\"col\"], [1, 2])\n+                    self.assertListEqual(dset[\"col\"][:], [1, 2])\n                     with dset.filter(lambda x: [i < 2 for i in x[\"col\"]], batched=True) as dset:\n-                        self.assertListEqual(dset[\"col\"], [1])\n+                        self.assertListEqual(dset[\"col\"][:], [1])\n \n     def test_filter_input_columns(self, in_memory):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -1957,8 +1957,8 @@ def test_filter_input_columns(self, in_memory):\n             with self._to(in_memory, tmp_dir, dset) as dset:\n                 with dset.filter(lambda x: x > 0, input_columns=[\"col_1\"]) as filtered_dset:\n                     self.assertListEqual(filtered_dset.column_names, dset.column_names)\n-                    self.assertListEqual(filtered_dset[\"col_1\"], [1, 2])\n-                    self.assertListEqual(filtered_dset[\"col_2\"], [\"b\", \"c\"])\n+                    self.assertListEqual(filtered_dset[\"col_1\"][:], [1, 2])\n+                    self.assertListEqual(filtered_dset[\"col_2\"][:], [\"b\", \"c\"])\n \n     def test_filter_fn_kwargs(self, in_memory):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -2323,12 +2323,12 @@ def test_shuffle(self, in_memory):\n                     # Reproducibility\n                     tmp_file = os.path.join(tmp_dir, \"test_2.arrow\")\n                     with dset.shuffle(seed=1234, indices_cache_file_name=tmp_file) as dset_shuffled_2:\n-                        self.assertListEqual(dset_shuffled[\"filename\"], dset_shuffled_2[\"filename\"])\n+                        self.assertSequenceEqual(dset_shuffled[\"filename\"], dset_shuffled_2[\"filename\"])\n \n                 # Compatible with temp_seed\n                 with temp_seed(42), dset.shuffle() as d1:\n                     with temp_seed(42), dset.shuffle() as d2, dset.shuffle() as d3:\n-                        self.assertListEqual(d1[\"filename\"], d2[\"filename\"])\n+                        self.assertSequenceEqual(d1[\"filename\"], d2[\"filename\"])\n                         self.assertEqual(d1._fingerprint, d2._fingerprint)\n                         self.assertNotEqual(d3[\"filename\"], d2[\"filename\"])\n                         self.assertNotEqual(d3._fingerprint, d2._fingerprint)\n@@ -2885,7 +2885,7 @@ def test_format_vectors(self, in_memory):\n             for col in columns:\n                 self.assertIsInstance(dset[0][col], (tf.Tensor, tf.RaggedTensor))\n                 self.assertIsInstance(dset[:2][col], (tf.Tensor, tf.RaggedTensor))\n-                self.assertIsInstance(dset[col], (tf.Tensor, tf.RaggedTensor))\n+                self.assertIsInstance(dset[col][:2], (tf.Tensor, tf.RaggedTensor))\n             self.assertTupleEqual(tuple(dset[:2][\"vec\"].shape), (2, 3))\n             self.assertTupleEqual(tuple(dset[\"vec\"][:2].shape), (2, 3))\n \n@@ -2894,10 +2894,10 @@ def test_format_vectors(self, in_memory):\n             self.assertIsNotNone(dset[:2])\n             self.assertIsInstance(dset[0][\"filename\"], np.str_)\n             self.assertIsInstance(dset[:2][\"filename\"], np.ndarray)\n-            self.assertIsInstance(dset[\"filename\"], np.ndarray)\n+            self.assertIsInstance(dset[\"filename\"][:], np.ndarray)\n             self.assertIsInstance(dset[0][\"vec\"], np.ndarray)\n             self.assertIsInstance(dset[:2][\"vec\"], np.ndarray)\n-            self.assertIsInstance(dset[\"vec\"], np.ndarray)\n+            self.assertIsInstance(dset[\"vec\"][:2], np.ndarray)\n             self.assertTupleEqual(dset[:2][\"vec\"].shape, (2, 3))\n             self.assertTupleEqual(dset[\"vec\"][:2].shape, (2, 3))\n \n@@ -2940,7 +2940,7 @@ def test_format_ragged_vectors(self, in_memory):\n             for col in columns:\n                 self.assertIsInstance(dset[0][col], tf.Tensor)\n                 self.assertIsInstance(dset[:2][col], tf.RaggedTensor if col == \"vec\" else tf.Tensor)\n-                self.assertIsInstance(dset[col], tf.RaggedTensor if col == \"vec\" else tf.Tensor)\n+                self.assertIsInstance(dset[col][:2], tf.RaggedTensor if col == \"vec\" else tf.Tensor)\n             # dim is None for ragged vectors in tensorflow\n             self.assertListEqual(dset[:2][\"vec\"].shape.as_list(), [2, None])\n             self.assertListEqual(dset[\"vec\"][:2].shape.as_list(), [2, None])\n@@ -2950,10 +2950,10 @@ def test_format_ragged_vectors(self, in_memory):\n             self.assertIsNotNone(dset[:2])\n             self.assertIsInstance(dset[0][\"filename\"], np.str_)\n             self.assertIsInstance(dset[:2][\"filename\"], np.ndarray)\n-            self.assertIsInstance(dset[\"filename\"], np.ndarray)\n+            self.assertIsInstance(dset[\"filename\"][:2], np.ndarray)\n             self.assertIsInstance(dset[0][\"vec\"], np.ndarray)\n             self.assertIsInstance(dset[:2][\"vec\"], np.ndarray)\n-            self.assertIsInstance(dset[\"vec\"], np.ndarray)\n+            self.assertIsInstance(dset[\"vec\"][:], np.ndarray)\n             # array is flat for ragged vectors in numpy\n             self.assertTupleEqual(dset[:2][\"vec\"].shape, (2,))\n             self.assertTupleEqual(dset[\"vec\"][:2].shape, (2,))\n@@ -2963,7 +2963,7 @@ def test_format_ragged_vectors(self, in_memory):\n             self.assertIsNotNone(dset[:2])\n             self.assertIsInstance(dset[0][\"filename\"], str)\n             self.assertIsInstance(dset[:2][\"filename\"], list)\n-            self.assertIsInstance(dset[\"filename\"], list)\n+            self.assertIsInstance(dset[\"filename\"][:2], list)\n             self.assertIsInstance(dset[0][\"vec\"], torch.Tensor)\n             self.assertIsInstance(dset[:2][\"vec\"][0], torch.Tensor)\n             self.assertIsInstance(dset[\"vec\"][0], torch.Tensor)\n@@ -3274,22 +3274,22 @@ def test_from_pandas(self):\n         data = {\"col_1\": [3, 2, 1, 0], \"col_2\": [\"a\", \"b\", \"c\", \"d\"]}\n         df = pd.DataFrame.from_dict(data)\n         with Dataset.from_pandas(df) as dset:\n-            self.assertListEqual(dset[\"col_1\"], data[\"col_1\"])\n-            self.assertListEqual(dset[\"col_2\"], data[\"col_2\"])\n+            self.assertSequenceEqual(dset[\"col_1\"], data[\"col_1\"])\n+            self.assertSequenceEqual(dset[\"col_2\"], data[\"col_2\"])\n             self.assertListEqual(list(dset.features.keys()), [\"col_1\", \"col_2\"])\n             self.assertDictEqual(dset.features, Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"string\")}))\n \n         features = Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"string\")})\n         with Dataset.from_pandas(df, features=features) as dset:\n-            self.assertListEqual(dset[\"col_1\"], data[\"col_1\"])\n-            self.assertListEqual(dset[\"col_2\"], data[\"col_2\"])\n+            self.assertSequenceEqual(dset[\"col_1\"], data[\"col_1\"])\n+            self.assertSequenceEqual(dset[\"col_2\"], data[\"col_2\"])\n             self.assertListEqual(list(dset.features.keys()), [\"col_1\", \"col_2\"])\n             self.assertDictEqual(dset.features, Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"string\")}))\n \n         features = Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"string\")})\n         with Dataset.from_pandas(df, features=features, info=DatasetInfo(features=features)) as dset:\n-            self.assertListEqual(dset[\"col_1\"], data[\"col_1\"])\n-            self.assertListEqual(dset[\"col_2\"], data[\"col_2\"])\n+            self.assertSequenceEqual(dset[\"col_1\"], data[\"col_1\"])\n+            self.assertSequenceEqual(dset[\"col_2\"], data[\"col_2\"])\n             self.assertListEqual(list(dset.features.keys()), [\"col_1\", \"col_2\"])\n             self.assertDictEqual(dset.features, Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"string\")}))\n \n@@ -3303,22 +3303,22 @@ def test_from_polars(self):\n         data = {\"col_1\": [3, 2, 1, 0], \"col_2\": [\"a\", \"b\", \"c\", \"d\"]}\n         df = pl.from_dict(data)\n         with Dataset.from_polars(df) as dset:\n-            self.assertListEqual(dset[\"col_1\"], data[\"col_1\"])\n-            self.assertListEqual(dset[\"col_2\"], data[\"col_2\"])\n+            self.assertSequenceEqual(dset[\"col_1\"], data[\"col_1\"])\n+            self.assertSequenceEqual(dset[\"col_2\"], data[\"col_2\"])\n             self.assertListEqual(list(dset.features.keys()), [\"col_1\", \"col_2\"])\n             self.assertDictEqual(dset.features, Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"large_string\")}))\n \n         features = Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"large_string\")})\n         with Dataset.from_polars(df, features=features) as dset:\n-            self.assertListEqual(dset[\"col_1\"], data[\"col_1\"])\n-            self.assertListEqual(dset[\"col_2\"], data[\"col_2\"])\n+            self.assertSequenceEqual(dset[\"col_1\"], data[\"col_1\"])\n+            self.assertSequenceEqual(dset[\"col_2\"], data[\"col_2\"])\n             self.assertListEqual(list(dset.features.keys()), [\"col_1\", \"col_2\"])\n             self.assertDictEqual(dset.features, Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"large_string\")}))\n \n         features = Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"large_string\")})\n         with Dataset.from_polars(df, features=features, info=DatasetInfo(features=features)) as dset:\n-            self.assertListEqual(dset[\"col_1\"], data[\"col_1\"])\n-            self.assertListEqual(dset[\"col_2\"], data[\"col_2\"])\n+            self.assertSequenceEqual(dset[\"col_1\"], data[\"col_1\"])\n+            self.assertSequenceEqual(dset[\"col_2\"], data[\"col_2\"])\n             self.assertListEqual(list(dset.features.keys()), [\"col_1\", \"col_2\"])\n             self.assertDictEqual(dset.features, Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"large_string\")}))\n \n@@ -3328,9 +3328,9 @@ def test_from_polars(self):\n     def test_from_dict(self):\n         data = {\"col_1\": [3, 2, 1, 0], \"col_2\": [\"a\", \"b\", \"c\", \"d\"], \"col_3\": pa.array([True, False, True, False])}\n         with Dataset.from_dict(data) as dset:\n-            self.assertListEqual(dset[\"col_1\"], data[\"col_1\"])\n-            self.assertListEqual(dset[\"col_2\"], data[\"col_2\"])\n-            self.assertListEqual(dset[\"col_3\"], data[\"col_3\"].to_pylist())\n+            self.assertSequenceEqual(dset[\"col_1\"], data[\"col_1\"])\n+            self.assertSequenceEqual(dset[\"col_2\"], data[\"col_2\"])\n+            self.assertSequenceEqual(dset[\"col_3\"], data[\"col_3\"].to_pylist())\n             self.assertListEqual(list(dset.features.keys()), [\"col_1\", \"col_2\", \"col_3\"])\n             self.assertDictEqual(\n                 dset.features, Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"string\"), \"col_3\": Value(\"bool\")})\n@@ -3338,9 +3338,9 @@ def test_from_dict(self):\n \n         features = Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"string\"), \"col_3\": Value(\"bool\")})\n         with Dataset.from_dict(data, features=features) as dset:\n-            self.assertListEqual(dset[\"col_1\"], data[\"col_1\"])\n-            self.assertListEqual(dset[\"col_2\"], data[\"col_2\"])\n-            self.assertListEqual(dset[\"col_3\"], data[\"col_3\"].to_pylist())\n+            self.assertSequenceEqual(dset[\"col_1\"], data[\"col_1\"])\n+            self.assertSequenceEqual(dset[\"col_2\"], data[\"col_2\"])\n+            self.assertSequenceEqual(dset[\"col_3\"], data[\"col_3\"].to_pylist())\n             self.assertListEqual(list(dset.features.keys()), [\"col_1\", \"col_2\", \"col_3\"])\n             self.assertDictEqual(\n                 dset.features, Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"string\"), \"col_3\": Value(\"bool\")})\n@@ -3348,9 +3348,9 @@ def test_from_dict(self):\n \n         features = Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"string\"), \"col_3\": Value(\"bool\")})\n         with Dataset.from_dict(data, features=features, info=DatasetInfo(features=features)) as dset:\n-            self.assertListEqual(dset[\"col_1\"], data[\"col_1\"])\n-            self.assertListEqual(dset[\"col_2\"], data[\"col_2\"])\n-            self.assertListEqual(dset[\"col_3\"], data[\"col_3\"].to_pylist())\n+            self.assertSequenceEqual(dset[\"col_1\"], data[\"col_1\"])\n+            self.assertSequenceEqual(dset[\"col_2\"], data[\"col_2\"])\n+            self.assertSequenceEqual(dset[\"col_3\"], data[\"col_3\"].to_pylist())\n             self.assertListEqual(list(dset.features.keys()), [\"col_1\", \"col_2\", \"col_3\"])\n             self.assertDictEqual(\n                 dset.features, Features({\"col_1\": Value(\"int64\"), \"col_2\": Value(\"string\"), \"col_3\": Value(\"bool\")})\n@@ -3359,9 +3359,9 @@ def test_from_dict(self):\n         features = Features({\"col_1\": Value(\"string\"), \"col_2\": Value(\"string\"), \"col_3\": Value(\"int32\")})\n         with Dataset.from_dict(data, features=features) as dset:\n             # the integers are converted to strings\n-            self.assertListEqual(dset[\"col_1\"], [str(x) for x in data[\"col_1\"]])\n-            self.assertListEqual(dset[\"col_2\"], data[\"col_2\"])\n-            self.assertListEqual(dset[\"col_3\"], [int(x) for x in data[\"col_3\"].to_pylist()])\n+            self.assertSequenceEqual(dset[\"col_1\"], [str(x) for x in data[\"col_1\"]])\n+            self.assertSequenceEqual(dset[\"col_2\"], data[\"col_2\"])\n+            self.assertSequenceEqual(dset[\"col_3\"], [int(x) for x in data[\"col_3\"].to_pylist()])\n             self.assertListEqual(list(dset.features.keys()), [\"col_1\", \"col_2\", \"col_3\"])\n             self.assertDictEqual(\n                 dset.features, Features({\"col_1\": Value(\"string\"), \"col_2\": Value(\"string\"), \"col_3\": Value(\"int32\")})\n@@ -3382,7 +3382,7 @@ def test_concatenate_mixed_memory_and_disk(self):\n             ):\n                 with concatenate_datasets([dset1, dset2, dset3]) as concatenated_dset:\n                     self.assertEqual(len(concatenated_dset), len(dset1) + len(dset2) + len(dset3))\n-                    self.assertListEqual(concatenated_dset[\"id\"], dset1[\"id\"] + dset2[\"id\"] + dset3[\"id\"])\n+                    self.assertSequenceEqual(concatenated_dset[\"id\"], dset1[\"id\"][:] + dset2[\"id\"][:] + dset3[\"id\"][:])\n \n     @require_transformers\n     @pytest.mark.integration\ndiff --git a/tests/test_dataset_dict.py b/tests/test_dataset_dict.py\nindex 72f84071e1c..651f2ec822e 100644\n--- a/tests/test_dataset_dict.py\n+++ b/tests/test_dataset_dict.py\n@@ -443,7 +443,7 @@ def test_shuffle(self):\n             dsets_shuffled = dsets.shuffle(\n                 seeds=seeds, indices_cache_file_names=indices_cache_file_names, load_from_cache_file=False\n             )\n-            self.assertListEqual(dsets_shuffled[\"train\"][\"filename\"], dsets_shuffled[\"test\"][\"filename\"])\n+            self.assertSequenceEqual(dsets_shuffled[\"train\"][\"filename\"], dsets_shuffled[\"test\"][\"filename\"])\n \n             self.assertEqual(len(dsets_shuffled[\"train\"]), 30)\n             self.assertEqual(dsets_shuffled[\"train\"][0][\"filename\"], \"my_name-train_028\")\n@@ -459,7 +459,7 @@ def test_shuffle(self):\n             dsets_shuffled_2 = dsets.shuffle(\n                 seeds=seeds, indices_cache_file_names=indices_cache_file_names_2, load_from_cache_file=False\n             )\n-            self.assertListEqual(dsets_shuffled[\"train\"][\"filename\"], dsets_shuffled_2[\"train\"][\"filename\"])\n+            self.assertSequenceEqual(dsets_shuffled[\"train\"][\"filename\"], dsets_shuffled_2[\"train\"][\"filename\"])\n \n             seeds = {\n                 \"train\": 1234,\n@@ -601,8 +601,8 @@ def test_align_labels_with_mapping(self):\n             }\n         )\n         dsets = dsets.align_labels_with_mapping(label2id, \"input_labels\")\n-        self.assertListEqual(train_expected_labels, dsets[\"train\"][\"input_labels\"])\n-        self.assertListEqual(test_expected_labels, dsets[\"test\"][\"input_labels\"])\n+        self.assertListEqual(train_expected_labels, dsets[\"train\"][\"input_labels\"][:])\n+        self.assertListEqual(test_expected_labels, dsets[\"test\"][\"input_labels\"][:])\n         train_aligned_label_names = [\n             dsets[\"train\"].features[\"input_labels\"].int2str(idx) for idx in dsets[\"train\"][\"input_labels\"]\n         ]\n", "problem_statement": "Add some iteration method on a dataset column (specific for inference)\n**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is.\r\n\r\nCurrently, `dataset[\"audio\"]` will load EVERY element in the dataset in RAM, which can be quite big for an audio dataset.\r\nHaving an iterator (or sequence) type of object, would make inference with `transformers` 's `pipeline` easier to use and not so memory hungry.\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\nFor a non breaking change:\r\n\r\n```python\r\nfor audio in dataset.iterate(\"audio\"):\r\n    # {\"array\": np.array(...), \"sampling_rate\":...}\r\n```\r\n\r\nFor a  breaking change solution (not necessary), changing the type of `dataset[\"audio\"]` to a sequence type so that\r\n\r\n```python\r\npipe = pipeline(model=\"...\")\r\nfor out in pipe(dataset[\"audio\"]):\r\n    # {\"text\":....}\r\n```\r\ncould work\r\n\r\n**Describe alternatives you've considered**\r\nA clear and concise description of any alternative solutions or features you've considered.\r\n\r\n```python\r\ndef iterate(dataset, key):\r\n    for item in dataset:\r\n        yield dataset[key]\r\n\r\nfor out in pipeline(iterate(dataset, \"audio\")):\r\n    # {\"array\": ...}\r\n```\r\n\r\nThis works but requires the helper function which feels slightly clunky.\r\n\r\n**Additional context**\r\nAdd any other context about the feature request here.\r\n\r\nThe context is actually to showcase better integration between  `pipeline` and `datasets` in the Quicktour demo: https://github.com/huggingface/transformers/pull/16723/files\r\n\r\n@lhoestq \r\n\n", "hints_text": "Thanks for the suggestion ! I agree it would be nice to have something directly in `datasets` to do something as simple as that\r\n\r\ncc @albertvillanova @mariosasko @polinaeterna What do you think if we have something similar to pandas `Series` that wouldn't bring everything in memory when doing `dataset[\"audio\"]` ? Currently it returns a list with all the decoded audio data in memory.\r\n\r\nIt would be a breaking change though, since `isinstance(dataset[\"audio\"], list)` wouldn't work anymore, but we could implement a `Sequence` so that `dataset[\"audio\"][0]` still works and only loads one item in memory.\r\n\r\nYour alternative suggestion with `iterate` is also sensible, though maybe less satisfactory in terms of experience IMO\nI agree that current behavior (decoding all audio file sin the dataset when accessing `dataset[\"audio\"]`) is not useful, IMHO. Indeed in our docs, we are constantly warning our collaborators not to do that.\r\n\r\nTherefore I upvote for a \"useful\" behavior of `dataset[\"audio\"]`. I don't think the breaking change is important in this case, as I guess no many people use it with its current behavior. Therefore, for me it seems reasonable to return a generator (instead of an in-memeory list) for \"special\" features, like Audio/Image.\r\n\r\n@lhoestq on the other hand I don't understand your proposal about Pandas-like... \nI recall I had the same idea while working on the `Image` feature, so I agree implementing something similar to `pd.Series` that lazily brings elements in memory would be beneficial.\n@lhoestq @mariosasko Could you please give a link to that new feature of `pandas.Series`? As far as I remember since I worked with pandas for more than 6 years, there was no lazy in-memory feature; it was everything in-memory; that was the reason why other frameworks were created, like Vaex or Dask, e.g. \nYea pandas doesn't do lazy loading. I was referring to pandas.Series to say that they have a dedicated class to represent a column ;)\n\n", "all_hints_text": "Thanks for the suggestion ! I agree it would be nice to have something directly in `datasets` to do something as simple as that\r\n\r\ncc @albertvillanova @mariosasko @polinaeterna What do you think if we have something similar to pandas `Series` that wouldn't bring everything in memory when doing `dataset[\"audio\"]` ? Currently it returns a list with all the decoded audio data in memory.\r\n\r\nIt would be a breaking change though, since `isinstance(dataset[\"audio\"], list)` wouldn't work anymore, but we could implement a `Sequence` so that `dataset[\"audio\"][0]` still works and only loads one item in memory.\r\n\r\nYour alternative suggestion with `iterate` is also sensible, though maybe less satisfactory in terms of experience IMO\nI agree that current behavior (decoding all audio file sin the dataset when accessing `dataset[\"audio\"]`) is not useful, IMHO. Indeed in our docs, we are constantly warning our collaborators not to do that.\r\n\r\nTherefore I upvote for a \"useful\" behavior of `dataset[\"audio\"]`. I don't think the breaking change is important in this case, as I guess no many people use it with its current behavior. Therefore, for me it seems reasonable to return a generator (instead of an in-memeory list) for \"special\" features, like Audio/Image.\r\n\r\n@lhoestq on the other hand I don't understand your proposal about Pandas-like... \nI recall I had the same idea while working on the `Image` feature, so I agree implementing something similar to `pd.Series` that lazily brings elements in memory would be beneficial.\n@lhoestq @mariosasko Could you please give a link to that new feature of `pandas.Series`? As far as I remember since I worked with pandas for more than 6 years, there was no lazy in-memory feature; it was everything in-memory; that was the reason why other frameworks were created, like Vaex or Dask, e.g. \nYea pandas doesn't do lazy loading. I was referring to pandas.Series to say that they have a dedicated class to represent a column ;)\n\n", "commit_urls": ["https://github.com/huggingface/datasets/commit/d1ea63535c96723215eb9fb27bf1347cbd086e0b", "https://github.com/huggingface/datasets/commit/74a355389963099cb6936f4adfc48006b7e1d0e0", "https://github.com/huggingface/datasets/commit/f042a29d9cabcbf25cc0c13ad1b096c0f2ce889f", "https://github.com/huggingface/datasets/commit/30091f44e0cac61344a854cb3c8b6977934761d8", "https://github.com/huggingface/datasets/commit/02b1f04a17db180a1121c5f7a6a3e3d9b6b2261a", "https://github.com/huggingface/datasets/commit/b2a587f89b16c1f4c81bdcaef33488896fabaaec", "https://github.com/huggingface/datasets/commit/65bfb43b44212c4812e137e37c3997a09b764b22", "https://github.com/huggingface/datasets/commit/e97106489ffe143000c34ae25f2d0067665a96e1"], "created_at": "2025-06-13T12:12:57Z", "classification": "Efficiency"}
{"repo": "deepset-ai/haystack", "pull_number": 9622, "instance_id": "deepset-ai__haystack-9622", "issue_numbers": [9577], "base_commit": "b9b1652fd47bb4ea6587505604a0451480f7e6de", "patch": "diff --git a/haystack/document_stores/in_memory/document_store.py b/haystack/document_stores/in_memory/document_store.py\nindex b5dc67c444..67d4b1d635 100644\n--- a/haystack/document_stores/in_memory/document_store.py\n+++ b/haystack/document_stores/in_memory/document_store.py\n@@ -68,6 +68,7 @@ def __init__(  # pylint: disable=too-many-positional-arguments\n         embedding_similarity_function: Literal[\"dot_product\", \"cosine\"] = \"dot_product\",\n         index: Optional[str] = None,\n         async_executor: Optional[ThreadPoolExecutor] = None,\n+        return_embedding: bool = True,\n     ):\n         \"\"\"\n         Initializes the DocumentStore.\n@@ -85,6 +86,7 @@ def __init__(  # pylint: disable=too-many-positional-arguments\n         :param async_executor:\n             Optional ThreadPoolExecutor to use for async calls. If not provided, a single-threaded\n             executor will be initialized and used.\n+        :param return_embedding: Whether to return the embedding of the retrieved Documents. Default is True.\n         \"\"\"\n         self.bm25_tokenization_regex = bm25_tokenization_regex\n         self.tokenizer = re.compile(bm25_tokenization_regex).findall\n@@ -118,6 +120,7 @@ def __init__(  # pylint: disable=too-many-positional-arguments\n             if async_executor is None\n             else async_executor\n         )\n+        self.return_embedding = return_embedding\n \n     def __del__(self):\n         \"\"\"\n@@ -355,6 +358,7 @@ def to_dict(self) -> Dict[str, Any]:\n             bm25_parameters=self.bm25_parameters,\n             embedding_similarity_function=self.embedding_similarity_function,\n             index=self.index,\n+            return_embedding=self.return_embedding,\n         )\n \n     @classmethod\n@@ -426,8 +430,15 @@ def filter_documents(self, filters: Optional[Dict[str, Any]] = None) -> List[Doc\n                 raise ValueError(\n                     \"Invalid filter syntax. See https://docs.haystack.deepset.ai/docs/metadata-filtering for details.\"\n                 )\n-            return [doc for doc in self.storage.values() if document_matches_filter(filters=filters, document=doc)]\n-        return list(self.storage.values())\n+            docs = [doc for doc in self.storage.values() if document_matches_filter(filters=filters, document=doc)]\n+        else:\n+            docs = list(self.storage.values())\n+\n+        if not self.return_embedding:\n+            for doc in docs:\n+                doc.embedding = None\n+\n+        return docs\n \n     def write_documents(self, documents: List[Document], policy: DuplicatePolicy = DuplicatePolicy.NONE) -> int:\n         \"\"\"\n@@ -542,7 +553,12 @@ def bm25_retrieval(\n \n             doc_fields = doc.to_dict()\n             doc_fields[\"score\"] = score\n+\n+            if not self.return_embedding and \"embedding\" in doc_fields:\n+                doc_fields.pop(\"embedding\")\n+\n             return_document = Document.from_dict(doc_fields)\n+\n             return_documents.append(return_document)\n \n         return return_documents\n@@ -553,7 +569,7 @@ def embedding_retrieval(  # pylint: disable=too-many-positional-arguments\n         filters: Optional[Dict[str, Any]] = None,\n         top_k: int = 10,\n         scale_score: bool = False,\n-        return_embedding: bool = False,\n+        return_embedding: Optional[bool] = False,\n     ) -> List[Document]:\n         \"\"\"\n         Retrieves documents that are most similar to the query embedding using a vector similarity metric.\n@@ -562,14 +578,24 @@ def embedding_retrieval(  # pylint: disable=too-many-positional-arguments\n         :param filters: A dictionary with filters to narrow down the search space.\n         :param top_k: The number of top documents to retrieve. Default is 10.\n         :param scale_score: Whether to scale the scores of the retrieved Documents. Default is False.\n-        :param return_embedding: Whether to return the embedding of the retrieved Documents. Default is False.\n+        :param return_embedding: Whether to return the embedding of the retrieved Documents.\n+            If not provided, the value of the `return_embedding` parameter set at component\n+            initialization will be used. Default is False.\n         :returns: A list of the top_k documents most relevant to the query.\n         \"\"\"\n         if len(query_embedding) == 0 or not isinstance(query_embedding[0], float):\n             raise ValueError(\"query_embedding should be a non-empty list of floats.\")\n \n-        filters = filters or {}\n-        all_documents = self.filter_documents(filters=filters)\n+        if filters:\n+            if \"operator\" not in filters and \"conditions\" not in filters:\n+                raise ValueError(\n+                    \"Invalid filter syntax. See https://docs.haystack.deepset.ai/docs/metadata-filtering for details.\"\n+                )\n+            all_documents = [\n+                doc for doc in self.storage.values() if document_matches_filter(filters=filters, document=doc)\n+            ]\n+        else:\n+            all_documents = list(self.storage.values())\n \n         documents_with_embeddings = [doc for doc in all_documents if doc.embedding is not None]\n         if len(documents_with_embeddings) == 0:\n@@ -587,12 +613,14 @@ def embedding_retrieval(  # pylint: disable=too-many-positional-arguments\n             embedding=query_embedding, documents=documents_with_embeddings, scale_score=scale_score\n         )\n \n+        resolved_return_embedding = self.return_embedding if return_embedding is None else return_embedding\n+\n         # create Documents with the similarity score for the top k results\n         top_documents = []\n         for doc, score in sorted(zip(documents_with_embeddings, scores), key=lambda x: x[1], reverse=True)[:top_k]:\n             doc_fields = doc.to_dict()\n             doc_fields[\"score\"] = score\n-            if return_embedding is False:\n+            if resolved_return_embedding is False:\n                 doc_fields[\"embedding\"] = None\n             top_documents.append(Document.from_dict(doc_fields))\n \ndiff --git a/releasenotes/notes/update-in-memory-document-store-17f555695caf9d52.yaml b/releasenotes/notes/update-in-memory-document-store-17f555695caf9d52.yaml\nnew file mode 100644\nindex 0000000000..92748412d3\n--- /dev/null\n+++ b/releasenotes/notes/update-in-memory-document-store-17f555695caf9d52.yaml\n@@ -0,0 +1,6 @@\n+---\n+enhancements:\n+  - |\n+    - Added `return_embedding` parameter inside `InMemoryDocumentStore::init` method.\n+    - Updated methods `bm25_retrieval`, and `filter_documents` to use `self.return_embedding` to determine whether embeddings are returned.\n+    - Updated tests (test_in_memory & test_in_memory_embedding_retriever) to reflect the changes in the `InMemoryDocumentStore`.\n", "test_patch": "diff --git a/test/components/retrievers/test_sentence_window_retriever.py b/test/components/retrievers/test_sentence_window_retriever.py\nindex e4ebaead95..65e21360fd 100644\n--- a/test/components/retrievers/test_sentence_window_retriever.py\n+++ b/test/components/retrievers/test_sentence_window_retriever.py\n@@ -76,6 +76,7 @@ def test_to_dict(self):\n                         \"bm25_tokenization_regex\": \"(?u)\\\\b\\\\w\\\\w+\\\\b\",\n                         \"embedding_similarity_function\": \"dot_product\",\n                         \"index\": ANY,\n+                        \"return_embedding\": True,\n                     },\n                 },\n                 \"window_size\": 3,\ndiff --git a/test/document_stores/test_in_memory.py b/test/document_stores/test_in_memory.py\nindex 118336eb54..eb48cb8373 100644\n--- a/test/document_stores/test_in_memory.py\n+++ b/test/document_stores/test_in_memory.py\n@@ -43,6 +43,7 @@ def test_to_dict(self):\n                 \"bm25_parameters\": {},\n                 \"embedding_similarity_function\": \"dot_product\",\n                 \"index\": store.index,\n+                \"return_embedding\": True,\n             },\n         }\n \n@@ -53,6 +54,7 @@ def test_to_dict_with_custom_init_parameters(self):\n             bm25_parameters={\"key\": \"value\"},\n             embedding_similarity_function=\"cosine\",\n             index=\"my_cool_index\",\n+            return_embedding=True,\n         )\n         data = store.to_dict()\n         assert data == {\n@@ -63,6 +65,7 @@ def test_to_dict_with_custom_init_parameters(self):\n                 \"bm25_parameters\": {\"key\": \"value\"},\n                 \"embedding_similarity_function\": \"cosine\",\n                 \"index\": \"my_cool_index\",\n+                \"return_embedding\": True,\n             },\n         }\n \n@@ -250,6 +253,47 @@ def test_bm25_retrieval_default_filter(self, document_store: InMemoryDocumentSto\n         results = document_store.bm25_retrieval(query=\"doesn't matter, top_k is 10\", top_k=10)\n         assert len(results) == 0\n \n+    def test_embedding_retrieval_return_embedding_false_on_store(self):\n+        # Initialize InMemoryDocumentStore with return_embedding=False\n+        docstore = InMemoryDocumentStore(embedding_similarity_function=\"cosine\", return_embedding=False)\n+        docs = [\n+            Document(content=\"Hello world\", embedding=[0.1, 0.2, 0.3, 0.4]),\n+            Document(content=\"Haystack supports multiple languages\", embedding=[1.0, 1.0, 1.0, 1.0]),\n+        ]\n+        docstore.write_documents(docs)\n+\n+        # embedding_retrieval should not return embeddings in the documents\n+        results = docstore.embedding_retrieval(query_embedding=[0.1, 0.1, 0.1, 0.1], top_k=2)\n+        assert all(doc.embedding is None for doc in results)\n+\n+        # bm25_retrieval should also not return embeddings\n+        bm25_results = docstore.bm25_retrieval(query=\"languages\", top_k=2)\n+        assert all(doc.embedding is None for doc in bm25_results)\n+\n+        # filter_documents should not return embeddings\n+        filtered_docs = docstore.filter_documents()\n+        assert all(doc.embedding is None for doc in filtered_docs)\n+\n+    def test_embedding_retrieval_override_return_embedding(self):\n+        docstore = InMemoryDocumentStore(embedding_similarity_function=\"cosine\", return_embedding=False)\n+        docs = [\n+            Document(content=\"Hello world\", embedding=[0.1, 0.2, 0.3, 0.4]),\n+            Document(content=\"Haystack supports multiple languages\", embedding=[1.0, 1.0, 1.0, 1.0]),\n+        ]\n+        docstore.write_documents(docs)\n+\n+        # Overriding return_embedding to True should return embeddings\n+        # Query for the embedding that matches both documents by cosine similarity\n+        results_with_embedding = docstore.embedding_retrieval(\n+            query_embedding=[0.1, 0.2, 0.3, 0.4], top_k=2, return_embedding=True\n+        )\n+\n+        # Assert that the retrieved documents have the expected embeddings\n+        assert len(results_with_embedding) == 2\n+\n+        assert results_with_embedding[0].embedding in ([1.0, 1.0, 1.0, 1.0], [0.1, 0.2, 0.3, 0.4])\n+        assert results_with_embedding[1].embedding in ([1.0, 1.0, 1.0, 1.0], [0.1, 0.2, 0.3, 0.4])\n+\n     def test_embedding_retrieval(self):\n         docstore = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n         # Tests if the embedding retrieval method returns the correct document based on the input query embedding.\n", "problem_statement": "Add `return_embedding` param to `FilterRetriever`\n**Is your feature request related to a problem? Please describe.**\nIt's not possible to remove the embeddings from the Documents that the FilterRetriever returns. These embeddings fill up the memory and context limit for LLMs if retrieved docs are passed directly to an LLM in Agent\n\n**Describe the solution you'd like**\nhave `return_embedding` param for FilterRetriever like we have in other retrievers\n\n**Describe alternatives you've considered**\nBased on the architectures, there are different approaches to eliminate this, such as using `outputs_to_string` in Agent\n\n**Additional context**\nN/A\n\n", "hints_text": "Maybe to add to this, not only should we add this option to the FilterRetriever but we should also add it to `InMemoryDocumentStore.bm25_retrieval` since often times we use BM25 retrieval even on documents that have embeddings. \n\nOne thing I noticed is that `filter_documents` used by `FilterRetriever` is a part of our `DocumentStore` protocol. So we will need to think if we add `return_embedding` to the protocol and force all downstream DocumentStore's to update or find a more flexible way to expand the protocol. \n\n**Update:** \nAs an alternative we could follow a similar approach as is used by the `OpenSearchDocumentStore` where we add a `return_embedding` parameter to the init method of `InMemoryDocumentStore` and use that within the necessary methods like bm25 retrieval, filter documents, and embedding retrieval\nhello here,\n\nHappy to help with this issue :)\n@sjrl \n\n> Update:\n> As an alternative we could follow a similar approach as is used by the OpenSearchDocumentStore where we add a\n> return_embedding parameter to the init method of InMemoryDocumentStore and use that within the necessary methods \n> like bm25 retrieval, filter documents, and embedding retrieval\n\nTotally agree with this \ud83e\udd1d , then we can be controlling it from a centralized position in the document store \ud83d\udc4d \n\n", "all_hints_text": "Maybe to add to this, not only should we add this option to the FilterRetriever but we should also add it to `InMemoryDocumentStore.bm25_retrieval` since often times we use BM25 retrieval even on documents that have embeddings. \n\nOne thing I noticed is that `filter_documents` used by `FilterRetriever` is a part of our `DocumentStore` protocol. So we will need to think if we add `return_embedding` to the protocol and force all downstream DocumentStore's to update or find a more flexible way to expand the protocol. \n\n**Update:** \nAs an alternative we could follow a similar approach as is used by the `OpenSearchDocumentStore` where we add a `return_embedding` parameter to the init method of `InMemoryDocumentStore` and use that within the necessary methods like bm25 retrieval, filter documents, and embedding retrieval\nhello here,\n\nHappy to help with this issue :)\n@sjrl \n\n> Update:\n> As an alternative we could follow a similar approach as is used by the OpenSearchDocumentStore where we add a\n> return_embedding parameter to the init method of InMemoryDocumentStore and use that within the necessary methods \n> like bm25 retrieval, filter documents, and embedding retrieval\n\nTotally agree with this \ud83e\udd1d , then we can be controlling it from a centralized position in the document store \ud83d\udc4d \n\n", "commit_urls": ["https://github.com/deepset-ai/haystack/commit/048544428117671d5bbdd6cbd51d759058182feb", "https://github.com/deepset-ai/haystack/commit/0f9dd431759bd62dabdf79aca3e0ae1bb19ffb70", "https://github.com/deepset-ai/haystack/commit/0d8fd01692a0a2f853589677d649725caca747ca", "https://github.com/deepset-ai/haystack/commit/326226ad66365f78d8721044817fc4d04267fc23", "https://github.com/deepset-ai/haystack/commit/f1b11c3b1f693a297c418becbfa3b79b381e40b5", "https://github.com/deepset-ai/haystack/commit/2b43f04232a1cb3ed2b36abf66f79ec1d23f59f2", "https://github.com/deepset-ai/haystack/commit/bee66b1abdf7211ef0112c6a559c04f1c89113fb", "https://github.com/deepset-ai/haystack/commit/136eed04c5f1097d4f65799ada7ce726035ce324", "https://github.com/deepset-ai/haystack/commit/07e4e1af2c0da189ca9c3e8f67d48e32a03a1949", "https://github.com/deepset-ai/haystack/commit/4cff023c95b221027053896b77b34f8ff6bad237", "https://github.com/deepset-ai/haystack/commit/f2e533d9721dd5ab83c3df107818847104f69d2f", "https://github.com/deepset-ai/haystack/commit/a077ca133e4f4c7e3a6ebaf6cef98cf8c843952b", "https://github.com/deepset-ai/haystack/commit/510c0a7acad2c13fc36e41f49364851ab8402c2c", "https://github.com/deepset-ai/haystack/commit/7c63e697d5a0ac28032228a43148a51d8eebe1b1", "https://github.com/deepset-ai/haystack/commit/56535ef421e16506f30c27ca8c612c240969051a", "https://github.com/deepset-ai/haystack/commit/dafd32f74d6d5c43bccac0b6070093cec6eab96b", "https://github.com/deepset-ai/haystack/commit/a0e85c9bee09ea1ac888eae90f65edbf55ff03a2", "https://github.com/deepset-ai/haystack/commit/e0d7a1260f51e555c6a1c6938063ebdb5b6b01a8", "https://github.com/deepset-ai/haystack/commit/1dea8bd84f3b62d4699986ff842cef300176aaa5", "https://github.com/deepset-ai/haystack/commit/318cf7bbaf850644ce99a82d182514ec8ed00dab", "https://github.com/deepset-ai/haystack/commit/6520a636aa6953576d2406e39495a38e9d229a02", "https://github.com/deepset-ai/haystack/commit/9b40fb860c79f0a5789cbefc348f49a36ebc2577"], "created_at": "2025-07-20T19:58:11Z", "classification": "Efficiency"}
{"repo": "jax-ml/jax", "pull_number": 29997, "instance_id": "jax-ml__jax-29997", "issue_numbers": [29774], "base_commit": "6d6ef7d6b13f7c04b120138e4a0b82fa0c0f4c30", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex ee199b88d75c..4569a5e56d9f 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -20,6 +20,7 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n   * Added `jax.P` which is an alias for `jax.sharding.PartitionSpec`.\n   * `jax.extend.core.primitives.pjit_p` has been renamed to `jit_p`, although\n     `pjit_p` is still exported as an alias for the time being.\n+  * Added {func}`jax.tree.reduce_associative`.\n \n * Breaking changes:\n   * {func}`jax.jit` now requires `fun` to be passed by position, and additional\ndiff --git a/docs/jax.tree.rst b/docs/jax.tree.rst\nindex 1a0ddaec86d0..7bb4c3e557c5 100644\n--- a/docs/jax.tree.rst\n+++ b/docs/jax.tree.rst\n@@ -20,6 +20,7 @@ List of Functions\n    map\n    map_with_path\n    reduce\n+   reduce_associative\n    structure\n    transpose\n    unflatten\ndiff --git a/docs/jax.tree_util.rst b/docs/jax.tree_util.rst\nindex a17a947af320..664fa0f3ce9e 100644\n--- a/docs/jax.tree_util.rst\n+++ b/docs/jax.tree_util.rst\n@@ -42,6 +42,7 @@ These APIs are now accessed via :mod:`jax.tree`.\n    tree_leaves\n    tree_map\n    tree_reduce\n+   tree_reduce_associative\n    tree_structure\n    tree_transpose\n    tree_unflatten\ndiff --git a/jax/_src/tree.py b/jax/_src/tree.py\nindex d1d3be41b917..870f74aeb0ed 100644\n--- a/jax/_src/tree.py\n+++ b/jax/_src/tree.py\n@@ -192,12 +192,55 @@ def reduce(function: Callable[[T, Any], T],\n     21\n \n   See Also:\n+    - :func:`jax.tree.reduce_associative`\n     - :func:`jax.tree.leaves`\n     - :func:`jax.tree.map`\n   \"\"\"\n   return tree_util.tree_reduce(function, tree, initializer, is_leaf=is_leaf)\n \n \n+def reduce_associative(\n+    operation: Callable[[T, T], T],\n+    tree: Any,\n+    *,\n+    identity: T | tree_util.Unspecified = tree_util.Unspecified(),\n+    is_leaf: Callable[[Any], bool] | None = None,\n+) -> T:\n+  \"\"\"Perform a reduction over a pytree with an associative binary operation.\n+\n+  This function exploits the fact that the operation is associative to perform\n+  the reduction in parallel (logarithmic depth).\n+\n+  Args:\n+    operation: the associative binary operation\n+    tree: the pytree to reduce\n+    identity: the identity element of the associative binary operation.\n+      This is used only when the tree is empty. It is optional otherwise.\n+    is_leaf: an optionally specified function that will be called at each\n+      flattening step. It should return a boolean, which indicates whether the\n+      flattening should traverse the current object, or if it should be stopped\n+      immediately, with the whole subtree being treated as a leaf.\n+\n+  Returns:\n+    result: the reduced value\n+\n+  Examples:\n+    >>> import jax\n+    >>> import operator\n+    >>> jax.tree.reduce_associative(operator.add, [1, (2, 3), [4, 5, 6]])\n+    21\n+\n+  See Also:\n+    - :func:`jax.tree.reduce`\n+  \"\"\"\n+  return tree_util.tree_reduce_associative(\n+      operation,\n+      tree,\n+      identity=identity,\n+      is_leaf=is_leaf,\n+  )\n+\n+\n def structure(tree: Any,\n               is_leaf: None | (Callable[[Any], bool]) = None) -> tree_util.PyTreeDef:\n   \"\"\"Gets the treedef for a pytree.\ndiff --git a/jax/_src/tree_util.py b/jax/_src/tree_util.py\nindex 48a52c5fb6aa..9a817e8df741 100644\n--- a/jax/_src/tree_util.py\n+++ b/jax/_src/tree_util.py\n@@ -458,6 +458,42 @@ def tree_reduce(function: Callable[[T, Any], T],\n     return functools.reduce(function, tree_leaves(tree, is_leaf=is_leaf), initializer)\n \n \n+class Unspecified:\n+  pass\n+\n+\n+def _parallel_reduce(\n+    sequence: list[T],\n+    operation: Callable[[T, T], T],\n+    identity: T | Unspecified = Unspecified(),\n+) -> T:\n+  length = len(sequence)\n+  if length == 0:\n+    if isinstance(identity, Unspecified):\n+      raise TypeError(\"Must specify identity for parallel reduction of empty sequence.\")\n+    return identity\n+  elif length == 1:\n+    return sequence[0]\n+  else:\n+    index = length // 2\n+    a = _parallel_reduce(sequence[:index], operation, identity)\n+    b = _parallel_reduce(sequence[index:], operation, identity)\n+    return operation(a, b)\n+\n+\n+@export\n+def tree_reduce_associative(\n+    operation: Callable[[T, T], T],\n+    tree: Any,\n+    *,\n+    identity: T | Unspecified = Unspecified(),\n+    is_leaf: Callable[[Any], bool] | None = None,\n+) -> T:\n+  \"\"\"Alias of :func:`jax.tree.reduce_associative`.\"\"\"\n+  sequence = tree_leaves(tree, is_leaf=is_leaf)\n+  return _parallel_reduce(sequence, operation, identity)\n+\n+\n @export\n def tree_all(tree: Any, *, is_leaf: Callable[[Any], bool] | None = None) -> bool:\n   \"\"\"Alias of :func:`jax.tree.all`.\"\"\"\ndiff --git a/jax/tree.py b/jax/tree.py\nindex 03ca503f3a41..854ee9c8ea7e 100644\n--- a/jax/tree.py\n+++ b/jax/tree.py\n@@ -27,6 +27,7 @@\n     map_with_path as map_with_path,\n     map as map,\n     reduce as reduce,\n+    reduce_associative as reduce_associative,\n     structure as structure,\n     transpose as transpose,\n     unflatten as unflatten,\ndiff --git a/jax/tree_util.py b/jax/tree_util.py\nindex a16f0c60c2e5..86a161d0f81b 100644\n--- a/jax/tree_util.py\n+++ b/jax/tree_util.py\n@@ -65,6 +65,7 @@\n     tree_map_with_path as tree_map_with_path,\n     tree_map as tree_map,\n     tree_reduce as tree_reduce,\n+    tree_reduce_associative as tree_reduce_associative,\n     tree_structure as tree_structure,\n     tree_transpose as tree_transpose,\n     tree_unflatten as tree_unflatten,\n", "test_patch": "diff --git a/tests/tree_util_test.py b/tests/tree_util_test.py\nindex a1e3ccbe265f..73e4bd71cd3c 100644\n--- a/tests/tree_util_test.py\n+++ b/tests/tree_util_test.py\n@@ -499,6 +499,13 @@ def testTreeReduceWithIsLeafArgument(self):\n                                 is_leaf=lambda l: isinstance(l, tuple))\n     self.assertEqual(out, (1, 2, 3, 4, 5, 6))\n \n+  def testTreeReduceAssociativeWithIsLeafArgument(self):\n+    out = tree_util.tree_reduce_associative(\n+        lambda x, y: x + y, [(1, 2), [(3, 4), (5, 6)]],\n+        is_leaf=lambda l: isinstance(l, tuple),\n+    )\n+    self.assertEqual(out, (1, 2, 3, 4, 5, 6))\n+\n   @parameterized.parameters(\n       tree_util.tree_leaves,\n       lambda tree, is_leaf: tree_util.tree_flatten(tree, is_leaf)[0])\n@@ -1501,6 +1508,23 @@ def test_tree_reduce_is_leaf(self):\n       tree_util.tree_reduce(func, obj, is_leaf=is_leaf),\n     )\n \n+  def test_tree_reduce_associative(self):\n+    func = lambda a, b: a + b\n+    obj = [1, 2, (3, 4)]\n+    self.assertEqual(\n+      jax.tree.reduce_associative(func, obj),\n+      tree_util.tree_reduce_associative(func, obj),\n+    )\n+\n+  def test_tree_reduce_associative_is_leaf(self):\n+    func = lambda a, b: a + b\n+    obj = [(1, 2), (3, 4)]\n+    is_leaf = lambda x: isinstance(x, tuple)\n+    self.assertEqual(\n+      jax.tree.reduce_associative(func, obj, is_leaf=is_leaf),\n+      tree_util.tree_reduce_associative(func, obj, is_leaf=is_leaf),\n+    )\n+\n   def test_tree_structure(self):\n     obj = [1, 2, (3, 4)]\n     self.assertEqual(\n", "problem_statement": "Add associative / parallel version of jax.tree.reduce\n[jax.tree.reduce](https://docs.jax.dev/en/latest/_autosummary/jax.tree.reduce.html) / [jax.tree_util.tree_reduce](https://docs.jax.dev/en/latest/_autosummary/jax.tree_util.tree_reduce.html) [uses](https://github.com/jax-ml/jax/blob/172d44b8c4d08c5e5cf597a82da0ba347024edcc/jax/_src/tree_util.py#L450) [functools.reduce](https://docs.python.org/3/library/functools.html#functools.reduce). Letting n be the number of leaves in the tree, this has a [depth / critical path length](https://en.wikipedia.org/wiki/Analysis_of_parallel_algorithms#Definitions) of O(n). Switching to parallel reduction would bring the critical path length down to O(log n). Example:\n\n```python3\nimport operator\n\ndef parallel_reduce(seq, op, identity):\n    if len(seq) == 0:\n        return identity\n    elif len(seq) == 1:\n        return seq[0]\n    else:\n        n = len(seq) // 2\n        a = parallel_reduce(seq[:n], op, identity)\n        b = parallel_reduce(seq[n:], op, identity)\n        return op(a, b)\n\ndef parallel_sum(seq):\n    return parallel_reduce(seq, operator.add, 0)\n\ndef parallel_prod(seq):\n    return parallel_reduce(seq, operator.mul, 1)\n\ndef parallel_all(seq):\n    return parallel_reduce(seq, operator.and_, True)\n\ndef parallel_any(seq):\n    return parallel_reduce(seq, operator.or_, False)\n```\n\nIf this sounds like a good idea, I can submit a PR for it.\n", "hints_text": "This approach would return incorrect results for reductions that aren\u2019t monoidal, and I don\u2019t know that there\u2019s any general way to detect if the user-supplied reduction meets this requirement.\nBut is any particular order already expected for `jax.tree.reduce` / `jax.tree.leaves`?\n\nIn either case, it could be made opt-in.\n@jakevdp I've realized that this belongs in a separate function, because the type of the `function` argument is different:\n\n```python3\nfunction : A \u00d7 B \u2192 A # reduce\nfunction : A \u00d7 A \u2192 A # parallel / associative reduce\n```\n\nThis is analogous to the difference between [`scan`](https://docs.jax.dev/en/latest/_autosummary/jax.lax.scan.html) and [`associative_scan`](https://docs.jax.dev/en/latest/_autosummary/jax.lax.associative_scan.html).\n\nTherefore, I suggest creating a new function.\n\nRelevant references:\n\n1. [Folding in parallel](https://okmij.org/ftp/Algorithms/map-monoid-reduce.html)\n2. [Parallel sequences: Fold, reduce and scan](https://www.cs.princeton.edu/courses/archive/fall14/cos326/lec/21-seq-divide-conquer.pdf)\n\n", "all_hints_text": "This approach would return incorrect results for reductions that aren\u2019t monoidal, and I don\u2019t know that there\u2019s any general way to detect if the user-supplied reduction meets this requirement.\nBut is any particular order already expected for `jax.tree.reduce` / `jax.tree.leaves`?\n\nIn either case, it could be made opt-in.\n@jakevdp I've realized that this belongs in a separate function, because the type of the `function` argument is different:\n\n```python3\nfunction : A \u00d7 B \u2192 A # reduce\nfunction : A \u00d7 A \u2192 A # parallel / associative reduce\n```\n\nThis is analogous to the difference between [`scan`](https://docs.jax.dev/en/latest/_autosummary/jax.lax.scan.html) and [`associative_scan`](https://docs.jax.dev/en/latest/_autosummary/jax.lax.associative_scan.html).\n\nTherefore, I suggest creating a new function.\n\nRelevant references:\n\n1. [Folding in parallel](https://okmij.org/ftp/Algorithms/map-monoid-reduce.html)\n2. [Parallel sequences: Fold, reduce and scan](https://www.cs.princeton.edu/courses/archive/fall14/cos326/lec/21-seq-divide-conquer.pdf)\n\n", "commit_urls": ["https://github.com/jax-ml/jax/commit/7343e14dce0f5b7fe4c73b6396d7a48a04e448b2"], "created_at": "2025-07-06T17:51:07Z", "classification": "Efficiency"}
{"repo": "BerriAI/litellm", "pull_number": 12251, "instance_id": "BerriAI__litellm-12251", "issue_numbers": [12107], "base_commit": "d720b3d369eca916c87cd044acc6d2b192b8519f", "patch": "diff --git a/litellm/__init__.py b/litellm/__init__.py\nindex e202f6dc4fbd..c66bd25459d5 100644\n--- a/litellm/__init__.py\n+++ b/litellm/__init__.py\n@@ -68,11 +68,15 @@\n from litellm.litellm_core_utils.logging_callback_manager import LoggingCallbackManager\n import httpx\n import dotenv\n+from litellm.llms.custom_httpx.async_client_cleanup import register_async_client_cleanup\n \n litellm_mode = os.getenv(\"LITELLM_MODE\", \"DEV\")  # \"PRODUCTION\", \"DEV\"\n if litellm_mode == \"DEV\":\n     dotenv.load_dotenv()\n \n+# Register async client cleanup to prevent resource leaks\n+register_async_client_cleanup()\n+\n ##################################################\n if set_verbose == True:\n     _turn_on_debug()\n@@ -1123,6 +1127,7 @@ def add_known_models():\n from .llms.nebius.chat.transformation import NebiusConfig\n from .main import *  # type: ignore\n from .integrations import *\n+from .llms.custom_httpx.async_client_cleanup import close_litellm_async_clients\n from .exceptions import (\n     AuthenticationError,\n     InvalidRequestError,\ndiff --git a/litellm/llms/custom_httpx/aiohttp_handler.py b/litellm/llms/custom_httpx/aiohttp_handler.py\nindex 5a1d4208656e..d9fc85877c3f 100644\n--- a/litellm/llms/custom_httpx/aiohttp_handler.py\n+++ b/litellm/llms/custom_httpx/aiohttp_handler.py\n@@ -47,6 +47,11 @@ def _get_async_client_session(\n             self.client_session = aiohttp.ClientSession()\n             return self.client_session\n \n+    async def close(self):\n+        \"\"\"Close the aiohttp client session if it exists.\"\"\"\n+        if self.client_session and not self.client_session.closed:\n+            await self.client_session.close()\n+\n     async def _make_common_async_call(\n         self,\n         async_client_session: Optional[ClientSession],\ndiff --git a/litellm/llms/custom_httpx/async_client_cleanup.py b/litellm/llms/custom_httpx/async_client_cleanup.py\nnew file mode 100644\nindex 000000000000..456025767640\n--- /dev/null\n+++ b/litellm/llms/custom_httpx/async_client_cleanup.py\n@@ -0,0 +1,83 @@\n+\"\"\"\n+Utility functions for cleaning up async HTTP clients to prevent resource leaks.\n+\"\"\"\n+import asyncio\n+\n+\n+async def close_litellm_async_clients():\n+    \"\"\"\n+    Close all cached async HTTP clients to prevent resource leaks.\n+\n+    This function iterates through all cached clients in litellm's in-memory cache\n+    and closes any aiohttp client sessions that are still open.\n+    \"\"\"\n+    # Import here to avoid circular import\n+    import litellm\n+    from litellm.llms.custom_httpx.aiohttp_handler import BaseLLMAIOHTTPHandler\n+\n+    cache_dict = getattr(litellm.in_memory_llm_clients_cache, \"cache_dict\", {})\n+\n+    for key, handler in cache_dict.items():\n+        # Handle BaseLLMAIOHTTPHandler instances (aiohttp_openai provider)\n+        if isinstance(handler, BaseLLMAIOHTTPHandler) and hasattr(handler, \"close\"):\n+            try:\n+                await handler.close()\n+            except Exception:\n+                # Silently ignore errors during cleanup\n+                pass\n+        \n+        # Handle AsyncHTTPHandler instances (used by Gemini and other providers)\n+        elif hasattr(handler, 'client'):\n+            client = handler.client\n+            # Check if the httpx client has an aiohttp transport\n+            if hasattr(client, '_transport') and hasattr(client._transport, 'aclose'):\n+                try:\n+                    await client._transport.aclose()\n+                except Exception:\n+                    # Silently ignore errors during cleanup\n+                    pass\n+            # Also close the httpx client itself\n+            if hasattr(client, 'aclose') and not client.is_closed:\n+                try:\n+                    await client.aclose()\n+                except Exception:\n+                    # Silently ignore errors during cleanup\n+                    pass\n+        \n+        # Handle any other objects with aclose method\n+        elif hasattr(handler, 'aclose'):\n+            try:\n+                await handler.aclose()\n+            except Exception:\n+                # Silently ignore errors during cleanup\n+                pass\n+\n+\n+def register_async_client_cleanup():\n+    \"\"\"\n+    Register the async client cleanup function to run at exit.\n+\n+    This ensures that all async HTTP clients are properly closed when the program exits.\n+    \"\"\"\n+    import atexit\n+\n+    def cleanup_wrapper():\n+        try:\n+            loop = asyncio.get_event_loop()\n+            if loop.is_running():\n+                # Schedule the cleanup coroutine\n+                loop.create_task(close_litellm_async_clients())\n+            else:\n+                # Run the cleanup coroutine\n+                loop.run_until_complete(close_litellm_async_clients())\n+        except Exception:\n+            # If we can't get an event loop or it's already closed, try creating a new one\n+            try:\n+                loop = asyncio.new_event_loop()\n+                loop.run_until_complete(close_litellm_async_clients())\n+                loop.close()\n+            except Exception:\n+                # Silently ignore errors during cleanup\n+                pass\n+\n+    atexit.register(cleanup_wrapper)\n", "test_patch": "diff --git a/tests/test_resource_cleanup.py b/tests/test_resource_cleanup.py\nnew file mode 100644\nindex 000000000000..41d56258be96\n--- /dev/null\n+++ b/tests/test_resource_cleanup.py\n@@ -0,0 +1,116 @@\n+\"\"\"\n+Test that async HTTP clients are properly cleaned up to prevent resource leaks.\n+Issue: https://github.com/BerriAI/litellm/issues/12107\n+\"\"\"\n+import asyncio\n+import os\n+import warnings\n+\n+import pytest\n+\n+import litellm\n+\n+\n+@pytest.mark.asyncio\n+async def test_acompletion_resource_cleanup():\n+    \"\"\"Test that acompletion doesn't leave unclosed client sessions.\"\"\"\n+    # Suppress warnings to check for them later\n+    with warnings.catch_warnings(record=True) as w:\n+        warnings.simplefilter(\"always\")\n+\n+        # Make an async completion call\n+        response = await litellm.acompletion(\n+            model=\"gemini/gemini-2.0-flash-lite-001\",\n+            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n+            mock_response=\"Hi there! How can I help you today?\",\n+        )\n+\n+        # Check that response was received\n+        assert (\n+            response.choices[0].message.content == \"Hi there! How can I help you today?\"\n+        )\n+\n+        # Manually close async clients\n+        await litellm.close_litellm_async_clients()\n+\n+        # Give a small delay for any warnings to appear\n+        await asyncio.sleep(0.1)\n+\n+        # Check for resource warnings\n+        resource_warnings = [\n+            warning\n+            for warning in w\n+            if \"Unclosed\" in str(warning.message)\n+            and (\n+                \"client session\" in str(warning.message)\n+                or \"connector\" in str(warning.message)\n+            )\n+        ]\n+\n+        # Should be no unclosed resource warnings\n+        assert (\n+            len(resource_warnings) == 0\n+        ), f\"Found unclosed resources: {[str(w.message) for w in resource_warnings]}\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_multiple_acompletion_calls_cleanup():\n+    \"\"\"Test that multiple acompletion calls reuse clients and don't leak resources.\"\"\"\n+    with warnings.catch_warnings(record=True) as w:\n+        warnings.simplefilter(\"always\")\n+\n+        # Make multiple async completion calls\n+        for i in range(3):\n+            response = await litellm.acompletion(\n+                model=\"gemini/gemini-2.0-flash-lite-001\",\n+                messages=[{\"role\": \"user\", \"content\": f\"Hello {i}\"}],\n+                mock_response=f\"Response {i}\",\n+            )\n+            assert response.choices[0].message.content == f\"Response {i}\"\n+\n+        # Clean up\n+        await litellm.close_litellm_async_clients()\n+\n+        # Give a small delay for any warnings to appear\n+        await asyncio.sleep(0.1)\n+\n+        # Check for resource warnings\n+        resource_warnings = [\n+            warning\n+            for warning in w\n+            if \"Unclosed\" in str(warning.message)\n+            and (\n+                \"client session\" in str(warning.message)\n+                or \"connector\" in str(warning.message)\n+            )\n+        ]\n+\n+        assert (\n+            len(resource_warnings) == 0\n+        ), f\"Found unclosed resources: {[str(w.message) for w in resource_warnings]}\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_cleanup_function_is_safe_to_call_multiple_times():\n+    \"\"\"Test that the cleanup function can be called multiple times safely.\"\"\"\n+    # This should not raise any errors\n+    await litellm.close_litellm_async_clients()\n+    await litellm.close_litellm_async_clients()\n+    await litellm.close_litellm_async_clients()\n+\n+    # Should still work after multiple cleanups\n+    response = await litellm.acompletion(\n+        model=\"gemini/gemini-2.0-flash-lite-001\",\n+        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n+        mock_response=\"Hi!\",\n+    )\n+    assert response.choices[0].message.content == \"Hi!\"\n+\n+    # Clean up again\n+    await litellm.close_litellm_async_clients()\n+\n+\n+if __name__ == \"__main__\":\n+    # Run the test\n+    asyncio.run(test_acompletion_resource_cleanup())\n+    print(\"\u2705 All tests passed!\")\n", "problem_statement": "[Bug]: Resource leakage when using Gemini models with acompletion\n### What happened?\n\nCode to reproduce problem:\n```\nimport litellm\nimport asyncio\n\nasync def main(): \n    response = await litellm.acompletion(\n        model=\"gemini/gemini-2.0-flash-lite-001\", \n        # model=\"vertex_ai/gemini-2.0-flash-lite-001\", # Same issue with Vertex AI\n        messages=[\n            {\"role\": \"user\", \"content\": \"Hello\"}\n        ],\n    )\n    print(response.choices[0].message.content)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### Relevant log output\n\n```shell\nHi there! How can I help you today?\n\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x12cebea50>\nUnclosed connector\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.73.2\n\n### Twitter / LinkedIn details\n\n_No response_\n", "hints_text": "@ishaan-jaff any context on this? \nI see this issue with non-gemini models as well. For example using `\"anthropic/claude-sonnet-4-20250514\"` with the above script I get the same unclosed connector error\n@colesmcintosh are you able to look into this, this week? \nAfter some digging, I came up with this workaround:\n\n```\nasync def close_litellm_async_clients():\n    for client in litellm.in_memory_llm_clients_cache.cache_dict.values():\n        await client.close()\n```\n\nCan be manually called but for my use case I'll register this atexit.\n\n\n", "all_hints_text": "@ishaan-jaff any context on this? \nI see this issue with non-gemini models as well. For example using `\"anthropic/claude-sonnet-4-20250514\"` with the above script I get the same unclosed connector error\n@colesmcintosh are you able to look into this, this week? \nAfter some digging, I came up with this workaround:\n\n```\nasync def close_litellm_async_clients():\n    for client in litellm.in_memory_llm_clients_cache.cache_dict.values():\n        await client.close()\n```\n\nCan be manually called but for my use case I'll register this atexit.\n\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/5a4d5360e01efe379d96c6a59f6e2870bd0a46a5", "https://github.com/BerriAI/litellm/commit/aba4b192d7452a16e3b721791163f5a28d218ec0", "https://github.com/BerriAI/litellm/commit/264f5eb6dd9161491772d49f091252b780a35180", "https://github.com/BerriAI/litellm/commit/dd90c3e9413bcc0962549f1b7f72cc4bff99100e", "https://github.com/BerriAI/litellm/commit/5a1fc9ff67d187e33f920669bcc98f717cf77f91", "https://github.com/BerriAI/litellm/commit/9ee8467b688a0d32535503637c178c6f576ac0b1", "https://github.com/BerriAI/litellm/commit/caf9c6103b949465fd7475acc575e5b949644c6c", "https://github.com/BerriAI/litellm/commit/f5d02a44cf94c40270a6bd9927901b02c3e253e8", "https://github.com/BerriAI/litellm/commit/b9337dccf0c8b67198237f9e68a712e17e82a472", "https://github.com/BerriAI/litellm/commit/a6da9db9cb568e4b207adefffdc8838cfad6d064", "https://github.com/BerriAI/litellm/commit/ea4b4db135a8bc91be4797077645d4b97bd424d7", "https://github.com/BerriAI/litellm/commit/f36339742cd4e50dc85dcc502190013be121dd13", "https://github.com/BerriAI/litellm/commit/7568f4cbede8b04f3e991a6987aabd35fc05d473"], "created_at": "2025-07-02T19:10:12Z", "classification": "Efficiency"}
{"repo": "BerriAI/litellm", "pull_number": 12596, "instance_id": "BerriAI__litellm-12596", "issue_numbers": [12539], "base_commit": "78baeae6d8b76abce000a88ea02957835ce7e4cc", "patch": "diff --git a/.circleci/config.yml b/.circleci/config.yml\nindex ecbe022e5628..2edbc32b1b68 100644\n--- a/.circleci/config.yml\n+++ b/.circleci/config.yml\n@@ -1321,6 +1321,7 @@ jobs:\n       # - run: python ./tests/documentation_tests/test_general_setting_keys.py\n       - run: python ./tests/code_coverage_tests/check_licenses.py\n       - run: python ./tests/code_coverage_tests/router_code_coverage.py\n+      - run: python ./tests/code_coverage_tests/test_ban_set_verbose.py\n       - run: python ./tests/code_coverage_tests/code_qa_check_tests.py\n       - run: python ./tests/code_coverage_tests/test_proxy_types_import.py\n       - run: python ./tests/code_coverage_tests/callback_manager_test.py\ndiff --git a/enterprise/enterprise_hooks/aporia_ai.py b/enterprise/enterprise_hooks/aporia_ai.py\nindex 2b427bea5ce0..d2184e92f2f4 100644\n--- a/enterprise/enterprise_hooks/aporia_ai.py\n+++ b/enterprise/enterprise_hooks/aporia_ai.py\n@@ -5,33 +5,32 @@\n # +-------------------------------------------------------------+\n #  Thank you users! We \u2764\ufe0f you! - Krrish & Ishaan\n \n-import sys\n import os\n+import sys\n \n sys.path.insert(\n     0, os.path.abspath(\"../..\")\n )  # Adds the parent directory to the system path\n-from typing import Optional, Literal, Any\n-import litellm\n+import json\n import sys\n-from litellm.proxy._types import UserAPIKeyAuth\n-from litellm.integrations.custom_guardrail import CustomGuardrail\n+from typing import Any, List, Literal, Optional\n+\n from fastapi import HTTPException\n+\n+import litellm\n from litellm._logging import verbose_proxy_logger\n-from litellm.proxy.guardrails.guardrail_helpers import should_proceed_based_on_metadata\n+from litellm.integrations.custom_guardrail import CustomGuardrail\n from litellm.litellm_core_utils.logging_utils import (\n     convert_litellm_response_object_to_str,\n )\n-from typing import List\n from litellm.llms.custom_httpx.http_handler import (\n     get_async_httpx_client,\n     httpxSpecialProvider,\n )\n-import json\n+from litellm.proxy._types import UserAPIKeyAuth\n+from litellm.proxy.guardrails.guardrail_helpers import should_proceed_based_on_metadata\n from litellm.types.guardrails import GuardrailEventHooks\n \n-litellm.set_verbose = True\n-\n GUARDRAIL_NAME = \"aporia\"\n \n \ndiff --git a/enterprise/enterprise_hooks/openai_moderation.py b/enterprise/enterprise_hooks/openai_moderation.py\nindex 1db932c853e5..ee8ac4950999 100644\n--- a/enterprise/enterprise_hooks/openai_moderation.py\n+++ b/enterprise/enterprise_hooks/openai_moderation.py\n@@ -5,21 +5,21 @@\n # +-------------------------------------------------------------+\n #  Thank you users! We \u2764\ufe0f you! - Krrish & Ishaan\n \n-import sys\n import os\n+import sys\n \n sys.path.insert(\n     0, os.path.abspath(\"../..\")\n )  # Adds the parent directory to the system path\n-from typing import Literal\n-import litellm\n import sys\n-from litellm.proxy._types import UserAPIKeyAuth\n-from litellm.integrations.custom_logger import CustomLogger\n+from typing import Literal\n+\n from fastapi import HTTPException\n-from litellm._logging import verbose_proxy_logger\n \n-litellm.set_verbose = True\n+import litellm\n+from litellm._logging import verbose_proxy_logger\n+from litellm.integrations.custom_logger import CustomLogger\n+from litellm.proxy._types import UserAPIKeyAuth\n \n \n class _ENTERPRISE_OpenAI_Moderation(CustomLogger):\ndiff --git a/enterprise/litellm_enterprise/enterprise_callbacks/llama_guard.py b/enterprise/litellm_enterprise/enterprise_callbacks/llama_guard.py\nindex a2d77f51a499..a44af55d4b1f 100644\n--- a/enterprise/litellm_enterprise/enterprise_callbacks/llama_guard.py\n+++ b/enterprise/litellm_enterprise/enterprise_callbacks/llama_guard.py\n@@ -25,8 +25,6 @@\n from litellm.proxy._types import UserAPIKeyAuth\n from litellm.types.utils import Choices, ModelResponse\n \n-litellm.set_verbose = True\n-\n \n class _ENTERPRISE_LlamaGuard(CustomLogger):\n     # Class variables or attributes\ndiff --git a/enterprise/litellm_enterprise/enterprise_callbacks/llm_guard.py b/enterprise/litellm_enterprise/enterprise_callbacks/llm_guard.py\nindex 59981154aa52..1475a94303e7 100644\n--- a/enterprise/litellm_enterprise/enterprise_callbacks/llm_guard.py\n+++ b/enterprise/litellm_enterprise/enterprise_callbacks/llm_guard.py\n@@ -19,8 +19,6 @@\n from litellm.secret_managers.main import get_secret_str\n from litellm.utils import get_formatted_prompt\n \n-litellm.set_verbose = True\n-\n \n class _ENTERPRISE_LLMGuard(CustomLogger):\n     # Class variables or attributes\ndiff --git a/litellm/proxy/guardrails/guardrail_hooks/aporia_ai/aporia_ai.py b/litellm/proxy/guardrails/guardrail_hooks/aporia_ai/aporia_ai.py\nindex 8c61c0c10490..85a0ab9dedd2 100644\n--- a/litellm/proxy/guardrails/guardrail_hooks/aporia_ai/aporia_ai.py\n+++ b/litellm/proxy/guardrails/guardrail_hooks/aporia_ai/aporia_ai.py\n@@ -17,7 +17,6 @@\n \n from fastapi import HTTPException\n \n-import litellm\n from litellm._logging import verbose_proxy_logger\n from litellm.integrations.custom_guardrail import (\n     CustomGuardrail,\n@@ -33,8 +32,6 @@\n from litellm.proxy._types import UserAPIKeyAuth\n from litellm.types.guardrails import GuardrailEventHooks\n \n-litellm.set_verbose = True\n-\n GUARDRAIL_NAME = \"aporia\"\n \n if TYPE_CHECKING:\n", "test_patch": "diff --git a/tests/code_coverage_tests/test_ban_set_verbose.py b/tests/code_coverage_tests/test_ban_set_verbose.py\nnew file mode 100644\nindex 000000000000..a003f666e7d3\n--- /dev/null\n+++ b/tests/code_coverage_tests/test_ban_set_verbose.py\n@@ -0,0 +1,140 @@\n+import ast\n+import os\n+\n+\n+def find_set_verbose_assignments(file_path):\n+    \"\"\"\n+    Finds all assignments of litellm.set_verbose = True in a given Python file.\n+    Returns a list of tuples (line_number, assignment_text).\n+    \"\"\"\n+    try:\n+        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n+            content = file.read()\n+            tree = ast.parse(content)\n+    except (SyntaxError, UnicodeDecodeError) as e:\n+        print(f\"Warning: Error parsing file {file_path}: {e}\")\n+        return []\n+\n+    assignments = []\n+    content_lines = content.splitlines()\n+\n+    for node in ast.walk(tree):\n+        if isinstance(node, ast.Assign):\n+            # Check if this is an assignment to litellm.set_verbose\n+            for target in node.targets:\n+                if isinstance(target, ast.Attribute):\n+                    # Check if it's litellm.set_verbose\n+                    if (isinstance(target.value, ast.Name) and \n+                        target.value.id == \"litellm\" and \n+                        target.attr == \"set_verbose\"):\n+                        \n+                        # Check if the value being assigned is True\n+                        if (isinstance(node.value, ast.Constant) and \n+                            node.value.value is True):\n+                            line_num = node.lineno\n+                            line_text = content_lines[line_num - 1].strip() if line_num <= len(content_lines) else \"\"\n+                            assignments.append((line_num, line_text))\n+                        elif (isinstance(node.value, ast.NameConstant) and \n+                              node.value.value is True):  # For older Python versions\n+                            line_num = node.lineno\n+                            line_text = content_lines[line_num - 1].strip() if line_num <= len(content_lines) else \"\"\n+                            assignments.append((line_num, line_text))\n+\n+    return assignments\n+\n+\n+def scan_litellm_files(base_dir):\n+    \"\"\"\n+    Scans all Python files in the litellm directory for set_verbose assignments.\n+    Returns a dictionary mapping file paths to lists of assignments.\n+    \"\"\"\n+    violations = {}\n+    litellm_dirs = [\n+        \"litellm\",\n+        \"enterprise\"\n+    ]\n+\n+    for litellm_dir in litellm_dirs:\n+        dir_path = os.path.join(base_dir, litellm_dir)\n+        if not os.path.exists(dir_path):\n+            print(f\"Warning: Directory {dir_path} does not exist.\")\n+            continue\n+\n+        print(f\"Scanning directory: {dir_path}\")\n+        for root, _, files in os.walk(dir_path):\n+            for file in files:\n+                if file.endswith(\".py\"):\n+                    file_path = os.path.join(root, file)\n+                    relative_path = os.path.relpath(file_path, base_dir)\n+                    \n+                    assignments = find_set_verbose_assignments(file_path)\n+                    if assignments:\n+                        violations[relative_path] = assignments\n+\n+    return violations\n+\n+\n+def test_no_hardcoded_set_verbose():\n+    \"\"\"\n+    Pytest-compatible test function that ensures no hardcoded litellm.set_verbose = True assignments exist.\n+    \"\"\"\n+    base_dir = \"./\"  # Adjust path as needed for your setup\n+    \n+    violations = scan_litellm_files(base_dir)\n+    \n+    if violations:\n+        violation_details = []\n+        total_violations = 0\n+        for file_path, assignments in violations.items():\n+            for line_num, line_text in assignments:\n+                violation_details.append(f\"{file_path}:{line_num} -> {line_text}\")\n+                total_violations += 1\n+        \n+        error_msg = (\n+            f\"Found {total_violations} prohibited litellm.set_verbose = True assignments:\\n\"\n+            + \"\\n\".join(violation_details) + \n+            \"\\n\\nREASON: litellm.set_verbose = True should not be hardcoded in production code. \"\n+            \"Instead, use environment variables or configuration files to control verbosity.\"\n+        )\n+        \n+        raise AssertionError(error_msg)\n+\n+\n+def main():\n+    \"\"\"\n+    Main function that scans for litellm.set_verbose = True assignments and fails if any are found.\n+    \"\"\"\n+    base_dir = \"./\"  # Adjust path as needed for your setup\n+    \n+    print(\"Scanning for litellm.set_verbose = True assignments...\")\n+    violations = scan_litellm_files(base_dir)\n+    \n+    if violations:\n+        print(\"\\n\u274c FOUND PROHIBITED litellm.set_verbose = True ASSIGNMENTS:\")\n+        print(\"=\" * 60)\n+        \n+        total_violations = 0\n+        for file_path, assignments in violations.items():\n+            print(f\"\\nFile: {file_path}\")\n+            for line_num, line_text in assignments:\n+                print(f\"  Line {line_num}: {line_text}\")\n+                total_violations += 1\n+        \n+        print(f\"\\n\ud83d\udcca Total violations found: {total_violations}\")\n+        print(\"\\n\ud83d\udeab REASON: litellm.set_verbose = True should not be hardcoded in production code.\")\n+        print(\"   Instead, use environment variables or configuration files to control verbosity.\")\n+        print(\"   Example alternatives:\")\n+        print(\"   - Use LITELLM_LOG=DEBUG environment variable\")\n+        print(\"   - Use litellm.set_verbose = os.getenv('LITELLM_VERBOSE', 'false').lower() == 'true'\")\n+        print(\"   - Use configuration-based verbosity settings\")\n+        \n+        raise Exception(\n+            f\"Found {total_violations} prohibited litellm.set_verbose = True assignments. \"\n+            \"Remove these hardcoded verbosity settings and use configuration-based approaches instead.\"\n+        )\n+    else:\n+        print(\"\u2705 No prohibited litellm.set_verbose = True assignments found.\")\n+\n+\n+if __name__ == \"__main__\":\n+    main() \n\\ No newline at end of file\n", "problem_statement": "[Bug]: Verbose log is enabled by default\n### What happened?\n\nVerbose logging is enabled unintentionally because of this:\nhttps://github.com/BerriAI/litellm/blob/749051105bb08e6a536230a0ac36c713cc01e852/litellm/proxy/guardrails/guardrail_hooks/aporia_ai/aporia_ai.py#L36\n\n### Relevant log output\n\n```shell\nlitellm  | ...\nlitellm  | Raw OpenAI Chunk\nlitellm  | ChatCompletionChunk(id='chatcmpl-BsPtLwE2sKCEZ1YpqCMgNooAHLFAr', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, refusal=None, role\n=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1752308939, model='gpt-4.1-nano-2025-04-14', object='chat.completion.chunk', service_tier='de\nfault', system_fingerprint=None, usage=None)\nlitellm  |\nlitellm  | model_response finish reason 3: None; response_obj={'text': ' you', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': Chat\nCompletionChunk(id='chatcmpl-BsPtLwE2sKCEZ1YpqCMgNooAHLFAr', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, refusal=None, role=None, tool_calls=None)\n, finish_reason=None, index=0, logprobs=None)], created=1752308939, model='gpt-4.1-nano-2025-04-14', object='chat.completion.chunk', service_tier='default', system_fingerpr\nint=None, usage=None), 'usage': None}\nlitellm  | original delta: {'content': ' you', 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}\nlitellm  | new delta: Delta(provider_specific_fields=None, refusal=None, content=' you', role=None, function_call=None, tool_calls=None, audio=None)\nlitellm  | model_response.choices[0].delta: Delta(provider_specific_fields=None, refusal=None, content=' you', role=None, function_call=None, tool_calls=None, audio\n=None); completion_obj: {'content': ' you'}\nlitellm  | self.sent_first_chunk: True\nlitellm  | completion_obj: {'content': ' you'}, model_response.choices[0]: StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None,\nrefusal=None, content=' you', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None), response_obj: {'text': ' you', 'is_finished': False, 'finish_reas\non': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-BsPtLwE2sKCEZ1YpqCMgNooAHLFAr', choices=[Choice(delta=ChoiceDelta(content=' you', function_c\nall=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1752308939, model='gpt-4.1-nano-2025-04-14', object='chat.complet\nion.chunk', service_tier='default', system_fingerprint=None, usage=None), 'usage': None}\nlitellm  | hold - False, model_response_str -  you\nlitellm  | choice_json: {'delta': {'content': ' you', 'function_call': None, 'refusal': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\nlitellm  | choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=' you', role=None,\nfunction_call=None, tool_calls=None, audio=None), logprobs=None)]\nlitellm  | self.sent_first_chunk: True\nlitellm  | returning model_response: ModelResponseStream(id='chatcmpl-BsPtLwE2sKCEZ1YpqCMgNooAHLFAr', created=1752308939, model='gpt-4.1-nano', object='chat.complet\nion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=' you', role=No\nne, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\nlitellm  | final returned processed chunk: ModelResponseStream(id='chatcmpl-BsPtLwE2sKCEZ1YpqCMgNooAHLFAr', created=1752308939, model='gpt-4.1-nano', object='chat.c\nompletion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=' you', r\nole=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\nlitellm  |\nlitellm  | Raw OpenAI Chunk\nlitellm  | ChatCompletionChunk(id='chatcmpl-BsPtLwE2sKCEZ1YpqCMgNooAHLFAr', choices=[Choice(delta=ChoiceDelta(content=' today', function_call=None, refusal=None, ro\nle=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1752308939, model='gpt-4.1-nano-2025-04-14', object='chat.completion.chunk', service_tier='\ndefault', system_fingerprint=None, usage=None)\nlitellm  |\nlitellm  | ...\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.74 / docker tag 19a2d497bbbf\n\n### Twitter / LinkedIn details\n\n_No response_\n", "hints_text": "The latest stable tag (litellm_stable_release_branch-stable 19a2d497bbbf) contains this bug.\nfixing \nThanks for your quick response!\n\n", "all_hints_text": "The latest stable tag (litellm_stable_release_branch-stable 19a2d497bbbf) contains this bug.\nfixing \nThanks for your quick response!\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/b05fdc9bb748bfb138ef926eb68f490d1e050a8c", "https://github.com/BerriAI/litellm/commit/b2006fe74902ba9a7e221af339cd3eda21d4eb8f", "https://github.com/BerriAI/litellm/commit/f40ed28e15275fc431ba8ad88a6f51a568f5b46e", "https://github.com/BerriAI/litellm/commit/a526b5b2b5d4b20c3f132ac1b8b024ab1dc2c165"], "created_at": "2025-07-15T00:23:32Z", "classification": "Efficiency"}
{"repo": "BerriAI/litellm", "pull_number": 13016, "instance_id": "BerriAI__litellm-13016", "issue_numbers": [12859], "base_commit": "2c38dc0de70fd2e6af3c9ee42dd014d9c7be1a60", "patch": "diff --git a/litellm/llms/github_copilot/chat/transformation.py b/litellm/llms/github_copilot/chat/transformation.py\nindex eff8acb5f540..4526e6247b4b 100644\n--- a/litellm/llms/github_copilot/chat/transformation.py\n+++ b/litellm/llms/github_copilot/chat/transformation.py\n@@ -1,7 +1,8 @@\n-from typing import Any, Optional, Tuple, cast\n+from typing import Any, Optional, Tuple, cast, List\n \n from litellm.exceptions import AuthenticationError\n from litellm.llms.openai.openai import OpenAIConfig\n+from litellm.types.llms.openai import AllMessageValues\n \n from ..authenticator import Authenticator\n from ..common_utils import GetAPIKeyError\n@@ -9,6 +10,7 @@\n \n class GithubCopilotConfig(OpenAIConfig):\n     GITHUB_COPILOT_API_BASE = \"https://api.githubcopilot.com/\"\n+\n     def __init__(\n         self,\n         api_key: Optional[str] = None,\n@@ -25,7 +27,9 @@ def _get_openai_compatible_provider_info(\n         api_key: Optional[str],\n         custom_llm_provider: str,\n     ) -> Tuple[Optional[str], Optional[str], str]:\n-        dynamic_api_base = self.authenticator.get_api_base() or self.GITHUB_COPILOT_API_BASE\n+        dynamic_api_base = (\n+            self.authenticator.get_api_base() or self.GITHUB_COPILOT_API_BASE\n+        )\n         try:\n             dynamic_api_key = self.authenticator.get_api_key()\n         except GetAPIKeyError as e:\n@@ -42,9 +46,44 @@ def _transform_messages(\n         model: str,\n     ):\n         import litellm\n-        disable_copilot_system_to_assistant = litellm.disable_copilot_system_to_assistant \n+\n+        disable_copilot_system_to_assistant = (\n+            litellm.disable_copilot_system_to_assistant\n+        )\n         if not disable_copilot_system_to_assistant:\n             for message in messages:\n                 if \"role\" in message and message[\"role\"] == \"system\":\n                     cast(Any, message)[\"role\"] = \"assistant\"\n         return messages\n+\n+    def validate_environment(\n+        self,\n+        headers: dict,\n+        model: str,\n+        messages: List[AllMessageValues],\n+        optional_params: dict,\n+        litellm_params: dict,\n+        api_key: Optional[str] = None,\n+        api_base: Optional[str] = None,\n+    ) -> dict:\n+        # Get base headers from parent\n+        validated_headers = super().validate_environment(\n+            headers, model, messages, optional_params, litellm_params, api_key, api_base\n+        )\n+\n+        # Add X-Initiator header based on message roles\n+        initiator = self._determine_initiator(messages)\n+        validated_headers[\"X-Initiator\"] = initiator\n+\n+        return validated_headers\n+\n+    def _determine_initiator(self, messages: List[AllMessageValues]) -> str:\n+        \"\"\"\n+        Determine if request is user or agent initiated based on message roles.\n+        Returns 'agent' if any message has role 'tool' or 'assistant', otherwise 'user'.\n+        \"\"\"\n+        for message in messages:\n+            role = message.get(\"role\")\n+            if role in [\"tool\", \"assistant\"]:\n+                return \"agent\"\n+        return \"user\"\n", "test_patch": "diff --git a/tests/test_litellm/llms/github_copilot/test_github_copilot_transformation.py b/tests/test_litellm/llms/github_copilot/test_github_copilot_transformation.py\nindex 388a50243f48..f21c123579db 100644\n--- a/tests/test_litellm/llms/github_copilot/test_github_copilot_transformation.py\n+++ b/tests/test_litellm/llms/github_copilot/test_github_copilot_transformation.py\n@@ -177,3 +177,188 @@ def test_transform_messages_disable_copilot_system_to_assistant(monkeypatch):\n         litellm.disable_copilot_system_to_assistant = original_flag\n \n \n+def test_x_initiator_header_user_request():\n+    \"\"\"Test that user-only messages result in X-Initiator: user header\"\"\"\n+    config = GithubCopilotConfig()\n+    \n+    # Mock the authenticator\n+    config.authenticator = MagicMock()\n+    config.authenticator.get_api_key.return_value = \"gh.test-key-123\"\n+    config.authenticator.get_api_base.return_value = None\n+\n+    messages = [\n+        {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n+        {\"role\": \"user\", \"content\": \"Hello!\"},\n+    ]\n+    \n+    headers = config.validate_environment(\n+        headers={},\n+        model=\"github_copilot/gpt-4\",\n+        messages=messages,\n+        optional_params={},\n+        litellm_params={},\n+        api_key=None,\n+        api_base=None,\n+    )\n+    \n+    assert headers[\"X-Initiator\"] == \"user\"\n+\n+\n+def test_x_initiator_header_agent_request_with_assistant():\n+    \"\"\"Test that messages with assistant role result in X-Initiator: agent header\"\"\"\n+    config = GithubCopilotConfig()\n+    \n+    # Mock the authenticator\n+    config.authenticator = MagicMock()\n+    config.authenticator.get_api_key.return_value = \"gh.test-key-123\"\n+    config.authenticator.get_api_base.return_value = None\n+\n+    messages = [\n+        {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n+        {\"role\": \"assistant\", \"content\": \"I can help you.\"},\n+    ]\n+    \n+    headers = config.validate_environment(\n+        headers={},\n+        model=\"github_copilot/gpt-4\", \n+        messages=messages,\n+        optional_params={},\n+        litellm_params={},\n+        api_key=None,\n+        api_base=None,\n+    )\n+    \n+    assert headers[\"X-Initiator\"] == \"agent\"\n+\n+\n+def test_x_initiator_header_agent_request_with_tool():\n+    \"\"\"Test that messages with tool role result in X-Initiator: agent header\"\"\"\n+    config = GithubCopilotConfig()\n+    \n+    # Mock the authenticator\n+    config.authenticator = MagicMock()\n+    config.authenticator.get_api_key.return_value = \"gh.test-key-123\"\n+    config.authenticator.get_api_base.return_value = None\n+\n+    messages = [\n+        {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n+        {\"role\": \"tool\", \"content\": \"Tool response.\", \"tool_call_id\": \"123\"},\n+    ]\n+    \n+    headers = config.validate_environment(\n+        headers={},\n+        model=\"github_copilot/gpt-4\", \n+        messages=messages,\n+        optional_params={},\n+        litellm_params={},\n+        api_key=None,\n+        api_base=None,\n+    )\n+    \n+    assert headers[\"X-Initiator\"] == \"agent\"\n+\n+\n+def test_x_initiator_header_mixed_messages_with_agent_roles():\n+    \"\"\"Test that mixed messages with agent roles (assistant/tool) result in X-Initiator: agent header\"\"\"\n+    config = GithubCopilotConfig()\n+    \n+    # Mock the authenticator  \n+    config.authenticator = MagicMock()\n+    config.authenticator.get_api_key.return_value = \"gh.test-key-123\"\n+    config.authenticator.get_api_base.return_value = None\n+\n+    messages = [\n+        {\"role\": \"user\", \"content\": \"Hello\"},\n+        {\"role\": \"assistant\", \"content\": \"Previous response.\"},\n+        {\"role\": \"user\", \"content\": \"Follow up question.\"},\n+    ]\n+    \n+    headers = config.validate_environment(\n+        headers={},\n+        model=\"github_copilot/gpt-4\",\n+        messages=messages, \n+        optional_params={},\n+        litellm_params={},\n+        api_key=None,\n+        api_base=None,\n+    )\n+    \n+    assert headers[\"X-Initiator\"] == \"agent\"\n+\n+\n+def test_x_initiator_header_user_only_messages():\n+    \"\"\"Test that user + system only messages result in X-Initiator: user header\"\"\"\n+    config = GithubCopilotConfig()\n+    \n+    # Mock the authenticator  \n+    config.authenticator = MagicMock()\n+    config.authenticator.get_api_key.return_value = \"gh.test-key-123\"\n+    config.authenticator.get_api_base.return_value = None\n+\n+    messages = [\n+        {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n+        {\"role\": \"user\", \"content\": \"Hello\"},\n+        {\"role\": \"user\", \"content\": \"Follow up question.\"},\n+    ]\n+    \n+    headers = config.validate_environment(\n+        headers={},\n+        model=\"github_copilot/gpt-4\",\n+        messages=messages, \n+        optional_params={},\n+        litellm_params={},\n+        api_key=None,\n+        api_base=None,\n+    )\n+    \n+    assert headers[\"X-Initiator\"] == \"user\"\n+\n+\n+def test_x_initiator_header_empty_messages():\n+    \"\"\"Test that empty messages result in X-Initiator: user header\"\"\"\n+    config = GithubCopilotConfig()\n+    \n+    # Mock the authenticator\n+    config.authenticator = MagicMock()\n+    config.authenticator.get_api_key.return_value = \"gh.test-key-123\"\n+    config.authenticator.get_api_base.return_value = None\n+\n+    messages = []\n+    \n+    headers = config.validate_environment(\n+        headers={},\n+        model=\"github_copilot/gpt-4\",\n+        messages=messages,\n+        optional_params={},\n+        litellm_params={},\n+        api_key=None,\n+        api_base=None,\n+    )\n+    \n+    assert headers[\"X-Initiator\"] == \"user\"\n+\n+\n+def test_x_initiator_header_system_only_messages():\n+    \"\"\"Test that system-only messages result in X-Initiator: user header\"\"\"\n+    config = GithubCopilotConfig()\n+    \n+    # Mock the authenticator\n+    config.authenticator = MagicMock()\n+    config.authenticator.get_api_key.return_value = \"gh.test-key-123\"\n+    config.authenticator.get_api_base.return_value = None\n+\n+    messages = [\n+        {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n+    ]\n+    \n+    headers = config.validate_environment(\n+        headers={},\n+        model=\"github_copilot/gpt-4\",\n+        messages=messages,\n+        optional_params={},\n+        litellm_params={},\n+        api_key=None,\n+        api_base=None,\n+    )\n+    \n+    assert headers[\"X-Initiator\"] == \"user\"\n", "problem_statement": "Reduce Premium Copilot requests\nWhen using the Copilot provider, the \"X-Initiator\" header should be set for non-user calls to not unnecessarily blow through premium requests. See https://github.com/sst/opencode/pull/595 for discussion and implementation. \n", "hints_text": "Related from codecompanion.nvim: https://github.com/olimorris/codecompanion.nvim/pull/1738\nSo we just need to set `  \"X-Initiator\": \"agent\"` in the requests ? \nSupposedly! The right ones, anyway, per that PR\n@Arithmomaniac should we just default to sending it ? \nI think the honest, behavior-matching behavior is https://github.com/ericc-ch/copilot-api/commit/b247302a4a7107d093e16c0209cc01931a06477d - you always send the header, watch the messages as they pass through to determine its value.\n\n", "all_hints_text": "Related from codecompanion.nvim: https://github.com/olimorris/codecompanion.nvim/pull/1738\nSo we just need to set `  \"X-Initiator\": \"agent\"` in the requests ? \nSupposedly! The right ones, anyway, per that PR\n@Arithmomaniac should we just default to sending it ? \nI think the honest, behavior-matching behavior is https://github.com/ericc-ch/copilot-api/commit/b247302a4a7107d093e16c0209cc01931a06477d - you always send the header, watch the messages as they pass through to determine its value.\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/04fdf4d648a3818b1aee33e018baffd7fec5a986"], "created_at": "2025-07-26T20:55:15Z", "classification": "Efficiency"}
{"repo": "BerriAI/litellm", "pull_number": 13140, "instance_id": "BerriAI__litellm-13140", "issue_numbers": [13120], "base_commit": "65ca4f66f654343382a713e680500d8d47313a94", "patch": "diff --git a/litellm/proxy/db/prisma_client.py b/litellm/proxy/db/prisma_client.py\nindex c4edc702b32b..02a75c0b0f71 100644\n--- a/litellm/proxy/db/prisma_client.py\n+++ b/litellm/proxy/db/prisma_client.py\n@@ -87,6 +87,12 @@ async def recreate_prisma_client(\n         self, new_db_url: str, http_client: Optional[Any] = None\n     ):\n         from prisma import Prisma  # type: ignore\n+        try:\n+            await self._original_prisma.disconnect()\n+        except Exception as e:\n+            verbose_proxy_logger.warning(\n+                f\"Failed to disconnect Prisma client: {e}\"\n+            )\n \n         if http_client is not None:\n             self._original_prisma = Prisma(http=http_client)\n", "test_patch": "diff --git a/tests/test_litellm/proxy/db/test_prisma_client.py b/tests/test_litellm/proxy/db/test_prisma_client.py\nindex c7e99aa75406..1d7459d5ccae 100644\n--- a/tests/test_litellm/proxy/db/test_prisma_client.py\n+++ b/tests/test_litellm/proxy/db/test_prisma_client.py\n@@ -1,6 +1,7 @@\n import json\n import os\n import sys\n+from unittest.mock import AsyncMock, Mock, patch\n \n import pytest\n from fastapi.testclient import TestClient\n@@ -10,7 +11,7 @@\n )  # Adds the parent directory to the system path\n \n \n-from litellm.proxy.db.prisma_client import should_update_prisma_schema\n+from litellm.proxy.db.prisma_client import PrismaWrapper, should_update_prisma_schema\n \n \n def test_should_update_prisma_schema(monkeypatch):\n@@ -33,3 +34,36 @@ def test_should_update_prisma_schema(monkeypatch):\n \n     monkeypatch.setenv(\"DISABLE_SCHEMA_UPDATE\", None)  # Set env var opposite to param\n     assert should_update_prisma_schema(False) == True  # Param False -> should update\n+\n+\n+@pytest.mark.asyncio\n+async def test_recreate_prisma_client_successful_disconnect():\n+    \"\"\"\n+    Test that recreate_prisma_client works normally when disconnect succeeds.\n+    \"\"\"\n+    # Mock the original prisma client\n+    mock_prisma = AsyncMock()\n+    \n+    # Create a PrismaWrapper instance\n+    wrapper = PrismaWrapper(original_prisma=mock_prisma, iam_token_db_auth=False)\n+    \n+    # Configure disconnect to succeed\n+    mock_prisma.disconnect.return_value = None\n+    \n+    # Mock the Prisma class constructor\n+    with patch(\"prisma.Prisma\") as mock_prisma_class:\n+        mock_new_prisma = AsyncMock()\n+        mock_prisma_class.return_value = mock_new_prisma\n+        \n+        # Call the method\n+        await wrapper.recreate_prisma_client(\"postgresql://new:new@localhost:5432/new\")\n+        \n+        # Verify that disconnect was called\n+        mock_prisma.disconnect.assert_called_once()\n+\n+        # Verify that a new Prisma client was created and connected\n+        mock_prisma_class.assert_called_once()\n+        mock_new_prisma.connect.assert_called_once()\n+        \n+        # Verify that the new client replaced the original\n+        assert wrapper._original_prisma == mock_new_prisma\n", "problem_statement": "[Bug]: Incorrect cleanup of DB connections when using AWS IAM authentication\n### What happened?\n\nWhen we enable `iam_token_db_auth` we see constant increase of RDS DB connections to a point where there is no available connections left and this causes eventual crash of the LiteLLM app container and restart. \n\nThis is accompanied with ever growing memory usage. \n\nWe are running ghcr.io/berriai/litellm:main-v1.73.6-stable\n\nThis the DB connection count before the change:\n\n<img width=\"1683\" height=\"555\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f89d6132-b192-4648-ba0d-28b2f175d4e8\" />\n\n### Relevant log output\n\n```shell\nlitellm.proxy_server.py::get_credentials() - Error getting credentials from DB - Error in connector: Error querying the database: FATAL: remaining connection slots are reserved for roles with privileges of the\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.73.6\n\n### Twitter / LinkedIn details\n\n_No response_\n", "hints_text": "\n\n", "all_hints_text": "\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/6f6171e767e41a1edf1665e2cc60489c6571d7f9", "https://github.com/BerriAI/litellm/commit/f1ad745eeac4cbf548882fd234d513878d9333c8", "https://github.com/BerriAI/litellm/commit/700ecbf87bef576dc629e7275e46605ad4764a49", "https://github.com/BerriAI/litellm/commit/c7210dd92c30e7997057a596b2218b778b72c422"], "created_at": "2025-07-30T16:56:07Z", "classification": "Efficiency"}
{"repo": "matplotlib/matplotlib", "pull_number": 20716, "instance_id": "matplotlib__matplotlib-20716", "issue_numbers": [127], "base_commit": "01e919a56be1bb4eedba70f221c0a22486074fc2", "patch": "diff --git a/doc/users/next_whats_new/type1_subset.rst b/doc/users/next_whats_new/type1_subset.rst\nnew file mode 100644\nindex 000000000000..b0ab0a4337e6\n--- /dev/null\n+++ b/doc/users/next_whats_new/type1_subset.rst\n@@ -0,0 +1,9 @@\n+PDF files created with usetex now embed subsets of Type 1 fonts\n+---------------------------------------------------------------\n+\n+When using the PDF backend with the usetex feature,\n+Matplotlib calls TeX to render the text and formulas in the figure.\n+The fonts that get used are usually \"Type 1\" fonts.\n+They used to be embedded in full\n+but are now limited to the glyphs that are actually used in the figure.\n+This reduces the size of the resulting PDF files.\ndiff --git a/lib/matplotlib/_type1font.py b/lib/matplotlib/_type1font.py\nindex 032b6a42ea63..33b22adbae73 100644\n--- a/lib/matplotlib/_type1font.py\n+++ b/lib/matplotlib/_type1font.py\n@@ -3,7 +3,7 @@\n \n This version reads pfa and pfb files and splits them for embedding in\n pdf files. It also supports SlantFont and ExtendFont transformations,\n-similarly to pdfTeX and friends. There is no support yet for subsetting.\n+similarly to pdfTeX and friends.\n \n Usage::\n \n@@ -11,6 +11,7 @@\n     clear_part, encrypted_part, finale = font.parts\n     slanted_font = font.transform({'slant': 0.167})\n     extended_font = font.transform({'extend': 1.2})\n+    subset_font = font.subset([ord(c) for c in 'Hello World'])\n \n Sources:\n \n@@ -25,6 +26,7 @@\n \n import binascii\n import functools\n+import itertools\n import logging\n import re\n import string\n@@ -637,8 +639,7 @@ def _parse_subrs(self, tokens, _data):\n \n         return array, next(tokens).endpos()\n \n-    @staticmethod\n-    def _parse_charstrings(tokens, _data):\n+    def _parse_charstrings(self, tokens, _data):\n         count_token = next(tokens)\n         if not count_token.is_number():\n             raise RuntimeError(\n@@ -660,7 +661,12 @@ def _parse_charstrings(tokens, _data):\n                     f\"Token following /{glyphname} in CharStrings definition \"\n                     f\"must be a number, was {nbytes_token}\"\n                 )\n-            next(tokens)  # usually RD or |-\n+            token = next(tokens)\n+            if not token.is_keyword(self._abbr['RD']):\n+                raise RuntimeError(\n+                    f\"Token preceding charstring must be {self._abbr['RD']}, \"\n+                    f\"was {token}\"\n+                )\n             binary_token = tokens.send(1+nbytes_token.value())\n             charstrings[glyphname] = binary_token.value()\n \n@@ -691,8 +697,7 @@ def _parse_encoding(tokens, _data):\n                 continue\n             encoding[index_token.value()] = name_token.value()\n \n-    @staticmethod\n-    def _parse_othersubrs(tokens, data):\n+    def _parse_othersubrs(self, tokens, data):\n         init_pos = None\n         while True:\n             token = next(tokens)\n@@ -700,7 +705,7 @@ def _parse_othersubrs(tokens, data):\n                 init_pos = token.pos\n             if token.is_delim():\n                 _expression(token, tokens, data)\n-            elif token.is_keyword('def', 'ND', '|-'):\n+            elif token.is_keyword('def', self._abbr['ND']):\n                 return data[init_pos:token.endpos()], token.endpos()\n \n     def transform(self, effects):\n@@ -755,7 +760,7 @@ def transform(self, effects):\n         fontmatrix = (\n             f\"[{' '.join(_format_approx(x, 6) for x in array)}]\"\n         )\n-        replacements = (\n+        newparts = self._replace(\n             [(x, f'/FontName/{fontname} def')\n              for x in self._pos['FontName']]\n             + [(x, f'/ItalicAngle {italicangle} def')\n@@ -765,11 +770,63 @@ def transform(self, effects):\n             + [(x, '') for x in self._pos.get('UniqueID', [])]\n         )\n \n+        return Type1Font((\n+            newparts[0],\n+            self._encrypt(newparts[1], 'eexec'),\n+            self.parts[2]\n+        ))\n+\n+    def with_encoding(self, encoding):\n+        \"\"\"\n+        Change the encoding of the font.\n+\n+        Parameters\n+        ----------\n+        encoding : dict\n+            A dictionary mapping character codes to glyph names.\n+\n+        Returns\n+        -------\n+        `Type1Font`\n+        \"\"\"\n+        newparts = self._replace(\n+            [(x, '') for x in self._pos.get('UniqueID', [])]\n+            + [(self._pos['Encoding'][0], self._postscript_encoding(encoding))]\n+        )\n+        return Type1Font((\n+            newparts[0],\n+            self._encrypt(newparts[1], 'eexec'),\n+            self.parts[2]\n+        ))\n+\n+    def _replace(self, replacements):\n+        \"\"\"\n+        Change the font according to `replacements`\n+\n+        Parameters\n+        ----------\n+        replacements : list of ((int, int), str)\n+            Each element is ((pos0, pos1), replacement) where pos0 and\n+            pos1 are indices to the original font data (parts[0] and the\n+            decrypted part concatenated). The data in the interval\n+            pos0:pos1 will be replaced by the replacement text. To\n+            accommodate binary data, the replacement is taken to be in\n+            Latin-1 encoding.\n+\n+            The case where pos0 is inside parts[0] and pos1 inside\n+            the decrypted part is not supported.\n+\n+        Returns\n+        -------\n+        (bytes, bytes)\n+            The new parts[0] and decrypted part (which needs to be\n+            encrypted in the transformed font).\n+        \"\"\"\n         data = bytearray(self.parts[0])\n         data.extend(self.decrypted)\n         len0 = len(self.parts[0])\n         for (pos0, pos1), value in sorted(replacements, reverse=True):\n-            data[pos0:pos1] = value.encode('ascii', 'replace')\n+            data[pos0:pos1] = value.encode('latin-1')\n             if pos0 < len(self.parts[0]):\n                 if pos1 >= len(self.parts[0]):\n                     raise RuntimeError(\n@@ -778,13 +835,275 @@ def transform(self, effects):\n                     )\n                 len0 += len(value) - pos1 + pos0\n \n-        data = bytes(data)\n-        return Type1Font((\n-            data[:len0],\n-            self._encrypt(data[len0:], 'eexec'),\n+        return bytes(data[:len0]), bytes(data[len0:])\n+\n+    def subset(self, characters, name_prefix):\n+        \"\"\"\n+        Return a new font that only defines the given characters.\n+\n+        Parameters\n+        ----------\n+        characters : sequence of bytes\n+            The subset of characters to include. These are indices into the\n+            font's encoding array. The encoding array of a Type-1 font can\n+            only include 256 characters, but other glyphs may be accessed\n+            via the seac operator.\n+        name_prefix : str\n+            Prefix to prepend to the font name.\n+\n+        Returns\n+        -------\n+        `Type1Font`\n+        \"\"\"\n+        characters = frozenset(characters)\n+        if _log.isEnabledFor(logging.DEBUG):\n+            _log.debug(\n+                \"Subsetting font %s to characters %s = %s\",\n+                self.prop['FontName'],\n+                sorted(characters),\n+                [self.prop['Encoding'].get(code) for code in sorted(characters)],\n+            )\n+        encoding = {code: glyph\n+                    for code, glyph in self.prop['Encoding'].items()\n+                    if code in characters}\n+        encoding[0] = '.notdef'\n+        # todo and done include strings (glyph names)\n+        todo = set(encoding.values())\n+        done = set()\n+        seen_subrs = {0, 1, 2, 3}\n+        while todo:\n+            glyph = todo.pop()\n+            called_glyphs, called_subrs = _CharstringSimulator(self).run(glyph)\n+            todo.update(called_glyphs - done)\n+            seen_subrs.update(called_subrs)\n+            done.add(glyph)\n+\n+        charstrings = self._subset_charstrings(done)\n+        subrs = self._subset_subrs(seen_subrs)\n+        newparts = self._replace(\n+            [(x, f'/FontName /{name_prefix}{self.prop[\"FontName\"]} def')\n+             for x in self._pos['FontName']]\n+            + [(self._pos['CharStrings'][0], charstrings),\n+               (self._pos['Subrs'][0], subrs),\n+               (self._pos['Encoding'][0], self._postscript_encoding(encoding))\n+               ] + [(x, '') for x in self._pos.get('UniqueID', [])]\n+        )\n+        return type(self)((\n+            newparts[0],\n+            self._encrypt(newparts[1], 'eexec'),\n             self.parts[2]\n         ))\n \n+    @staticmethod\n+    def _charstring_tokens(data):\n+        \"\"\"Parse a Type-1 charstring\n+\n+        Yield opcode names and integer parameters.\n+        \"\"\"\n+        data = iter(data)\n+        for byte in data:\n+            if 32 <= byte <= 246:\n+                yield byte - 139\n+            elif 247 <= byte <= 250:\n+                byte2 = next(data)\n+                yield (byte-247) * 256 + byte2 + 108\n+            elif 251 <= byte <= 254:\n+                byte2 = next(data)\n+                yield -(byte-251)*256 - byte2 - 108\n+            elif byte == 255:\n+                bs = bytes(itertools.islice(data, 4))\n+                yield struct.unpack('>i', bs)[0]\n+            elif byte == 12:\n+                byte1 = next(data)\n+                yield {\n+                    0: 'dotsection',\n+                    1: 'vstem3',\n+                    2: 'hstem3',\n+                    6: 'seac',\n+                    7: 'sbw',\n+                    12: 'div',\n+                    16: 'callothersubr',\n+                    17: 'pop',\n+                    33: 'setcurrentpoint'\n+                }[byte1]\n+            else:\n+                yield {\n+                    1: 'hstem',\n+                    3: 'vstem',\n+                    4: 'vmoveto',\n+                    5: 'rlineto',\n+                    6: 'hlineto',\n+                    7: 'vlineto',\n+                    8: 'rrcurveto',\n+                    9: 'closepath',\n+                    10: 'callsubr',\n+                    11: 'return',\n+                    13: 'hsbw',\n+                    14: 'endchar',\n+                    21: 'rmoveto',\n+                    22: 'hmoveto',\n+                    30: 'vhcurveto',\n+                    31: 'hvcurveto'\n+                }[byte]\n+\n+    def _postscript_encoding(self, encoding):\n+        \"\"\"Return a PostScript encoding array for the encoding.\"\"\"\n+        return '\\n'.join([\n+            '/Encoding 256 array\\n0 1 255 { 1 index exch /.notdef put} for',\n+            *(\n+                f'dup {i} /{glyph} put'\n+                for i, glyph in sorted(encoding.items())\n+                if glyph != '.notdef'\n+            ),\n+            'readonly def\\n',\n+        ])\n+\n+    def _subset_charstrings(self, glyphs):\n+        \"\"\"Return a PostScript CharStrings array for the glyphs.\"\"\"\n+        charstrings = self.prop['CharStrings']\n+        lenIV = self.prop.get('lenIV', 4)\n+        ordered = sorted(glyphs)\n+        encrypted = [\n+            self._encrypt(charstrings[glyph], 'charstring', lenIV).decode('latin-1')\n+            for glyph in ordered\n+        ]\n+        RD, ND = self._abbr['RD'], self._abbr['ND']\n+        return '\\n'.join([\n+            f'/CharStrings {len(ordered)} dict dup begin',\n+            *(\n+                f'/{glyph} {len(enc)} {RD} {enc} {ND}'\n+                for glyph, enc in zip(ordered, encrypted)\n+            ),\n+            'end\\n',\n+        ])\n+\n+    def _subset_subrs(self, indices):\n+        \"\"\"Return a PostScript Subrs array for the subroutines.\"\"\"\n+        # we can't remove subroutines, we just replace unused ones with a stub\n+        subrs = self.prop['Subrs']\n+        n_subrs = len(subrs)\n+        lenIV = self.prop.get('lenIV', 4)\n+        stub = self._encrypt(b'\\x0b', 'charstring', lenIV).decode('latin-1')\n+        encrypted = [\n+            self._encrypt(subrs[i], 'charstring', lenIV).decode('latin-1')\n+            if i in indices\n+            else stub\n+            for i in range(n_subrs)\n+        ]\n+        RD, ND, NP = self._abbr['RD'], self._abbr['ND'], self._abbr['NP']\n+        return '\\n'.join([\n+            f'/Subrs {n_subrs} array',\n+            *(\n+                f'dup {i} {len(enc)} {RD} {enc} {NP}'\n+                for i, enc in enumerate(encrypted)\n+            ),\n+        ])\n+\n+\n+class _CharstringSimulator:\n+    __slots__ = ('font', 'buildchar_stack', 'postscript_stack', 'glyphs', 'subrs')\n+\n+    def __init__(self, font):\n+        self.font = font\n+        self.buildchar_stack = []\n+        self.postscript_stack = []\n+        self.glyphs = set()\n+        self.subrs = set()\n+\n+    def run(self, glyph_or_subr):\n+        \"\"\"Run the charstring interpreter on a glyph or subroutine.\n+\n+        This does not actually execute the code but simulates it to find out\n+        which subroutines get called when executing the glyph or subroutine.\n+\n+        Parameters\n+        ----------\n+        glyph_or_subr : str or int\n+            The name of the glyph or the index of the subroutine to simulate.\n+\n+        Returns\n+        -------\n+        glyphs : set[str]\n+            The set of glyph names called by the glyph or subroutine.\n+        subrs : set[int]\n+            The set of subroutines called by the glyph or subroutine.\n+        \"\"\"\n+        if isinstance(glyph_or_subr, str):\n+            program = self.font.prop['CharStrings'][glyph_or_subr]\n+            self.glyphs.add(glyph_or_subr)\n+        else:\n+            program = self.font.prop['Subrs'][glyph_or_subr]\n+            self.subrs.add(glyph_or_subr)\n+        for opcode in self.font._charstring_tokens(program):\n+            if opcode in ('return', 'endchar'):\n+                return self.glyphs, self.subrs\n+            self._step(opcode)\n+        else:\n+            font_name = self.font.prop.get('FontName', '(unknown)')\n+            _log.info(\n+                f\"Glyph or subr {glyph_or_subr} in font {font_name} does not end \"\n+                \"with return or endchar\"\n+            )\n+            return self.glyphs, self.subrs\n+\n+    def _step(self, opcode):\n+        \"\"\"Run one step in the charstring interpreter.\"\"\"\n+        match opcode:\n+            case int():\n+                self.buildchar_stack.append(opcode)\n+            case (\n+                'hsbw' | 'sbw' | 'closepath' | 'hlineto' | 'hmoveto' | 'hcurveto' |\n+                'hvcurveto' | 'rlineto' | 'rmoveto' | 'rrcurveto' | 'vhcurveto' |\n+                'vlineto' | 'vmoveto' | 'dotsection' | 'hstem' | 'hstem3' |\n+                'vstem' | 'vstem3' | 'setcurrentpoint'\n+            ):\n+                self.buildchar_stack.clear()\n+            case 'seac':  # Standard Encoding Accented Character\n+                codes = self.buildchar_stack[3:5]\n+                self.glyphs.update(_StandardEncoding[int(x)] for x in codes)\n+                self.buildchar_stack.clear()\n+            case 'div':\n+                num1, num2 = self.buildchar_stack[-2:]\n+                if num2 == 0:\n+                    _log.warning(\n+                        f\"Division by zero in font {self.font.prop['FontName']}\"\n+                    )\n+                    self.buildchar_stack[-2:] = [0]\n+                else:\n+                    self.buildchar_stack[-2:] = [num1/num2]\n+            case 'callothersubr':\n+                n, othersubr = self.buildchar_stack[-2:]\n+                if not isinstance(n, int):\n+                    _log.warning(\n+                        f\"callothersubr {othersubr} with non-integer argument \"\n+                        f\"count in font {self.font.prop['FontName']}\"\n+                    )\n+                    n = int(n)\n+                args = self.buildchar_stack[-2-n:-2]\n+                if othersubr == 3:\n+                    self.postscript_stack.append(args[0])\n+                else:\n+                    self.postscript_stack.extend(args[::-1])\n+                self.buildchar_stack[-2-n:] = []\n+            case 'callsubr':\n+                subr = self.buildchar_stack.pop()\n+                if not isinstance(subr, int):\n+                    _log.warning(\n+                        f\"callsubr with non-integer argument {subr} in font \"\n+                        f\"{self.font.prop['FontName']}\"\n+                    )\n+                    subr = int(subr)\n+                self.run(subr)\n+            case 'pop':\n+                if not self.postscript_stack:\n+                    _log.warning(\n+                        f\"pop with empty stack in font {self.font.prop['FontName']}\"\n+                    )\n+                    self.postscript_stack.append(0)\n+                self.buildchar_stack.append(self.postscript_stack.pop())\n+            case _:\n+                raise RuntimeError(f'opcode {opcode}')\n+\n \n _StandardEncoding = {\n     **{ord(letter): letter for letter in string.ascii_letters},\ndiff --git a/lib/matplotlib/backends/backend_pdf.py b/lib/matplotlib/backends/backend_pdf.py\nindex 073ca05bc172..6f3cb8afa1ac 100644\n--- a/lib/matplotlib/backends/backend_pdf.py\n+++ b/lib/matplotlib/backends/backend_pdf.py\n@@ -722,8 +722,6 @@ def __init__(self, filename, metadata=None):\n         self._internal_font_seq = (Name(f'F{i}') for i in itertools.count(1))\n         self._fontNames = {}     # maps filenames to internal font names\n         self._dviFontInfo = {}   # maps dvi font names to embedding information\n-        # differently encoded Type-1 fonts may share the same descriptor\n-        self._type1Descriptors = {}\n         self._character_tracker = _backend_pdf_ps.CharacterTracker()\n \n         self.alphaStates = {}   # maps alpha values to graphics state objects\n@@ -767,8 +765,7 @@ def __init__(self, filename, metadata=None):\n \n     fontNames = _api.deprecated(\"3.11\")(property(lambda self: self._fontNames))\n     dviFontInfo = _api.deprecated(\"3.11\")(property(lambda self: self._dviFontInfo))\n-    type1Descriptors = _api.deprecated(\"3.11\")(\n-        property(lambda self: self._type1Descriptors))\n+    type1Descriptors = _api.deprecated(\"3.11\")(property(lambda _: {}))\n \n     def newPage(self, width, height):\n         self.endStream()\n@@ -808,7 +805,14 @@ def newTextnote(self, text, positionRect=[-100, -100, 0, 0]):\n                    }\n         self.pageAnnotations.append(theNote)\n \n-    def _get_subsetted_psname(self, ps_name, charmap):\n+    @staticmethod\n+    def _get_subset_prefix(charset):\n+        \"\"\"\n+        Get a prefix for a subsetted font name.\n+\n+        The prefix is six uppercase letters followed by a plus sign;\n+        see PDF reference section 5.5.3 Font Subsets.\n+        \"\"\"\n         def toStr(n, base):\n             if n < base:\n                 return string.ascii_uppercase[n]\n@@ -818,11 +822,15 @@ def toStr(n, base):\n                 )\n \n         # encode to string using base 26\n-        hashed = hash(frozenset(charmap.keys())) % ((sys.maxsize + 1) * 2)\n+        hashed = hash(charset) % ((sys.maxsize + 1) * 2)\n         prefix = toStr(hashed, 26)\n \n         # get first 6 characters from prefix\n-        return prefix[:6] + \"+\" + ps_name\n+        return prefix[:6] + \"+\"\n+\n+    @staticmethod\n+    def _get_subsetted_psname(ps_name, charmap):\n+        return PdfFile._get_subset_prefix(frozenset(charmap.keys())) + ps_name\n \n     def finalize(self):\n         \"\"\"Write out the various deferred objects and the pdf end matter.\"\"\"\n@@ -994,53 +1002,60 @@ def _embedTeXFont(self, fontinfo):\n         _log.debug('Embedding TeX font %s - fontinfo=%s',\n                    fontinfo.dvifont.texname, fontinfo.__dict__)\n \n-        # Widths\n-        widthsObject = self.reserveObject('font widths')\n-        tfm = fontinfo.dvifont._tfm\n-        # convert from TeX's 12.20 representation to 1/1000 text space units.\n-        widths = [(1000 * metrics.tex_width) >> 20\n-                  if (metrics := tfm.get_metrics(char)) else 0\n-                  for char in range(max(tfm._glyph_metrics, default=-1) + 1)]\n-        self.writeObject(widthsObject, widths)\n-\n-        # Font dictionary\n+        # The font dictionary is the top-level object describing a font\n         fontdictObject = self.reserveObject('font dictionary')\n         fontdict = {\n             'Type':      Name('Font'),\n             'Subtype':   Name('Type1'),\n-            'FirstChar': 0,\n-            'LastChar':  len(widths) - 1,\n-            'Widths':    widthsObject,\n-            }\n-\n-        # Encoding (if needed)\n-        if fontinfo.encodingfile is not None:\n-            fontdict['Encoding'] = {\n-                'Type': Name('Encoding'),\n-                'Differences': [\n-                    0, *map(Name, dviread._parse_enc(fontinfo.encodingfile))],\n-            }\n+        }\n \n-        # We have a font file to embed - read it in and apply any effects\n+        # Read the font file and apply any encoding changes and effects\n         t1font = _type1font.Type1Font(fontinfo.fontfile)\n+        if fontinfo.encodingfile is not None:\n+            t1font = t1font.with_encoding(\n+                {i: c for i, c in enumerate(dviread._parse_enc(fontinfo.encodingfile))}\n+            )\n         if fontinfo.effects:\n             t1font = t1font.transform(fontinfo.effects)\n-        fontdict['BaseFont'] = Name(t1font.prop['FontName'])\n \n-        # Font descriptors may be shared between differently encoded\n-        # Type-1 fonts, so only create a new descriptor if there is no\n-        # existing descriptor for this font.\n-        effects = (fontinfo.effects.get('slant', 0.0),\n-                   fontinfo.effects.get('extend', 1.0))\n-        fontdesc = self._type1Descriptors.get((fontinfo.fontfile, effects))\n-        if fontdesc is None:\n-            fontdesc = self.createType1Descriptor(t1font)\n-            self._type1Descriptors[(fontinfo.fontfile, effects)] = fontdesc\n-        fontdict['FontDescriptor'] = fontdesc\n+        # Reduce the font to only the glyphs used in the document, get the encoding\n+        # for that subset, and compute various properties based on the encoding.\n+        chars = frozenset(self._character_tracker.used[fontinfo.dvifont.fname])\n+        t1font = t1font.subset(chars, self._get_subset_prefix(chars))\n+        fontdict['BaseFont'] = Name(t1font.prop['FontName'])\n+        # createType1Descriptor writes the font data as a side effect\n+        fontdict['FontDescriptor'] = self.createType1Descriptor(t1font)\n+        encoding = t1font.prop['Encoding']\n+        fontdict['Encoding'] = self._generate_encoding(encoding)\n+        fc = fontdict['FirstChar'] = min(encoding.keys(), default=0)\n+        lc = fontdict['LastChar'] = max(encoding.keys(), default=255)\n+\n+        # Convert glyph widths from TeX 12.20 fixed point to 1/1000 text space units\n+        tfm = fontinfo.dvifont._tfm\n+        widths = [(1000 * metrics.tex_width) >> 20\n+                  if (metrics := tfm.get_metrics(char)) else 0\n+                  for char in range(fc, lc + 1)]\n+        fontdict['Widths'] = widthsObject = self.reserveObject('glyph widths')\n+        self.writeObject(widthsObject, widths)\n \n         self.writeObject(fontdictObject, fontdict)\n         return fontdictObject\n \n+\n+    def _generate_encoding(self, encoding):\n+        prev = -2\n+        result = []\n+        for code, name in sorted(encoding.items()):\n+            if code != prev + 1:\n+                result.append(code)\n+            prev = code\n+            result.append(Name(name))\n+        return {\n+            'Type': Name('Encoding'),\n+            'Differences': result\n+        }\n+\n+\n     @_api.delete_parameter(\"3.11\", \"fontfile\")\n     def createType1Descriptor(self, t1font, fontfile=None):\n         # Create and write the font descriptor and the font file\n@@ -1077,6 +1092,14 @@ def createType1Descriptor(self, t1font, fontfile=None):\n         if 0:\n             flags |= 1 << 18\n \n+        encoding = t1font.prop['Encoding']\n+        charset = ''.join(\n+            sorted(\n+                f'/{c}' for c in encoding.values()\n+                if c != '.notdef'\n+            )\n+        )\n+\n         descriptor = {\n             'Type':        Name('FontDescriptor'),\n             'FontName':    Name(t1font.prop['FontName']),\n@@ -1090,6 +1113,7 @@ def createType1Descriptor(self, t1font, fontfile=None):\n             'FontFile':    fontfileObject,\n             'FontFamily':  t1font.prop['FamilyName'],\n             'StemV':       50,  # TODO\n+            'CharSet':     charset,\n             # (see also revision 3874; but not all TeX distros have AFM files!)\n             # 'FontWeight': a number where 400 = Regular, 700 = Bold\n         }\n@@ -2267,6 +2291,7 @@ def draw_tex(self, gc, x, y, s, prop, angle, *, mtext=None):\n                 seq += [['font', pdfname, dvifont.size]]\n                 oldfont = dvifont\n             seq += [['text', x1, y1, [bytes([glyph])], x1+width]]\n+            self.file._character_tracker.track(dvifont, chr(glyph))\n \n         # Find consecutive text strings with constant y coordinate and\n         # combine into a sequence of strings and kerns, or just one\ndiff --git a/lib/matplotlib/dviread.py b/lib/matplotlib/dviread.py\nindex a588979f5fad..9e8b6a5facf5 100644\n--- a/lib/matplotlib/dviread.py\n+++ b/lib/matplotlib/dviread.py\n@@ -17,17 +17,17 @@\n               ...\n \"\"\"\n \n-from collections import namedtuple\n import dataclasses\n import enum\n-from functools import cache, lru_cache, partial, wraps\n import logging\n import os\n-from pathlib import Path\n import re\n import struct\n import subprocess\n import sys\n+from collections import namedtuple\n+from functools import cache, lru_cache, partial, wraps\n+from pathlib import Path\n \n import numpy as np\n \n@@ -583,6 +583,9 @@ class DviFont:\n     Attributes\n     ----------\n     texname : bytes\n+    fname : str\n+       Compatibility shim so that DviFont can be used with\n+       ``_backend_pdf_ps.CharacterTracker``; not a real filename.\n     size : float\n        Size of the font in Adobe points, converted from the slightly\n        smaller TeX points.\n@@ -602,6 +605,18 @@ def __init__(self, scale, tfm, texname, vf):\n         (1000 * self._tfm.width.get(char, 0)) >> 20\n         for char in range(max(self._tfm.width, default=-1) + 1)]))\n \n+    @property\n+    def fname(self):\n+        \"\"\"A fake filename\"\"\"\n+        return self.texname.decode('latin-1')\n+\n+    def _get_fontmap(self, string):\n+        \"\"\"Get the mapping from characters to the font that includes them.\n+\n+        Each value maps to self; there is no fallback mechanism for DviFont.\n+        \"\"\"\n+        return {char: self for char in string}\n+\n     def __eq__(self, other):\n         return (type(self) is type(other)\n                 and self.texname == other.texname and self.size == other.size)\n@@ -1161,8 +1176,8 @@ def _fontfile(cls, suffix, texname):\n \n \n if __name__ == '__main__':\n-    from argparse import ArgumentParser\n     import itertools\n+    from argparse import ArgumentParser\n \n     import fontTools.agl\n \ndiff --git a/lib/matplotlib/dviread.pyi b/lib/matplotlib/dviread.pyi\nindex 41799c083218..12a9215b5308 100644\n--- a/lib/matplotlib/dviread.pyi\n+++ b/lib/matplotlib/dviread.pyi\n@@ -66,6 +66,8 @@ class DviFont:\n     def __ne__(self, other: object) -> bool: ...\n     @property\n     def widths(self) -> list[int]: ...\n+    @property\n+    def fname(self) -> str: ...\n \n class Vf(Dvi):\n     def __init__(self, filename: str | os.PathLike) -> None: ...\n", "test_patch": "diff --git a/.github/workflows/tests.yml b/.github/workflows/tests.yml\nindex 7a197a9d4aa8..2a48276707ce 100644\n--- a/.github/workflows/tests.yml\n+++ b/.github/workflows/tests.yml\n@@ -64,8 +64,10 @@ jobs:\n             python-version: '3.12'\n             # https://github.com/matplotlib/matplotlib/issues/29844\n             pygobject-ver: '<3.52.0'\n-          - os: ubuntu-22.04\n+          - name-suffix: \"(Extra TeX packages)\"\n+            os: ubuntu-22.04\n             python-version: '3.13'\n+            extra-packages: 'texlive-fonts-extra texlive-lang-cyrillic'\n             # https://github.com/matplotlib/matplotlib/issues/29844\n             pygobject-ver: '<3.52.0'\n           - name-suffix: \"Free-threaded\"\n@@ -142,7 +144,8 @@ jobs:\n               texlive-latex-recommended \\\n               texlive-luatex \\\n               texlive-pictures \\\n-              texlive-xetex\n+              texlive-xetex \\\n+              ${{ matrix.extra-packages }}\n             if [[ \"${{ matrix.name-suffix }}\" != '(Minimum Versions)' ]]; then\n               sudo apt-get install -yy --no-install-recommends ffmpeg poppler-utils\n             fi\ndiff --git a/lib/matplotlib/tests/baseline_images/test_backend_pdf/font-bitstream-charter.pdf b/lib/matplotlib/tests/baseline_images/test_backend_pdf/font-bitstream-charter.pdf\nnew file mode 100644\nindex 000000000000..c8f9411fb3d9\nBinary files /dev/null and b/lib/matplotlib/tests/baseline_images/test_backend_pdf/font-bitstream-charter.pdf differ\ndiff --git a/lib/matplotlib/tests/baseline_images/test_backend_pdf/font-dejavusans.pdf b/lib/matplotlib/tests/baseline_images/test_backend_pdf/font-dejavusans.pdf\nnew file mode 100644\nindex 000000000000..fd907dee6687\nBinary files /dev/null and b/lib/matplotlib/tests/baseline_images/test_backend_pdf/font-dejavusans.pdf differ\ndiff --git a/lib/matplotlib/tests/baseline_images/test_backend_pdf/font-heuristica.pdf b/lib/matplotlib/tests/baseline_images/test_backend_pdf/font-heuristica.pdf\nnew file mode 100644\nindex 000000000000..ca9b38d09b89\nBinary files /dev/null and b/lib/matplotlib/tests/baseline_images/test_backend_pdf/font-heuristica.pdf differ\ndiff --git a/lib/matplotlib/tests/test_backend_pdf.py b/lib/matplotlib/tests/test_backend_pdf.py\nindex dc349e8dfa35..f126fb543e78 100644\n--- a/lib/matplotlib/tests/test_backend_pdf.py\n+++ b/lib/matplotlib/tests/test_backend_pdf.py\n@@ -16,7 +16,7 @@\n from matplotlib.backends._backend_pdf_ps import get_glyphs_subset, font_as_file\n from matplotlib.backends.backend_pdf import PdfPages\n from matplotlib.patches import Rectangle\n-from matplotlib.testing import _gen_multi_font_text\n+from matplotlib.testing import _gen_multi_font_text, _has_tex_package\n from matplotlib.testing.decorators import check_figures_equal, image_comparison\n from matplotlib.testing._markers import needs_usetex\n \n@@ -428,3 +428,53 @@ def test_truetype_conversion(recwarn):\n             font=Path(__file__).parent / \"data/mpltest.ttf\", fontsize=80)\n     ax.set_xticks([])\n     ax.set_yticks([])\n+\n+\n+@pytest.mark.skipif(not _has_tex_package(\"heuristica\"),\n+                    reason=\"LaTeX lacks heuristica package\")\n+@image_comparison([\"font-heuristica.pdf\"])\n+def test_font_heuristica():\n+    # Heuristica uses the callothersubr operator for some glyphs\n+    mpl.rcParams['text.latex.preamble'] = '\\n'.join((\n+        r'\\usepackage{heuristica}',\n+        r'\\usepackage[T1]{fontenc}',\n+        r'\\usepackage[utf8]{inputenc}'\n+    ))\n+    fig, ax = plt.subplots()\n+    ax.text(0.1, 0.1, r\"BHTem fi ffl 1234\", usetex=True, fontsize=50)\n+    ax.set_xticks([])\n+    ax.set_yticks([])\n+\n+\n+@pytest.mark.skipif(not _has_tex_package(\"DejaVuSans\"),\n+                    reason=\"LaTeX lacks DejaVuSans package\")\n+@image_comparison([\"font-dejavusans.pdf\"])\n+def test_font_dejavusans():\n+    # DejaVuSans uses the seac operator to compose characters with diacritics\n+    mpl.rcParams['text.latex.preamble'] = '\\n'.join((\n+        r'\\usepackage{DejaVuSans}',\n+        r'\\usepackage[T1]{fontenc}',\n+        r'\\usepackage[utf8]{inputenc}'\n+    ))\n+\n+    fig, ax = plt.subplots()\n+    ax.text(0.1, 0.1, r\"\\textsf{\u00f1\u00e4\u00f6 ABCDabcd}\", usetex=True, fontsize=50)\n+    ax.text(0.1, 0.3, r\"\\textsf{fi ffl 1234}\", usetex=True, fontsize=50)\n+    ax.set_xticks([])\n+    ax.set_yticks([])\n+\n+\n+@pytest.mark.skipif(not _has_tex_package(\"charter\"),\n+                    reason=\"LaTeX lacks charter package\")\n+@image_comparison([\"font-bitstream-charter.pdf\"])\n+def test_font_bitstream_charter():\n+    mpl.rcParams['text.latex.preamble'] = '\\n'.join((\n+        r'\\usepackage{charter}',\n+        r'\\usepackage[T1]{fontenc}',\n+        r'\\usepackage[utf8]{inputenc}'\n+    ))\n+    fig, ax = plt.subplots()\n+    ax.text(0.1, 0.1, r\"\u00e5\u00fc\u0161 ABCDabcd\", usetex=True, fontsize=50)\n+    ax.text(0.1, 0.3, r\"fi ffl 1234\", usetex=True, fontsize=50)\n+    ax.set_xticks([])\n+    ax.set_yticks([])\ndiff --git a/lib/matplotlib/tests/test_usetex.py b/lib/matplotlib/tests/test_usetex.py\nindex 0b6d6d5e5535..95eb69325622 100644\n--- a/lib/matplotlib/tests/test_usetex.py\n+++ b/lib/matplotlib/tests/test_usetex.py\n@@ -1,3 +1,4 @@\n+import re\n from tempfile import TemporaryFile\n \n import numpy as np\n@@ -156,6 +157,69 @@ def test_missing_psfont(fmt, monkeypatch):\n         fig.savefig(tmpfile, format=fmt)\n \n \n+def test_pdf_type1_font_subsetting():\n+    \"\"\"Test that fonts in PDF output are properly subset.\"\"\"\n+    pikepdf = pytest.importorskip(\"pikepdf\")\n+\n+    mpl.rcParams[\"text.usetex\"] = True\n+    mpl.rcParams[\"text.latex.preamble\"] = r\"\\usepackage{amssymb}\"\n+    fig, ax = plt.subplots()\n+    ax.text(0.2, 0.7, r\"$\\int_{-\\infty}^{\\aleph}\\sqrt{\\alpha\\beta\\gamma}\\mathrm{d}x$\")\n+    ax.text(0.2, 0.5, r\"$\\mathfrak{x}\\circledcirc\\mathfrak{y}\\in\\mathbb{R}$\")\n+\n+    with TemporaryFile() as tmpfile:\n+        fig.savefig(tmpfile, format=\"pdf\")\n+        tmpfile.seek(0)\n+        pdf = pikepdf.Pdf.open(tmpfile)\n+\n+        length = {}\n+        page = pdf.pages[0]\n+        for font_name, font in page.Resources.Font.items():\n+            assert font.Subtype == \"/Type1\", (\n+                f\"Font {font_name}={font} is not a Type 1 font\"\n+            )\n+\n+            # Subsetted font names have a 6-character tag followed by a '+'\n+            base_font = str(font[\"/BaseFont\"]).removeprefix(\"/\")\n+            assert re.match(r\"^[A-Z]{6}\\+\", base_font), (\n+                f\"Font {font_name}={base_font} lacks a subset indicator tag\"\n+            )\n+            assert \"/FontFile\" in font.FontDescriptor, (\n+                f\"Type 1 font {font_name}={base_font} is not embedded\"\n+            )\n+            _, original_name = base_font.split(\"+\", 1)\n+            length[original_name] = len(bytes(font[\"/FontDescriptor\"][\"/FontFile\"]))\n+\n+    print(\"Embedded font stream lengths:\", length)\n+    # We should have several fonts, each much smaller than the original.\n+    # I get under 10kB on my system for each font, but allow 15kB in case\n+    # of differences in the font files.\n+    assert {\n+        'CMEX10',\n+        'CMMI12',\n+        'CMR12',\n+        'CMSY10',\n+        'CMSY8',\n+        'EUFM10',\n+        'MSAM10',\n+        'MSBM10',\n+    }.issubset(length), \"Missing expected fonts in the PDF\"\n+    for font_name, length in length.items():\n+        assert length < 15_000, (\n+            f\"Font {font_name}={length} is larger than expected\"\n+        )\n+\n+    # For comparison, lengths without subsetting on my system:\n+    #  'CMEX10': 29686\n+    #  'CMMI12': 36176\n+    #  'CMR12': 32157\n+    #  'CMSY10': 32004\n+    #  'CMSY8': 32061\n+    #  'EUFM10': 20546\n+    #  'MSAM10': 31199\n+    #  'MSBM10': 34129\n+\n+\n try:\n     _old_gs_version = mpl._get_executable_info('gs').version < parse_version('9.55')\n except mpl.ExecutableNotFoundError:\n", "problem_statement": "When text.usetex=True with pdf backend, full subset of latex fonts is embedded into pdf file\nSaving plots to pdf, using latex for font rendering (i.e. setting rcParams['text.usetex']=True) results in very large file size (~150kb for simple line plot with some mathematical latex expressions). This is due to all fonts being fully embedded into pdf. \n\nWhen resulting pdf is postprocessed, so that only subset of fonts is embedded, file size is reduced drastically (e.g. from 150kb to 15kb).\n\nThe embedded fonts are of FontType=1.\n\nPdf backend should support embedding of subset of fonts when rcParams[\"text.usetex\"]=True is used.\n\n", "hints_text": "Needs Type1 subsetting, which is somewhat nontrivial. On my list of things to do eventually.\n\n@jkseppan What is the state of this?\n\nNothing yet. There's a plan for a larger-scope refactoring of text handling in [MEP14](https://github.com/matplotlib/matplotlib/wiki/Mep14) and that would help with subsetting too.\n\nUpdating after a couple years again: same as last comment\n\n", "all_hints_text": "Needs Type1 subsetting, which is somewhat nontrivial. On my list of things to do eventually.\n\n@jkseppan What is the state of this?\n\nNothing yet. There's a plan for a larger-scope refactoring of text handling in [MEP14](https://github.com/matplotlib/matplotlib/wiki/Mep14) and that would help with subsetting too.\n\nUpdating after a couple years again: same as last comment\nThis issue has been marked \"inactive\" because it has been 365 days since the last comment. If this issue is still present in recent Matplotlib releases, or the feature request is still wanted, please leave a comment and this label will be removed. If there are no updates in another 30 days, this issue will be automatically closed, but you are free to re-open or create a new issue if needed. We value issue reports, and this procedure is meant to help us resurface and prioritize issues that have not been addressed yet, not make them disappear.  Thanks for your help!\nThis issue is mad old \n@jkseppan Did a version of this go in with the font fallback work we did last summer?\nThe Type 1 font subsetting PR is still open, I think.\nI finally updated the PR. It had a small conflict with something related to the font fallbacks, and a bug that caused the fonts to break on some viewers. It would be a good idea to test this with a wide variety of PDF viewers.\n\n", "commit_urls": ["https://github.com/matplotlib/matplotlib/commit/76d8b3cfdd9247834f25f4c2aacd7f6dc863e5b8", "https://github.com/matplotlib/matplotlib/commit/22198e9de2285d76bec1a3013011568076947f8a", "https://github.com/matplotlib/matplotlib/commit/53355cae4f15bd672cc9829719f6a7c68fb5a8f7", "https://github.com/matplotlib/matplotlib/commit/c77a459690da733028a60dce5f0991db3bc5701f", "https://github.com/matplotlib/matplotlib/commit/8abab038d76854bd117b65bfc635a5af2c324548", "https://github.com/matplotlib/matplotlib/commit/d5ab3b02bbaf05ee2998803150b3e2444980ca3e"], "created_at": "2021-07-22T13:35:28Z", "classification": "Efficiency"}
{"repo": "numpy/numpy", "pull_number": 29358, "instance_id": "numpy__numpy-29358", "issue_numbers": [29355], "base_commit": "4816e4d31b2ce4ce5ca689cf9538e0c1aa41b3f2", "patch": "diff --git a/numpy/_core/src/umath/ufunc_object.c b/numpy/_core/src/umath/ufunc_object.c\nindex 485364af1ff2..fd9ae9f1e41d 100644\n--- a/numpy/_core/src/umath/ufunc_object.c\n+++ b/numpy/_core/src/umath/ufunc_object.c\n@@ -3727,6 +3727,8 @@ PyUFunc_GenericReduction(PyUFuncObject *ufunc,\n     if (ret == NULL) {\n         goto fail;\n     }\n+    \n+    Py_XDECREF(out);\n \n     Py_DECREF(signature[0]);\n     Py_DECREF(signature[1]);\n@@ -3753,6 +3755,8 @@ PyUFunc_GenericReduction(PyUFuncObject *ufunc,\n     return wrapped_result;\n \n fail:\n+    Py_XDECREF(out);\n+\n     Py_XDECREF(signature[0]);\n     Py_XDECREF(signature[1]);\n     Py_XDECREF(signature[2]);\n", "test_patch": "diff --git a/numpy/_core/tests/test_ufunc.py b/numpy/_core/tests/test_ufunc.py\nindex 9c5489a614e8..84e31bd1bc3c 100644\n--- a/numpy/_core/tests/test_ufunc.py\n+++ b/numpy/_core/tests/test_ufunc.py\n@@ -3003,6 +3003,45 @@ def test_reduce_casterrors(offset):\n     assert out[()] < value * offset\n \n \n+@pytest.mark.skipif(not HAS_REFCOUNT, reason=\"Python lacks refcounts\")\n+def test_reduction_no_reference_leak():\n+    # Test that the generic reduction does not leak references.\n+    # gh-29358\n+    arr = np.array([1, 2, 3], dtype=np.int32)\n+    count = sys.getrefcount(arr)\n+\n+    np.add.reduce(arr, dtype=np.int32, initial=0)\n+    assert count == sys.getrefcount(arr)\n+\n+    np.add.accumulate(arr, dtype=np.int32)\n+    assert count == sys.getrefcount(arr)\n+\n+    np.add.reduceat(arr, [0, 1], dtype=np.int32)\n+    assert count == sys.getrefcount(arr)\n+\n+    # with `out=` the reference count is not changed\n+    out = np.empty((), dtype=np.int32)\n+    out_count = sys.getrefcount(out)\n+\n+    np.add.reduce(arr, dtype=np.int32, out=out, initial=0)\n+    assert count == sys.getrefcount(arr)\n+    assert out_count == sys.getrefcount(out)\n+\n+    out = np.empty(arr.shape, dtype=np.int32)\n+    out_count = sys.getrefcount(out)\n+\n+    np.add.accumulate(arr, dtype=np.int32, out=out)\n+    assert count == sys.getrefcount(arr)\n+    assert out_count == sys.getrefcount(out)\n+\n+    out = np.empty((2,), dtype=np.int32)\n+    out_count = sys.getrefcount(out)\n+\n+    np.add.reduceat(arr, [0, 1], dtype=np.int32, out=out)\n+    assert count == sys.getrefcount(arr)\n+    assert out_count == sys.getrefcount(out)\n+\n+\n def test_object_reduce_cleanup_on_failure():\n     # Test cleanup, including of the initial value (manually provided or not)\n     with pytest.raises(TypeError):\n", "problem_statement": "BUG: Memory leak using `out=` parameter in numpy version 2.3\n### Describe the issue:\n\nThere seems to be a memory leak introduced in numpy 2.3.0.  I've noticed this when using `numpy.cumsum` with the `out=` parameter.  It looks as if data for the array passed in the `out` parameter is not being released. \n\n### Reproduce the code example:\n\n```python\nimport numpy as np\nimport pytest\n\n# (also needs pytest-memray installed)\n\n@pytest.mark.limit_leaks(\"100 KB\")\ndef test_does_not_leak() -> None:\n    for _ in range(100):\n        size = 1000\n        x = np.ones(size, np.float64)\n        v = np.empty(size, np.float64)\n        np.cumsum(x, out=v)\n```\n\n### Error message:\n\n```shell\nTest was allowed to leak 100.0KiB per location but at least one location leaked more\n----------------------------------------------------------------------------------------------------- memray-leaked-memory ------------------------------------------------------------------------------------------------------\nList of leaked allocations:\n    - 781.2KiB allocated here:\n        default_malloc:<unknown>:0\n        PyDataMem_UserNEW:<unknown>:0\n        PyArray_NewFromDescr_int:<unknown>:0\n        PyArray_Empty_int:<unknown>:0\n        array_empty:<unknown>:0\n        ...\n```\n\n### Python and NumPy Versions:\n\nTested on python 3.11\nnumpy:\n   * numpy 2.3.0 leaks\n   * numpy 2.2.6 does not leak\n\n\n\n### Runtime Environment:\n\n```\n>>> numpy.show_runtime()\n[{'numpy_version': '2.3.0',\n  'python': '3.11.13 (main, Jun  9 2025, 16:09:23) [GCC 13.1.1 20230614 (Red '\n            'Hat 13.1.1-4)]',\n  'uname': uname_result(system='Linux', node='ab21b3c99130', release='6.6.87.2-microsoft-standard-WSL2', version='#1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025', machine='x86_64')},\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\n                      'found': ['SSSE3',\n                                'SSE41',\n                                'POPCNT',\n                                'SSE42',\n                                'AVX',\n                                'F16C',\n                                'FMA3',\n                                'AVX2'],\n                      'not_found': ['AVX512F',\n                                    'AVX512CD',\n                                    'AVX512_KNL',\n                                    'AVX512_KNM',\n                                    'AVX512_SKX',\n                                    'AVX512_CLX',\n                                    'AVX512_CNL',\n                                    'AVX512_ICL',\n                                    'AVX512_SPR']}},\n {'architecture': 'Haswell',\n  'filepath': '/dependencies/opt/bb/lib/python3.11/site-packages/numpy.libs/libscipy_openblas64_-56d6093b.so',\n  'internal_api': 'openblas',\n  'num_threads': 22,\n  'prefix': 'libscipy_openblas',\n  'threading_layer': 'pthreads',\n  'user_api': 'blas',\n  'version': '0.3.29'}]\n```\n\n### Context for the issue:\n\nThis is problematic for long running python services, as it can result in unbounded memory growth and eventual service failure.\n\n", "hints_text": "Thanks for the report, seems like a result reference is leaked here.\n\n", "all_hints_text": "Thanks for the report, seems like a result reference is leaked here.\n\n", "commit_urls": ["https://github.com/numpy/numpy/commit/d09d1354700f604f44e368fb053de66dd4fbf41b", "https://github.com/numpy/numpy/commit/c89f9fd588dd29f19b12118d96b4595b9ffbb8a5", "https://github.com/numpy/numpy/commit/edfee27e48db03a5d366ad03e249d608e9e72ee5", "https://github.com/numpy/numpy/commit/61a0e96a496118339ed100f26363f1f780373456", "https://github.com/numpy/numpy/commit/e06cc4d81bf3e30bc5690ab58984edc497d59411", "https://github.com/numpy/numpy/commit/c49330162a4fb8709474252702cf6ad48af235e7", "https://github.com/numpy/numpy/commit/a690b1bf4b05b236e223531b4cfe4115ffbb0599", "https://github.com/numpy/numpy/commit/b6fa00ce411ef2e2a65cc349c603617087289fe3"], "created_at": "2025-07-11T00:44:16Z", "classification": "Efficiency"}
{"repo": "pydantic/pydantic", "pull_number": 11898, "instance_id": "pydantic__pydantic-11898", "issue_numbers": [11122, 11870, 11876, 11978], "base_commit": "9d6b2bfd74c8f42cac5a17c8031ff08d50bf5579", "patch": "diff --git a/pydantic/_internal/_fields.py b/pydantic/_internal/_fields.py\nindex a862f60ffe..ccd0face31 100644\n--- a/pydantic/_internal/_fields.py\n+++ b/pydantic/_internal/_fields.py\n@@ -326,18 +326,15 @@ def collect_model_fields(  # noqa: C901\n                 # Note that we only do this for method descriptors for now, we might want to\n                 # extend this to any descriptor in the future (by simply checking for\n                 # `hasattr(assigned_value.default, '__get__')`).\n-                assigned_value.default = assigned_value.default.__get__(None, cls)\n-\n-            # The `from_annotated_attribute()` call below mutates the assigned `Field()`, so make a copy:\n-            original_assignment = (\n-                copy(assigned_value) if not evaluated and isinstance(assigned_value, FieldInfo_) else assigned_value\n-            )\n+                default = assigned_value.default.__get__(None, cls)\n+                assigned_value.default = default\n+                assigned_value._attributes_set['default'] = default\n \n             field_info = FieldInfo_.from_annotated_attribute(ann_type, assigned_value, _source=AnnotationSource.CLASS)\n             # Store the original annotation and assignment value that should be used to rebuild the field info later.\n             # Note that the assignment is always stored as the annotation might contain a type var that is later\n             #  parameterized with an unknown forward reference (and we'll need it to rebuild the field info):\n-            field_info._original_assignment = original_assignment\n+            field_info._original_assignment = assigned_value\n             if not evaluated:\n                 field_info._complete = False\n                 field_info._original_annotation = ann_type\ndiff --git a/pydantic/fields.py b/pydantic/fields.py\nindex cdcd5d56e0..7b0b73f73f 100644\n--- a/pydantic/fields.py\n+++ b/pydantic/fields.py\n@@ -10,13 +10,13 @@\n from copy import copy\n from dataclasses import Field as DataclassField\n from functools import cached_property\n-from typing import Annotated, Any, ClassVar, Literal, TypeVar, cast, overload\n+from typing import Annotated, Any, ClassVar, Literal, TypeVar, cast, final, overload\n from warnings import warn\n \n import annotated_types\n import typing_extensions\n from pydantic_core import PydanticUndefined\n-from typing_extensions import TypeAlias, Unpack, deprecated\n+from typing_extensions import Self, TypeAlias, Unpack, deprecated\n from typing_inspection import typing_objects\n from typing_inspection.introspection import UNKNOWN, AnnotationSource, ForbiddenQualifier, Qualifier, inspect_annotation\n \n@@ -97,6 +97,7 @@ class _FieldInfoInputs(_FromFieldInfoInputs, total=False):\n     default: Any\n \n \n+@final\n class FieldInfo(_repr.Representation):\n     \"\"\"This class holds information about a field.\n \n@@ -213,7 +214,7 @@ def __init__(self, **kwargs: Unpack[_FieldInfoInputs]) -> None:\n \n         See the signature of `pydantic.fields.Field` for more details about the expected arguments.\n         \"\"\"\n-        self._attributes_set = {k: v for k, v in kwargs.items() if v is not _Unset}\n+        self._attributes_set = {k: v for k, v in kwargs.items() if v is not _Unset and k not in self.metadata_lookup}\n         kwargs = {k: _DefaultValues.get(k) if v is _Unset else v for k, v in kwargs.items()}  # type: ignore\n         self.annotation = kwargs.get('annotation')\n \n@@ -340,30 +341,12 @@ class MyModel(pydantic.BaseModel):\n         final = 'final' in inspected_ann.qualifiers\n         metadata = inspected_ann.metadata\n \n-        if not metadata:\n-            # No metadata, e.g. `field: int`, or `field: Final[str]`:\n-            field_info = FieldInfo(annotation=type_expr, frozen=final or None)\n-            field_info._qualifiers = inspected_ann.qualifiers\n-            return field_info\n-\n-        # With metadata, e.g. `field: Annotated[int, Field(...), Gt(1)]`:\n-        field_info_annotations = [a for a in metadata if isinstance(a, FieldInfo)]\n-        field_info = FieldInfo.merge_field_infos(*field_info_annotations, annotation=type_expr)\n-\n-        new_field_info = copy(field_info)\n-        new_field_info.annotation = type_expr\n-        new_field_info.frozen = final or field_info.frozen\n-        field_metadata: list[Any] = []\n-        for a in metadata:\n-            if typing_objects.is_deprecated(a):\n-                new_field_info.deprecated = a.message\n-            elif not isinstance(a, FieldInfo):\n-                field_metadata.append(a)\n-            else:\n-                field_metadata.extend(a.metadata)\n-            new_field_info.metadata = field_metadata\n-        new_field_info._qualifiers = inspected_ann.qualifiers\n-        return new_field_info\n+        attr_overrides = {'annotation': type_expr}\n+        if final:\n+            attr_overrides['frozen'] = True\n+        field_info = FieldInfo._construct(metadata, **attr_overrides)\n+        field_info._qualifiers = inspected_ann.qualifiers\n+        return field_info\n \n     @staticmethod\n     def from_annotated_attribute(\n@@ -417,58 +400,130 @@ class MyModel(pydantic.BaseModel):\n         final = 'final' in inspected_ann.qualifiers\n         metadata = inspected_ann.metadata\n \n-        if isinstance(default, FieldInfo):\n-            # e.g. `field: int = Field(...)`\n-            default.annotation = type_expr\n-            default.metadata += metadata\n-            merged_default = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                default,\n-                annotation=default.annotation,\n-            )\n-            merged_default.frozen = final or merged_default.frozen\n-            merged_default._qualifiers = inspected_ann.qualifiers\n-            return merged_default\n-\n-        if isinstance(default, dataclasses.Field):\n-            # `collect_dataclass_fields()` passes the dataclass Field as a default.\n-            pydantic_field = FieldInfo._from_dataclass_field(default)\n-            pydantic_field.annotation = type_expr\n-            pydantic_field.metadata += metadata\n-            pydantic_field = FieldInfo.merge_field_infos(\n-                *[x for x in metadata if isinstance(x, FieldInfo)],\n-                pydantic_field,\n-                annotation=pydantic_field.annotation,\n-            )\n-            pydantic_field.frozen = final or pydantic_field.frozen\n-            pydantic_field.init_var = 'init_var' in inspected_ann.qualifiers\n-            pydantic_field.init = getattr(default, 'init', None)\n-            pydantic_field.kw_only = getattr(default, 'kw_only', None)\n-            pydantic_field._qualifiers = inspected_ann.qualifiers\n-            return pydantic_field\n-\n-        if not metadata:\n-            # No metadata, e.g. `field: int = ...`, or `field: Final[str] = ...`:\n-            field_info = FieldInfo(annotation=type_expr, default=default, frozen=final or None)\n-            field_info._qualifiers = inspected_ann.qualifiers\n+        # HACK 1: the order in which the metadata is merged is inconsistent; we need to prepend\n+        # metadata from the assignment at the beginning of the metadata. Changing this is only\n+        # possible in v3 (at least). See https://github.com/pydantic/pydantic/issues/10507\n+        prepend_metadata: list[Any] | None = None\n+        attr_overrides = {'annotation': type_expr}\n+        if final:\n+            attr_overrides['frozen'] = True\n+\n+        # HACK 2: FastAPI is subclassing `FieldInfo` and historically expected the actual\n+        # instance's type to be preserved when constructing new models with its subclasses as assignments.\n+        # This code is never reached by Pydantic itself, and in an ideal world this shouldn't be necessary.\n+        if not metadata and isinstance(default, FieldInfo) and type(default) is not FieldInfo:\n+            field_info = default._copy()\n+            field_info._attributes_set.update(attr_overrides)\n+            for k, v in attr_overrides.items():\n+                setattr(field_info, k, v)\n             return field_info\n \n-        # With metadata, e.g. `field: Annotated[int, Field(...), Gt(1)] = ...`:\n-        field_infos = [a for a in metadata if isinstance(a, FieldInfo)]\n-        field_info = FieldInfo.merge_field_infos(*field_infos, annotation=type_expr, default=default)\n-        field_metadata: list[Any] = []\n-        for a in metadata:\n-            if typing_objects.is_deprecated(a):\n-                field_info.deprecated = a.message\n-            elif not isinstance(a, FieldInfo):\n-                field_metadata.append(a)\n-            else:\n-                field_metadata.extend(a.metadata)\n-        field_info.metadata = field_metadata\n+        if isinstance(default, FieldInfo):\n+            default_copy = default._copy()  # Copy unnecessary when we remove HACK 1.\n+            prepend_metadata = default_copy.metadata\n+            default_copy.metadata = []\n+            metadata = metadata + [default_copy]\n+        elif isinstance(default, dataclasses.Field):\n+            from_field = FieldInfo._from_dataclass_field(default)\n+            prepend_metadata = from_field.metadata  # Unnecessary when we remove HACK 1.\n+            from_field.metadata = []\n+            metadata = metadata + [from_field]\n+            if 'init_var' in inspected_ann.qualifiers:\n+                attr_overrides['init_var'] = True\n+            if (init := getattr(default, 'init', None)) is not None:\n+                attr_overrides['init'] = init\n+            if (kw_only := getattr(default, 'kw_only', None)) is not None:\n+                attr_overrides['kw_only'] = kw_only\n+        else:\n+            # `default` is the actual default value\n+            attr_overrides['default'] = default\n+\n+        field_info = FieldInfo._construct(\n+            prepend_metadata + metadata if prepend_metadata is not None else metadata, **attr_overrides\n+        )\n         field_info._qualifiers = inspected_ann.qualifiers\n         return field_info\n \n+    @classmethod\n+    def _construct(cls, metadata: list[Any], **attr_overrides: Any) -> Self:\n+        \"\"\"Construct the final `FieldInfo` instance, by merging the possibly existing `FieldInfo` instances from the metadata.\n+\n+        With the following example:\n+\n+        ```python {test=\"skip\" lint=\"skip\"}\n+        class Model(BaseModel):\n+            f: Annotated[int, Gt(1), Field(description='desc', lt=2)]\n+        ```\n+\n+        `metadata` refers to the metadata elements of the `Annotated` form. This metadata is iterated over from left to right:\n+\n+        - If the element is a `Field()` function (which is itself a `FieldInfo` instance), the field attributes (such as\n+          `description`) are saved to be set on the final `FieldInfo` instance.\n+          On the other hand, some kwargs (such as `lt`) are stored as `metadata` (see `FieldInfo.__init__()`, calling\n+          `FieldInfo._collect_metadata()`). In this case, the final metadata list is extended with the one from this instance.\n+        - Else, the element is considered as a single metadata object, and is appended to the final metadata list.\n+\n+        Args:\n+            metadata: The list of metadata elements to merge together. If the `FieldInfo` instance to be constructed is for\n+                a field with an assigned `Field()`, this `Field()` assignment should be added as the last element of the\n+                provided metadata.\n+            **attr_overrides: Extra attributes that should be set on the final merged `FieldInfo` instance.\n+\n+        Returns:\n+            The final merged `FieldInfo` instance.\n+        \"\"\"\n+        merged_metadata: list[Any] = []\n+        merged_kwargs: dict[str, Any] = {}\n+\n+        for meta in metadata:\n+            if isinstance(meta, FieldInfo):\n+                merged_metadata.extend(meta.metadata)\n+\n+                new_js_extra: JsonDict | None = None\n+                current_js_extra = meta.json_schema_extra\n+                if current_js_extra is not None and 'json_schema_extra' in merged_kwargs:\n+                    # We need to merge `json_schema_extra`'s:\n+                    existing_js_extra = merged_kwargs['json_schema_extra']\n+                    if isinstance(existing_js_extra, dict):\n+                        if isinstance(current_js_extra, dict):\n+                            new_js_extra = {\n+                                **existing_js_extra,\n+                                **current_js_extra,\n+                            }\n+                        elif callable(current_js_extra):\n+                            warn(\n+                                'Composing `dict` and `callable` type `json_schema_extra` is not supported. '\n+                                'The `callable` type is being ignored. '\n+                                \"If you'd like support for this behavior, please open an issue on pydantic.\",\n+                                UserWarning,\n+                            )\n+                    elif callable(existing_js_extra) and isinstance(current_js_extra, dict):\n+                        warn(\n+                            'Composing `dict` and `callable` type `json_schema_extra` is not supported. '\n+                            'The `callable` type is being ignored. '\n+                            \"If you'd like support for this behavior, please open an issue on pydantic.\",\n+                            UserWarning,\n+                        )\n+\n+                merged_kwargs.update(meta._attributes_set)\n+                if new_js_extra is not None:\n+                    merged_kwargs['json_schema_extra'] = new_js_extra\n+            elif typing_objects.is_deprecated(meta):\n+                merged_kwargs['deprecated'] = meta\n+            else:\n+                merged_metadata.append(meta)\n+\n+        merged_kwargs.update(attr_overrides)\n+        merged_field_info = cls(**merged_kwargs)\n+        merged_field_info.metadata = merged_metadata\n+        return merged_field_info\n+\n     @staticmethod\n+    @typing_extensions.deprecated(\n+        \"The 'merge_field_infos()' method is deprecated and will be removed in a future version. \"\n+        'If you relied on this method, please open an issue in the Pydantic issue tracker.',\n+        category=None,\n+    )\n     def merge_field_infos(*field_infos: FieldInfo, **overrides: Any) -> FieldInfo:\n         \"\"\"Merge `FieldInfo` instances keeping only explicitly set attributes.\n \n@@ -479,7 +534,7 @@ def merge_field_infos(*field_infos: FieldInfo, **overrides: Any) -> FieldInfo:\n         \"\"\"\n         if len(field_infos) == 1:\n             # No merging necessary, but we still need to make a copy and apply the overrides\n-            field_info = copy(field_infos[0])\n+            field_info = field_infos[0]._copy()\n             field_info._attributes_set.update(overrides)\n \n             default_override = overrides.pop('default', PydanticUndefined)\n@@ -700,6 +755,19 @@ def apply_typevars_map(\n             self._complete = False\n             self._original_annotation = self.annotation\n \n+    def _copy(self) -> Self:\n+        \"\"\"Return a copy of the `FieldInfo` instance.\"\"\"\n+        # Note: we can't define a custom `__copy__()`, as `FieldInfo` is being subclassed\n+        # by some third-party libraries with extra attributes defined (and as `FieldInfo`\n+        # is slotted, we can't make a copy of the `__dict__`).\n+        copied = copy(self)\n+        for attr_name in ('metadata', '_attributes_set', '_qualifiers'):\n+            # Apply \"deep-copy\" behavior on collections attributes:\n+            value = getattr(copied, attr_name).copy()\n+            setattr(copied, attr_name, value)\n+\n+        return copied\n+\n     def __repr_args__(self) -> ReprArgs:\n         yield 'annotation', _repr.PlainRepr(_repr.display_as_type(self.annotation))\n         yield 'required', self.is_required()\ndiff --git a/uv.lock b/uv.lock\nindex 8ccf284f78..636c39143e 100644\n--- a/uv.lock\n+++ b/uv.lock\n@@ -2721,14 +2721,14 @@ wheels = [\n \n [[package]]\n name = \"typing-inspection\"\n-version = \"0.4.0\"\n+version = \"0.4.1\"\n source = { registry = \"https://pypi.org/simple\" }\n dependencies = [\n     { name = \"typing-extensions\" },\n ]\n-sdist = { url = \"https://files.pythonhosted.org/packages/82/5c/e6082df02e215b846b4b8c0b887a64d7d08ffaba30605502639d44c06b82/typing_inspection-0.4.0.tar.gz\", hash = \"sha256:9765c87de36671694a67904bf2c96e395be9c6439bb6c87b5142569dcdd65122\", size = 76222, upload-time = \"2025-02-25T17:27:59.638Z\" }\n+sdist = { url = \"https://files.pythonhosted.org/packages/f8/b1/0c11f5058406b3af7609f121aaa6b609744687f1d158b3c3a5bf4cc94238/typing_inspection-0.4.1.tar.gz\", hash = \"sha256:6ae134cc0203c33377d43188d4064e9b357dba58cff3185f22924610e70a9d28\", size = 75726, upload-time = \"2025-05-21T18:55:23.885Z\" }\n wheels = [\n-    { url = \"https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl\", hash = \"sha256:50e72559fcd2a6367a19f7a7e610e6afcb9fac940c650290eed893d61386832f\", size = 14125, upload-time = \"2025-02-25T17:27:57.754Z\" },\n+    { url = \"https://files.pythonhosted.org/packages/17/69/cd203477f944c353c31bade965f880aa1061fd6bf05ded0726ca845b6ff7/typing_inspection-0.4.1-py3-none-any.whl\", hash = \"sha256:389055682238f53b04f7badcb49b989835495a96700ced5dab2d8feae4b26f51\", size = 14552, upload-time = \"2025-05-21T18:55:22.152Z\" },\n ]\n \n [[package]]\n", "test_patch": "diff --git a/tests/test_annotated.py b/tests/test_annotated.py\nindex 52e09d8b8a..48cb2d9424 100644\n--- a/tests/test_annotated.py\n+++ b/tests/test_annotated.py\n@@ -505,28 +505,6 @@ class AnnotatedFieldModel(BaseModel):\n         }\n     ]\n \n-    # Ensure that the inner annotation does not override the outer, even for metadata:\n-    class AnnotatedFieldModel2(BaseModel):\n-        foo: 'Annotated[String, Field(min_length=3)]' = Field(description='hello', min_length=2)\n-\n-    AnnotatedFieldModel2(foo='00')\n-\n-    class AnnotatedFieldModel4(BaseModel):\n-        foo: 'Annotated[String, Field(min_length=3)]' = Field(description='hello', min_length=4)\n-\n-    with pytest.raises(ValidationError) as exc_info:\n-        AnnotatedFieldModel4(foo='00')\n-\n-    assert exc_info.value.errors(include_url=False) == [\n-        {\n-            'loc': ('foo',),\n-            'input': '00',\n-            'ctx': {'min_length': 4},\n-            'msg': 'String should have at least 4 characters',\n-            'type': 'string_too_short',\n-        }\n-    ]\n-\n \n def test_tzinfo_validator_example_pattern() -> None:\n     \"\"\"Test that tzinfo custom validator pattern works as explained in the examples/validators docs.\"\"\"\ndiff --git a/tests/test_deprecated_fields.py b/tests/test_deprecated_fields.py\nindex ec26fe2625..b17f037ede 100644\n--- a/tests/test_deprecated_fields.py\n+++ b/tests/test_deprecated_fields.py\n@@ -252,8 +252,19 @@ class Model(BaseModel):\n     Test = int\n \n     Model.model_rebuild()\n-    assert Model.model_fields['a'].deprecated == 'test'\n+    assert isinstance(Model.model_fields['a'].deprecated, deprecated)\n+    assert Model.model_fields['a'].deprecated.message == 'test'\n \n     m = Model()\n \n     pytest.warns(DeprecationWarning, lambda: m.a, match='test')\n+\n+\n+def test_deprecated_field_with_assignment() -> None:\n+    class Model(BaseModel):\n+        # A buggy implementation made it so that deprecated wouldn't\n+        # appear on the `FieldInfo`:\n+        a: Annotated[int, deprecated('test')] = Field(default=1)\n+\n+    assert isinstance(Model.model_fields['a'].deprecated, deprecated)\n+    assert Model.model_fields['a'].deprecated.message == 'test'\ndiff --git a/tests/test_discriminated_union.py b/tests/test_discriminated_union.py\nindex fe21fabf4f..41eecf0449 100644\n--- a/tests/test_discriminated_union.py\n+++ b/tests/test_discriminated_union.py\n@@ -2254,3 +2254,47 @@ class Bar(BaseModel):\n     final_schema = gen_schema.clean_schema(disc_union_ref)\n \n     assert final_schema['type'] == 'tagged-union'\n+\n+\n+def test_recursive_discriminated_union() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/11978\"\"\"\n+\n+    F = TypeVar('F', bound=BaseModel)\n+\n+    class Not(BaseModel, Generic[F]):\n+        operand: F = Field()\n+\n+    class Label(BaseModel):\n+        prop: Literal['label'] = 'label'\n+\n+    def filter_discriminator(v):\n+        if isinstance(v, dict):\n+            if 'not' in v:\n+                return 'not'\n+            else:\n+                return v.get('prop')\n+\n+        if isinstance(v, Not):\n+            return 'not'\n+        else:\n+            return getattr(v, 'prop', None)\n+\n+    ParagraphFilterExpression = Annotated[\n+        Union[\n+            Annotated[Not['ParagraphFilterExpression'], Tag('not')],\n+            Annotated[Label, Tag('label')],\n+        ],\n+        Discriminator(filter_discriminator),\n+    ]\n+\n+    FieldFilterExpression = Annotated[\n+        Union[\n+            Annotated[Not['FieldFilterExpression'], Tag('not')],\n+            Annotated[Label, Tag('label')],\n+        ],\n+        Discriminator(filter_discriminator),\n+    ]\n+\n+    class FilterExpression(BaseModel):\n+        field: FieldFilterExpression\n+        paragraph: ParagraphFilterExpression\ndiff --git a/tests/test_fields.py b/tests/test_fields.py\nindex 1a50ff2a45..db5b3883b6 100644\n--- a/tests/test_fields.py\n+++ b/tests/test_fields.py\n@@ -1,9 +1,23 @@\n-from typing import Union\n+from typing import Annotated, Final, Union\n \n import pytest\n+from annotated_types import Gt\n+from pydantic_core import PydanticUndefined\n \n import pydantic.dataclasses\n-from pydantic import BaseModel, ConfigDict, Field, PydanticUserError, RootModel, ValidationError, computed_field, fields\n+from pydantic import (\n+    AfterValidator,\n+    BaseModel,\n+    ConfigDict,\n+    Field,\n+    PydanticUserError,\n+    RootModel,\n+    ValidationError,\n+    computed_field,\n+    create_model,\n+    fields,\n+)\n+from pydantic.fields import FieldInfo\n \n \n def test_field_info_annotation_keyword_argument():\n@@ -188,3 +202,78 @@ class Model(BaseModel):\n     Model.model_rebuild()\n \n     assert Model.model_fields['f'].description == 'test doc'\n+\n+\n+def test_final_to_frozen_with_assignment() -> None:\n+    class Model(BaseModel):\n+        # A buggy implementation made it so that `frozen` wouldn't\n+        # be set on the `FieldInfo`:\n+        b: Annotated[Final[int], ...] = Field(alias='test')\n+\n+    assert Model.model_fields['b'].frozen\n+\n+\n+def test_metadata_preserved_with_assignment() -> None:\n+    def func1(v):\n+        pass\n+\n+    def func2(v):\n+        pass\n+\n+    class Model(BaseModel):\n+        # A buggy implementation made it so that the first validator\n+        # would be dropped:\n+        a: Annotated[int, AfterValidator(func1), Field(gt=1), AfterValidator(func2)] = Field(...)\n+\n+    metadata = Model.model_fields['a'].metadata\n+\n+    assert isinstance(metadata[0], AfterValidator)\n+    assert isinstance(metadata[1], Gt)\n+    assert isinstance(metadata[2], AfterValidator)\n+\n+\n+def test_reused_field_not_mutated() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/11876\"\"\"\n+\n+    Ann = Annotated[int, Field()]\n+\n+    class Foo(BaseModel):\n+        f: Ann = 50\n+\n+    class Bar(BaseModel):\n+        f: Annotated[Ann, Field()]\n+\n+    assert Bar.model_fields['f'].default is PydanticUndefined\n+\n+\n+def test_no_duplicate_metadata_with_assignment_and_rebuild() -> None:\n+    \"\"\"https://github.com/pydantic/pydantic/issues/11870\"\"\"\n+\n+    class Model(BaseModel):\n+        f: Annotated['Int', Gt(1)] = Field()\n+\n+    Int = int\n+\n+    Model.model_rebuild()\n+\n+    assert len(Model.model_fields['f'].metadata) == 1\n+\n+\n+def test_fastapi_compatibility_hack() -> None:\n+    class Body(FieldInfo):\n+        \"\"\"A reproduction of the FastAPI's `Body` param.\"\"\"\n+\n+        pass\n+\n+    field = Body()\n+    # Assigning after doesn't update `_attributes_set`, which is currently\n+    # relied on to merge `FieldInfo` instances during field creation.\n+    # This is also what the FastAPI code is doing in some places.\n+    # The FastAPI compatibility hack makes it so that it still works.\n+    field.default = 1\n+\n+    Model = create_model('Model', f=(int, field))\n+    model_field = Model.model_fields['f']\n+\n+    assert isinstance(model_field, Body)\n+    assert not model_field.is_required()\ndiff --git a/tests/test_json_schema.py b/tests/test_json_schema.py\nindex 452b0d4912..0ebbaa3a8a 100644\n--- a/tests/test_json_schema.py\n+++ b/tests/test_json_schema.py\n@@ -6679,13 +6679,16 @@ class Model(BaseModel):\n \n \n def test_warn_on_mixed_compose() -> None:\n-    with pytest.warns(\n-        PydanticJsonSchemaWarning, match='Composing `dict` and `callable` type `json_schema_extra` is not supported.'\n-    ):\n+    with pytest.warns(UserWarning, match='Composing `dict` and `callable` type `json_schema_extra` is not supported.'):\n \n-        class Model(BaseModel):\n+        class Model1(BaseModel):\n             field: Annotated[int, Field(json_schema_extra={'a': 'dict'}), Field(json_schema_extra=lambda x: x.pop('a'))]  # type: ignore\n \n+    with pytest.warns(UserWarning, match='Composing `dict` and `callable` type `json_schema_extra` is not supported.'):\n+\n+        class Model2(BaseModel):\n+            field: Annotated[int, Field(json_schema_extra=lambda x: x.pop('a')), Field(json_schema_extra={'a': 'dict'})]  # type: ignore\n+\n \n def test_blank_title_is_respected() -> None:\n     class Model(BaseModel):\n", "problem_statement": "`Field()` and `FieldInfo`/metadata ordering inconsistencies and bugs\n### Initial Checks\n\n- [X] I confirm that I'm using Pydantic V2\n\n### Description\n\nClosely related: https://github.com/pydantic/pydantic/issues/10036\n\nThe current `Field()` function and `FieldInfo` logic and implementation is currently quite messy and not that performant [^1].\n\nSeems like the implementation grew over time and it is currently really hard to reason about (and as a consequence, is a source of bugs). While we wanted to rework the implementation with @sydney-runkle, it turns out being really difficult because of backwards compatibility reasons (many 3rd party libraries rely on the `FieldInfo` class and its method, even though we never explicitly provided support for this).\n\nhttps://github.com/pydantic/pydantic/pull/11109 is a first step in cleaning the implementation. However, I'll describe the ideal API here and how can we work this out.\n\n## `Field()` vs `FieldInfo`\n\nA major design flaw exists between the `Field()` function and the `FieldInfo` class. For each field of a model, a single `FieldInfo` instance is used and exposed via `model_fields`:\n\n```python\nclass Model(BaseModel):\n    f: Annotated[int, Field(alias='my_alias')]\n\nModel.model_fields['f']\n#> FieldInfo(annotation=int, alias='my_alias')\n```\n\nHowever, the `Field()` function also returns a `FieldInfo` instance. This is confusing because while the `Field()` function can be used to specify _most_ of the metadata, there are `FieldInfo` attributes that can't be specified (e.g. `annotation`, `evaluated`). This results in ugly checks:\n\nhttps://github.com/pydantic/pydantic/blob/d5f4bde014f12e0b2d03d46d77925d808053ff16/pydantic/fields.py#L276-L278\n\nand an implementation hard to follow. As an example, the [`from_annotated_attribute`](https://github.com/pydantic/pydantic/blob/d5f4bde014f12e0b2d03d46d77925d808053ff16/pydantic/fields.py#L343) method (which duplicates some logic from [`from_annotation`](https://github.com/pydantic/pydantic/blob/d5f4bde014f12e0b2d03d46d77925d808053ff16/pydantic/fields.py#L281)) is really hard to read and the complex implementation results in some bugs:\n\n```python\nfrom pydantic import BaseModel, AfterValidator\n\ndef func1(v): pass\ndef func2(v): pass\n\nclass Model(BaseModel):\n    a: Annotated[int, AfterValidator(func1), Field(gt=1), AfterValidator(func2)] = Field(...)\n\nModel.model_fields['a'].metadata\n#> [Gt(gt=1), AfterValidator(func=<function func2 at 0x7c2889945f80>)]\n# Some metadata is lost?\n```\n\n```python\nclass Model(BaseModel):\n    a: Annotated[Final[int], ...]\n    b: Annotated[Final[int], ...] = Field(alias='test')\n\nModel.model_fields['a'].frozen\n#> True\nModel.model_fields['b'].frozen\n#> None\n```\n\n```python\nclass Model(BaseModel):\n    a: Annotated[int, deprecated(\"test\")]\n    b: Annotated[int, deprecated(\"test\")] = Field(default=1)\n\nModel.model_fields['a'].deprecated\n#> \"test\"\nModel.model_fields['b'].deprecated\n#> None\n```\n\nIt also directly mutates the assigned `FieldInfo` instance, which isn't thread safe (see `test_annotated`), and requires some workarounds here:\n\nhttps://github.com/pydantic/pydantic/blob/f6725ba21e4409cbd867427203adf85c39e42630/pydantic/_internal/_fields.py#L233-L236\n\nAlso causes issues in: https://github.com/pydantic/pydantic/issues/11876.\n\n## Alternative implementation\n\nHere is a description of an alternative implementation (that doesn't take backwards compatibility into account):\n\n- The `Field()` function returns a different class instance: let's call it `FieldSpec`. It is a simple container class, only representing the kwargs passed to the `Field()` function.\n- A single `FieldInfo` instance is created for each field, and holds all the data of the field: the annotation, default value, alias, etc **AND** a list of metadata. Upon creation of the class, it is responsible for iterating over the metadata and for each element:\n  - If the element is a `FieldSpec` instance: fetch metadata info where the order of metadata _does not_ matter. For example: `alias`, `default`, `strict`. Metadata info where the order of metadata matters, it is converted into the right metadata class (e.g. `gt=1` -> `Gt(1)`).\n  - If the element is not a `FieldSpec` instance, it is left as is.\n- If the field was created using an [annotated assignment statement](https://docs.python.org/3/reference/simple_stmts.html#annassign):\n  - If the assignment is a default value (e.g. `f: int = 1`), we set `FieldInfo.default` with the default value at the end of the `FieldInfo` instantiation.\n  - If the assignment is a `FielSpec` instance: assume it will be last in the list of metadata.\n\nHere is an example:\n\n```python\nclass Model(BaseModel):\n    a: Annotated[int, Gt(1), Field(lt=2, alias='alias'), Field(strict=True)]\n    b: Annotated[int, Gt(1)] = Field(gt=2)\n\nModel.model_fields\n{\n    'a': FieldInfo(annotation=int, alias='alias', strict=True, metadata=[Gt(1), Lt(2)],\n    'b': FieldInfo(annotation=int, metadata=[Gt(2)]),\n}\n```\n\nThis resolves some annotation order inconsistencies (e.g. no need to have the ugly workaround in https://github.com/pydantic/pydantic/pull/9279/), but not all:\n\n```python\nclass Model(BaseModel):\n    a: Annotated[int, AfterValidator(lambda v: v + 1), Field(ge=1)]\n\nModel(a=0)\n#> Model(a=1)\n# Arguably wrong, *after* validators are meant to be applied after Pydantic validation, and the `ge` constraint\n# can be considered as being part of this Pydantic validation.\n```\n\n[^1]: Taking the `k8s_v2.py` benchmark file from [#10285](https://github.com/pydantic/pydantic/issues/10285), we see that fields collection is taking ~750ms, almost 1/6th of the total time.\n\n### Example Code\n\n_No response_\n\n### Python, Pydantic & OS Version\n\n```Text\n2.10\n```\n\nRegression in Pydantic 2.11.0: PydanticUserError Raised for Tagged Unions with Discriminator\n### Initial Checks\n\n- [x] I confirm that I'm using Pydantic V2\n\n### Description\n\nIn this example code, I started to catch the exception `PydanticUserError`:\n```\npydantic.errors.PydanticUserError: `Tag` not provided for choice {'type': 'tagged-union', 'choices': {'ref': {'type': 'definition-ref', 'schema_ref': '__main__.ReferenceObject:110773795946080', 'metadata': {'pydantic_internal_union_tag_key': 'ref'}}, 'object': {'type': 'tagged-union', 'choices': {'boolean': {'type': 'definition-ref', 'schema_ref': '__main__.BooleanSchemaObject:110773796544064'}, 'array': {'type': 'definition-ref', 'schema_ref': '__main__.ArraySchemaObject:110773796549760'}}, 'discriminator': 'type', 'metadata': {'pydantic_internal_union_tag_key': 'object'}}}, 'discriminator': <function reference_discriminator at 0x77835a3ebd90>} used with `Discriminator`\n```\n\nIf I change field `items` in ArraySchemaObject to\n```\n        items: Annotated[ReferenceObject, Tag('ref')] | Annotated['SchemaGroup', Tag('object')] = Field(\n            discriminator=Discriminator(reference_discriminator)\n        )\n```\nor\n```\n        items: Annotated[\n            Annotated[ReferenceObject, Tag('ref')] | Annotated['SchemaGroup', Tag('object')],\n            Discriminator(reference_discriminator),\n        ]\n```\nit starts to work.\n\nAdditionally, I can remove class ComponentsObject and it starts to work too.\n\nThis problem started in version 2.11.0, in 2.10.6 it worked correctly.\n\n### Example Code\n\n```Python\nfrom typing import Literal, Annotated\n\nfrom pydantic import VERSION, Field, BaseModel, Tag, Discriminator, PydanticUserError\nfrom pydantic_core import core_schema\n\ntry:\n    class ReferenceObject(BaseModel):\n        ref: str\n\n\n    def reference_discriminator(v) -> str:\n        return 'ref' if '$ref' in v else 'object'\n\n\n    class BooleanSchemaObject(BaseModel):\n        type: Literal['boolean'] = 'boolean'\n\n\n    class ArraySchemaObject(BaseModel):\n        type: Literal['array'] = 'array'\n        items: Annotated[\n            Annotated[ReferenceObject, Tag('ref')] | Annotated['SchemaGroup', Tag('object')],\n            Discriminator(reference_discriminator),\n        ] = Field()\n\n\n    class SchemaGroup:\n        def __get_pydantic_core_schema__(self, handler):\n            return core_schema.tagged_union_schema(choices={\n                'boolean': handler.generate_schema(BooleanSchemaObject),\n                'array': handler.generate_schema(ArraySchemaObject),\n            }, discriminator='type')\n\n\n    class ComponentsObject(BaseModel):\n        schemas: Annotated[\n            Annotated[ReferenceObject, Tag('ref')] | Annotated[SchemaGroup, Tag('object')],\n            Discriminator(reference_discriminator),\n        ]\n\n\n    print('Pydantic version: %s, error: false' % VERSION)\nexcept PydanticUserError:\n    print('Pydantic version: %s, error: true' % VERSION)\n```\n\n### Python, Pydantic & OS Version\n\n```Text\nNot working version:\n             pydantic version: 2.11.4\n        pydantic-core version: 2.33.2\n          pydantic-core build: profile=release pgo=false\n               python version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\n                     platform: Linux-6.8.0-58-generic-x86_64-with-glibc2.35\n             related packages: typing_extensions-4.12.2\n                       commit: unknown\n\nWorking version:\n             pydantic version: 2.10.6\n        pydantic-core version: 2.27.2\n          pydantic-core build: profile=release pgo=false\n                 install path: /home/sanya2013/.local/lib/python3.10/site-packages/pydantic\n               python version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\n                     platform: Linux-6.8.0-58-generic-x86_64-with-glibc2.35\n             related packages: typing_extensions-4.12.2\n                       commit: unknown\n```\nField default gets mutated globally when setting it in a specific model\n### Initial Checks\n\n- [x] I confirm that I'm using Pydantic V2\n\n### Description\n\nWhen defining a custom type with a Field annotation, its default gets mutated globally whenever it is set in a specific model. See the minimal reproduction example below.\n\n### Example Code\n\n```Python\nfrom pydantic import BaseModel, Field\nfrom typing import Annotated\n\nSimilarityLevel = Annotated[int, Field()]\n\nclass Foo(BaseModel):\n    sl: SimilarityLevel = 50\n\nclass Bar(BaseModel):\n    sl: Annotated[SimilarityLevel, Field()]\n\nprint(Bar.model_fields[\"sl\"])\n```\n\nOutput:\n\n```\nFieldInfo(annotation=int, required=False, default=50)\n```\n\n### Python, Pydantic & OS Version\n\n```Text\n             pydantic version: 2.11.4\n        pydantic-core version: 2.33.2\n          pydantic-core build: profile=release pgo=false\n               python version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0]\n                     platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\n             related packages: pydantic-extra-types-2.10.4 fastapi-0.115.12 pydantic-settings-2.9.1 mypy-1.15.0 typing_extensions-4.13.2\n                       commit: unknown\n```\nError building schema with tagged generic enums in 2.11.6\n### Initial Checks\n\n- [x] I confirm that I'm using Pydantic V2\n\n### Description\n\nAfter upgrading to 2.11.6, the example below started failing with error:\n\n```\npydantic.errors.PydanticUserError: `Tag` not provided for choice {'type': 'tagged-union', 'choices': {'not': {'type': 'definition-ref', 'schema_ref': '__main__.Not:310200368[ParagraphFilterExpression:str-140647796960000]', 'metadata': {'pydantic_internal_union_tag_key': 'not'}}, 'label': {'type': 'definition-ref', 'schema_ref': '__main__.Label:310499760', 'metadata': {'pydantic_internal_union_tag_key': 'label'}}}, 'discriminator': <function filter_discriminator at 0x7feb1e6e94e0>} used with `Discriminator`\n\nFor further information visit https://errors.pydantic.dev/2.11/u/callable-discriminator-no-tag\n```\n\nThis used to work in pydantic 2.11.4. I bisected the changes and this change in behaviour was introduced by https://github.com/pydantic/pydantic/pull/11946.\n\n### Example Code\n\nThis is a minimal reproducing example. The original code is in https://github.com/nuclia/nucliadb/blob/main/nucliadb_models/src/nucliadb_models/filters.py\n\n```Python\nfrom pydantic import AliasChoices, BaseModel, Discriminator, Field, Tag, field_validator, model_validator\nfrom typing import Annotated, Self, TypeVar, Generic, Any, Literal, Optional, Union\n\nF = TypeVar(\"F\", bound=BaseModel)\n\nclass Not(BaseModel, Generic[F], extra=\"forbid\"):\n    operand: F = Field(\n        serialization_alias=\"not\", validation_alias=AliasChoices(\"operand\", \"not\")\n    )\n\n\nclass Label(BaseModel, extra=\"forbid\"):\n    prop: Literal[\"label\"] = \"label\"\n    label: str\n\nclass Keyword(BaseModel, extra=\"forbid\"):\n    prop: Literal[\"keyword\"] = \"keyword\"\n    word: str\n\ndef filter_discriminator(v: Any) -> Optional[str]:\n    if isinstance(v, dict):\n        if \"not\" in v:\n            return \"not\"\n        else:\n            return v.get(\"prop\")\n\n    if isinstance(v, Not):\n        return \"not\"\n    else:\n        return getattr(v, \"prop\", None)\n\nParagraphFilterExpression = Annotated[\n    Union[\n        Annotated[Not[\"ParagraphFilterExpression\"], Tag(\"not\")],\n        Annotated[Label, Tag(\"label\")],\n    ],\n    Discriminator(filter_discriminator),\n]\n\n\nFieldFilterExpression = Annotated[\n    Union[\n        Annotated[Not[\"FieldFilterExpression\"], Tag(\"not\")],\n        Annotated[Keyword, Tag(\"keyword\")],\n    ],\n    Discriminator(filter_discriminator),\n]\n\nclass FilterExpression(BaseModel, extra=\"forbid\"):\n    field: Optional[FieldFilterExpression]\n    paragraph: Optional[ParagraphFilterExpression]\n```\n\n### Python, Pydantic & OS Version\n\n```Text\npydantic version: 2.11.6\n        pydantic-core version: 2.33.2\n          pydantic-core build: profile=release pgo=false\n               python version: 3.13.3 (main, Apr  9 2025, 04:03:52) [Clang 20.1.0 ]\n                     platform: Linux-6.14.9-arch1-1-x86_64-with-glibc2.41\n             related packages: mypy-1.15.0 typing_extensions-4.14.0 fastapi-0.115.11 pydantic-settings-2.8.1\n                       commit: unknown\n```\n", "hints_text": "\n\nMRE:\n\n```python\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nfrom annotated_types import Gt\n\nclass Model(BaseModel):\n    a: Annotated['Int',  Gt(1)] = Field()\n\nInt = int\n\nModel.model_rebuild()\nModel.model_fields['a'].metadata\n# [Gt(1), Gt(1)]\n```\n\n\n\n\n\n", "all_hints_text": "\n\nMRE:\n\n```python\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nfrom annotated_types import Gt\n\nclass Model(BaseModel):\n    a: Annotated['Int',  Gt(1)] = Field()\n\nInt = int\n\nModel.model_rebuild()\nModel.model_fields['a'].metadata\n# [Gt(1), Gt(1)]\n```\nFixed in https://github.com/pydantic/pydantic/pull/11902 for 2.11.5.\n\n\n\nThis is fixed by [this refactor](https://github.com/pydantic/pydantic/pull/11898) that will land in 2.12, so I'll try to find an alternative fix for 2.11.\nFixed in 2.11.7, which I've just released.\n\n", "commit_urls": ["https://github.com/pydantic/pydantic/commit/2c2ad979c456194f41245e82c03c5593ceebedb7", "https://github.com/pydantic/pydantic/commit/44bae6a3389d8e8a66074786ff55fe5761c464b3", "https://github.com/pydantic/pydantic/commit/c0641b2c944e6e1360908e4b1075a72121419892", "https://github.com/pydantic/pydantic/commit/fca06a7d9224a58ed4f8cd81b56028d1cf2d119a", "https://github.com/pydantic/pydantic/commit/0d3ec278d652977a49db934dbbe408834122dcbd", "https://github.com/pydantic/pydantic/commit/6b16f47b12591425b34aadb2726f584b9c220331", "https://github.com/pydantic/pydantic/commit/5fa0a550ef7c1cd9a95d5d3af48246e91e244938", "https://github.com/pydantic/pydantic/commit/6d2ab349f4de66711b3c20b82de2e85edd39deb3", "https://github.com/pydantic/pydantic/commit/b7a7350041f1f923a28bb2e171dcd449563834ee", "https://github.com/pydantic/pydantic/commit/dacead835977c9d145e63c9843b4600791eccb36", "https://github.com/pydantic/pydantic/commit/35c4f9516ce4c2a44f720f39e2595015eb7300ec", "https://github.com/pydantic/pydantic/commit/3ce521803303fcc7626134621bb374e0e6563ae1"], "created_at": "2025-05-21T20:31:56Z", "classification": "Efficiency"}
{"repo": "huggingface/pytorch-image-models", "pull_number": 2501, "instance_id": "huggingface__pytorch-image-models-2501", "issue_numbers": [2435], "base_commit": "290aa418d4be7921d769090d13742613aa222de1", "patch": "diff --git a/timm/models/beit.py b/timm/models/beit.py\nindex 2ee5fbb01e..5b5973801d 100644\n--- a/timm/models/beit.py\n+++ b/timm/models/beit.py\n@@ -615,7 +615,10 @@ def forward_intermediates(\n         else:\n             blocks = self.blocks[:max_index + 1]\n         for i, blk in enumerate(blocks):\n-            x = blk(x, shared_rel_pos_bias=rel_pos_bias)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x, shared_rel_pos_bias=rel_pos_bias)\n+            else:\n+                x = blk(x, shared_rel_pos_bias=rel_pos_bias)\n             if i in take_indices:\n                 # normalize intermediates with final norm layer if enabled\n                 intermediates.append(self.norm(x) if norm else x)\ndiff --git a/timm/models/byobnet.py b/timm/models/byobnet.py\nindex 18da1f2dc7..e37d25b6ef 100644\n--- a/timm/models/byobnet.py\n+++ b/timm/models/byobnet.py\n@@ -1508,7 +1508,10 @@ def forward_intermediates(\n             stages = self.stages[:max_index]\n         for stage in stages:\n             feat_idx += 1\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint_seq(stage, x)\n+            else:\n+                x = stage(x)\n             if not exclude_final_conv and feat_idx == last_idx:\n                 # default feature_info for this model uses final_conv as the last feature output (if present)\n                 x = self.final_conv(x)\ndiff --git a/timm/models/cait.py b/timm/models/cait.py\nindex 28e14ec756..2c500ec3dd 100644\n--- a/timm/models/cait.py\n+++ b/timm/models/cait.py\n@@ -18,7 +18,7 @@\n from timm.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, use_fused_attn\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n-from ._manipulate import checkpoint_seq\n+from ._manipulate import checkpoint, checkpoint_seq\n from ._registry import register_model, generate_default_cfgs\n \n __all__ = ['Cait', 'ClassAttn', 'LayerScaleBlockClassAttn', 'LayerScaleBlock', 'TalkingHeadAttn']\n@@ -373,7 +373,10 @@ def forward_intermediates(\n         else:\n             blocks = self.blocks[:max_index + 1]\n         for i, blk in enumerate(blocks):\n-            x = blk(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x)\n+            else:\n+                x = blk(x)\n             if i in take_indices:\n                 # normalize intermediates with final norm layer if enabled\n                 intermediates.append(self.norm(x) if norm else x)\ndiff --git a/timm/models/crossvit.py b/timm/models/crossvit.py\nindex f3d52f8e49..0e1de2fadd 100644\n--- a/timm/models/crossvit.py\n+++ b/timm/models/crossvit.py\n@@ -14,21 +14,16 @@\n NOTE: model names have been renamed from originals to represent actual input res all *_224 -> *_240 and *_384 -> *_408\n \n Modifications and additions for timm hacked together by / Copyright 2021, Ross Wightman\n+Modified from Timm. https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n \"\"\"\n \n # Copyright IBM All Rights Reserved.\n # SPDX-License-Identifier: Apache-2.0\n \n-\n-\"\"\"\n-Modified from Timm. https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n-\n-\"\"\"\n from functools import partial\n from typing import List, Optional, Tuple\n \n import torch\n-import torch.hub\n import torch.nn as nn\n \n from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\ndiff --git a/timm/models/davit.py b/timm/models/davit.py\nindex a82f2e5fae..22b4a1a05f 100644\n--- a/timm/models/davit.py\n+++ b/timm/models/davit.py\n@@ -25,7 +25,7 @@\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n from ._features_fx import register_notrace_function\n-from ._manipulate import checkpoint_seq\n+from ._manipulate import checkpoint, checkpoint_seq\n from ._registry import generate_default_cfgs, register_model\n \n __all__ = ['DaVit']\n@@ -671,7 +671,10 @@ def forward_intermediates(\n             stages = self.stages[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 if norm and feat_idx == last_idx:\n                     x_inter = self.norm_pre(x)  # applying final norm to last intermediate\ndiff --git a/timm/models/dla.py b/timm/models/dla.py\nindex 666acd9d9c..197060e4e6 100644\n--- a/timm/models/dla.py\n+++ b/timm/models/dla.py\n@@ -10,7 +10,6 @@\n \n import torch\n import torch.nn as nn\n-import torch.nn.functional as F\n \n from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n from timm.layers import create_classifier\ndiff --git a/timm/models/efficientnet.py b/timm/models/efficientnet.py\nindex 245f544066..7f2f5aa341 100644\n--- a/timm/models/efficientnet.py\n+++ b/timm/models/efficientnet.py\n@@ -259,9 +259,11 @@ def forward_intermediates(\n             blocks = self.blocks\n         else:\n             blocks = self.blocks[:max_index]\n-        for blk in blocks:\n-            feat_idx += 1\n-            x = blk(x)\n+        for feat_idx, blk in enumerate(blocks, start=1):\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint_seq(blk, x)\n+            else:\n+                x = blk(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \ndiff --git a/timm/models/efficientvit_mit.py b/timm/models/efficientvit_mit.py\nindex 80fbe43314..8b35a04c87 100644\n--- a/timm/models/efficientvit_mit.py\n+++ b/timm/models/efficientvit_mit.py\n@@ -789,7 +789,10 @@ def forward_intermediates(\n             stages = self.stages[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint_seq(stages, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \n@@ -943,7 +946,10 @@ def forward_intermediates(\n             stages = self.stages[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint_seq(stages, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \ndiff --git a/timm/models/efficientvit_msra.py b/timm/models/efficientvit_msra.py\nindex f3c8db74cd..80c5d99995 100644\n--- a/timm/models/efficientvit_msra.py\n+++ b/timm/models/efficientvit_msra.py\n@@ -18,7 +18,7 @@\n from timm.layers import SqueezeExcite, SelectAdaptivePool2d, trunc_normal_, _assert\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n-from ._manipulate import checkpoint_seq\n+from ._manipulate import checkpoint, checkpoint_seq\n from ._registry import register_model, generate_default_cfgs\n \n \n@@ -510,7 +510,10 @@ def forward_intermediates(\n             stages = self.stages[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \ndiff --git a/timm/models/eva.py b/timm/models/eva.py\nindex 61301616d6..21f0b9dc35 100644\n--- a/timm/models/eva.py\n+++ b/timm/models/eva.py\n@@ -731,7 +731,10 @@ def forward_intermediates(\n         else:\n             blocks = self.blocks[:max_index + 1]\n         for i, blk in enumerate(blocks):\n-            x = blk(x, rope=rot_pos_embed)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x, rope=rot_pos_embed)\n+            else:\n+                x = blk(x, rope=rot_pos_embed)\n             if i in take_indices:\n                 intermediates.append(self.norm(x) if norm else x)\n \ndiff --git a/timm/models/fasternet.py b/timm/models/fasternet.py\nindex d73f49a265..b9e4aed249 100644\n--- a/timm/models/fasternet.py\n+++ b/timm/models/fasternet.py\n@@ -142,7 +142,7 @@ def __init__(\n     def forward(self, x: torch.Tensor) -> torch.Tensor:\n         x = self.downsample(x)\n         if self.grad_checkpointing and not torch.jit.is_scripting():\n-            x = checkpoint_seq(self.blocks, x, flatten=True)\n+            x = checkpoint_seq(self.blocks, x)\n         else:\n             x = self.blocks(x)\n         return x\ndiff --git a/timm/models/focalnet.py b/timm/models/focalnet.py\nindex aa3237925f..3c2bd75643 100644\n--- a/timm/models/focalnet.py\n+++ b/timm/models/focalnet.py\n@@ -274,7 +274,7 @@ def forward(self, x):\n         x = self.downsample(x)\n         for blk in self.blocks:\n             if self.grad_checkpointing and not torch.jit.is_scripting():\n-                x = checkpoint.checkpoint(blk, x)\n+                x = checkpoint(blk, x)\n             else:\n                 x = blk(x)\n         return x\ndiff --git a/timm/models/gcvit.py b/timm/models/gcvit.py\nindex 214619de9b..367e5dfff5 100644\n--- a/timm/models/gcvit.py\n+++ b/timm/models/gcvit.py\n@@ -361,7 +361,7 @@ def forward(self, x):\n         global_query = self.global_norm(global_query.permute(0, 2, 3, 1))\n         for blk in self.blocks:\n             if self.grad_checkpointing and not torch.jit.is_scripting():\n-                x = checkpoint.checkpoint(blk, x)\n+                x = checkpoint(blk, x, global_query)\n             else:\n                 x = blk(x, global_query)\n         x = self.norm(x)\ndiff --git a/timm/models/ghostnet.py b/timm/models/ghostnet.py\nindex a2ddad4696..126d638f43 100644\n--- a/timm/models/ghostnet.py\n+++ b/timm/models/ghostnet.py\n@@ -727,7 +727,10 @@ def forward_intermediates(\n             stages = self.blocks[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages, start=1):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint_seq(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \ndiff --git a/timm/models/hgnet.py b/timm/models/hgnet.py\nindex 212cbb58ff..5c49f9ddca 100644\n--- a/timm/models/hgnet.py\n+++ b/timm/models/hgnet.py\n@@ -345,7 +345,7 @@ def __init__(\n     def forward(self, x):\n         x = self.downsample(x)\n         if self.grad_checkpointing and not torch.jit.is_scripting():\n-            x = checkpoint_seq(self.blocks, x, flatten=False)\n+            x = checkpoint_seq(self.blocks, x)\n         else:\n             x = self.blocks(x)\n         return x\ndiff --git a/timm/models/hiera.py b/timm/models/hiera.py\nindex 2c16a9d63e..fa9d6d2833 100644\n--- a/timm/models/hiera.py\n+++ b/timm/models/hiera.py\n@@ -24,7 +24,7 @@\n # --------------------------------------------------------\n import math\n from functools import partial\n-from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n+from typing import Dict, List, Optional, Tuple, Type, Union\n \n import torch\n import torch.nn as nn\n@@ -719,7 +719,10 @@ def forward_intermediates(\n         else:\n             blocks = self.blocks[:max_index + 1]\n         for i, blk in enumerate(blocks):\n-            x = blk(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x)\n+            else:\n+                x = blk(x)\n             if i in take_indices:\n                 x_int = self.reroll(x, i, mask=mask)\n                 intermediates.append(x_int.permute(0, 3, 1, 2) if output_fmt == 'NCHW' else x_int)\ndiff --git a/timm/models/hieradet_sam2.py b/timm/models/hieradet_sam2.py\nindex 6cd2592a95..fbd7ce28ec 100644\n--- a/timm/models/hieradet_sam2.py\n+++ b/timm/models/hieradet_sam2.py\n@@ -1,12 +1,11 @@\n import math\n from copy import deepcopy\n from functools import partial\n-from typing import Callable, Dict, List, Optional, Tuple, Union\n+from typing import Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n-from torch.jit import Final\n \n from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n from timm.layers import PatchEmbed, Mlp, DropPath, ClNormMlpClassifierHead, LayerScale, \\\n@@ -14,8 +13,8 @@\n \n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n-from ._manipulate import named_apply, checkpoint_seq, adapt_input_conv\n-from ._registry import generate_default_cfgs, register_model, register_model_deprecations\n+from ._manipulate import named_apply, checkpoint\n+from ._registry import generate_default_cfgs, register_model\n \n \n def window_partition(x, window_size: Tuple[int, int]):\n@@ -289,6 +288,7 @@ def __init__(\n         norm_layer = get_norm_layer(norm_layer)\n         act_layer = get_act_layer(act_layer)\n         assert len(stages) == len(window_spec)\n+        self.grad_checkpointing = False\n         self.num_classes = num_classes\n         self.window_spec = window_spec\n         self.output_fmt = 'NHWC'\n@@ -471,7 +471,10 @@ def forward_intermediates(\n         else:\n             blocks = self.blocks[:max_index + 1]\n         for i, blk in enumerate(blocks):\n-            x = blk(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x)\n+            else:\n+                x = blk(x)\n             if i in take_indices:\n                 x_out = x.permute(0, 3, 1, 2) if output_fmt == 'NCHW' else x\n                 intermediates.append(x_out)\n@@ -503,8 +506,11 @@ def prune_intermediate_layers(\n     def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n         x = self.patch_embed(x)  # BHWC\n         x = self._pos_embed(x)\n-        for i, blk in enumerate(self.blocks):\n-            x = blk(x)\n+        for blk in self.blocks:\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x)\n+            else:\n+                x = blk(x)\n         return x\n \n     def forward_head(self, x, pre_logits: bool = False) -> torch.Tensor:\ndiff --git a/timm/models/hrnet.py b/timm/models/hrnet.py\nindex 75b157d67d..92ee3511cf 100644\n--- a/timm/models/hrnet.py\n+++ b/timm/models/hrnet.py\n@@ -13,7 +13,6 @@\n \n import torch\n import torch.nn as nn\n-import torch.nn.functional as F\n \n from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n from timm.layers import create_classifier\ndiff --git a/timm/models/inception_resnet_v2.py b/timm/models/inception_resnet_v2.py\nindex 7fdfee41ed..d691be7a8f 100644\n--- a/timm/models/inception_resnet_v2.py\n+++ b/timm/models/inception_resnet_v2.py\n@@ -5,7 +5,6 @@\n from functools import partial\n import torch\n import torch.nn as nn\n-import torch.nn.functional as F\n \n from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n from timm.layers import create_classifier, ConvNormAct\ndiff --git a/timm/models/inception_v3.py b/timm/models/inception_v3.py\nindex 8cb1a151df..a55521c3de 100644\n--- a/timm/models/inception_v3.py\n+++ b/timm/models/inception_v3.py\n@@ -4,7 +4,6 @@\n Licensed BSD-Clause 3 https://github.com/pytorch/vision/blob/master/LICENSE\n \"\"\"\n from functools import partial\n-from typing import Optional\n \n import torch\n import torch.nn as nn\ndiff --git a/timm/models/levit.py b/timm/models/levit.py\nindex 577fc5f2d7..a4c9ce628a 100644\n--- a/timm/models/levit.py\n+++ b/timm/models/levit.py\n@@ -34,7 +34,7 @@\n from timm.layers import to_ntuple, to_2tuple, get_act_layer, DropPath, trunc_normal_, ndgrid\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n-from ._manipulate import checkpoint_seq\n+from ._manipulate import checkpoint, checkpoint_seq\n from ._registry import generate_default_cfgs, register_model\n \n __all__ = ['Levit']\n@@ -671,7 +671,10 @@ def forward_intermediates(\n         else:\n             stages = self.stages[:max_index + 1]\n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 if self.use_conv:\n                     intermediates.append(x)\ndiff --git a/timm/models/metaformer.py b/timm/models/metaformer.py\nindex 2e93e01b16..3364a79563 100644\n--- a/timm/models/metaformer.py\n+++ b/timm/models/metaformer.py\n@@ -41,7 +41,7 @@\n     use_fused_attn\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n-from ._manipulate import checkpoint_seq\n+from ._manipulate import checkpoint, checkpoint_seq\n from ._registry import generate_default_cfgs, register_model\n \n __all__ = ['MetaFormer']\n@@ -631,7 +631,10 @@ def forward_intermediates(\n             stages = self.stages[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \ndiff --git a/timm/models/mlp_mixer.py b/timm/models/mlp_mixer.py\nindex b024fba478..838e0e0117 100644\n--- a/timm/models/mlp_mixer.py\n+++ b/timm/models/mlp_mixer.py\n@@ -49,7 +49,7 @@\n from timm.layers import PatchEmbed, Mlp, GluMlp, GatedMlp, DropPath, lecun_normal_, to_2tuple\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n-from ._manipulate import named_apply, checkpoint_seq\n+from ._manipulate import named_apply, checkpoint, checkpoint_seq\n from ._registry import generate_default_cfgs, register_model, register_model_deprecations\n \n __all__ = ['MixerBlock', 'MlpMixer']  # model_registry will add each entrypoint fn to this\n@@ -406,7 +406,10 @@ def forward_intermediates(\n         else:\n             blocks = self.blocks[:max_index + 1]\n         for i, blk in enumerate(blocks):\n-            x = blk(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x)\n+            else:\n+                x = blk(x)\n             if i in take_indices:\n                 # normalize intermediates with final norm layer if enabled\n                 intermediates.append(self.norm(x) if norm else x)\ndiff --git a/timm/models/mobilenetv3.py b/timm/models/mobilenetv3.py\nindex 8e25674b63..eb87bb38d8 100644\n--- a/timm/models/mobilenetv3.py\n+++ b/timm/models/mobilenetv3.py\n@@ -227,9 +227,11 @@ def forward_intermediates(\n             blocks = self.blocks\n         else:\n             blocks = self.blocks[:max_index]\n-        for blk in blocks:\n-            feat_idx += 1\n-            x = blk(x)\n+        for feat_idx, blk in enumerate(blocks, start=1):\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint_seq(blk, x)\n+            else:\n+                x = blk(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \ndiff --git a/timm/models/mvitv2.py b/timm/models/mvitv2.py\nindex c048a07277..01c4550ed8 100644\n--- a/timm/models/mvitv2.py\n+++ b/timm/models/mvitv2.py\n@@ -681,7 +681,7 @@ def __init__(\n     def forward(self, x, feat_size: List[int]):\n         for blk in self.blocks:\n             if self.grad_checkpointing and not torch.jit.is_scripting():\n-                x, feat_size = checkpoint.checkpoint(blk, x, feat_size)\n+                x, feat_size = checkpoint(blk, x, feat_size)\n             else:\n                 x, feat_size = blk(x, feat_size)\n         return x, feat_size\ndiff --git a/timm/models/naflexvit.py b/timm/models/naflexvit.py\nindex 29e8ba28e7..5794fdf5e0 100644\n--- a/timm/models/naflexvit.py\n+++ b/timm/models/naflexvit.py\n@@ -20,14 +20,14 @@\n import math\n from dataclasses import dataclass, fields, replace\n from functools import partial\n-from typing import Callable, Dict, List, Optional, Set, Tuple, Type, Union, Final, Any, Literal\n+from typing import Callable, Dict, List, Optional, Set, Tuple, Type, Union, Any\n \n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n from torch.nn.utils.rnn import pad_sequence\n \n-from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n+from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n from timm.layers import (\n     AttentionPoolLatent,\n     Mlp,\n@@ -35,14 +35,13 @@\n     get_act_layer,\n     get_norm_layer,\n     LayerNorm,\n-    LayerType,\n     _assert,\n )\n from timm.models._builder import build_model_with_cfg\n from timm.models._features import feature_take_indices\n from timm.models._features_fx import register_notrace_function, register_notrace_module\n from timm.models._registry import register_model, generate_default_cfgs\n-from timm.models._manipulate import checkpoint_seq, named_apply\n+from timm.models._manipulate import checkpoint, checkpoint_seq, named_apply\n \n from .vision_transformer import Block, global_pool_nlc\n \n@@ -1202,7 +1201,7 @@ def forward_intermediates(\n             output_dict: bool = False,\n             patch_coord: Optional[torch.Tensor] = None,\n             patch_valid: Optional[torch.Tensor] = None,\n-            mask: Optional[torch.Tensor] = None,\n+            attn_mask: Optional[torch.Tensor] = None,\n     ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]], Dict[str, Any]]:\n         \"\"\" Forward features that returns intermediates.\n \n@@ -1217,7 +1216,7 @@ def forward_intermediates(\n             output_dict: Return outputs as a dictionary with 'image_features' and 'image_intermediates' keys\n             patch_coord: Optional patch coordinates [B, N, 2] for NaFlex mode\n             patch_valid: Optional patch type indicators (1=patch, 0=padding) for NaFlex\n-            mask: Optional attention mask\n+            attn_mask: Optional attention mask for masked attention\n         Returns:\n             A tuple with (final_features, intermediates), a list of intermediate features, or a dictionary containing\n             'image_features' and 'image_intermediates' (and optionally 'image_intermediates_prefix')\n@@ -1241,8 +1240,8 @@ def forward_intermediates(\n             H, W = self.embeds.dynamic_feat_size((height, width))\n \n         # Create attention mask if patch_type is provided and mask is not\n-        if mask is None and patch_valid is not None:\n-            mask = create_attention_mask(patch_valid, self.num_prefix_tokens, patches.dtype)\n+        if attn_mask is None and patch_valid is not None:\n+            attn_mask = create_attention_mask(patch_valid, self.num_prefix_tokens, patches.dtype)\n \n         # Forward pass through embedding\n         x = self.embeds(patches, patch_coord=patch_coord)\n@@ -1255,7 +1254,12 @@ def forward_intermediates(\n             blocks = self.blocks[:max_index + 1]\n \n         for i, blk in enumerate(blocks):\n-            x = blk(x, attn_mask=mask)\n+            if attn_mask is not None:\n+                x = blk(x, attn_mask=attn_mask)\n+            elif self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x)\n+            else:\n+                x = blk(x)\n             if i in take_indices:\n                 # normalize intermediates with final norm layer if enabled\n                 intermediates.append(self.norm(x) if norm else x)\ndiff --git a/timm/models/nasnet.py b/timm/models/nasnet.py\nindex a0d2d8a734..8edee45c86 100644\n--- a/timm/models/nasnet.py\n+++ b/timm/models/nasnet.py\n@@ -3,11 +3,9 @@\n  https://github.com/Cadene/pretrained-models.pytorch\n \"\"\"\n from functools import partial\n-from typing import Optional\n \n import torch\n import torch.nn as nn\n-import torch.nn.functional as F\n \n from timm.layers import ConvNormAct, create_conv2d, create_pool2d, create_classifier\n from ._builder import build_model_with_cfg\ndiff --git a/timm/models/nextvit.py b/timm/models/nextvit.py\nindex 2f232e2990..402a9d76ea 100644\n--- a/timm/models/nextvit.py\n+++ b/timm/models/nextvit.py\n@@ -17,8 +17,7 @@\n from timm.layers import ClassifierHead\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n-from ._features_fx import register_notrace_function\n-from ._manipulate import checkpoint_seq\n+from ._manipulate import checkpoint, checkpoint_seq\n from ._registry import generate_default_cfgs, register_model\n \n __all__ = ['NextViT']\n@@ -595,7 +594,10 @@ def forward_intermediates(\n             stages = self.stages[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 if feat_idx == last_idx:\n                     x_inter = self.norm(x) if norm else x\ndiff --git a/timm/models/nfnet.py b/timm/models/nfnet.py\nindex d848f81eba..68b8b1b6d6 100644\n--- a/timm/models/nfnet.py\n+++ b/timm/models/nfnet.py\n@@ -19,7 +19,7 @@\n from collections import OrderedDict\n from dataclasses import dataclass, replace\n from functools import partial\n-from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+from typing import Any, Callable, Dict, Optional, Tuple\n \n import torch\n import torch.nn as nn\n@@ -304,7 +304,7 @@ def create_stem(\n     if 'deep' in stem_type:\n         if 'quad' in stem_type:\n             # 4 deep conv stack as in NFNet-F models\n-            assert not 'pool' in stem_type\n+            assert 'pool' not in stem_type\n             stem_chs = (out_chs // 8, out_chs // 4, out_chs // 2, out_chs)\n             strides = (2, 1, 1, 2)\n             stem_stride = 4\ndiff --git a/timm/models/pnasnet.py b/timm/models/pnasnet.py\nindex 20d17945b5..7f33aaeabb 100644\n--- a/timm/models/pnasnet.py\n+++ b/timm/models/pnasnet.py\n@@ -10,7 +10,6 @@\n \n import torch\n import torch.nn as nn\n-import torch.nn.functional as F\n \n from timm.layers import ConvNormAct, create_conv2d, create_pool2d, create_classifier\n from ._builder import build_model_with_cfg\ndiff --git a/timm/models/pvt_v2.py b/timm/models/pvt_v2.py\nindex bb1baf6645..0259a1f64e 100644\n--- a/timm/models/pvt_v2.py\n+++ b/timm/models/pvt_v2.py\n@@ -267,7 +267,7 @@ def forward(self, x):\n         x = x.reshape(B, -1, C)\n         for blk in self.blocks:\n             if self.grad_checkpointing and not torch.jit.is_scripting():\n-                x = checkpoint.checkpoint(blk, x, feat_size)\n+                x = checkpoint(blk, x, feat_size)\n             else:\n                 x = blk(x, feat_size)\n         x = self.norm(x)\ndiff --git a/timm/models/rdnet.py b/timm/models/rdnet.py\nindex a3a205fff6..5764b6ed82 100644\n--- a/timm/models/rdnet.py\n+++ b/timm/models/rdnet.py\n@@ -281,6 +281,27 @@ def __init__(\n \n         named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)\n \n+    @torch.jit.ignore\n+    def group_matcher(self, coarse=False):\n+        assert not coarse, \"coarse grouping is not implemented for RDNet\"\n+        return dict(\n+            stem=r'^stem',\n+            blocks=r'^dense_stages\\.(\\d+)',\n+        )\n+\n+    @torch.jit.ignore\n+    def set_grad_checkpointing(self, enable=True):\n+        for s in self.dense_stages:\n+            s.grad_checkpointing = enable\n+\n+    @torch.jit.ignore\n+    def get_classifier(self) -> nn.Module:\n+        return self.head.fc\n+\n+    def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n+        self.num_classes = num_classes\n+        self.head.reset(num_classes, global_pool)\n+\n     def forward_intermediates(\n             self,\n             x: torch.Tensor,\n@@ -350,14 +371,6 @@ def prune_intermediate_layers(\n             self.reset_classifier(0, '')\n         return take_indices\n \n-    @torch.jit.ignore\n-    def get_classifier(self) -> nn.Module:\n-        return self.head.fc\n-\n-    def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):\n-        self.num_classes = num_classes\n-        self.head.reset(num_classes, global_pool)\n-\n     def forward_features(self, x):\n         x = self.stem(x)\n         x = self.dense_stages(x)\n@@ -372,19 +385,6 @@ def forward(self, x):\n         x = self.forward_head(x)\n         return x\n \n-    @torch.jit.ignore\n-    def group_matcher(self, coarse=False):\n-        assert not coarse, \"coarse grouping is not implemented for RDNet\"\n-        return dict(\n-            stem=r'^stem',\n-            blocks=r'^dense_stages\\.(\\d+)',\n-        )\n-\n-    @torch.jit.ignore\n-    def set_grad_checkpointing(self, enable=True):\n-        for s in self.dense_stages:\n-            s.grad_checkpointing = enable\n-\n \n def _init_weights(module, name=None, head_init_scale=1.0):\n     if isinstance(module, nn.Conv2d):\ndiff --git a/timm/models/repghost.py b/timm/models/repghost.py\nindex a75d9d8506..c5a7d93a4f 100644\n--- a/timm/models/repghost.py\n+++ b/timm/models/repghost.py\n@@ -336,7 +336,10 @@ def forward_intermediates(\n             stages = self.blocks[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages, start=1):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint_seq(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \ndiff --git a/timm/models/repvit.py b/timm/models/repvit.py\nindex 190f4b5298..3641d6f70c 100644\n--- a/timm/models/repvit.py\n+++ b/timm/models/repvit.py\n@@ -23,7 +23,7 @@\n from timm.layers import SqueezeExcite, trunc_normal_, to_ntuple, to_2tuple\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n-from ._manipulate import checkpoint_seq\n+from ._manipulate import checkpoint, checkpoint_seq\n from ._registry import register_model, generate_default_cfgs\n \n __all__ = ['RepVit']\n@@ -367,7 +367,10 @@ def forward_intermediates(\n             stages = self.stages[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \ndiff --git a/timm/models/resnetv2.py b/timm/models/resnetv2.py\nindex 38cd89ce11..0b78e7b44d 100644\n--- a/timm/models/resnetv2.py\n+++ b/timm/models/resnetv2.py\n@@ -397,6 +397,8 @@ def __init__(\n             **block_kwargs: Any,\n     ):\n         super(ResNetStage, self).__init__()\n+        self.grad_checkpointing = False\n+\n         first_dilation = 1 if dilation in (1, 2) else 2\n         layer_kwargs = dict(act_layer=act_layer, conv_layer=conv_layer, norm_layer=norm_layer)\n         proj_layer = DownsampleAvg if avg_down else DownsampleConv\n@@ -431,7 +433,10 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n         Returns:\n             Output tensor.\n         \"\"\"\n-        x = self.blocks(x)\n+        if self.grad_checkpointing and not torch.jit.is_scripting():\n+            x = checkpoint_seq(self.blocks, x)\n+        else:\n+            x = self.blocks(x)\n         return x\n \n \n@@ -604,7 +609,6 @@ def __init__(\n         )\n \n         self.init_weights(zero_init_last=zero_init_last)\n-        self.grad_checkpointing = False\n \n     @torch.jit.ignore\n     def init_weights(self, zero_init_last: bool = True) -> None:\n@@ -631,7 +635,8 @@ def group_matcher(self, coarse: bool = False) -> Dict[str, Any]:\n     @torch.jit.ignore\n     def set_grad_checkpointing(self, enable: bool = True) -> None:\n         \"\"\"Enable or disable gradient checkpointing.\"\"\"\n-        self.grad_checkpointing = enable\n+        for s in self.stages:\n+            s.grad_checkpointing = enable\n \n     @torch.jit.ignore\n     def get_classifier(self) -> nn.Module:\n@@ -731,10 +736,7 @@ def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n             Feature tensor.\n         \"\"\"\n         x = self.stem(x)\n-        if self.grad_checkpointing and not torch.jit.is_scripting():\n-            x = checkpoint_seq(self.stages, x, flatten=True)\n-        else:\n-            x = self.stages(x)\n+        x = self.stages(x)\n         x = self.norm(x)\n         return x\n \ndiff --git a/timm/models/rexnet.py b/timm/models/rexnet.py\nindex 04e284158c..77b801db87 100644\n--- a/timm/models/rexnet.py\n+++ b/timm/models/rexnet.py\n@@ -22,7 +22,7 @@\n from ._builder import build_model_with_cfg\n from ._efficientnet_builder import efficientnet_init_weights\n from ._features import feature_take_indices\n-from ._manipulate import checkpoint_seq\n+from ._manipulate import checkpoint, checkpoint_seq\n from ._registry import generate_default_cfgs, register_model\n \n __all__ = ['RexNet']  # model_registry will add each entrypoint fn to this\n@@ -382,7 +382,10 @@ def forward_intermediates(\n             stages = self.features[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \n@@ -426,7 +429,7 @@ def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         x = self.stem(x)\n         if self.grad_checkpointing and not torch.jit.is_scripting():\n-            x = checkpoint_seq(self.features, x, flatten=True)\n+            x = checkpoint_seq(self.features, x)\n         else:\n             x = self.features(x)\n         return x\ndiff --git a/timm/models/selecsls.py b/timm/models/selecsls.py\nindex fdfa16c318..dc19ff41d1 100644\n--- a/timm/models/selecsls.py\n+++ b/timm/models/selecsls.py\n@@ -13,7 +13,6 @@\n \n import torch\n import torch.nn as nn\n-import torch.nn.functional as F\n \n from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n from timm.layers import create_classifier\ndiff --git a/timm/models/shvit.py b/timm/models/shvit.py\nindex be3e206ee8..c165f1a280 100644\n--- a/timm/models/shvit.py\n+++ b/timm/models/shvit.py\n@@ -11,7 +11,6 @@\n   year={2024}\n }\n \"\"\"\n-import re\n from typing import Any, Dict, List, Optional, Set, Tuple, Union\n \n import torch\n@@ -245,7 +244,7 @@ def __init__(\n     def forward(self, x: torch.Tensor) -> torch.Tensor:\n         x = self.downsample(x)\n         if self.grad_checkpointing and not torch.jit.is_scripting():\n-            x = checkpoint_seq(self.blocks, x, flatten=True)\n+            x = checkpoint_seq(self.blocks, x)\n         else:\n             x = self.blocks(x)\n         return x\n@@ -429,7 +428,7 @@ def checkpoint_filter_fn(state_dict: Dict[str, torch.Tensor], model: nn.Module)\n     state_dict = state_dict.get('model', state_dict)\n \n     # out_dict = {}\n-    #\n+    # import re\n     # replace_rules = [\n     #     (re.compile(r'^blocks1\\.'), 'stages.0.blocks.'),\n     #     (re.compile(r'^blocks2\\.'), 'stages.1.blocks.'),\ndiff --git a/timm/models/starnet.py b/timm/models/starnet.py\nindex 646fd324b2..9ed32a85d7 100644\n--- a/timm/models/starnet.py\n+++ b/timm/models/starnet.py\n@@ -198,7 +198,10 @@ def forward_intermediates(\n             stages = self.stages[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint_seq(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 if norm and feat_idx == last_idx:\n                     x_inter = self.norm(x)  # applying final norm last intermediate\n@@ -233,7 +236,7 @@ def prune_intermediate_layers(\n     def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n         x = self.stem(x)\n         if self.grad_checkpointing and not torch.jit.is_scripting():\n-            x = checkpoint_seq(self.stages, x, flatten=True)\n+            x = checkpoint_seq(self.stages, x)\n         else:\n             x = self.stages(x)\n         x = self.norm(x)\ndiff --git a/timm/models/swiftformer.py b/timm/models/swiftformer.py\nindex 5998c233fd..38df6f1638 100644\n--- a/timm/models/swiftformer.py\n+++ b/timm/models/swiftformer.py\n@@ -304,7 +304,7 @@ def __init__(\n     def forward(self, x: torch.Tensor) -> torch.Tensor:\n         x = self.downsample(x)\n         if self.grad_checkpointing and not torch.jit.is_scripting():\n-            x = checkpoint_seq(self.blocks, x, flatten=True)\n+            x = checkpoint_seq(self.blocks, x)\n         else:\n             x = self.blocks(x)\n         return x\ndiff --git a/timm/models/swin_transformer.py b/timm/models/swin_transformer.py\nindex 7eeae8316b..e17f16746c 100644\n--- a/timm/models/swin_transformer.py\n+++ b/timm/models/swin_transformer.py\n@@ -24,7 +24,7 @@\n \n from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n from timm.layers import PatchEmbed, Mlp, DropPath, ClassifierHead, to_2tuple, to_ntuple, trunc_normal_, \\\n-    _assert, use_fused_attn, resize_rel_pos_bias_table, resample_patch_embed, ndgrid\n+    use_fused_attn, resize_rel_pos_bias_table, resample_patch_embed, ndgrid\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n from ._features_fx import register_notrace_function\ndiff --git a/timm/models/swin_transformer_v2.py b/timm/models/swin_transformer_v2.py\nindex f7b758aa8b..35c0daa8ac 100644\n--- a/timm/models/swin_transformer_v2.py\n+++ b/timm/models/swin_transformer_v2.py\n@@ -619,7 +619,7 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n \n         for blk in self.blocks:\n             if self.grad_checkpointing and not torch.jit.is_scripting():\n-                x = checkpoint.checkpoint(blk, x)\n+                x = checkpoint(blk, x)\n             else:\n                 x = blk(x)\n         return x\ndiff --git a/timm/models/swin_transformer_v2_cr.py b/timm/models/swin_transformer_v2_cr.py\nindex c490fa23ca..1ef3164fd9 100644\n--- a/timm/models/swin_transformer_v2_cr.py\n+++ b/timm/models/swin_transformer_v2_cr.py\n@@ -29,7 +29,7 @@\n # --------------------------------------------------------\n import logging\n import math\n-from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Type, Union\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n \n import torch\n import torch.nn as nn\n@@ -636,7 +636,7 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n         for block in self.blocks:\n             # Perform checkpointing if utilized\n             if self.grad_checkpointing and not torch.jit.is_scripting():\n-                x = checkpoint.checkpoint(block, x)\n+                x = checkpoint(block, x)\n             else:\n                 x = block(x)\n         x = bhwc_to_bchw(x)\ndiff --git a/timm/models/tiny_vit.py b/timm/models/tiny_vit.py\nindex 366eef7092..39bacc850c 100644\n--- a/timm/models/tiny_vit.py\n+++ b/timm/models/tiny_vit.py\n@@ -22,7 +22,7 @@\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n from ._features_fx import register_notrace_module\n-from ._manipulate import checkpoint_seq\n+from ._manipulate import checkpoint, checkpoint_seq\n from ._registry import register_model, generate_default_cfgs\n \n \n@@ -570,7 +570,10 @@ def forward_intermediates(\n             stages = self.stages[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \ndiff --git a/timm/models/tnt.py b/timm/models/tnt.py\nindex fa6e1fc9e7..0ecd8e72a4 100644\n--- a/timm/models/tnt.py\n+++ b/timm/models/tnt.py\n@@ -386,7 +386,10 @@ def forward_intermediates(\n             blocks = self.blocks[:max_index + 1]\n \n         for i, blk in enumerate(blocks):\n-            pixel_embed, patch_embed = blk(pixel_embed, patch_embed)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                pixel_embed, patch_embed = checkpoint(blk, pixel_embed, patch_embed)\n+            else:\n+                pixel_embed, patch_embed = blk(pixel_embed, patch_embed)\n             if i in take_indices:\n                 # normalize intermediates with final norm layer if enabled\n                 intermediates.append(self.norm(patch_embed) if norm else patch_embed)\ndiff --git a/timm/models/tresnet.py b/timm/models/tresnet.py\nindex 0fb76fa40c..2c452e4707 100644\n--- a/timm/models/tresnet.py\n+++ b/timm/models/tresnet.py\n@@ -15,7 +15,7 @@\n from timm.layers import SpaceToDepth, BlurPool2d, ClassifierHead, SEModule, ConvNormAct, DropPath\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n-from ._manipulate import checkpoint_seq\n+from ._manipulate import checkpoint, checkpoint_seq\n from ._registry import register_model, generate_default_cfgs, register_model_deprecations\n \n __all__ = ['TResNet']  # model_registry will add each entrypoint fn to this\n@@ -263,7 +263,10 @@ def forward_intermediates(\n             stages = self.body[:max_index + 1]\n \n         for feat_idx, stage in enumerate(stages):\n-            x = stage(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(stage, x)\n+            else:\n+                x = stage(x)\n             if feat_idx in take_indices:\n                 intermediates.append(x)\n \ndiff --git a/timm/models/vision_transformer.py b/timm/models/vision_transformer.py\nindex b57e2f213c..05e435e9e9 100644\n--- a/timm/models/vision_transformer.py\n+++ b/timm/models/vision_transformer.py\n@@ -68,7 +68,7 @@\n )\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n-from ._manipulate import named_apply, checkpoint_seq, adapt_input_conv\n+from ._manipulate import named_apply, checkpoint, checkpoint_seq, adapt_input_conv\n from ._registry import generate_default_cfgs, register_model, register_model_deprecations\n \n __all__ = ['VisionTransformer']  # model_registry will add each entrypoint fn to this\n@@ -824,7 +824,12 @@ def forward_intermediates(\n         else:\n             blocks = self.blocks[:max_index + 1]\n         for i, blk in enumerate(blocks):\n-            x = blk(x, attn_mask=attn_mask)\n+            if attn_mask is not None:\n+                x = blk(x, attn_mask=attn_mask)\n+            elif self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x)\n+            else:\n+                x = blk(x)\n             if i in take_indices:\n                 # normalize intermediates with final norm layer if enabled\n                 intermediates.append(self.norm(x) if norm else x)\ndiff --git a/timm/models/vision_transformer_hybrid.py b/timm/models/vision_transformer_hybrid.py\nindex 4cf3a7664b..0ff4823497 100644\n--- a/timm/models/vision_transformer_hybrid.py\n+++ b/timm/models/vision_transformer_hybrid.py\n@@ -13,15 +13,14 @@\n \n Hacked together by / Copyright 2020, Ross Wightman\n \"\"\"\n-import math\n from functools import partial\n-from typing import Dict, List, Optional, Tuple, Type, Union\n+from typing import Dict, Tuple, Type, Union\n \n import torch\n import torch.nn as nn\n \n from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n-from timm.layers import StdConv2dSame, StdConv2d, ConvNormAct, to_2tuple, to_ntuple, HybridEmbed\n+from timm.layers import StdConv2dSame, StdConv2d, ConvNormAct, to_ntuple, HybridEmbed\n \n from ._builder import build_model_with_cfg\n from ._registry import generate_default_cfgs, register_model, register_model_deprecations\ndiff --git a/timm/models/vision_transformer_relpos.py b/timm/models/vision_transformer_relpos.py\nindex 030c24dc69..dcccba73ba 100644\n--- a/timm/models/vision_transformer_relpos.py\n+++ b/timm/models/vision_transformer_relpos.py\n@@ -427,7 +427,10 @@ def forward_intermediates(\n         else:\n             blocks = self.blocks[:max_index + 1]\n         for i, blk in enumerate(blocks):\n-            x = blk(x, shared_rel_pos=shared_rel_pos)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x, shared_rel_pos=shared_rel_pos)\n+            else:\n+                x = blk(x, shared_rel_pos=shared_rel_pos)\n             if i in take_indices:\n                 # normalize intermediates with final norm layer if enabled\n                 intermediates.append(self.norm(x) if norm else x)\ndiff --git a/timm/models/vision_transformer_sam.py b/timm/models/vision_transformer_sam.py\nindex 75bb12e56f..df70f4a251 100644\n--- a/timm/models/vision_transformer_sam.py\n+++ b/timm/models/vision_transformer_sam.py\n@@ -24,7 +24,7 @@\n from ._builder import build_model_with_cfg\n from ._features import feature_take_indices\n from ._features_fx import register_notrace_function\n-from ._manipulate import checkpoint_seq\n+from ._manipulate import checkpoint, checkpoint_seq\n from ._registry import generate_default_cfgs, register_model\n \n # model_registry will add each entrypoint fn to this\n@@ -579,7 +579,10 @@ def forward_intermediates(\n         else:\n             blocks = self.blocks[:max_index + 1]\n         for i, blk in enumerate(blocks):\n-            x = blk(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x)\n+            else:\n+                x = blk(x)\n             if i in take_indices:\n                 # make output BCHW\n                 if norm:\ndiff --git a/timm/models/volo.py b/timm/models/volo.py\nindex f76a8361a3..f417dc6df1 100644\n--- a/timm/models/volo.py\n+++ b/timm/models/volo.py\n@@ -1039,7 +1039,10 @@ def forward_intermediates(\n                 # add positional encoding after outlooker blocks\n                 x = x + self.pos_embed\n                 x = self.pos_drop(x)\n-            x = block(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(block, x)\n+            else:\n+                x = block(x)\n             if idx in take_indices:\n                 if norm and idx >= 2:\n                     x_inter = self.norm(x)\ndiff --git a/timm/models/xcit.py b/timm/models/xcit.py\nindex 250749f1cf..271578adf8 100644\n--- a/timm/models/xcit.py\n+++ b/timm/models/xcit.py\n@@ -478,7 +478,10 @@ def forward_intermediates(\n         else:\n             blocks = self.blocks[:max_index + 1]\n         for i, blk in enumerate(blocks):\n-            x = blk(x, Hp, Wp)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x, Hp, Wp)\n+            else:\n+                x = blk(x, Hp, Wp)\n             if i in take_indices:\n                 # normalize intermediates with final norm layer if enabled\n                 intermediates.append(self.norm(x) if norm else x)\n@@ -494,7 +497,10 @@ def forward_intermediates(\n         # NOTE not supporting return of class tokens\n         x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1)\n         for blk in self.cls_attn_blocks:\n-            x = blk(x)\n+            if self.grad_checkpointing and not torch.jit.is_scripting():\n+                x = checkpoint(blk, x)\n+            else:\n+                x = blk(x)\n \n         x = self.norm(x)\n \n", "test_patch": "diff --git a/tests/test_models.py b/tests/test_models.py\nindex b4686a3efe..58dad77e27 100644\n--- a/tests/test_models.py\n+++ b/tests/test_models.py\n@@ -186,6 +186,18 @@ def test_model_forward(model_name, batch_size):\n     assert outputs.shape[0] == batch_size\n     assert not torch.isnan(outputs).any(), 'Output included NaNs'\n \n+    # Test that grad-checkpointing, if supported, doesn't cause model failures or change in output\n+    try:\n+        model.set_grad_checkpointing()\n+    except:\n+        # throws if not supported, that's fine\n+        pass\n+    else:\n+        outputs2 = model(inputs)\n+        if isinstance(outputs, tuple):\n+            outputs2 = torch.cat(outputs2)\n+        assert torch.allclose(outputs, outputs2, rtol=1e-4, atol=1e-5), 'Output does not match'\n+\n \n @pytest.mark.base\n @pytest.mark.timeout(timeout120)\n@@ -529,6 +541,20 @@ def test_model_forward_intermediates(model_name, batch_size):\n     output2 = model.forward_features(inpt)\n     assert torch.allclose(output, output2)\n \n+    # Test that grad-checkpointing, if supported\n+    try:\n+        model.set_grad_checkpointing()\n+    except:\n+        # throws if not supported, that's fine\n+        pass\n+    else:\n+        output3, _ = model.forward_intermediates(\n+            inpt,\n+            output_fmt=output_fmt,\n+        )\n+        assert torch.allclose(output, output3, rtol=1e-4, atol=1e-5), 'Output does not match'\n+\n+\n \n def _create_fx_model(model, train=False):\n     # This block of code does a bit of juggling to handle any case where there are multiple outputs in train mode\n@@ -717,4 +743,4 @@ def test_model_forward_torchscript_with_features_fx(model_name, batch_size):\n \n         for tensor in outputs:\n             assert tensor.shape[0] == batch_size\n-            assert not torch.isnan(tensor).any(), 'Output included NaNs'\n\\ No newline at end of file\n+            assert not torch.isnan(tensor).any(), 'Output included NaNs'\n", "problem_statement": "[FEATURE] Gradient checkpointing in `forward_intermediates()`\n**Is your feature request related to a problem? Please describe.**\nI rely on the `forward_intermediates()` API for object detection models, and I'm experimenting with ViT-g and would like to try gradient checkpointing. \n\n**Describe the solution you'd like**\nIn `VisionTransformer.forward_features()` we have:\n\n```python\nif self.grad_checkpointing and not torch.jit.is_scripting():\n    x = checkpoint_seq(self.blocks, x)\n```\n\nI'm thinking something like this could work in `VisionTransformer.forward_intermediates()`:\n\n```python\nfor i, blk in enumerate(blocks):\n    if self.grad_checkpointing and not torch.jit.is_scripting():\n        x = checkpoint_module(blk, x)\n    else:\n        x = blk(x)\n```\n\nI called this `checkpoint_module()` but I _think_ we could just use `checkpoint_seq()` directly, based on the code? Either way, is this as simple as I think it would be, or am I missing something? I haven't used gradient checkpointing a lot so I'm not entirely sure. \n\nI'm happy to submit a PR for a few models if it's as simple as calling `checkpoint_seq()` in `forward_intermediates()` as I've outlined above. I'm not sure how many models use this API and/or `self.grad_checkpointing`, and whether you want this to be supported in all of them.\n\n", "hints_text": "I just noticed for ConvNeXt the gradient checkpointing is done within a ConvNeXt stage, which means it would work as is for `forward_intermediates()`. So maybe this feature request is specific to VisionTransformer (or other models whose gradient checkpointing won't work within `forward_intermediates()`).\nAlso, shouldn't this be called activation checkpointing not gradient checkpointing? Just want to make sure I'm not misunderstanding the implementation / goal here. I'm guessing the name comes from the HuggingFace trainer flag, but is a bit of a misnomer? \n@collinmccarthy you are correct on all counts, I didn't explicitly support this when I added foward_intermediates() as I was focused on getting it working / integrated and then didn't revisit. \n\nStage based ones that needed to push the logic into the stages should still work.\n\nActivation checkpointing makes more sense as the name / description of what's going on, but historically it was often called gradient checkpointing so it persisted.  Not going to change that now.\n\nIf you've tried the above additions and it works a PR would be welcome for any models that you happen to be working with. \n\nShould use my checkpoint wrapper around the torch one (changes the reentrant arg)\n\n```\nfrom ._manipulate import checkpoint\n\n...\n\ndef forward_intermediates(self, x, ...):\n\n        ...\n\n        for blk in self.blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(blk, x)\n            else:\n                x = blk(x)\n\n```\nThanks, all this sounds great. I'll submit a PR soon for just `VisionTransformer`, for now, and if I run across other models I need in the future I'll submit PRs for those and reference this issue. \n\n", "all_hints_text": "I just noticed for ConvNeXt the gradient checkpointing is done within a ConvNeXt stage, which means it would work as is for `forward_intermediates()`. So maybe this feature request is specific to VisionTransformer (or other models whose gradient checkpointing won't work within `forward_intermediates()`).\nAlso, shouldn't this be called activation checkpointing not gradient checkpointing? Just want to make sure I'm not misunderstanding the implementation / goal here. I'm guessing the name comes from the HuggingFace trainer flag, but is a bit of a misnomer? \n@collinmccarthy you are correct on all counts, I didn't explicitly support this when I added foward_intermediates() as I was focused on getting it working / integrated and then didn't revisit. \n\nStage based ones that needed to push the logic into the stages should still work.\n\nActivation checkpointing makes more sense as the name / description of what's going on, but historically it was often called gradient checkpointing so it persisted.  Not going to change that now.\n\nIf you've tried the above additions and it works a PR would be welcome for any models that you happen to be working with. \n\nShould use my checkpoint wrapper around the torch one (changes the reentrant arg)\n\n```\nfrom ._manipulate import checkpoint\n\n...\n\ndef forward_intermediates(self, x, ...):\n\n        ...\n\n        for blk in self.blocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(blk, x)\n            else:\n                x = blk(x)\n\n```\nThanks, all this sounds great. I'll submit a PR soon for just `VisionTransformer`, for now, and if I run across other models I need in the future I'll submit PRs for those and reference this issue. \n\n", "commit_urls": ["https://github.com/huggingface/pytorch-image-models/commit/57f85542da8061a7c1c90832ddae1da419b75a58", "https://github.com/huggingface/pytorch-image-models/commit/a80348a8c8935da4b0dd2fc37dc80961f59468ef", "https://github.com/huggingface/pytorch-image-models/commit/b0b28e29aab9190d994b5b73630b714a3daf828c", "https://github.com/huggingface/pytorch-image-models/commit/d1e7779828a3f5b00a6d37f2793b17f56cde0645", "https://github.com/huggingface/pytorch-image-models/commit/c9f9c30dfad1b7327e00f350d87268fe23fe4180", "https://github.com/huggingface/pytorch-image-models/commit/0638708731f7635d274d1884ccfc2f98cfc4221a", "https://github.com/huggingface/pytorch-image-models/commit/03fa149147b1695dc7ebce06737623d39b7dd34f", "https://github.com/huggingface/pytorch-image-models/commit/cd1542aa3977004249c3d1cb682002d8a1dda33a", "https://github.com/huggingface/pytorch-image-models/commit/f71c3d1a9c49ea85011fdbe94921da0dd81edf8e", "https://github.com/huggingface/pytorch-image-models/commit/2579b12b336edf25332a2eea11f5cd7316228f83", "https://github.com/huggingface/pytorch-image-models/commit/869bac2515ccc6bd70e50275f5f29a75df01363c", "https://github.com/huggingface/pytorch-image-models/commit/b6692ed5e7c6d944d782765d556ae620d68a19db", "https://github.com/huggingface/pytorch-image-models/commit/1f9eb663715f40e4a5a78e57180f8f0240198ec2", "https://github.com/huggingface/pytorch-image-models/commit/725071e02026dd6e1a626264a18a4d5c51df7ca4", "https://github.com/huggingface/pytorch-image-models/commit/b1a9a9e28af3a3867820cfafa6a2d571a684b1dc", "https://github.com/huggingface/pytorch-image-models/commit/b3a87738dce3989a951ac47671abd190e05610b3"], "created_at": "2025-05-28T21:25:56Z", "classification": "Efficiency"}
{"repo": "Lightning-AI/pytorch-lightning", "pull_number": 20868, "instance_id": "Lightning-AI__pytorch-lightning-20868", "issue_numbers": [12567], "base_commit": "fafc2395884877f94cbc2bf83a83e3fc0f1d5c2d", "patch": "diff --git a/src/lightning/pytorch/callbacks/model_summary.py b/src/lightning/pytorch/callbacks/model_summary.py\nindex 03f50d65bf1e9..ee9ff2f3bd902 100644\n--- a/src/lightning/pytorch/callbacks/model_summary.py\n+++ b/src/lightning/pytorch/callbacks/model_summary.py\n@@ -68,6 +68,9 @@ def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -\n         model_size = model_summary.model_size\n         total_training_modes = model_summary.total_training_modes\n \n+        # todo Add `total_flops` in DeepSpeedSummary.\n+        total_flops = model_summary.total_flops if hasattr(model_summary, \"total_flops\") else 0\n+\n         if trainer.is_global_zero:\n             self.summarize(\n                 summary_data,\n@@ -75,6 +78,7 @@ def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -\n                 trainable_parameters,\n                 model_size,\n                 total_training_modes,\n+                total_flops=total_flops,\n                 **self._summarize_kwargs,\n             )\n \n@@ -92,6 +96,7 @@ def summarize(\n         trainable_parameters: int,\n         model_size: float,\n         total_training_modes: dict[str, int],\n+        total_flops: int,\n         **summarize_kwargs: Any,\n     ) -> None:\n         summary_table = _format_summary_table(\n@@ -99,6 +104,7 @@ def summarize(\n             trainable_parameters,\n             model_size,\n             total_training_modes,\n+            total_flops,\n             *summary_data,\n         )\n         log.info(\"\\n\" + summary_table)\ndiff --git a/src/lightning/pytorch/callbacks/rich_model_summary.py b/src/lightning/pytorch/callbacks/rich_model_summary.py\nindex e4027f0dedcb1..817aeeb655a7a 100644\n--- a/src/lightning/pytorch/callbacks/rich_model_summary.py\n+++ b/src/lightning/pytorch/callbacks/rich_model_summary.py\n@@ -72,6 +72,7 @@ def summarize(\n         trainable_parameters: int,\n         model_size: float,\n         total_training_modes: dict[str, int],\n+        total_flops: int,\n         **summarize_kwargs: Any,\n     ) -> None:\n         from rich import get_console\n@@ -86,6 +87,7 @@ def summarize(\n         table.add_column(\"Type\")\n         table.add_column(\"Params\", justify=\"right\")\n         table.add_column(\"Mode\")\n+        table.add_column(\"FLOPs\", justify=\"right\")\n \n         column_names = list(zip(*summary_data))[0]\n \n@@ -113,5 +115,6 @@ def summarize(\n         grid.add_row(f\"[bold]Total estimated model params size (MB)[/]: {parameters[3]}\")\n         grid.add_row(f\"[bold]Modules in train mode[/]: {total_training_modes['train']}\")\n         grid.add_row(f\"[bold]Modules in eval mode[/]: {total_training_modes['eval']}\")\n+        grid.add_row(f\"[bold]Total FLOPs[/]: {get_human_readable_count(total_flops)}\")\n \n         console.print(grid)\ndiff --git a/src/lightning/pytorch/utilities/model_summary/model_summary.py b/src/lightning/pytorch/utilities/model_summary/model_summary.py\nindex 6a5baf2c1e04a..98d74ff63ea5f 100644\n--- a/src/lightning/pytorch/utilities/model_summary/model_summary.py\n+++ b/src/lightning/pytorch/utilities/model_summary/model_summary.py\n@@ -22,10 +22,12 @@\n import torch\n import torch.nn as nn\n from torch import Tensor\n+from torch.utils.flop_counter import FlopCounterMode\n from torch.utils.hooks import RemovableHandle\n \n import lightning.pytorch as pl\n from lightning.fabric.utilities.distributed import _is_dtensor\n+from lightning.fabric.utilities.imports import _TORCH_GREATER_EQUAL_2_4\n from lightning.pytorch.utilities.model_helpers import _ModuleMode\n from lightning.pytorch.utilities.rank_zero import WarningCache\n \n@@ -180,29 +182,31 @@ class ModelSummary:\n         ...\n         >>> model = LitModel()\n         >>> ModelSummary(model, max_depth=1)  # doctest: +NORMALIZE_WHITESPACE\n-          | Name | Type       | Params | Mode  | In sizes  | Out sizes\n-        --------------------------------------------------------------------\n-        0 | net  | Sequential | 132 K  | train | [10, 256] | [10, 512]\n-        --------------------------------------------------------------------\n+          | Name | Type       | Params | Mode  | FLOPs | In sizes  | Out sizes\n+        ----------------------------------------------------------------------------\n+        0 | net  | Sequential | 132 K  | train | 2.6 M | [10, 256] | [10, 512]\n+        ----------------------------------------------------------------------------\n         132 K     Trainable params\n         0         Non-trainable params\n         132 K     Total params\n         0.530     Total estimated model params size (MB)\n         3         Modules in train mode\n         0         Modules in eval mode\n+        2.6 M     Total Flops\n         >>> ModelSummary(model, max_depth=-1)  # doctest: +NORMALIZE_WHITESPACE\n-          | Name  | Type        | Params | Mode  | In sizes  | Out sizes\n-        ----------------------------------------------------------------------\n-        0 | net   | Sequential  | 132 K  | train | [10, 256] | [10, 512]\n-        1 | net.0 | Linear      | 131 K  | train | [10, 256] | [10, 512]\n-        2 | net.1 | BatchNorm1d | 1.0 K  | train | [10, 512] | [10, 512]\n-        ----------------------------------------------------------------------\n+          | Name  | Type        | Params | Mode  | FLOPs | In sizes  | Out sizes\n+        ------------------------------------------------------------------------------\n+        0 | net   | Sequential  | 132 K  | train | 2.6 M | [10, 256] | [10, 512]\n+        1 | net.0 | Linear      | 131 K  | train | 2.6 M | [10, 256] | [10, 512]\n+        2 | net.1 | BatchNorm1d | 1.0 K  | train | 0     | [10, 512] | [10, 512]\n+        ------------------------------------------------------------------------------\n         132 K     Trainable params\n         0         Non-trainable params\n         132 K     Total params\n         0.530     Total estimated model params size (MB)\n         3         Modules in train mode\n         0         Modules in eval mode\n+        2.6 M     Total Flops\n \n     \"\"\"\n \n@@ -212,6 +216,13 @@ def __init__(self, model: \"pl.LightningModule\", max_depth: int = 1) -> None:\n         if not isinstance(max_depth, int) or max_depth < -1:\n             raise ValueError(f\"`max_depth` can be -1, 0 or > 0, got {max_depth}.\")\n \n+        # The max-depth needs to be plus one because the root module is already counted as depth 0.\n+        self._flop_counter = FlopCounterMode(\n+            mods=None if _TORCH_GREATER_EQUAL_2_4 else self._model,\n+            display=False,\n+            depth=max_depth + 1,\n+        )\n+\n         self._max_depth = max_depth\n         self._layer_summary = self.summarize()\n         # 1 byte -> 8 bits\n@@ -279,6 +290,22 @@ def total_layer_params(self) -> int:\n     def model_size(self) -> float:\n         return self.total_parameters * self._precision_megabytes\n \n+    @property\n+    def total_flops(self) -> int:\n+        return self._flop_counter.get_total_flops()\n+\n+    @property\n+    def flop_counts(self) -> dict[str, dict[Any, int]]:\n+        flop_counts = self._flop_counter.get_flop_counts()\n+        ret = {\n+            name: flop_counts.get(\n+                f\"{type(self._model).__name__}.{name}\",\n+                {},\n+            )\n+            for name in self.layer_names\n+        }\n+        return ret\n+\n     def summarize(self) -> dict[str, LayerSummary]:\n         summary = OrderedDict((name, LayerSummary(module)) for name, module in self.named_modules)\n         if self._model.example_input_array is not None:\n@@ -307,8 +334,18 @@ def _forward_example_input(self) -> None:\n         mode.capture(model)\n         model.eval()\n \n+        # FlopCounterMode does not support ScriptModules before torch 2.4.0, so we use a null context\n+        flop_context = (\n+            contextlib.nullcontext()\n+            if (\n+                not _TORCH_GREATER_EQUAL_2_4\n+                and any(isinstance(m, torch.jit.ScriptModule) for m in self._model.modules())\n+            )\n+            else self._flop_counter\n+        )\n+\n         forward_context = contextlib.nullcontext() if trainer is None else trainer.precision_plugin.forward_context()\n-        with torch.no_grad(), forward_context:\n+        with torch.no_grad(), forward_context, flop_context:\n             # let the model hooks collect the input- and output shapes\n             if isinstance(input_, (list, tuple)):\n                 model(*input_)\n@@ -330,6 +367,7 @@ def _get_summary_data(self) -> list[tuple[str, list[str]]]:\n             (\"Type\", self.layer_types),\n             (\"Params\", list(map(get_human_readable_count, self.param_nums))),\n             (\"Mode\", [\"train\" if mode else \"eval\" for mode in self.training_modes]),\n+            (\"FLOPs\", list(map(get_human_readable_count, (sum(x.values()) for x in self.flop_counts.values())))),\n         ]\n         if self._model.example_input_array is not None:\n             arrays.append((\"In sizes\", [str(x) for x in self.in_sizes]))\n@@ -349,6 +387,7 @@ def _add_leftover_params_to_summary(self, arrays: list[tuple[str, list[str]]], t\n         layer_summaries[\"Type\"].append(NOT_APPLICABLE)\n         layer_summaries[\"Params\"].append(get_human_readable_count(total_leftover_params))\n         layer_summaries[\"Mode\"].append(NOT_APPLICABLE)\n+        layer_summaries[\"FLOPs\"].append(NOT_APPLICABLE)\n         if \"In sizes\" in layer_summaries:\n             layer_summaries[\"In sizes\"].append(NOT_APPLICABLE)\n         if \"Out sizes\" in layer_summaries:\n@@ -361,8 +400,16 @@ def __str__(self) -> str:\n         trainable_parameters = self.trainable_parameters\n         model_size = self.model_size\n         total_training_modes = self.total_training_modes\n-\n-        return _format_summary_table(total_parameters, trainable_parameters, model_size, total_training_modes, *arrays)\n+        total_flops = self.total_flops\n+\n+        return _format_summary_table(\n+            total_parameters,\n+            trainable_parameters,\n+            model_size,\n+            total_training_modes,\n+            total_flops,\n+            *arrays,\n+        )\n \n     def __repr__(self) -> str:\n         return str(self)\n@@ -383,6 +430,7 @@ def _format_summary_table(\n     trainable_parameters: int,\n     model_size: float,\n     total_training_modes: dict[str, int],\n+    total_flops: int,\n     *cols: tuple[str, list[str]],\n ) -> str:\n     \"\"\"Takes in a number of arrays, each specifying a column in the summary table, and combines them all into one big\n@@ -423,6 +471,8 @@ def _format_summary_table(\n     summary += \"Modules in train mode\"\n     summary += \"\\n\" + s.format(total_training_modes[\"eval\"], 10)\n     summary += \"Modules in eval mode\"\n+    summary += \"\\n\" + s.format(get_human_readable_count(total_flops), 10)\n+    summary += \"Total Flops\"\n \n     return summary\n \n", "test_patch": "diff --git a/tests/tests_pytorch/callbacks/test_model_summary.py b/tests/tests_pytorch/callbacks/test_model_summary.py\nindex 215176ee2376b..07676801fcfb2 100644\n--- a/tests/tests_pytorch/callbacks/test_model_summary.py\n+++ b/tests/tests_pytorch/callbacks/test_model_summary.py\n@@ -65,6 +65,9 @@ def summarize(\n             assert summary_data[4][0] == \"Mode\"\n             assert summary_data[4][1][0] == \"train\"\n \n+            assert summary_data[5][0] == \"FLOPs\"\n+            assert all(isinstance(x, str) for x in summary_data[5][1])\n+\n             assert total_training_modes == {\"train\": 1, \"eval\": 0}\n \n     model = BoringModel()\ndiff --git a/tests/tests_pytorch/callbacks/test_rich_model_summary.py b/tests/tests_pytorch/callbacks/test_rich_model_summary.py\nindex 7534c23d5679c..af385bb1a9b39 100644\n--- a/tests/tests_pytorch/callbacks/test_rich_model_summary.py\n+++ b/tests/tests_pytorch/callbacks/test_rich_model_summary.py\n@@ -62,10 +62,11 @@ def example_input_array(self) -> Any:\n         trainable_parameters=1,\n         model_size=1,\n         total_training_modes=summary.total_training_modes,\n+        total_flops=1,\n     )\n \n     # ensure that summary was logged + the breakdown of model parameters\n     assert mock_console.call_count == 2\n     # assert that the input summary data was converted correctly\n     args, _ = mock_table_add_row.call_args_list[0]\n-    assert args[1:] == (\"0\", \"layer\", \"Linear\", \"66  \", \"train\", \"[4, 32]\", \"[4, 2]\")\n+    assert args[1:] == (\"0\", \"layer\", \"Linear\", \"66  \", \"train\", \"512  \", \"[4, 32]\", \"[4, 2]\")\ndiff --git a/tests/tests_pytorch/utilities/test_model_summary.py b/tests/tests_pytorch/utilities/test_model_summary.py\nindex 54c5572d01767..85825b5ea749d 100644\n--- a/tests/tests_pytorch/utilities/test_model_summary.py\n+++ b/tests/tests_pytorch/utilities/test_model_summary.py\n@@ -173,6 +173,7 @@ def test_empty_model_summary_shapes(max_depth):\n     assert summary.in_sizes == []\n     assert summary.out_sizes == []\n     assert summary.param_nums == []\n+    assert summary.total_flops == 0\n \n \n @pytest.mark.parametrize(\"max_depth\", [-1, 1])\n", "problem_statement": "\ud83d\ude80 Add FLOPs count to model summary\n## \ud83d\ude80 Feature\r\n\r\nAdd FLOPs count in model summary.\r\n\r\n### Motivation\r\n\r\nImprovements in model development are increasingly evaluated using the FLOPs count (e.g., [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)). However, there is no standardized way to compute FLOPs count, though many libraries exist (e.g., [1](https://github.com/1adrianb/pytorch-estimate-flops), [2](https://github.com/facebookresearch/fvcore/blob/main/docs/flop_count.md)). It would be great to add this functionality to the model summary in pytorch lightning.\n\ncc @borda @kaushikb11 @awaelchli @rohitgr7 @akihironitta\n", "hints_text": "Prior issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/3337\nThis issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n\nIt could be implemented by adapting https://dev-discuss.pytorch.org/t/the-ideal-pytorch-flop-counter-with-torch-dispatch/505 but PyTorch itself does not provide a solution upstream: https://github.com/pytorch/pytorch/issues/5013\nAn implementation of the ideal flop counter is here: \r\n\r\nhttps://github.com/pytorch-labs/torcheval/blob/main/torcheval/tools/module_summary.py\r\n\r\nhttps://github.com/pytorch-labs/torcheval/blob/main/torcheval/tools/flops.py\nThanks for the link @ananthsub. For anybody reading, this is how you would use it:\r\n\r\n```python\r\nimport torch\r\nfrom torcheval.tools.module_summary import get_module_summary\r\nfrom pytorch_lightning.demos.boring_classes import BoringModel\r\n\r\nmodel = BoringModel()\r\nsummary = get_module_summary(model, torch.randn(2, 32))\r\nprint(summary.flops_forward, summary.flops_backward)\r\n```\r\n\r\nStill, we'll want to add FLOPs support to our `ModelSummary` class (torcheval's `ModuleSummary` looks quite similar :eyes:), so leaving this issue open.\nA FLOP counter was added to PyTorch: https://github.com/pytorch/pytorch/pull/95751\n#18848 added this small utility (to be released with 2.2)\r\n\r\n```python\r\nfrom lightning.fabric.utilities import measure_flops\r\n\r\nwith torch.device(\"meta\"):\r\n    model = MyModel()\r\n    x = torch.randn(2, 32)\r\n\r\nmodel_fwd = lambda: model(x)\r\nfwd_flops = measure_flops(model, model_fwd)\r\n\r\nmodel_loss = lambda y: y.sum()\r\nfwd_and_bwd_flops = measure_flops(model, model_fwd, model_loss)\r\n```\nHow do you go with counting flops for billion-parameter models that get OOM when running the meta device?\nHi @carmocca, I\u2019ve added FLOPs support to the `ModelSummary` in #20868.  \nFeel free to check it out if you or others are looking to include FLOPs in the summary callback.\n\n", "all_hints_text": "Prior issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/3337\nThis issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n\nIt could be implemented by adapting https://dev-discuss.pytorch.org/t/the-ideal-pytorch-flop-counter-with-torch-dispatch/505 but PyTorch itself does not provide a solution upstream: https://github.com/pytorch/pytorch/issues/5013\nAn implementation of the ideal flop counter is here: \r\n\r\nhttps://github.com/pytorch-labs/torcheval/blob/main/torcheval/tools/module_summary.py\r\n\r\nhttps://github.com/pytorch-labs/torcheval/blob/main/torcheval/tools/flops.py\nThanks for the link @ananthsub. For anybody reading, this is how you would use it:\r\n\r\n```python\r\nimport torch\r\nfrom torcheval.tools.module_summary import get_module_summary\r\nfrom pytorch_lightning.demos.boring_classes import BoringModel\r\n\r\nmodel = BoringModel()\r\nsummary = get_module_summary(model, torch.randn(2, 32))\r\nprint(summary.flops_forward, summary.flops_backward)\r\n```\r\n\r\nStill, we'll want to add FLOPs support to our `ModelSummary` class (torcheval's `ModuleSummary` looks quite similar :eyes:), so leaving this issue open.\nA FLOP counter was added to PyTorch: https://github.com/pytorch/pytorch/pull/95751\n#18848 added this small utility (to be released with 2.2)\r\n\r\n```python\r\nfrom lightning.fabric.utilities import measure_flops\r\n\r\nwith torch.device(\"meta\"):\r\n    model = MyModel()\r\n    x = torch.randn(2, 32)\r\n\r\nmodel_fwd = lambda: model(x)\r\nfwd_flops = measure_flops(model, model_fwd)\r\n\r\nmodel_loss = lambda y: y.sum()\r\nfwd_and_bwd_flops = measure_flops(model, model_fwd, model_loss)\r\n```\nHow do you go with counting flops for billion-parameter models that get OOM when running the meta device?\nHi @carmocca, I\u2019ve added FLOPs support to the `ModelSummary` in #20868.  \nFeel free to check it out if you or others are looking to include FLOPs in the summary callback.\n\n", "commit_urls": ["https://github.com/Lightning-AI/pytorch-lightning/commit/b44a35bdc68241cf24b9bd6a14240188ef4d04aa", "https://github.com/Lightning-AI/pytorch-lightning/commit/be6c17a5b4408d0e6ae20175a33c3fca28822a24", "https://github.com/Lightning-AI/pytorch-lightning/commit/b9837e48aa3d657f0190928ce4b380fe78406a97", "https://github.com/Lightning-AI/pytorch-lightning/commit/0f40bf181af055b25b3b542600df663fe2ff18b4", "https://github.com/Lightning-AI/pytorch-lightning/commit/5473ea5b3735a97f4630392a35eb14eedd4f40e6", "https://github.com/Lightning-AI/pytorch-lightning/commit/d720e36610c01e818f980e60590bfdc68493d61a", "https://github.com/Lightning-AI/pytorch-lightning/commit/81a08ec1626a07072bcab6e1372b5ac390e33945", "https://github.com/Lightning-AI/pytorch-lightning/commit/4b7eddfa2fd52869d7233384ded4d6057a990e35", "https://github.com/Lightning-AI/pytorch-lightning/commit/61b9abc8b85f9703b7b37fc9e8f0863a5b769bdb"], "created_at": "2025-05-31T08:33:02Z", "classification": "Efficiency"}
{"repo": "huggingface/smolagents", "pull_number": 1417, "instance_id": "huggingface__smolagents-1417", "issue_numbers": [1262], "base_commit": "7113cf6b3987c7f93ea17f9963fc32fd6cee2846", "patch": "diff --git a/src/smolagents/agents.py b/src/smolagents/agents.py\nindex 28c109c49..468da08c8 100644\n--- a/src/smolagents/agents.py\n+++ b/src/smolagents/agents.py\n@@ -462,7 +462,7 @@ def _run_stream(\n                 planning_start_time = time.time()\n                 planning_step = None\n                 for element in self._generate_planning_step(\n-                    task, is_first_step=(self.step_number == 1), step=self.step_number\n+                    task, is_first_step=(len(self.memory.steps) == 1), step=self.step_number\n                 ):\n                     yield element\n                     planning_step = element\n@@ -617,7 +617,8 @@ def _generate_planning_step(\n                     }\n                 ],\n             }\n-            input_messages = [plan_update_pre] + memory_messages + [plan_update_post]\n+            # remove last message from memory_messages because it is the current task\n+            input_messages = [plan_update_pre] + memory_messages[:-1] + [plan_update_post]\n             if self.stream_outputs and hasattr(self.model, \"generate_stream\"):\n                 plan_message_content = \"\"\n                 input_tokens, output_tokens = 0, 0\n", "test_patch": "diff --git a/tests/test_agents.py b/tests/test_agents.py\nindex f92449df4..1c3565a32 100644\n--- a/tests/test_agents.py\n+++ b/tests/test_agents.py\n@@ -45,7 +45,11 @@\n     populate_template,\n )\n from smolagents.default_tools import DuckDuckGoSearchTool, FinalAnswerTool, PythonInterpreterTool, VisitWebpageTool\n-from smolagents.memory import ActionStep, PlanningStep\n+from smolagents.memory import (\n+    ActionStep,\n+    PlanningStep,\n+    TaskStep,\n+)\n from smolagents.models import (\n     ChatMessage,\n     ChatMessageToolCall,\n@@ -55,7 +59,7 @@\n     Model,\n     TransformersModel,\n )\n-from smolagents.monitoring import AgentLogger, LogLevel\n+from smolagents.monitoring import AgentLogger, LogLevel, TokenUsage\n from smolagents.tools import Tool, tool\n from smolagents.utils import (\n     BASE_BUILTIN_MODULES,\n@@ -223,6 +227,15 @@ def generate(self, messages, stop_sequences=None):\n             )\n \n \n+class FakeCodeModelPlanning(Model):\n+    def generate(self, messages, stop_sequences=None):\n+        return ChatMessage(\n+            role=\"assistant\",\n+            content=\"llm plan\",\n+            token_usage=TokenUsage(input_tokens=10, output_tokens=10),\n+        )\n+\n+\n class FakeCodeModelError(Model):\n     def generate(self, messages, stop_sequences=None):\n         prompt = str(messages)\n@@ -629,6 +642,30 @@ def generate(self, messages, stop_sequences=None):\n         assert len(agent.memory.steps) == 2\n         assert \"Generation failed\" in str(e)\n \n+    def test_planning_step_with_injected_memory(self):\n+        \"\"\"Test that planning step uses update plan prompts when memory is injected before run.\"\"\"\n+        agent = CodeAgent(tools=[], planning_interval=1, model=FakeCodeModelPlanning())\n+        task = \"Continuous task\"\n+\n+        # Inject memory before run\n+        previous_step = TaskStep(task=\"Previous user request\")\n+        agent.memory.steps.append(previous_step)\n+\n+        # Run the agent\n+        agent.run(task, reset=False)\n+\n+        # Verify that the planning step used update plan prompts\n+        planning_steps = [step for step in agent.memory.steps if isinstance(step, PlanningStep)]\n+        assert len(planning_steps) > 0\n+\n+        # Check that the planning step's model input messages contain the injected memory\n+        planning_step = planning_steps[0]\n+        assert len(planning_step.model_input_messages) == 3  # system message + memory messages + user message\n+        assert planning_step.model_input_messages[0][\"role\"] == \"system\"\n+        assert task in planning_step.model_input_messages[0][\"content\"][0][\"text\"]\n+        assert planning_step.model_input_messages[1][\"role\"] == \"user\"\n+        assert \"Previous user request\" in planning_step.model_input_messages[1][\"content\"][0][\"text\"]\n+\n \n class CustomFinalAnswerTool(FinalAnswerTool):\n     def forward(self, answer) -> str:\n", "problem_statement": "[BUG] Agent memory not used when planning step\n**Describe the bug**\nAgent memory should be used for the planning step, whatever the `step_number` is (which is relative the last `Task` step).\nWhen you converse with the agent with a `planning_interval > 0` then you can get weird planning from the 2nd message (task).\n\nExample:\n\n_(consider I've already asked my agent images of president Trump dancing)_\n\n> User: Give me more images\n\nAt this point, the planning will mostly be about getting the context of those image, asking more information from the user and stuff like that. Most of those planned steps __are already known__ from previous messages.\nSo yes, when step 1 is starting, memory is injected and LLM will understand those questions are already answered. But we've done a useless planning in the mean time ...\nCurrently, I am considering deactivating the planning step if it is not a new conversation, due to this behavior.\n\n**Code to reproduce the error**\nYou just need `planning_interval` set and greather than 0.\nThen converse with your agent and do not repeat yourself (e.g. `give me more`) and look at plannings.\n\n**Expected behavior**\nWhen there is a memory, it should be used with the planning.\n\n**Packages version:**\n`v1.13.0`\n\n\n", "hints_text": "Thanks for raising this!\n\nThe current behavior *does* include agent memory in the planning step, with two intentional exclusions: previous **planning steps** and the **system prompt**. This design ensures that the agent's new plan is informed by the broader task context and past interactions, without being constrained by earlier plans that may no longer be relevant.\n\nIf you're observing unexpected planning behavior starting from the second task message when using a `planning_interval > 0`, that might point to a bug or edge case worth investigating. Could you share a minimal reproducible example so we can take a closer look?\n\nHello @albertvillanova  I may not have been clear enough.\nI am considering a conversation here: Task -> Answer -> Task -> __HERE__.\n\n> The current behavior does include agent memory in the planning step, with two intentional exclusions: previous planning steps and the system prompt.\n\nYes, but it applies only __within__ the run of a Task __AND if it is not the first step__ (which means is _does not_ work __between__ tasks, which is my problem).\n\n--- \n\nBelow links will target tag `v1.15` so they remains stable over time, but it's the same in `main` as I write this comment.\nAs mentionned above,  >>> I am considering that the user already sent messages and the agent replied. <<<\n\nWhen `run` starts for the nth Task, the `step_number` is reset to `1`. [link](https://github.com/huggingface/smolagents/blob/v1.15.0/src/smolagents/agents.py#L357).\n\nIf we activated the planning step (let's assume we did), the `is_first_step` is computed as `is_first_step=(self.step_number == 1)` [link](https://github.com/huggingface/smolagents/blob/v1.15.0/src/smolagents/agents.py#L366C27-L366C64)\n\nSo at that point, the planning will be executed with `is_first_step=True`.\nAnd keep in mind as well that this flag drives whether we inject memory or not [link](https://github.com/huggingface/smolagents/blob/v1.15.0/src/smolagents/agents.py#L438-L452)\n\nMy problem is exactly this. It should have been resolved to `False`.\n\n`is_first_step=(self.step_number == 1)` is IMHO to much \"single Task\" oriented and my proposal is to change it to something like `is_first_step=len(self.memory.steps) == 0`.\n\n---\n\nWhy ? \n\nAs said in my first message, a user can input:\n\n> Can you find others with red ?\n\nCurrent agent planning is 99% about finding the context around that question.\nBut if memory was injected __when there is a memory, not when it's the first step of the task__ then it would have immediatly understoold that I was talking about flowers.\n\nAgain, since the memory is injected as step 1, the agent will get back on correct tracks.\nBut yeah, in this case (which is just standard conversation case) the planning is just useless after the 1st Task.\n\n", "all_hints_text": "Thanks for raising this!\n\nThe current behavior *does* include agent memory in the planning step, with two intentional exclusions: previous **planning steps** and the **system prompt**. This design ensures that the agent's new plan is informed by the broader task context and past interactions, without being constrained by earlier plans that may no longer be relevant.\n\nIf you're observing unexpected planning behavior starting from the second task message when using a `planning_interval > 0`, that might point to a bug or edge case worth investigating. Could you share a minimal reproducible example so we can take a closer look?\n\nHello @albertvillanova  I may not have been clear enough.\nI am considering a conversation here: Task -> Answer -> Task -> __HERE__.\n\n> The current behavior does include agent memory in the planning step, with two intentional exclusions: previous planning steps and the system prompt.\n\nYes, but it applies only __within__ the run of a Task __AND if it is not the first step__ (which means is _does not_ work __between__ tasks, which is my problem).\n\n--- \n\nBelow links will target tag `v1.15` so they remains stable over time, but it's the same in `main` as I write this comment.\nAs mentionned above,  >>> I am considering that the user already sent messages and the agent replied. <<<\n\nWhen `run` starts for the nth Task, the `step_number` is reset to `1`. [link](https://github.com/huggingface/smolagents/blob/v1.15.0/src/smolagents/agents.py#L357).\n\nIf we activated the planning step (let's assume we did), the `is_first_step` is computed as `is_first_step=(self.step_number == 1)` [link](https://github.com/huggingface/smolagents/blob/v1.15.0/src/smolagents/agents.py#L366C27-L366C64)\n\nSo at that point, the planning will be executed with `is_first_step=True`.\nAnd keep in mind as well that this flag drives whether we inject memory or not [link](https://github.com/huggingface/smolagents/blob/v1.15.0/src/smolagents/agents.py#L438-L452)\n\nMy problem is exactly this. It should have been resolved to `False`.\n\n`is_first_step=(self.step_number == 1)` is IMHO to much \"single Task\" oriented and my proposal is to change it to something like `is_first_step=len(self.memory.steps) == 0`.\n\n---\n\nWhy ? \n\nAs said in my first message, a user can input:\n\n> Can you find others with red ?\n\nCurrent agent planning is 99% about finding the context around that question.\nBut if memory was injected __when there is a memory, not when it's the first step of the task__ then it would have immediatly understoold that I was talking about flowers.\n\nAgain, since the memory is injected as step 1, the agent will get back on correct tracks.\nBut yeah, in this case (which is just standard conversation case) the planning is just useless after the 1st Task.\nThanks for your explanation, @gael-ft.\n\nIf I understand correctly, you are assuming that 2 consecutive tasks are related, so the 2nd task can skip planning if the first one has already handled it. \n\nThat makes sense in some scenarios, but what about if the second task is completely unrelated to the 1st one?\n\n", "commit_urls": ["https://github.com/huggingface/smolagents/commit/0a2d48b5de108e86149abb05442779a87fd60719", "https://github.com/huggingface/smolagents/commit/d8bd6c4c98aef89a5fe01fe7998aa4603356f5d1", "https://github.com/huggingface/smolagents/commit/0a0ccc52607d25da9ac6199cee7bcbd40b282e81", "https://github.com/huggingface/smolagents/commit/daf454448c4569edbd69e5ce22f4653ec842e1fe", "https://github.com/huggingface/smolagents/commit/f9aec0f972e9d68902d5a4d7b5c990c826a1633c"], "created_at": "2025-06-06T18:06:39Z", "classification": "Efficiency"}
