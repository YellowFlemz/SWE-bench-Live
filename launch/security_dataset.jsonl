{"repo": "apache/airflow", "pull_number": 51991, "instance_id": "apache__airflow-51991", "issue_numbers": [17500], "base_commit": "c27ec6d2eb9dbcdf299f2dc96e2896a5b371ca09", "patch": "diff --git a/providers/hashicorp/src/airflow/providers/hashicorp/_internal_client/vault_client.py b/providers/hashicorp/src/airflow/providers/hashicorp/_internal_client/vault_client.py\nindex edc87827857af..aa73ffdc8b7db 100644\n--- a/providers/hashicorp/src/airflow/providers/hashicorp/_internal_client/vault_client.py\n+++ b/providers/hashicorp/src/airflow/providers/hashicorp/_internal_client/vault_client.py\n@@ -153,6 +153,15 @@ def __init__(\n                 raise VaultError(\"The 'radius' authentication type requires 'radius_host'\")\n             if not radius_secret:\n                 raise VaultError(\"The 'radius' authentication type requires 'radius_secret'\")\n+        if auth_type == \"gcp\":\n+            if not gcp_scopes:\n+                raise VaultError(\"The 'gcp' authentication type requires 'gcp_scopes'\")\n+            if not role_id:\n+                raise VaultError(\"The 'gcp' authentication type requires 'role_id'\")\n+            if not gcp_key_path and not gcp_keyfile_dict:\n+                raise VaultError(\n+                    \"The 'gcp' authentication type requires 'gcp_key_path' or 'gcp_keyfile_dict'\"\n+                )\n \n         self.kv_engine_version = kv_engine_version or 2\n         self.url = url\n@@ -303,13 +312,41 @@ def _auth_gcp(self, _client: hvac.Client) -> None:\n         )\n \n         scopes = _get_scopes(self.gcp_scopes)\n-        credentials, _ = get_credentials_and_project_id(\n+        credentials, project_id = get_credentials_and_project_id(\n             key_path=self.gcp_key_path, keyfile_dict=self.gcp_keyfile_dict, scopes=scopes\n         )\n+\n+        import json\n+        import time\n+\n+        import googleapiclient\n+\n+        if self.gcp_keyfile_dict:\n+            creds = self.gcp_keyfile_dict\n+        elif self.gcp_key_path:\n+            with open(self.gcp_key_path) as f:\n+                creds = json.load(f)\n+\n+        service_account = creds[\"client_email\"]\n+\n+        # Generate a payload for subsequent \"signJwt()\" call\n+        # Reference: https://googleapis.dev/python/google-auth/latest/reference/google.auth.jwt.html#google.auth.jwt.Credentials\n+        now = int(time.time())\n+        expires = now + 900  # 15 mins in seconds, can't be longer.\n+        payload = {\"iat\": now, \"exp\": expires, \"sub\": credentials, \"aud\": f\"vault/{self.role_id}\"}\n+        body = {\"payload\": json.dumps(payload)}\n+        name = f\"projects/{project_id}/serviceAccounts/{service_account}\"\n+\n+        # Perform the GCP API call\n+        iam = googleapiclient.discovery.build(\"iam\", \"v1\", credentials=credentials)\n+        request = iam.projects().serviceAccounts().signJwt(name=name, body=body)\n+        resp = request.execute()\n+        jwt = resp[\"signedJwt\"]\n+\n         if self.auth_mount_point:\n-            _client.auth.gcp.configure(credentials=credentials, mount_point=self.auth_mount_point)\n+            _client.auth.gcp.login(role=self.role_id, jwt=jwt, mount_point=self.auth_mount_point)\n         else:\n-            _client.auth.gcp.configure(credentials=credentials)\n+            _client.auth.gcp.login(role=self.role_id, jwt=jwt)\n \n     def _auth_azure(self, _client: hvac.Client) -> None:\n         if self.auth_mount_point:\n", "test_patch": "diff --git a/providers/hashicorp/tests/unit/hashicorp/_internal_client/test_vault_client.py b/providers/hashicorp/tests/unit/hashicorp/_internal_client/test_vault_client.py\nindex 8b98a814394d5..c9239b75a99e4 100644\n--- a/providers/hashicorp/tests/unit/hashicorp/_internal_client/test_vault_client.py\n+++ b/providers/hashicorp/tests/unit/hashicorp/_internal_client/test_vault_client.py\n@@ -16,6 +16,8 @@\n # under the License.\n from __future__ import annotations\n \n+import json\n+import time\n from unittest import mock\n from unittest.mock import mock_open, patch\n \n@@ -253,86 +255,217 @@ def test_azure_missing_tenant_id(self, mock_hvac):\n                 secret_id=\"pass\",\n             )\n \n+    @mock.patch(\"builtins.open\", create=True)\n     @mock.patch(\"airflow.providers.google.cloud.utils.credentials_provider._get_scopes\")\n     @mock.patch(\"airflow.providers.google.cloud.utils.credentials_provider.get_credentials_and_project_id\")\n-    @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")\n-    def test_gcp(self, mock_hvac, mock_get_credentials, mock_get_scopes):\n+    @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac.Client\")\n+    @mock.patch(\"googleapiclient.discovery.build\")\n+    def test_gcp(self, mock_google_build, mock_hvac_client, mock_get_credentials, mock_get_scopes, mock_open):\n+        # Mock the content of the file 'path.json'\n+        mock_file = mock.MagicMock()\n+        mock_file.read.return_value = '{\"client_email\": \"service_account_email\"}'\n+        mock_open.return_value.__enter__.return_value = mock_file\n+\n         mock_client = mock.MagicMock()\n-        mock_hvac.Client.return_value = mock_client\n+        mock_hvac_client.return_value = mock_client\n         mock_get_scopes.return_value = [\"scope1\", \"scope2\"]\n         mock_get_credentials.return_value = (\"credentials\", \"project_id\")\n+\n+        # Mock the current time to use for iat and exp\n+        current_time = int(time.time())\n+        iat = current_time\n+        exp = iat + 3600  # 1 hour after iat\n+\n+        # Mock the signJwt API to return the expected payload\n+        mock_sign_jwt = (\n+            mock_google_build.return_value.projects.return_value.serviceAccounts.return_value.signJwt\n+        )\n+        mock_sign_jwt.return_value.execute.return_value = {\"signedJwt\": \"mocked_jwt\"}\n+\n         vault_client = _VaultClient(\n             auth_type=\"gcp\",\n             gcp_key_path=\"path.json\",\n             gcp_scopes=\"scope1,scope2\",\n+            role_id=\"role\",\n             url=\"http://localhost:8180\",\n             session=None,\n         )\n-        client = vault_client.client\n-        mock_hvac.Client.assert_called_with(url=\"http://localhost:8180\", session=None)\n+\n+        # Preserve the original json.dumps\n+        original_json_dumps = json.dumps\n+\n+        # Inject the mocked payload into the JWT signing process\n+        with mock.patch(\"json.dumps\") as mock_json_dumps:\n+\n+            def mocked_json_dumps(payload):\n+                # Override the payload to inject controlled iat and exp values\n+                payload[\"iat\"] = iat\n+                payload[\"exp\"] = exp\n+                return original_json_dumps(payload)  # Use the original json.dumps\n+\n+            mock_json_dumps.side_effect = mocked_json_dumps\n+\n+            client = vault_client.client  # Trigger the Vault client creation\n+\n+        # Validate that the HVAC client and other mocks are called correctly\n+        mock_hvac_client.assert_called_with(url=\"http://localhost:8180\", session=None)\n         mock_get_scopes.assert_called_with(\"scope1,scope2\")\n         mock_get_credentials.assert_called_with(\n             key_path=\"path.json\", keyfile_dict=None, scopes=[\"scope1\", \"scope2\"]\n         )\n-        mock_hvac.Client.assert_called_with(url=\"http://localhost:8180\", session=None)\n-        client.auth.gcp.configure.assert_called_with(\n-            credentials=\"credentials\",\n-        )\n+\n+        # Extract the arguments passed to the mocked signJwt API\n+        args, kwargs = mock_sign_jwt.call_args\n+        payload = json.loads(kwargs[\"body\"][\"payload\"])\n+\n+        # Assert iat and exp values are as expected\n+        assert payload[\"iat\"] == iat\n+        assert payload[\"exp\"] == exp\n+        assert abs(payload[\"exp\"] - (payload[\"iat\"] + 3600)) < 10  # Validate exp is 3600 seconds after iat\n+\n+        client.auth.gcp.login.assert_called_with(role=\"role\", jwt=\"mocked_jwt\")\n         client.is_authenticated.assert_called_with()\n         assert vault_client.kv_engine_version == 2\n \n+    @mock.patch(\"builtins.open\", create=True)\n     @mock.patch(\"airflow.providers.google.cloud.utils.credentials_provider._get_scopes\")\n     @mock.patch(\"airflow.providers.google.cloud.utils.credentials_provider.get_credentials_and_project_id\")\n-    @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")\n-    def test_gcp_different_auth_mount_point(self, mock_hvac, mock_get_credentials, mock_get_scopes):\n-        mock_client = mock.MagicMock()\n-        mock_hvac.Client.return_value = mock_client\n+    @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac.Client\")\n+    @mock.patch(\"googleapiclient.discovery.build\")\n+    def test_gcp_different_auth_mount_point(\n+        self, mock_google_build, mock_hvac_client, mock_get_credentials, mock_get_scopes, mock_open\n+    ):\n+        # Mock the content of the file 'path.json'\n+        mock_file = mock.MagicMock()\n+        mock_file.read.return_value = '{\"client_email\": \"service_account_email\"}'\n+        mock_open.return_value.__enter__.return_value = mock_file\n+\n+        mock_client = mock.MagicMock()\n+        mock_hvac_client.return_value = mock_client\n         mock_get_scopes.return_value = [\"scope1\", \"scope2\"]\n         mock_get_credentials.return_value = (\"credentials\", \"project_id\")\n+\n+        mock_sign_jwt = (\n+            mock_google_build.return_value.projects.return_value.serviceAccounts.return_value.signJwt\n+        )\n+        mock_sign_jwt.return_value.execute.return_value = {\"signedJwt\": \"mocked_jwt\"}\n+\n+        # Generate realistic iat and exp values\n+        current_time = int(time.time())\n+        iat = current_time\n+        exp = current_time + 3600  # 1 hour later\n+\n         vault_client = _VaultClient(\n             auth_type=\"gcp\",\n             gcp_key_path=\"path.json\",\n             gcp_scopes=\"scope1,scope2\",\n+            role_id=\"role\",\n             url=\"http://localhost:8180\",\n             auth_mount_point=\"other\",\n             session=None,\n         )\n-        client = vault_client.client\n-        mock_hvac.Client.assert_called_with(url=\"http://localhost:8180\", session=None)\n+\n+        # Preserve the original json.dumps\n+        original_json_dumps = json.dumps\n+\n+        # Inject the mocked payload into the JWT signing process\n+        with mock.patch(\"json.dumps\") as mock_json_dumps:\n+\n+            def mocked_json_dumps(payload):\n+                # Override the payload to inject controlled iat and exp values\n+                payload[\"iat\"] = iat\n+                payload[\"exp\"] = exp\n+                return original_json_dumps(payload)  # Use the original json.dumps\n+\n+            mock_json_dumps.side_effect = mocked_json_dumps\n+\n+            client = vault_client.client  # Trigger the Vault client creation\n+\n+        # Assertions\n+        mock_hvac_client.assert_called_with(url=\"http://localhost:8180\", session=None)\n         mock_get_scopes.assert_called_with(\"scope1,scope2\")\n         mock_get_credentials.assert_called_with(\n             key_path=\"path.json\", keyfile_dict=None, scopes=[\"scope1\", \"scope2\"]\n         )\n-        mock_hvac.Client.assert_called_with(url=\"http://localhost:8180\", session=None)\n-        client.auth.gcp.configure.assert_called_with(credentials=\"credentials\", mount_point=\"other\")\n+        # Extract the arguments passed to the mocked signJwt API\n+        args, kwargs = mock_sign_jwt.call_args\n+        payload = json.loads(kwargs[\"body\"][\"payload\"])\n+\n+        # Assert iat and exp values are as expected\n+        assert payload[\"iat\"] == iat\n+        assert payload[\"exp\"] == exp\n+        assert abs(payload[\"exp\"] - (payload[\"iat\"] + 3600)) < 10  # Validate exp is 3600 seconds after iat\n+\n+        client.auth.gcp.login.assert_called_with(role=\"role\", jwt=\"mocked_jwt\", mount_point=\"other\")\n         client.is_authenticated.assert_called_with()\n         assert vault_client.kv_engine_version == 2\n \n+    @mock.patch(\n+        \"builtins.open\", new_callable=mock_open, read_data='{\"client_email\": \"service_account_email\"}'\n+    )\n     @mock.patch(\"airflow.providers.google.cloud.utils.credentials_provider._get_scopes\")\n     @mock.patch(\"airflow.providers.google.cloud.utils.credentials_provider.get_credentials_and_project_id\")\n-    @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")\n-    def test_gcp_dict(self, mock_hvac, mock_get_credentials, mock_get_scopes):\n+    @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac.Client\")\n+    @mock.patch(\"googleapiclient.discovery.build\")\n+    def test_gcp_dict(\n+        self, mock_google_build, mock_hvac_client, mock_get_credentials, mock_get_scopes, mock_file\n+    ):\n         mock_client = mock.MagicMock()\n-        mock_hvac.Client.return_value = mock_client\n+        mock_hvac_client.return_value = mock_client\n         mock_get_scopes.return_value = [\"scope1\", \"scope2\"]\n         mock_get_credentials.return_value = (\"credentials\", \"project_id\")\n+\n+        mock_sign_jwt = (\n+            mock_google_build.return_value.projects.return_value.serviceAccounts.return_value.signJwt\n+        )\n+        mock_sign_jwt.return_value.execute.return_value = {\"signedJwt\": \"mocked_jwt\"}\n+\n+        # Generate realistic iat and exp values\n+        current_time = int(time.time())\n+        iat = current_time\n+        exp = current_time + 3600  # 1 hour later\n+\n         vault_client = _VaultClient(\n             auth_type=\"gcp\",\n-            gcp_keyfile_dict={\"key\": \"value\"},\n+            gcp_keyfile_dict={\"client_email\": \"service_account_email\"},\n             gcp_scopes=\"scope1,scope2\",\n+            role_id=\"role\",\n             url=\"http://localhost:8180\",\n             session=None,\n         )\n-        client = vault_client.client\n-        mock_hvac.Client.assert_called_with(url=\"http://localhost:8180\", session=None)\n+\n+        # Preserve the original json.dumps\n+        original_json_dumps = json.dumps\n+\n+        # Inject the mocked payload into the JWT signing process\n+        with mock.patch(\"json.dumps\") as mock_json_dumps:\n+\n+            def mocked_json_dumps(payload):\n+                # Override the payload to inject controlled iat and exp values\n+                payload[\"iat\"] = iat\n+                payload[\"exp\"] = exp\n+                return original_json_dumps(payload)  # Use the original json.dumps\n+\n+            mock_json_dumps.side_effect = mocked_json_dumps\n+\n+            client = vault_client.client  # Trigger the Vault client creation\n+\n+        # Assertions\n+        mock_hvac_client.assert_called_with(url=\"http://localhost:8180\", session=None)\n         mock_get_scopes.assert_called_with(\"scope1,scope2\")\n         mock_get_credentials.assert_called_with(\n-            key_path=None, keyfile_dict={\"key\": \"value\"}, scopes=[\"scope1\", \"scope2\"]\n-        )\n-        mock_hvac.Client.assert_called_with(url=\"http://localhost:8180\", session=None)\n-        client.auth.gcp.configure.assert_called_with(\n-            credentials=\"credentials\",\n+            key_path=None, keyfile_dict={\"client_email\": \"service_account_email\"}, scopes=[\"scope1\", \"scope2\"]\n         )\n+        # Extract the arguments passed to the mocked signJwt API\n+        args, kwargs = mock_sign_jwt.call_args\n+        payload = json.loads(kwargs[\"body\"][\"payload\"])\n+\n+        # Assert iat and exp values are as expected\n+        assert payload[\"iat\"] == iat\n+        assert payload[\"exp\"] == exp\n+        assert abs(payload[\"exp\"] - (payload[\"iat\"] + 3600)) < 10  # Validate exp is 3600 seconds after iat\n+\n+        client.auth.gcp.login.assert_called_with(role=\"role\", jwt=\"mocked_jwt\")\n         client.is_authenticated.assert_called_with()\n         assert vault_client.kv_engine_version == 2\n \ndiff --git a/providers/hashicorp/tests/unit/hashicorp/hooks/test_vault.py b/providers/hashicorp/tests/unit/hashicorp/hooks/test_vault.py\nindex fd0862329e7ef..fa573eb6624a1 100644\n--- a/providers/hashicorp/tests/unit/hashicorp/hooks/test_vault.py\n+++ b/providers/hashicorp/tests/unit/hashicorp/hooks/test_vault.py\n@@ -18,7 +18,7 @@\n \n import re\n from unittest import mock\n-from unittest.mock import PropertyMock, mock_open, patch\n+from unittest.mock import MagicMock, PropertyMock, mock_open, patch\n \n import pytest\n from hvac.exceptions import VaultError\n@@ -431,7 +431,10 @@ def test_azure_dejson(self, mock_hvac, mock_get_connection):\n     @mock.patch(\"airflow.providers.google.cloud.utils.credentials_provider.get_credentials_and_project_id\")\n     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")\n     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")\n-    def test_gcp_init_params(self, mock_hvac, mock_get_connection, mock_get_credentials, mock_get_scopes):\n+    @mock.patch(\"googleapiclient.discovery.build\")\n+    def test_gcp_init_params(\n+        self, mock_build, mock_hvac, mock_get_connection, mock_get_credentials, mock_get_scopes\n+    ):\n         mock_client = mock.MagicMock()\n         mock_hvac.Client.return_value = mock_client\n         mock_connection = self.get_mock_connection()\n@@ -439,6 +442,17 @@ def test_gcp_init_params(self, mock_hvac, mock_get_connection, mock_get_credenti\n         mock_get_scopes.return_value = [\"scope1\", \"scope2\"]\n         mock_get_credentials.return_value = (\"credentials\", \"project_id\")\n \n+        # Mock googleapiclient.discovery.build chain\n+        mock_service = MagicMock()\n+        mock_projects = MagicMock()\n+        mock_service_accounts = MagicMock()\n+        mock_sign_jwt = MagicMock()\n+        mock_sign_jwt.execute.return_value = {\"signedJwt\": \"mocked_jwt\"}\n+        mock_service_accounts.signJwt.return_value = mock_sign_jwt\n+        mock_projects.serviceAccounts.return_value = mock_service_accounts\n+        mock_service.projects.return_value = mock_projects\n+        mock_build.return_value = mock_service\n+\n         connection_dict = {}\n \n         mock_connection.extra_dejson.get.side_effect = connection_dict.get\n@@ -447,20 +461,24 @@ def test_gcp_init_params(self, mock_hvac, mock_get_connection, mock_get_credenti\n             \"auth_type\": \"gcp\",\n             \"gcp_key_path\": \"path.json\",\n             \"gcp_scopes\": \"scope1,scope2\",\n+            \"role_id\": \"role\",\n             \"session\": None,\n         }\n \n-        test_hook = VaultHook(**kwargs)\n-        test_client = test_hook.get_conn()\n+        with patch(\n+            \"builtins.open\", mock_open(read_data='{\"client_email\": \"service_account_email\"}')\n+        ) as mock_file:\n+            test_hook = VaultHook(**kwargs)\n+            test_client = test_hook.get_conn()\n+            mock_file.assert_called_with(\"path.json\")\n+\n         mock_get_connection.assert_called_with(\"vault_conn_id\")\n         mock_get_scopes.assert_called_with(\"scope1,scope2\")\n         mock_get_credentials.assert_called_with(\n             key_path=\"path.json\", keyfile_dict=None, scopes=[\"scope1\", \"scope2\"]\n         )\n         mock_hvac.Client.assert_called_with(url=\"http://localhost:8180\", session=None)\n-        test_client.auth.gcp.configure.assert_called_with(\n-            credentials=\"credentials\",\n-        )\n+        test_client.auth.gcp.login.assert_called_with(role=\"role\", jwt=\"mocked_jwt\")\n         test_client.is_authenticated.assert_called_with()\n         assert test_hook.vault_client.kv_engine_version == 2\n \n@@ -468,7 +486,10 @@ def test_gcp_init_params(self, mock_hvac, mock_get_connection, mock_get_credenti\n     @mock.patch(\"airflow.providers.google.cloud.utils.credentials_provider.get_credentials_and_project_id\")\n     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")\n     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")\n-    def test_gcp_dejson(self, mock_hvac, mock_get_connection, mock_get_credentials, mock_get_scopes):\n+    @mock.patch(\"googleapiclient.discovery.build\")\n+    def test_gcp_dejson(\n+        self, mock_build, mock_hvac, mock_get_connection, mock_get_credentials, mock_get_scopes\n+    ):\n         mock_client = mock.MagicMock()\n         mock_hvac.Client.return_value = mock_client\n         mock_connection = self.get_mock_connection()\n@@ -476,29 +497,45 @@ def test_gcp_dejson(self, mock_hvac, mock_get_connection, mock_get_credentials,\n         mock_get_scopes.return_value = [\"scope1\", \"scope2\"]\n         mock_get_credentials.return_value = (\"credentials\", \"project_id\")\n \n+        # Mock googleapiclient.discovery.build chain\n+        mock_service = MagicMock()\n+        mock_projects = MagicMock()\n+        mock_service_accounts = MagicMock()\n+        mock_sign_jwt = MagicMock()\n+        mock_sign_jwt.execute.return_value = {\"signedJwt\": \"mocked_jwt\"}\n+        mock_service_accounts.signJwt.return_value = mock_sign_jwt\n+        mock_projects.serviceAccounts.return_value = mock_service_accounts\n+        mock_service.projects.return_value = mock_projects\n+        mock_build.return_value = mock_service\n+\n         connection_dict = {\n             \"auth_type\": \"gcp\",\n             \"gcp_key_path\": \"path.json\",\n             \"gcp_scopes\": \"scope1,scope2\",\n+            \"role_id\": \"role\",\n         }\n \n         mock_connection.extra_dejson.get.side_effect = connection_dict.get\n         kwargs = {\n             \"vault_conn_id\": \"vault_conn_id\",\n             \"session\": None,\n+            \"role_id\": \"role\",\n         }\n \n-        test_hook = VaultHook(**kwargs)\n-        test_client = test_hook.get_conn()\n+        with patch(\n+            \"builtins.open\", mock_open(read_data='{\"client_email\": \"service_account_email\"}')\n+        ) as mock_file:\n+            test_hook = VaultHook(**kwargs)\n+            test_client = test_hook.get_conn()\n+            mock_file.assert_called_with(\"path.json\")\n+\n         mock_get_connection.assert_called_with(\"vault_conn_id\")\n         mock_get_scopes.assert_called_with(\"scope1,scope2\")\n         mock_get_credentials.assert_called_with(\n             key_path=\"path.json\", keyfile_dict=None, scopes=[\"scope1\", \"scope2\"]\n         )\n         mock_hvac.Client.assert_called_with(url=\"http://localhost:8180\", session=None)\n-        test_client.auth.gcp.configure.assert_called_with(\n-            credentials=\"credentials\",\n-        )\n+        test_client.auth.gcp.login.assert_called_with(role=\"role\", jwt=\"mocked_jwt\")\n         test_client.is_authenticated.assert_called_with()\n         assert test_hook.vault_client.kv_engine_version == 2\n \n@@ -506,7 +543,10 @@ def test_gcp_dejson(self, mock_hvac, mock_get_connection, mock_get_credentials,\n     @mock.patch(\"airflow.providers.google.cloud.utils.credentials_provider.get_credentials_and_project_id\")\n     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")\n     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")\n-    def test_gcp_dict_dejson(self, mock_hvac, mock_get_connection, mock_get_credentials, mock_get_scopes):\n+    @mock.patch(\"googleapiclient.discovery.build\")\n+    def test_gcp_dict_dejson(\n+        self, mock_build, mock_hvac, mock_get_connection, mock_get_credentials, mock_get_scopes\n+    ):\n         mock_client = mock.MagicMock()\n         mock_hvac.Client.return_value = mock_client\n         mock_connection = self.get_mock_connection()\n@@ -514,16 +554,29 @@ def test_gcp_dict_dejson(self, mock_hvac, mock_get_connection, mock_get_credenti\n         mock_get_scopes.return_value = [\"scope1\", \"scope2\"]\n         mock_get_credentials.return_value = (\"credentials\", \"project_id\")\n \n+        # Mock googleapiclient.discovery.build chain\n+        mock_service = MagicMock()\n+        mock_projects = MagicMock()\n+        mock_service_accounts = MagicMock()\n+        mock_sign_jwt = MagicMock()\n+        mock_sign_jwt.execute.return_value = {\"signedJwt\": \"mocked_jwt\"}\n+        mock_service_accounts.signJwt.return_value = mock_sign_jwt\n+        mock_projects.serviceAccounts.return_value = mock_service_accounts\n+        mock_service.projects.return_value = mock_projects\n+        mock_build.return_value = mock_service\n+\n         connection_dict = {\n             \"auth_type\": \"gcp\",\n-            \"gcp_keyfile_dict\": '{\"key\": \"value\"}',\n+            \"gcp_keyfile_dict\": '{\"client_email\": \"service_account_email\"}',\n             \"gcp_scopes\": \"scope1,scope2\",\n+            \"role_id\": \"role\",\n         }\n \n         mock_connection.extra_dejson.get.side_effect = connection_dict.get\n         kwargs = {\n             \"vault_conn_id\": \"vault_conn_id\",\n             \"session\": None,\n+            \"role_id\": \"role\",\n         }\n \n         test_hook = VaultHook(**kwargs)\n@@ -531,12 +584,10 @@ def test_gcp_dict_dejson(self, mock_hvac, mock_get_connection, mock_get_credenti\n         mock_get_connection.assert_called_with(\"vault_conn_id\")\n         mock_get_scopes.assert_called_with(\"scope1,scope2\")\n         mock_get_credentials.assert_called_with(\n-            key_path=None, keyfile_dict={\"key\": \"value\"}, scopes=[\"scope1\", \"scope2\"]\n+            key_path=None, keyfile_dict={\"client_email\": \"service_account_email\"}, scopes=[\"scope1\", \"scope2\"]\n         )\n         mock_hvac.Client.assert_called_with(url=\"http://localhost:8180\", session=None)\n-        test_client.auth.gcp.configure.assert_called_with(\n-            credentials=\"credentials\",\n-        )\n+        test_client.auth.gcp.login.assert_called_with(role=\"role\", jwt=\"mocked_jwt\")\n         test_client.is_authenticated.assert_called_with()\n         assert test_hook.vault_client.kv_engine_version == 2\n \n", "problem_statement": "Incorrect implementation of GCP auth type in VaultBackend?\n# Short\r\n\r\nI'm pretty sure the authentication flow for GCP in the `airflow.providers.hashicorp.secrets.vault.VaultBackend` is incorrect and not usable. The Vault API used [`/auth/gcp/config`](https://www.vaultproject.io/api/auth/gcp#configure) has a different purpose, [`/auth/gcp/login`](https://www.vaultproject.io/api/auth/gcp#login) should be used instead.\r\n\r\nI've seen the `/auth/<type>/config` API being used in a couple other auth types, e.g. RADIUS and Azure, so the problem is probably broader than just GCP.\r\n\r\nIt's a bold statement as it means nobody has really used `VaultBackend` with these auth types. I may be wrong, but I would be surprised. \r\n\r\n# Details\r\n\r\nI've analysed Airflow's `VaultBackend` code while researching how to integrate Vault with GCP for a different project. Here's the discrepancy I found between the current implementation and the Vault's design.\r\n\r\n## Current implementation\r\n\r\nGCP auth is implemented in [vault_client.py#L277](https://github.com/apache/airflow/blob/main/airflow/providers/hashicorp/_internal_client/vault_client.py#L277). The implementation delegates to [official Vault client's method](https://github.com/hvac/hvac/blob/develop/hvac/api/auth_methods/gcp.py#L22) that calls `/auth/gcp/config` while providing the Airflow's GCP credentials to the call.\r\n\r\nThis is however an **administrative API** to configure Vault's capability to verify JWT tokens via GCP API. The credentials expected are credentials for Vault to use when communicating with GCP IAM API, **not application's credentials**. This is explained in [/auth/gcp/config API docs](https://www.vaultproject.io/api/auth/gcp#configure). The API doesn't respond with any data, so from the point of view of `VaultBackend` this call is basically just a no-op.\r\n\r\n[/auth/gcp/config](https://www.vaultproject.io/api/auth/gcp#configure) call requires `X-Vault-Token` header with a token that has proper permissions. I'm guessing either this auth method was not tested at all or some administrative (root?) Vault token was passed (by mistake?) during manual tests? I haven't seen any integration tests for the auth methods, I understand they would be difficult/costly.\r\n\r\n## Expected implementation\r\n\r\nThe proper API to use is [/auth/gcp/login](https://www.vaultproject.io/api/auth/gcp#login). It requires a signed JWT token issued by [IAM's signJwt](https://cloud.google.com/iam/docs/reference/credentials/rest/v1/projects.serviceAccounts/signJwt) based on application's GCP credentials. It returns a Vault token to be passed in subsequent requests.\r\n\r\nThis is described in the [Vault docs on GCP auth](https://www.vaultproject.io/docs/auth/gcp#iam-login) and on Google blog along with an implementation example [Authenticating to HashiCorp Vault using Google Cloud IAM](https://opensource.googleblog.com/2017/08/hashicorp-vault-and-google-cloud-iam.html).\r\n\r\n---\r\n\r\nAm I missing something? What do you think about this?\n", "hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template!\n\nCC: @potiuk \nUnfortunately I ran into this very issue trying to configure Vault as secrets backend using GCP Auth. The \"credentials\" returned by the `/auth/gcp/config` call cannot be serialized into JSON , triggering the following:\r\n\r\n```\r\n[2021-11-24 07:45:00,858] {taskinstance.py:1463} ERROR - Task failed with exception\r\nTraceback (most recent call last):\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py\", line 1165, in _run_raw_task\r\n    self._prepare_and_execute_task_with_callbacks(context, task)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py\", line 1283, in _prepare_and_execute_task_with_callbacks\r\n    result = self._execute_task(context, task_copy)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py\", line 1313, in _execute_task\r\n    result = task_copy.execute(context=context)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py\", line 150, in execute\r\n    return_value = self.execute_callable()\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py\", line 161, in execute_callable\r\n    return self.python_callable(*self.op_args, **self.op_kwargs)\r\n  File \"/opt/airflow/dags/fa43208e955dbded1f92788fdabc5945cc1cabfb/vault_test.py\", line 8, in get_secrets\r\n    conn = BaseHook.get_connection(kwargs['my_conn_id'])\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/base.py\", line 68, in get_connection\r\n    conn = Connection.get_connection_from_secrets(conn_id)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/models/connection.py\", line 376, in get_connection_from_secrets\r\n    conn = secrets_backend.get_connection(conn_id=conn_id)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/secrets/vault.py\", line 226, in get_connection\r\n    response = self.get_response(conn_id)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/secrets/vault.py\", line 193, in get_response\r\n    return self.vault_client.get_secret(secret_path=secret_path)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/_internal_client/vault_client.py\", line 375, in get_secret\r\n    response = self.client.secrets.kv.v2.read_secret_version(\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/_internal_client/vault_client.py\", line 205, in client\r\n    if not self._client.is_authenticated():\r\n  File \"/usr/local/lib/python3.8/functools.py\", line 967, in __get__\r\n    val = self.func(instance)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/_internal_client/vault_client.py\", line 228, in _client\r\n    self._auth_gcp(_client)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/_internal_client/vault_client.py\", line 307, in _auth_gcp\r\n    _client.auth.gcp.configure(credentials=credentials)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/hvac/api/auth_methods/gcp.py\", line 59, in configure\r\n    return self._adapter.post(\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/hvac/adapters.py\", line 121, in post\r\n    return self.request(\"post\", url, **kwargs)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/hvac/adapters.py\", line 356, in request\r\n    response = super(JSONAdapter, self).request(*args, **kwargs)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/hvac/adapters.py\", line 305, in request\r\n    response = self.session.request(\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/requests/sessions.py\", line 528, in request\r\n    prep = self.prepare_request(req)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/requests/sessions.py\", line 456, in prepare_request\r\n    p.prepare(\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/requests/models.py\", line 319, in prepare\r\n    self.prepare_body(data, files, json)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/requests/models.py\", line 471, in prepare_body\r\n    body = complexjson.dumps(json, allow_nan=False)\r\n  File \"/usr/local/lib/python3.8/json/__init__.py\", line 234, in dumps\r\n    return cls(\r\n  File \"/usr/local/lib/python3.8/json/encoder.py\", line 199, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/local/lib/python3.8/json/encoder.py\", line 257, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/usr/local/lib/python3.8/json/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type Credentials is not JSON serializable\r\n\r\n```\nThis issue has been automatically marked as stale because it has been open for 365 days without any activity. There has been several Airflow releases since last activity on this issue. Kindly asking to recheck the report against latest Airflow version and let us know if the issue is reproducible. The issue will be closed in next 30 days if no further activity occurs from the issue author.\nThis issue has been closed because it has not received response from the issue author.\nPlease reopen.\nWhy? Can you confirm it is still open in the latest version of Google Provider @fpopic ? Can you then open a new issue with new stack trace showing the problem ? Otherwise (if there is no new data on latest versions ) there is no way anyone is able to take any action on it since it's open for 2 years. \r\n\r\nBut if you still have the issue, really the best way from your side to get attention is to provide more fresh information  - ideally in a new issue.\nI will try with the latest provider and write my outcome here. ~ 6 months ago I was getting the same error as @pbetkier.\nPlease assign https://github.com/messam88\nNot sure what you are asking for @fpopic -> but a) this issue is closed, b) we cannot assign anyone who is not a contributor or has not commented themselves on the issue.\n\n", "all_hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template!\n\nCC: @potiuk \nUnfortunately I ran into this very issue trying to configure Vault as secrets backend using GCP Auth. The \"credentials\" returned by the `/auth/gcp/config` call cannot be serialized into JSON , triggering the following:\r\n\r\n```\r\n[2021-11-24 07:45:00,858] {taskinstance.py:1463} ERROR - Task failed with exception\r\nTraceback (most recent call last):\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py\", line 1165, in _run_raw_task\r\n    self._prepare_and_execute_task_with_callbacks(context, task)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py\", line 1283, in _prepare_and_execute_task_with_callbacks\r\n    result = self._execute_task(context, task_copy)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py\", line 1313, in _execute_task\r\n    result = task_copy.execute(context=context)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py\", line 150, in execute\r\n    return_value = self.execute_callable()\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py\", line 161, in execute_callable\r\n    return self.python_callable(*self.op_args, **self.op_kwargs)\r\n  File \"/opt/airflow/dags/fa43208e955dbded1f92788fdabc5945cc1cabfb/vault_test.py\", line 8, in get_secrets\r\n    conn = BaseHook.get_connection(kwargs['my_conn_id'])\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/base.py\", line 68, in get_connection\r\n    conn = Connection.get_connection_from_secrets(conn_id)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/models/connection.py\", line 376, in get_connection_from_secrets\r\n    conn = secrets_backend.get_connection(conn_id=conn_id)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/secrets/vault.py\", line 226, in get_connection\r\n    response = self.get_response(conn_id)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/secrets/vault.py\", line 193, in get_response\r\n    return self.vault_client.get_secret(secret_path=secret_path)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/_internal_client/vault_client.py\", line 375, in get_secret\r\n    response = self.client.secrets.kv.v2.read_secret_version(\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/_internal_client/vault_client.py\", line 205, in client\r\n    if not self._client.is_authenticated():\r\n  File \"/usr/local/lib/python3.8/functools.py\", line 967, in __get__\r\n    val = self.func(instance)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/_internal_client/vault_client.py\", line 228, in _client\r\n    self._auth_gcp(_client)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/hashicorp/_internal_client/vault_client.py\", line 307, in _auth_gcp\r\n    _client.auth.gcp.configure(credentials=credentials)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/hvac/api/auth_methods/gcp.py\", line 59, in configure\r\n    return self._adapter.post(\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/hvac/adapters.py\", line 121, in post\r\n    return self.request(\"post\", url, **kwargs)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/hvac/adapters.py\", line 356, in request\r\n    response = super(JSONAdapter, self).request(*args, **kwargs)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/hvac/adapters.py\", line 305, in request\r\n    response = self.session.request(\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/requests/sessions.py\", line 528, in request\r\n    prep = self.prepare_request(req)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/requests/sessions.py\", line 456, in prepare_request\r\n    p.prepare(\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/requests/models.py\", line 319, in prepare\r\n    self.prepare_body(data, files, json)\r\n  File \"/home/airflow/.local/lib/python3.8/site-packages/requests/models.py\", line 471, in prepare_body\r\n    body = complexjson.dumps(json, allow_nan=False)\r\n  File \"/usr/local/lib/python3.8/json/__init__.py\", line 234, in dumps\r\n    return cls(\r\n  File \"/usr/local/lib/python3.8/json/encoder.py\", line 199, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/local/lib/python3.8/json/encoder.py\", line 257, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/usr/local/lib/python3.8/json/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type Credentials is not JSON serializable\r\n\r\n```\nThis issue has been automatically marked as stale because it has been open for 365 days without any activity. There has been several Airflow releases since last activity on this issue. Kindly asking to recheck the report against latest Airflow version and let us know if the issue is reproducible. The issue will be closed in next 30 days if no further activity occurs from the issue author.\nThis issue has been closed because it has not received response from the issue author.\nPlease reopen.\nWhy? Can you confirm it is still open in the latest version of Google Provider @fpopic ? Can you then open a new issue with new stack trace showing the problem ? Otherwise (if there is no new data on latest versions ) there is no way anyone is able to take any action on it since it's open for 2 years. \r\n\r\nBut if you still have the issue, really the best way from your side to get attention is to provide more fresh information  - ideally in a new issue.\nI will try with the latest provider and write my outcome here. ~ 6 months ago I was getting the same error as @pbetkier.\nPlease assign https://github.com/messam88\nNot sure what you are asking for @fpopic -> but a) this issue is closed, b) we cannot assign anyone who is not a contributor or has not commented themselves on the issue.\n@potiuk @mik-laj can you reopen the issue, please? I wrote update here https://github.com/apache/airflow/pull/35855#issuecomment-2544097512. Not sure how to proceed with Closed Issue and Closed PR by bot.\r\n\n@potiuk I can confirm that this issue still exists.\n\nInstead of login (`auth/{gcp}/login`), airflow calls configure (`auth/{gcp}/config`).\nReferences:\n- https://github.com/apache/airflow/blob/main/providers/hashicorp/src/airflow/providers/hashicorp/_internal_client/vault_client.py#L306\n- https://github.com/apache/airflow/blob/main/providers/hashicorp/src/airflow/providers/hashicorp/_internal_client/vault_client.py#L308\n\nIt should be changed to `_client.auth.gcp.login(...)`.\n\n@fpopic thanks for opening https://github.com/apache/airflow/pull/35855, this is a good start. \n\nMoreover, for the same reason, `azure` and `radius` authn won't work as well.\n@aaabramov -> I suggest to open a new issue about it. Ass I already commented above - tt makes very little sense to comment on an issue opened 3 years ago. Quite likely it's related- but different issue and many of the things that were valid 3 years ago are not valid any more. And there is no point in tagging me again - I might or might not be involved in any way in diagnosing and solving the issue. Possibly people from google will get interested when you open a new issue (and link to that one marking it  as imilar) and they migh help with it. Or other people might get interest and implement it. \n\nBy commenting on a closed issue you are limiting your visibility and a chance that someone will look at it. Highly recommend to open a new issue.\n@pbetkier can you create a clone issue and I will make a new PR?\n> [@pbetkier](https://github.com/pbetkier) can you create a clone issue and I will make a new PR?\n\n@fpopic -> if you have a fix, you can create PR regardless from having an issue. We do not need issues in Airflow. We need PRs solving them and we are perfectly fine to have PRs just solving issues describing issues the are solving. We do not need double-accounting for that.\n@potiuk Here it is https://github.com/apache/airflow/pull/35855 just I don't have authorisation to reopen it.\nI can't reopen it either. You need to recreate it by pushing a new branch and creating new PR.\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/48429a7bc9ec21dc1e79800906dcd5ac9b7f2ecc", "https://github.com/apache/airflow/commit/f3b91f65db4f5c29aae737b5ad3a54c1d7231859", "https://github.com/apache/airflow/commit/3cd82a0134d51a5b92025bce973b7dece8b6e1db", "https://github.com/apache/airflow/commit/b04067cbcc28756961c62b2459f7f9868d2aa930", "https://github.com/apache/airflow/commit/3566858a70046d3ceda5831a8e801548496bda79", "https://github.com/apache/airflow/commit/549d0025b91d8428a66ba1eac6e67a8041fbead4", "https://github.com/apache/airflow/commit/92811268c5aaf91acd8dbf18554cf0620e519140", "https://github.com/apache/airflow/commit/ba28a987e6cdddd26cb21c7772b7a02be4a8898b"], "created_at": "2025-06-21T09:24:37Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 51780, "instance_id": "apache__airflow-51780", "issue_numbers": [50423], "base_commit": "f6141fffc11c7d3e874688d51d98989c50d1269b", "patch": "diff --git a/airflow-core/src/airflow/settings.py b/airflow-core/src/airflow/settings.py\nindex c137c80b56fba..08d7ac7af5ba7 100644\n--- a/airflow-core/src/airflow/settings.py\n+++ b/airflow-core/src/airflow/settings.py\n@@ -616,7 +616,9 @@ def initialize():\n     # The webservers import this file from models.py with the default settings.\n \n     if not os.environ.get(\"PYTHON_OPERATORS_VIRTUAL_ENV_MODE\", None):\n-        configure_orm()\n+        is_worker = os.environ.get(\"_AIRFLOW__REEXECUTED_PROCESS\") == \"1\"\n+        if not is_worker:\n+            configure_orm()\n     configure_action_logging()\n \n     # mask the sensitive_config_values\ndiff --git a/task-sdk/src/airflow/sdk/execution_time/task_runner.py b/task-sdk/src/airflow/sdk/execution_time/task_runner.py\nindex 5ad5d4df45feb..e4292deca2a10 100644\n--- a/task-sdk/src/airflow/sdk/execution_time/task_runner.py\n+++ b/task-sdk/src/airflow/sdk/execution_time/task_runner.py\n@@ -35,8 +35,9 @@\n import attrs\n import lazy_object_proxy\n import structlog\n-from pydantic import AwareDatetime, ConfigDict, Field, JsonValue\n+from pydantic import AwareDatetime, ConfigDict, Field, JsonValue, TypeAdapter\n \n+from airflow.configuration import conf\n from airflow.dag_processing.bundles.base import BaseDagBundle, BundleVersionLock\n from airflow.dag_processing.bundles.manager import DagBundlesManager\n from airflow.exceptions import AirflowInactiveAssetInInletOrOutletException\n@@ -97,6 +98,7 @@\n )\n from airflow.sdk.execution_time.xcom import XCom\n from airflow.utils.net import get_hostname\n+from airflow.utils.platform import getuser\n from airflow.utils.timezone import coerce_datetime\n \n if TYPE_CHECKING:\n@@ -642,6 +644,7 @@ def parse(what: StartupDetails, log: Logger) -> RuntimeTaskInstance:\n #   accessible wherever needed during task execution without modifying every layer of the call stack.\n SUPERVISOR_COMMS: CommsDecoder[ToTask, ToSupervisor]\n \n+\n # State machine!\n # 1. Start up (receive details from supervisor)\n # 2. Execution (run task code, possibly send requests)\n@@ -651,13 +654,18 @@ def parse(what: StartupDetails, log: Logger) -> RuntimeTaskInstance:\n def startup() -> tuple[RuntimeTaskInstance, Context, Logger]:\n     # The parent sends us a StartupDetails message un-prompted. After this, every single message is only sent\n     # in response to us sending a request.\n-    msg = SUPERVISOR_COMMS._get_response()\n+    log = structlog.get_logger(logger_name=\"task\")\n+\n+    if os.environ.get(\"_AIRFLOW__REEXECUTED_PROCESS\") == \"1\" and os.environ.get(\"_AIRFLOW__STARTUP_MSG\"):\n+        # entrypoint of re-exec process\n+        msg = TypeAdapter(StartupDetails).validate_json(os.environ[\"_AIRFLOW__STARTUP_MSG\"])\n+        log.debug(\"Using serialized startup message from environment\", msg=msg)\n+    else:\n+        # normal entry point\n+        msg = SUPERVISOR_COMMS._get_response()  # type: ignore[assignment]\n \n     if not isinstance(msg, StartupDetails):\n         raise RuntimeError(f\"Unhandled startup message {type(msg)} {msg}\")\n-\n-    log = structlog.get_logger(logger_name=\"task\")\n-\n     # setproctitle causes issue on Mac OS: https://github.com/benoitc/gunicorn/issues/3021\n     os_type = sys.platform\n     if os_type == \"darwin\":\n@@ -677,6 +685,34 @@ def startup() -> tuple[RuntimeTaskInstance, Context, Logger]:\n         ti.log_url = get_log_url_from_ti(ti)\n     log.debug(\"DAG file parsed\", file=msg.dag_rel_path)\n \n+    run_as_user = getattr(ti.task, \"run_as_user\", None) or conf.get(\n+        \"core\", \"default_impersonation\", fallback=None\n+    )\n+\n+    if os.environ.get(\"_AIRFLOW__REEXECUTED_PROCESS\") != \"1\" and run_as_user and run_as_user != getuser():\n+        # enters here for re-exec process\n+        os.environ[\"_AIRFLOW__REEXECUTED_PROCESS\"] = \"1\"\n+        # store startup message in environment for re-exec process\n+        os.environ[\"_AIRFLOW__STARTUP_MSG\"] = msg.model_dump_json()\n+        os.set_inheritable(SUPERVISOR_COMMS.socket.fileno(), True)\n+\n+        # Import main directly from the module instead of re-executing the file.\n+        # This ensures that when other parts modules import\n+        # airflow.sdk.execution_time.task_runner, they get the same module instance\n+        # with the properly initialized SUPERVISOR_COMMS global variable.\n+        # If we re-executed the module with `python -m`, it would load as __main__ and future\n+        # imports would get a fresh copy without the initialized globals.\n+        rexec_python_code = \"from airflow.sdk.execution_time.task_runner import main; main()\"\n+        cmd = [\"sudo\", \"-E\", \"-H\", \"-u\", run_as_user, sys.executable, \"-c\", rexec_python_code]\n+        log.info(\n+            \"Running command\",\n+            command=cmd,\n+        )\n+        os.execvp(\"sudo\", cmd)\n+\n+        # ideally, we should never reach here, but if we do, we should return None, None, None\n+        return None, None, None\n+\n     return ti, ti.get_template_context(), log\n \n \n", "test_patch": "diff --git a/task-sdk/tests/task_sdk/execution_time/test_task_runner.py b/task-sdk/tests/task_sdk/execution_time/test_task_runner.py\nindex 5ce460c455bf4..9745297f8c7f0 100644\n--- a/task-sdk/tests/task_sdk/execution_time/test_task_runner.py\n+++ b/task-sdk/tests/task_sdk/execution_time/test_task_runner.py\n@@ -644,6 +644,91 @@ def execute(self, context):\n     mock_supervisor_comms.assert_has_calls(expected_calls)\n \n \n+@patch(\"os.execvp\")\n+@patch(\"os.set_inheritable\")\n+def test_task_run_with_user_impersonation(\n+    mock_set_inheritable, mock_execvp, mocked_parse, make_ti_context, time_machine, mock_supervisor_comms\n+):\n+    class CustomOperator(BaseOperator):\n+        def execute(self, context):\n+            print(\"Hi from CustomOperator!\")\n+\n+    task = CustomOperator(task_id=\"impersonation_task\", run_as_user=\"airflowuser\")\n+    instant = timezone.datetime(2024, 12, 3, 10, 0)\n+\n+    what = StartupDetails(\n+        ti=TaskInstance(\n+            id=uuid7(),\n+            task_id=\"impersonation_task\",\n+            dag_id=\"basic_dag\",\n+            run_id=\"c\",\n+            try_number=1,\n+        ),\n+        dag_rel_path=\"\",\n+        bundle_info=FAKE_BUNDLE,\n+        ti_context=make_ti_context(),\n+        start_date=timezone.utcnow(),\n+    )\n+\n+    mocked_parse(what, \"basic_dag\", task)\n+    time_machine.move_to(instant, tick=False)\n+\n+    mock_supervisor_comms._get_response.return_value = what\n+    mock_supervisor_comms.socket.fileno.return_value = 42\n+\n+    with mock.patch.dict(os.environ, {}, clear=True):\n+        startup()\n+\n+        assert os.environ[\"_AIRFLOW__REEXECUTED_PROCESS\"] == \"1\"\n+        assert \"_AIRFLOW__STARTUP_MSG\" in os.environ\n+\n+        mock_set_inheritable.assert_called_once_with(42, True)\n+        actual_cmd = mock_execvp.call_args.args[1]\n+\n+        assert actual_cmd[:5] == [\"sudo\", \"-E\", \"-H\", \"-u\", \"airflowuser\"]\n+        assert \"python -c\" in actual_cmd[5] + \" \" + actual_cmd[6]\n+        assert actual_cmd[7] == \"from airflow.sdk.execution_time.task_runner import main; main()\"\n+\n+\n+@patch(\"airflow.sdk.execution_time.task_runner.getuser\")\n+def test_task_run_with_user_impersonation_default_user(\n+    mock_get_user, mocked_parse, make_ti_context, time_machine, mock_supervisor_comms\n+):\n+    class CustomOperator(BaseOperator):\n+        def execute(self, context):\n+            print(\"Hi from CustomOperator!\")\n+\n+    task = CustomOperator(task_id=\"impersonation_task\", run_as_user=\"default_user\")\n+    instant = timezone.datetime(2024, 12, 3, 10, 0)\n+\n+    what = StartupDetails(\n+        ti=TaskInstance(\n+            id=uuid7(),\n+            task_id=\"impersonation_task\",\n+            dag_id=\"basic_dag\",\n+            run_id=\"c\",\n+            try_number=1,\n+        ),\n+        dag_rel_path=\"\",\n+        bundle_info=FAKE_BUNDLE,\n+        ti_context=make_ti_context(),\n+        start_date=timezone.utcnow(),\n+    )\n+\n+    mocked_parse(what, \"basic_dag\", task)\n+    time_machine.move_to(instant, tick=False)\n+\n+    mock_supervisor_comms._get_response.return_value = what\n+    mock_supervisor_comms.socket.fileno.return_value = 42\n+    mock_get_user.return_value = \"default_user\"\n+\n+    with mock.patch.dict(os.environ, {}, clear=True):\n+        startup()\n+\n+        assert \"_AIRFLOW__REEXECUTED_PROCESS\" not in os.environ\n+        assert \"_AIRFLOW__STARTUP_MSG\" not in os.environ\n+\n+\n @pytest.mark.parametrize(\n     [\"command\", \"rendered_command\"],\n     [\n", "problem_statement": "Handle `run_as_user` / impersonation in the Supervisor\n`run_as_user` currently isn\u2019t supported with Task SDK execution path. We should fix that\n", "hints_text": "@amoghrajesh Could you check on this?\nSure, i will check this one out\nThe effort seems to be more involved and complicated with the nature of how we run tasks in Airflow 3, so given multiple unknowns (approach, limitations, etc), moving it to 3.0.3.\n\n", "all_hints_text": "@amoghrajesh Could you check on this?\nSure, i will check this one out\nThe effort seems to be more involved and complicated with the nature of how we run tasks in Airflow 3, so given multiple unknowns (approach, limitations, etc), moving it to 3.0.3.\nI will catch up with Ash on this one and make progress on it. The current approach has certain limitations, so have to look a step back to figure out what can work for us.\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/48f33a05f43f09ebf7d3deba7e6697d940ab5745", "https://github.com/apache/airflow/commit/22a44cbce14b0ce5fdaab0466549c8c336f5dc9a", "https://github.com/apache/airflow/commit/1372214abfd52a59e8a468adf06160876495c424", "https://github.com/apache/airflow/commit/dcc491c235525e9dd05cb4f6dc3c4dabc12467e3", "https://github.com/apache/airflow/commit/d48608201037ea180404777e64307617b5825307", "https://github.com/apache/airflow/commit/595b5500e2ad9e198e6a965b3632d982658adc22", "https://github.com/apache/airflow/commit/4975725d5d6e34ccfbeec614c5cff5785137379b", "https://github.com/apache/airflow/commit/c014ebecd3035d710b263b206532f0f54111c36d", "https://github.com/apache/airflow/commit/8ad4db594ba7b953c2c5047d6f938a4f81f719e3", "https://github.com/apache/airflow/commit/05c78f1ae71520362491503ad8e16266d9666a69", "https://github.com/apache/airflow/commit/f45647afbcfcacd056f9bab0a314e1029bde4563", "https://github.com/apache/airflow/commit/a1c0dd27024a24261bacbda6b8cf27c9a2671326", "https://github.com/apache/airflow/commit/6b37dcd4afe9cb6e1bb7c3473fbfc98715712dae", "https://github.com/apache/airflow/commit/f3b72393c646929637f71758f21d823bb3604204", "https://github.com/apache/airflow/commit/5ba66b53d0a935b410db291b1ae54811ac49e08e", "https://github.com/apache/airflow/commit/a5c2a1ec9121085e33a17509c109b35c869c2b6c", "https://github.com/apache/airflow/commit/54ee8c5f0a2e81295ba5da1d6ac9090e7813d269", "https://github.com/apache/airflow/commit/e890bad8399624a08bf1f24b95e02dafb054567e", "https://github.com/apache/airflow/commit/b84df96ae55ac577a21cacd7dbdc7d44e26127c9", "https://github.com/apache/airflow/commit/5a7f6d6e66a62dd63683124eed0b0d4400ad6b4d", "https://github.com/apache/airflow/commit/5a8e13e88204ff709e81eddfd6f0efdb4c29bd59", "https://github.com/apache/airflow/commit/010d2b2744061942eb6a5192a42582710a6046d9", "https://github.com/apache/airflow/commit/6ee1cb02839fac30819fb12d045cb314c8534ad9", "https://github.com/apache/airflow/commit/706542f6b59fb79340bf7d495408fef19e16b06c"], "created_at": "2025-06-16T11:45:09Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 52024, "instance_id": "apache__airflow-52024", "issue_numbers": [50595], "base_commit": "23284618d0ab08ee53f7ec9172c8141b0faa75c3", "patch": "diff --git a/chart/templates/rbac/job-launcher-role.yaml b/chart/templates/rbac/job-launcher-role.yaml\nnew file mode 100644\nindex 0000000000000..5135f2d8169d0\n--- /dev/null\n+++ b/chart/templates/rbac/job-launcher-role.yaml\n@@ -0,0 +1,67 @@\n+{{/*\n+ Licensed to the Apache Software Foundation (ASF) under one\n+ or more contributor license agreements.  See the NOTICE file\n+ distributed with this work for additional information\n+ regarding copyright ownership.  The ASF licenses this file\n+ to you under the Apache License, Version 2.0 (the\n+ \"License\"); you may not use this file except in compliance\n+ with the License.  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+ Unless required by applicable law or agreed to in writing,\n+ software distributed under the License is distributed on an\n+ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ KIND, either express or implied.  See the License for the\n+ specific language governing permissions and limitations\n+ under the License.\n+*/}}\n+\n+################################\n+## Airflow Job Launcher Role\n+#################################\n+{{- if and .Values.rbac.create .Values.allowJobLaunching }}\n+apiVersion: rbac.authorization.k8s.io/v1\n+{{- if .Values.multiNamespaceMode }}\n+kind: ClusterRole\n+{{- else }}\n+kind: Role\n+{{- end }}\n+metadata:\n+  {{- if not .Values.multiNamespaceMode }}\n+  name: {{ include \"airflow.fullname\" . }}-job-launcher-role\n+  namespace: \"{{ .Release.Namespace }}\"\n+  {{- else }}\n+  name: {{ .Release.Namespace }}-{{ include \"airflow.fullname\" . }}-job-launcher-role\n+  {{- end }}\n+  labels:\n+    tier: airflow\n+    release: {{ .Release.Name }}\n+    chart: \"{{ .Chart.Name }}-{{ .Chart.Version }}\"\n+    heritage: {{ .Release.Service }}\n+    {{- if .Values.multiNamespaceMode }}\n+    namespace: \"{{ .Release.Namespace }}\"\n+    {{- end }}\n+    {{- with .Values.labels }}\n+      {{- toYaml . | nindent 4 }}\n+    {{- end }}\n+rules:\n+  - apiGroups:\n+      - \"batch\"\n+    resources:\n+      - \"jobs\"\n+    verbs:\n+      - \"create\"\n+      - \"list\"\n+      - \"get\"\n+      - \"patch\"\n+      - \"watch\"\n+      - \"delete\"\n+  - apiGroups:\n+      - \"batch\"\n+    resources:\n+      - \"jobs/status\"\n+    verbs:\n+      - \"get\"\n+      - watch\"\n+{{- end }}\ndiff --git a/chart/templates/rbac/job-launcher-rolebinding.yaml b/chart/templates/rbac/job-launcher-rolebinding.yaml\nnew file mode 100644\nindex 0000000000000..d0ceb2ff08044\n--- /dev/null\n+++ b/chart/templates/rbac/job-launcher-rolebinding.yaml\n@@ -0,0 +1,79 @@\n+{{/*\n+ Licensed to the Apache Software Foundation (ASF) under one\n+ or more contributor license agreements.  See the NOTICE file\n+ distributed with this work for additional information\n+ regarding copyright ownership.  The ASF licenses this file\n+ to you under the Apache License, Version 2.0 (the\n+ \"License\"); you may not use this file except in compliance\n+ with the License.  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+ Unless required by applicable law or agreed to in writing,\n+ software distributed under the License is distributed on an\n+ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ KIND, either express or implied.  See the License for the\n+ specific language governing permissions and limitations\n+ under the License.\n+*/}}\n+\n+################################\n+## Airflow Job Launcher Role Binding\n+#################################\n+{{- if and .Values.rbac.create .Values.allowJobLaunching }}\n+{{- $schedulerLaunchExecutors := list \"LocalExecutor\" \"LocalKubernetesExecutor\" \"KubernetesExecutor\" \"CeleryKubernetesExecutor\" }}\n+{{- $workerLaunchExecutors := list \"CeleryExecutor\" \"LocalKubernetesExecutor\" \"KubernetesExecutor\" \"CeleryKubernetesExecutor\" }}\n+{{- $executors := split \",\" .Values.executor }}\n+apiVersion: rbac.authorization.k8s.io/v1\n+{{- if .Values.multiNamespaceMode }}\n+kind: ClusterRoleBinding\n+{{- else }}\n+kind: RoleBinding\n+{{- end }}\n+metadata:\n+  {{- if not .Values.multiNamespaceMode }}\n+  namespace: \"{{ .Release.Namespace }}\"\n+  name: {{ include \"airflow.fullname\" . }}-job-launcher-rolebinding\n+  {{- else }}\n+  name: {{ .Release.Namespace }}-{{ include \"airflow.fullname\" . }}-job-launcher-rolebinding\n+  {{- end }}\n+  labels:\n+    tier: airflow\n+    release: {{ .Release.Name }}\n+    chart: \"{{ .Chart.Name }}-{{ .Chart.Version }}\"\n+    heritage: {{ .Release.Service }}\n+    {{- if .Values.multiNamespaceMode }}\n+    namespace: \"{{ .Release.Namespace }}\"\n+    {{- end }}\n+    {{- with .Values.labels }}\n+      {{- toYaml . | nindent 4 }}\n+    {{- end }}\n+roleRef:\n+  apiGroup: rbac.authorization.k8s.io\n+  {{- if .Values.multiNamespaceMode }}\n+  kind: ClusterRole\n+  name: {{ .Release.Namespace }}-{{ include \"airflow.fullname\" . }}-job-launcher-role\n+  {{- else }}\n+  kind: Role\n+  name: {{ include \"airflow.fullname\" . }}-job-launcher-role\n+  {{- end }}\n+subjects:\n+  {{- $schedulerAdded := false }}\n+  {{- range $executor := $executors }}\n+  {{- if and (has $executor $schedulerLaunchExecutors) (not $schedulerAdded) }}\n+  {{- $schedulerAdded = true }}\n+  - kind: ServiceAccount\n+    name: {{ include \"scheduler.serviceAccountName\" $ }}\n+    namespace: \"{{ $.Release.Namespace }}\"\n+  {{- end }}\n+  {{- end }}\n+  {{- $workerAdded := false }}\n+  {{- range $executor := $executors }}\n+  {{- if and (has $executor $workerLaunchExecutors) (not $workerAdded) }}\n+  {{- $workerAdded = true }}\n+  - kind: ServiceAccount\n+    name: {{ include \"worker.serviceAccountName\" $ }}\n+    namespace: \"{{ $.Release.Namespace }}\"\n+  {{- end }}\n+  {{- end }}\n+{{- end }}\ndiff --git a/chart/values.schema.json b/chart/values.schema.json\nindex 01654f9042f9f..eea46988546f6 100644\n--- a/chart/values.schema.json\n+++ b/chart/values.schema.json\n@@ -695,6 +695,12 @@\n             \"x-docsSection\": \"Airflow\",\n             \"default\": true\n         },\n+        \"allowJobLaunching\": {\n+            \"description\": \"Whether various Airflow components launch jobs.\",\n+            \"type\": \"boolean\",\n+            \"x-docsSection\": \"Airflow\",\n+            \"default\": false\n+        },\n         \"images\": {\n             \"description\": \"Images.\",\n             \"type\": \"object\",\ndiff --git a/chart/values.yaml b/chart/values.yaml\nindex dd57e4778226f..384b856cca060 100644\n--- a/chart/values.yaml\n+++ b/chart/values.yaml\n@@ -379,6 +379,7 @@ executor: \"CeleryExecutor\"\n # If this is true and using CeleryExecutor/KubernetesExecutor/CeleryKubernetesExecutor, the workers\n # will be able to launch pods.\n allowPodLaunching: true\n+allowJobLaunching: false\n \n # Environment variables for all airflow containers\n env: []\n", "test_patch": "diff --git a/helm-tests/tests/helm_tests/airflow_aux/test_job_launcher_role.py b/helm-tests/tests/helm_tests/airflow_aux/test_job_launcher_role.py\nnew file mode 100644\nindex 0000000000000..bf59f0497a094\n--- /dev/null\n+++ b/helm-tests/tests/helm_tests/airflow_aux/test_job_launcher_role.py\n@@ -0,0 +1,117 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+from __future__ import annotations\n+\n+import jmespath\n+import pytest\n+from chart_utils.helm_template_generator import render_chart\n+\n+\n+class TestJobLauncher:\n+    \"\"\"Tests job launcher RBAC.\"\"\"\n+\n+    @pytest.mark.parametrize(\n+        \"executor, rbac, allow, expected_accounts\",\n+        [\n+            (\"CeleryKubernetesExecutor\", True, True, [\"scheduler\", \"worker\"]),\n+            (\"KubernetesExecutor\", True, True, [\"scheduler\", \"worker\"]),\n+            (\"CeleryExecutor\", True, True, [\"worker\"]),\n+            (\"LocalExecutor\", True, True, [\"scheduler\"]),\n+            (\"LocalExecutor\", False, False, []),\n+            (\"CeleryExecutor,KubernetesExecutor\", True, True, [\"scheduler\", \"worker\"]),\n+        ],\n+    )\n+    def test_job_launcher_rolebinding(self, executor, rbac, allow, expected_accounts):\n+        docs = render_chart(\n+            values={\n+                \"rbac\": {\"create\": rbac},\n+                \"allowJobLaunching\": allow,\n+                \"executor\": executor,\n+            },\n+            show_only=[\"templates/rbac/job-launcher-rolebinding.yaml\"],\n+        )\n+        if expected_accounts:\n+            for idx, suffix in enumerate(expected_accounts):\n+                assert f\"release-name-airflow-{suffix}\" == jmespath.search(f\"subjects[{idx}].name\", docs[0])\n+        else:\n+            assert docs == []\n+\n+    @pytest.mark.parametrize(\n+        \"multiNamespaceMode, namespace, expectedRole, expectedRoleBinding\",\n+        [\n+            (\n+                True,\n+                \"namespace\",\n+                \"namespace-release-name-job-launcher-role\",\n+                \"namespace-release-name-job-launcher-rolebinding\",\n+            ),\n+            (\n+                True,\n+                \"other-ns\",\n+                \"other-ns-release-name-job-launcher-role\",\n+                \"other-ns-release-name-job-launcher-rolebinding\",\n+            ),\n+            (False, \"namespace\", \"release-name-job-launcher-role\", \"release-name-job-launcher-rolebinding\"),\n+        ],\n+    )\n+    def test_job_launcher_rolebinding_multi_namespace(\n+        self, multiNamespaceMode, namespace, expectedRole, expectedRoleBinding\n+    ):\n+        docs = render_chart(\n+            namespace=namespace,\n+            values={\"allowJobLaunching\": True, \"multiNamespaceMode\": multiNamespaceMode},\n+            show_only=[\"templates/rbac/job-launcher-rolebinding.yaml\"],\n+        )\n+\n+        actualRoleBinding = jmespath.search(\"metadata.name\", docs[0])\n+        assert actualRoleBinding == expectedRoleBinding\n+\n+        actualRoleRef = jmespath.search(\"roleRef.name\", docs[0])\n+        assert actualRoleRef == expectedRole\n+\n+        actualKind = jmespath.search(\"kind\", docs[0])\n+        actualRoleRefKind = jmespath.search(\"roleRef.kind\", docs[0])\n+        if multiNamespaceMode:\n+            assert actualKind == \"ClusterRoleBinding\"\n+            assert actualRoleRefKind == \"ClusterRole\"\n+        else:\n+            assert actualKind == \"RoleBinding\"\n+            assert actualRoleRefKind == \"Role\"\n+\n+    @pytest.mark.parametrize(\n+        \"multiNamespaceMode, namespace, expectedRole\",\n+        [\n+            (True, \"namespace\", \"namespace-release-name-job-launcher-role\"),\n+            (True, \"other-ns\", \"other-ns-release-name-job-launcher-role\"),\n+            (False, \"namespace\", \"release-name-job-launcher-role\"),\n+        ],\n+    )\n+    def test_job_launcher_role_multi_namespace(self, multiNamespaceMode, namespace, expectedRole):\n+        docs = render_chart(\n+            namespace=namespace,\n+            values={\"allowJobLaunching\": True, \"multiNamespaceMode\": multiNamespaceMode},\n+            show_only=[\"templates/rbac/job-launcher-role.yaml\"],\n+        )\n+\n+        actualRole = jmespath.search(\"metadata.name\", docs[0])\n+        assert actualRole == expectedRole\n+\n+        actualKind = jmespath.search(\"kind\", docs[0])\n+        if multiNamespaceMode:\n+            assert actualKind == \"ClusterRole\"\n+        else:\n+            assert actualKind == \"Role\"\n", "problem_statement": "KubernetesJobOperator is not creating jobs using official chart\n### Official Helm Chart version\n\n1.15.0\n\n### Apache Airflow version\n\n2.9.3\n\n### Kubernetes Version\n\n1.30.11\n\n### Helm Chart configuration\n\n_No response_\n\n### Docker Image customizations\n\nOfficial image\n\n### What happened\n\n_No response_\n\n### What you think should happen instead\n\n```\nkubernetes.client.exceptions.ApiException: (403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'Audit-Id': 'dfbb1c2d-dcb8-49d8-baf5-ac246888f021', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'X-Kubernetes-Pf-Flowschema-Uid': '039c3e74-af96-47d4-9d83-a3630033db89', 'X-Kubernetes-Pf-Prioritylevel-Uid': 'cbb01a5d-c107-49cf-84ec-2140416514a1', 'Date': 'Tue, 14 May 2025 09:54:14 GMT', 'Content-Length': '323'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"jobs.batch is forbidden: User \\\"system:serviceaccount:data-dev:airflow-scheduler\\\" cannot create resource \\\"jobs\\\" in API group \\\"batch\\\" in the namespace \\\"data-dev\\\"\",\"reason\":\"Forbidden\",\"details\":{\"group\":\"batch\",\"kind\":\"jobs\"},\"code\":403}\n```\n\n### How to reproduce\n\nUpdated kubernetes Pod Operator to Job Operator and it is failing for missing permission to create kube job.\n\nKubernetesPodOperator(\n        task_id=\"run_custom_pod\",\n        namespace=\"airflow\",\n        image=\"busybox:latest\",  # Main container image\n        name=\"custom-pod\",\n        on_finish_action=\"keep_pod\",\n        init_containers=[{\"name\": f\"init-container-{i}\", \"image\": \"\", \"entrypoint\": []} for i in range(2)]\n    )\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\n/assign\nI'll raise a PR for this.\nHi @rbm897, I noticed you mentioned raising a PR for this issue. Do you have any updates or are you still working on it? I\u2019ve implemented a solution and would like to contribute a PR if this is still open or would you be open to collaborating/reviewing my solution?. Thanks!\nHi @HsiuChuanHsu  I have raised a PR, please help to review https://github.com/apache/airflow/pull/52024. Thanks\n\n", "all_hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\n/assign\nI'll raise a PR for this.\nHi @rbm897, I noticed you mentioned raising a PR for this issue. Do you have any updates or are you still working on it? I\u2019ve implemented a solution and would like to contribute a PR if this is still open or would you be open to collaborating/reviewing my solution?. Thanks!\nHi @HsiuChuanHsu  I have raised a PR, please help to review https://github.com/apache/airflow/pull/52024. Thanks\nHi @eladkal Please help to review #52024 \n\n", "commit_urls": ["https://github.com/apache/airflow/commit/6496adcd373cbe28e8c576cb1f0f8f2ad3dbb6cc", "https://github.com/apache/airflow/commit/848b5100f79809a54c8220eef6b3aae947965388", "https://github.com/apache/airflow/commit/b46f73e0b554a860d90db21daf3d252c3f68cd30", "https://github.com/apache/airflow/commit/72c31d0ec0e9f27e6f3f035cd0c9599017206a20", "https://github.com/apache/airflow/commit/19467d8a93bdf4797387ff7f49a663363b86c2f9", "https://github.com/apache/airflow/commit/f0b94cd419dd061a86214eecca01544d6a33a619", "https://github.com/apache/airflow/commit/4b768d62b0bae0add557fda27f63c9552363b1ce"], "created_at": "2025-06-22T13:36:54Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 51256, "instance_id": "apache__airflow-51256", "issue_numbers": [50941], "base_commit": "9d6ae2d39e0c48ce0e46d778b8e1343748902d79", "patch": "diff --git a/providers/git/src/airflow/providers/git/hooks/git.py b/providers/git/src/airflow/providers/git/hooks/git.py\nindex 22cfde132a52b..86ee583dec13d 100644\n--- a/providers/git/src/airflow/providers/git/hooks/git.py\n+++ b/providers/git/src/airflow/providers/git/hooks/git.py\n@@ -48,7 +48,7 @@ def get_ui_field_behaviour(cls) -> dict[str, Any]:\n         return {\n             \"hidden_fields\": [\"schema\"],\n             \"relabeling\": {\n-                \"login\": \"Username\",\n+                \"login\": \"Username or Access Token name\",\n                 \"host\": \"Repository URL\",\n                 \"password\": \"Access Token (optional)\",\n             },\n@@ -68,6 +68,7 @@ def __init__(\n         super().__init__()\n         connection = self.get_connection(git_conn_id)\n         self.repo_url = repo_url or connection.host\n+        self.user_name = connection.login or \"user\"\n         self.auth_token = connection.password\n         self.private_key = connection.extra_dejson.get(\"private_key\")\n         self.key_file = connection.extra_dejson.get(\"key_file\")\n@@ -89,7 +90,7 @@ def _process_git_auth_url(self):\n         if not isinstance(self.repo_url, str):\n             return\n         if self.auth_token and self.repo_url.startswith(\"https://\"):\n-            self.repo_url = self.repo_url.replace(\"https://\", f\"https://{self.auth_token}@\")\n+            self.repo_url = self.repo_url.replace(\"https://\", f\"https://{self.user_name}:{self.auth_token}@\")\n         elif not self.repo_url.startswith(\"git@\") or not self.repo_url.startswith(\"https://\"):\n             self.repo_url = os.path.expanduser(self.repo_url)\n \n", "test_patch": "diff --git a/providers/git/tests/unit/git/hooks/test_git.py b/providers/git/tests/unit/git/hooks/test_git.py\nindex 11302e59331f2..bdc86830f8ad7 100644\n--- a/providers/git/tests/unit/git/hooks/test_git.py\n+++ b/providers/git/tests/unit/git/hooks/test_git.py\n@@ -110,11 +110,11 @@ def setup_class(cls) -> None:\n         \"conn_id, hook_kwargs, expected_repo_url\",\n         [\n             (CONN_DEFAULT, {}, AIRFLOW_GIT),\n-            (CONN_HTTPS, {}, f\"https://{ACCESS_TOKEN}@github.com/apache/airflow.git\"),\n+            (CONN_HTTPS, {}, f\"https://user:{ACCESS_TOKEN}@github.com/apache/airflow.git\"),\n             (\n                 CONN_HTTPS,\n                 {\"repo_url\": \"https://github.com/apache/zzzairflow\"},\n-                f\"https://{ACCESS_TOKEN}@github.com/apache/zzzairflow\",\n+                f\"https://user:{ACCESS_TOKEN}@github.com/apache/zzzairflow\",\n             ),\n             (CONN_ONLY_PATH, {}, \"path/to/repo\"),\n         ],\n", "problem_statement": "The git password/token for GitDagBundle when connecting to Gitlab currently needs to include the token name\n### Apache Airflow Provider(s)\n\ngit\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-git==0.0.2\n\n### Apache Airflow version\n\n3.0.1\n\n### Operating System\n\napache-airflow-providers-git==0.0.2\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nRunning on a local minikube instance\n\n### What happened\n\nI setup the GitDagBundle for the dag processor to connect to my Gitlab repository as described in https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/dag-bundles.html with the git connection being defined as \n\n```\n  dag_processor:\n    dag_bundle_config_list: '[{\"name\":\"dag-bundle\",\"classpath\":\"airflow.providers.git.bundles.git.GitDagBundle\",\"kwargs\":{\"git_conn_id\":\"git-dags-conn\",\"subdir\":\"\",\"tracking_ref\":\"main\"}}]'\n\n---\n\nairflow connections add 'git-dags-conn' \\\n    --conn-type git \\\n    --conn-login \"$GITLAB_TOKEN_NAME\" \\\n    --conn-password \"$GITLAB_TOKEN_VALUE\" \\\n    --conn-host 'https://gitlab.com/$GITLAB_USER/$GITLAB_REPO.git'\n```\n\nBut when I do this, the dag proc errors out with an 128 error and text message indicating that the authentication failed.\nAfter some trials, I turned out that I had to specify the connection as \n\n```\nairflow connections add 'git-dags-conn' \\\n    --conn-type git \\\n    --conn-login \"$GITLAB_TOKEN_NAME\" \\\n    --conn-password \"$GITLAB_TOKEN_NAME:$GITLAB_TOKEN_VALUE\" \\\n    --conn-host 'https://gitlab.com/$GITLAB_USER/$GITLAB_REPO.git'\n```\n\nand then the dag processor worked as expected.\n\n### What you think should happen instead\n\nI should not have to specify the password as `TOKEN_NAME:TOKEN_PWD`, it should take the provided username\n\n### How to reproduce\n\nCreate a gitlab repo and a personal token, and insert the settings as decribed above.\n\n### Anything else\n\nI did not test this against a Github repository, I can imagine that the behaviour is different there\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "hints_text": "This error was printing my full password in the logs because I did not include the token name in the password field:\n\n```\ncmdline: git clone -v --bare -- https://*****@gitlab.domain.com/airflow.git /tmp/airflow/dag_bundles/gitlab-dags/bare\n\nstderr: 'Cloning into bare repository '/tmp/airflow/dag_bundles/gitlab-dags/bare'...\n\nfatal: could not read Password for 'https://gldt/-@gitlab.domain.com': No such device or address\n```\n\n\n\n\n", "all_hints_text": "This error was printing my full password in the logs because I did not include the token name in the password field:\n\n```\ncmdline: git clone -v --bare -- https://*****@gitlab.domain.com/airflow.git /tmp/airflow/dag_bundles/gitlab-dags/bare\n\nstderr: 'Cloning into bare repository '/tmp/airflow/dag_bundles/gitlab-dags/bare'...\n\nfatal: could not read Password for 'https://gldt/-@gitlab.domain.com': No such device or address\n```\n\n\n\nMay I know how did you pass the config in the helm chart. I was working on the same use case with multi git branch.\n> May I know how did you pass the config in the helm chart. I was working on the same use case with multi git branch.\n\n@sushanth2901 I've added the config.dag_processor.dag_bundle_config_list as specified in the \"what happened\" section above in the config section, and then added the git connection through the cli after connecting to a pod, but you could also specify it through the UI or as an environment variable (https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#storing-connections-in-environment-variables), preferably through a secret that gets added through the corresponding extraEnvFrom field. Or is it something else you have problems with? Happy to help, but I'd probably need more details\nThank you @GrumpyCat51 It worked for me but there are few issues I'm facing.\n\n1) The git i created through UI it didn't work since I'm facing \"no host URL found\" even though I provided URL. So, I deleted it and created through airflow cli then it worked for me but the problem is few times if dag is restarted or deleted the new pod it not picking up and facing same issue, so i had to create the connection again.\n\n\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/33a22fc36f5b573cd93fdc3fdf2d651e9a65306e", "https://github.com/apache/airflow/commit/f0d966a694fc6e2b6246abcfa4c07ee20915d878"], "created_at": "2025-05-31T04:04:55Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 51191, "instance_id": "apache__airflow-51191", "issue_numbers": [51190], "base_commit": "8edbf34b3b3ae5fca733c5ea49a3f80d722e548f", "patch": "diff --git a/chart/values.schema.json b/chart/values.schema.json\nindex 9636d3a07e566..2795a342a93c3 100644\n--- a/chart/values.schema.json\n+++ b/chart/values.schema.json\n@@ -5454,11 +5454,47 @@\n                             },\n                             \"value\": {\n                                 \"type\": \"string\"\n+                            },\n+                            \"valueFrom\": {\n+                                \"type\": \"object\",\n+                                \"properties\": {\n+                                    \"configMapKeyRef\": {\n+                                        \"$ref\": \"#/definitions/io.k8s.api.core.v1.ConfigMapKeySelector\",\n+                                        \"description\": \"Selects a key of a ConfigMap.\"\n+                                    },\n+                                    \"secretKeyRef\": {\n+                                        \"$ref\": \"#/definitions/io.k8s.api.core.v1.SecretKeySelector\",\n+                                        \"description\": \"Selects a key of a secret in the pod's namespace\"\n+                                    }\n+                                },\n+                                \"anyOf\": [\n+                                    {\n+                                        \"required\": [\n+                                            \"configMapKeyRef\"\n+                                        ]\n+                                    },\n+                                    {\n+                                        \"required\": [\n+                                            \"secretKeyRef\"\n+                                        ]\n+                                    }\n+                                ]\n                             }\n                         },\n                         \"required\": [\n-                            \"name\",\n-                            \"value\"\n+                            \"name\"\n+                        ],\n+                        \"anyOf\": [\n+                            {\n+                                \"required\": [\n+                                    \"value\"\n+                                ]\n+                            },\n+                            {\n+                                \"required\": [\n+                                    \"valueFrom\"\n+                                ]\n+                            }\n                         ],\n                         \"additionalProperties\": false\n                     }\n", "test_patch": "diff --git a/helm-tests/tests/helm_tests/airflow_core/test_api_server.py b/helm-tests/tests/helm_tests/airflow_core/test_api_server.py\nindex 97b3b64690e23..77bf475b6b739 100644\n--- a/helm-tests/tests/helm_tests/airflow_core/test_api_server.py\n+++ b/helm-tests/tests/helm_tests/airflow_core/test_api_server.py\n@@ -94,15 +94,32 @@ def test_should_add_extraEnvs(self):\n         docs = render_chart(\n             values={\n                 \"apiServer\": {\n-                    \"env\": [{\"name\": \"TEST_ENV_1\", \"value\": \"test_env_1\"}],\n+                    \"env\": [\n+                        {\"name\": \"TEST_ENV_1\", \"value\": \"test_env_1\"},\n+                        {\n+                            \"name\": \"TEST_ENV_2\",\n+                            \"valueFrom\": {\"configMapKeyRef\": {\"name\": \"test-config\", \"key\": \"test-key\"}},\n+                        },\n+                        {\n+                            \"name\": \"TEST_ENV_3\",\n+                            \"valueFrom\": {\"secretKeyRef\": {\"name\": \"test-secret\", \"key\": \"test-key\"}},\n+                        },\n+                    ],\n                 },\n             },\n             show_only=[\"templates/api-server/api-server-deployment.yaml\"],\n         )\n \n-        assert {\"name\": \"TEST_ENV_1\", \"value\": \"test_env_1\"} in jmespath.search(\n-            \"spec.template.spec.containers[0].env\", docs[0]\n-        )\n+        env_result = jmespath.search(\"spec.template.spec.containers[0].env\", docs[0])\n+        assert {\"name\": \"TEST_ENV_1\", \"value\": \"test_env_1\"} in env_result\n+        assert {\n+            \"name\": \"TEST_ENV_2\",\n+            \"valueFrom\": {\"configMapKeyRef\": {\"name\": \"test-config\", \"key\": \"test-key\"}},\n+        } in env_result\n+        assert {\n+            \"name\": \"TEST_ENV_3\",\n+            \"valueFrom\": {\"secretKeyRef\": {\"name\": \"test-secret\", \"key\": \"test-key\"}},\n+        } in env_result\n \n     def test_should_add_extra_volume_and_extra_volume_mount(self):\n         docs = render_chart(\n", "problem_statement": "incorrect helm chart validation for apiServer.env field\n### Official Helm Chart version\n\nmain (development)\n\n### Apache Airflow version\n\n3.0.0\n\n### Kubernetes Version\n\n1.31\n\n### Helm Chart configuration\n\n```yaml\nairflowVersion: 3.0.0\napiServer:\n  env:\n    - name: AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS\n      value: \"true\"\n    - name: AIRFLOW__API_AUTH__JWT_SECRET\n      valueFrom:\n        secretKeyRef:\n          name: airflow-webserver-secret\n          key: webserver-secret-key\n```\n\n### Docker Image customizations\n\n_No response_\n\n### What happened\n\nI am currently trying to deploy the latest Airflow. During deployment I ran into an issue with `AIRFLOW__API_AUTH__JWT_SECRET` not being set. This is already reported here https://github.com/apache/airflow/issues/50538\n\nTo work around this I decided to pass a custom env variable via the `apiServer.env` value. Because this is a secret value, I wanted to use a `valueFrom` configuration. This is supported by the other components, like `webserver` and th same implementation is used by the templates, however, when executing helm, it produces the following validation error \n\n```\nError: values don't meet the specifications of the schema(s) in the following chart(s):\nairflow:\n- apiServer.env.1: value is required\n- apiServer.env.1: Additional property valueFrom is not allowed\n```\n\n\n\n### What you think should happen instead\n\nThe template supports this feature, only the `values.schema.json` needs to be updated, copying the configuration used for the other components. \n\n\nI have already validated that the chart works, once the schema is updated. I am prepared to submit a PR, if it is allowed.\n\n### How to reproduce\n\nYou can reproduce using the provided values file and then in the airflow repository run \n\n```bash\n$ helm template local ./chart --values bug-values.yaml\n```\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "hints_text": "\n\n", "all_hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/b42e2c063375ce7a373dbae52d8ed0b2f6741daf", "https://github.com/apache/airflow/commit/4f01bfe941a3df07e9cb8f207773a344f1712364", "https://github.com/apache/airflow/commit/80d1ecfdd253bf08aae1f4a2baa501d1dc8492ff", "https://github.com/apache/airflow/commit/1977904e8927717302b0c83e8e614d76c74afc9f"], "created_at": "2025-05-29T08:40:47Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 51462, "instance_id": "apache__airflow-51462", "issue_numbers": [51325], "base_commit": "b739115fdfa233c2d90498b94ad4324610943a69", "patch": "diff --git a/providers/fab/src/airflow/providers/fab/auth_manager/fab_auth_manager.py b/providers/fab/src/airflow/providers/fab/auth_manager/fab_auth_manager.py\nindex 074e26c8c7c08..06e0239c00ae2 100644\n--- a/providers/fab/src/airflow/providers/fab/auth_manager/fab_auth_manager.py\n+++ b/providers/fab/src/airflow/providers/fab/auth_manager/fab_auth_manager.py\n@@ -564,8 +564,8 @@ def _is_authorized_dag(\n             # Check whether the user has permissions to access a specific DAG\n             resource_dag_name = permissions.resource_name(details.id, RESOURCE_DAG)\n             return self._is_authorized(method=method, resource_type=resource_dag_name, user=user)\n-\n-        return False\n+        authorized_dags = self.get_authorized_dag_ids(user=user, method=method)\n+        return len(authorized_dags) > 0\n \n     def _is_authorized_dag_run(\n         self,\n@@ -590,8 +590,8 @@ def _is_authorized_dag_run(\n             # Check whether the user has permissions to access a specific DAG Run permission on a DAG Level\n             resource_dag_name = permissions.resource_name(details.id, RESOURCE_DAG_RUN)\n             return self._is_authorized(method=method, resource_type=resource_dag_name, user=user)\n-\n-        return False\n+        authorized_dags = self.get_authorized_dag_ids(user=user, method=method)\n+        return len(authorized_dags) > 0\n \n     @staticmethod\n     def _get_fab_action(method: ResourceMethod) -> str:\n", "test_patch": "diff --git a/providers/fab/tests/unit/fab/auth_manager/test_fab_auth_manager.py b/providers/fab/tests/unit/fab/auth_manager/test_fab_auth_manager.py\nindex 5e517be3efe30..b4d99b6928409 100644\n--- a/providers/fab/tests/unit/fab/auth_manager/test_fab_auth_manager.py\n+++ b/providers/fab/tests/unit/fab/auth_manager/test_fab_auth_manager.py\n@@ -279,6 +279,62 @@ def test_is_authorized(self, api_name, method, user_permissions, expected_result\n                 [(ACTION_CAN_READ, \"resource_test\")],\n                 False,\n             ),\n+            # With specific DAG permissions but no specific DAG requested\n+            (\n+                \"GET\",\n+                None,\n+                None,\n+                [(ACTION_CAN_READ, \"DAG:test_dag_id\")],\n+                True,\n+            ),\n+            # With multiple specific DAG permissions, no specific DAG requested\n+            (\n+                \"GET\",\n+                None,\n+                None,\n+                [(ACTION_CAN_READ, \"DAG:test_dag_id\"), (ACTION_CAN_READ, \"DAG:test_dag_id2\")],\n+                True,\n+            ),\n+            # With specific DAG permissions but wrong method\n+            (\n+                \"POST\",\n+                None,\n+                None,\n+                [(ACTION_CAN_READ, \"DAG:test_dag_id\")],\n+                True,\n+            ),\n+            # With correct method permissions for specific DAG\n+            (\n+                \"POST\",\n+                None,\n+                None,\n+                [(ACTION_CAN_CREATE, \"DAG:test_dag_id\")],\n+                True,\n+            ),\n+            # With EDIT permission on specific DAG, no specific DAG requested\n+            (\n+                \"PUT\",\n+                None,\n+                None,\n+                [(ACTION_CAN_EDIT, \"DAG:test_dag_id\")],\n+                True,\n+            ),\n+            # With DELETE permission on specific DAG, no specific DAG requested\n+            (\n+                \"DELETE\",\n+                None,\n+                None,\n+                [(ACTION_CAN_DELETE, \"DAG:test_dag_id\")],\n+                True,\n+            ),\n+            # Mixed permissions - some DAG, some non-DAG\n+            (\n+                \"GET\",\n+                None,\n+                None,\n+                [(ACTION_CAN_READ, \"DAG:test_dag_id\"), (ACTION_CAN_READ, \"resource_test\")],\n+                True,\n+            ),\n             # Scenario 2 #\n             # With global permissions on DAGs\n             (\n@@ -348,10 +404,52 @@ def test_is_authorized(self, api_name, method, user_permissions, expected_result\n                 [(ACTION_CAN_READ, RESOURCE_TASK_INSTANCE)],\n                 False,\n             ),\n+            # DAG sub-entity with specific DAG permissions but no specific DAG requested\n+            (\n+                \"GET\",\n+                DagAccessEntity.RUN,\n+                None,\n+                [(ACTION_CAN_READ, \"DAG:test_dag_id\"), (ACTION_CAN_READ, RESOURCE_DAG_RUN)],\n+                True,\n+            ),\n+            # DAG sub-entity access with no DAG permissions, no specific DAG requested\n+            (\n+                \"GET\",\n+                DagAccessEntity.RUN,\n+                None,\n+                [(ACTION_CAN_READ, RESOURCE_DAG_RUN)],\n+                True,\n+            ),\n+            # DAG sub-entity with specific DAG permissions but missing sub-entity permission\n+            (\n+                \"GET\",\n+                DagAccessEntity.TASK_INSTANCE,\n+                None,\n+                [(ACTION_CAN_READ, \"DAG:test_dag_id\")],\n+                False,\n+            ),\n+            # Multiple DAG access entities with proper permissions\n+            (\n+                \"DELETE\",\n+                DagAccessEntity.TASK,\n+                None,\n+                [(ACTION_CAN_EDIT, \"DAG:test_dag_id\"), (ACTION_CAN_DELETE, RESOURCE_TASK_INSTANCE)],\n+                True,\n+            ),\n+            # User with specific DAG permissions but wrong method for sub-entity\n+            (\n+                \"POST\",\n+                DagAccessEntity.RUN,\n+                None,\n+                [(ACTION_CAN_READ, \"DAG:test_dag_id\"), (ACTION_CAN_READ, RESOURCE_DAG_RUN)],\n+                True,\n+            ),\n         ],\n     )\n+    @mock.patch.object(FabAuthManager, \"get_authorized_dag_ids\")\n     def test_is_authorized_dag(\n         self,\n+        mock_get_authorized_dag_ids,\n         method,\n         dag_access_entity,\n         dag_details,\n@@ -359,8 +457,13 @@ def test_is_authorized_dag(\n         expected_result,\n         auth_manager_with_appbuilder,\n     ):\n+        dag_permissions = [perm[1] for perm in user_permissions if perm[1].startswith(\"DAG:\")]\n+        dag_ids = {perm.replace(\"DAG:\", \"\") for perm in dag_permissions}\n+        mock_get_authorized_dag_ids.return_value = dag_ids\n+\n         user = Mock()\n         user.perms = user_permissions\n+        user.id = 1\n         result = auth_manager_with_appbuilder.is_authorized_dag(\n             method=method, access_entity=dag_access_entity, details=dag_details, user=user\n         )\n", "problem_statement": "DAG-level access control requires global \"can read on DAGs\" permission\n### Apache Airflow version\n\n3.0.1\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nDAG-level access control using the `access_control` parameter is not working as expected. Users with specific DAG permissions (e.g., `can read on DAG:test_dag_1`) cannot access the DAG list page without also having the global `can read on DAGs` permission, which defeats the purpose of DAG-level access control.\n\n### What you think should happen instead?\n\nUsers should be able to access DAGs when they have specific DAG permissions defined in the DAG's `access_control`, without requiring the global `can read on DAGs` permission. The DAG list should show only the DAGs the user has explicit access to.\n\n\n### How to reproduce\n\n# How to reproduce\n\n1. Create a custom role (e.g., `ABC_Viewer`) with the following permissions:\n\n[can read on DAG:test_dag_1, can read on DAG Dependencies, can read on DAG Code, can read on DAG Runs, can read on DAG Versions, can read on DAG Warnings, can read on Assets, can read on Asset Aliases, can read on Backfills, can read on Cluster Activity, can read on Configurations, can read on Pools, can read on ImportError, can read on Jobs, can read on My Password, can edit on My Password, can read on My Profile, can edit on My Profile, can read on SLA Misses, can read on Task Instances, can read on Task Logs, can read on XComs, can read on Website, menu access on Browse, menu access on DAGs, menu access on DAG Dependencies, menu access on DAG Runs, menu access on Assets, menu access on Cluster Activity, menu access on Documentation, menu access on Docs, menu access on Jobs, menu access on SLA Misses, menu access on Task Instances]\n\n\n2. Create a DAG with access control:\n```\npythondag = DAG(\n    dag_id='test_dag_1',\n    start_date=datetime(2024, 1, 1),\n    schedule=None,\n    catchup=False,\n    access_control={\n        'ABC_Viewer': {'can_read'}\n    }\n)\n\ntask1 = EmptyOperator(\n    task_id='test_dag_1',\n    dag=pythondag\n)\n```\n3. Assign the `ABC_Viewer` role to a user (verified in Security >  Users that the user has this role)\n4. Login as that user and navigate to the DAG list page and you will see a `403` \n\n# Note\n- **Without** global `can read on DAGs` permission: User gets a 403 Forbidden error when accessing the DAG list page\n- **With** global `can read on DAGs` permission: User can see ALL DAGs, not just the ones specified in access_control\n\n# Expected Result\nUser should be able to access the DAG list page and see only test_dag_1 (the DAG they have explicit access to via access_control), without needing the global can read on DAGs permission.\n\n### Operating System\n\nDebian GNU/Linux 12\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-fab==2.0.2\n\n\n### Deployment\n\nOther Docker-based deployment\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\ncc: @pierrejeambrun \nThanks for reporting the issue, indeed, I think this should be the case for any listing endpoint. We should have the same issue for listing other resources (connection, variables etc...).\n\n\nMarking as good first issue, it's a great way to get started with the api server contributions.\nHi, I'd like to take this one on, could you please assign it to me? :)\nThanks folks, I believe we need to change the following https://github.com/apache/airflow/blob/6846a480a79ba978e7755366e658e96a683a48b8/providers/fab/src/airflow/providers/fab/auth_manager/fab_auth_manager.py#L563-L568\n\nand \n\nhttps://github.com/apache/airflow/blob/6846a480a79ba978e7755366e658e96a683a48b8/providers/fab/src/airflow/providers/fab/auth_manager/fab_auth_manager.py#L589-L594\n\nto \n```\nif details and details.id:\n    # Check whether the user has permissions to access a specific DAG\n    resource_dag_name = permissions.resource_name(details.id, RESOURCE_DAG)\n    return self._is_authorized(method=method, resource_type=resource_dag_name, user=user)\n\nelse:\n    authorized_dags = self.get_authorized_dag_ids(user=user, method=method)\n    return len(authorized_dags) > 0\n```\n\nI patched over the code and this change seems to work. \n@willyyang nice, will you be submitting a PR?\nhttps://github.com/apache/airflow/pull/51462 @nailo2c @pierrejeambrun @jroachgolf84 \n\n", "all_hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\ncc: @pierrejeambrun \nThanks for reporting the issue, indeed, I think this should be the case for any listing endpoint. We should have the same issue for listing other resources (connection, variables etc...).\n\n\nMarking as good first issue, it's a great way to get started with the api server contributions.\nHi, I'd like to take this one on, could you please assign it to me? :)\nThanks folks, I believe we need to change the following https://github.com/apache/airflow/blob/6846a480a79ba978e7755366e658e96a683a48b8/providers/fab/src/airflow/providers/fab/auth_manager/fab_auth_manager.py#L563-L568\n\nand \n\nhttps://github.com/apache/airflow/blob/6846a480a79ba978e7755366e658e96a683a48b8/providers/fab/src/airflow/providers/fab/auth_manager/fab_auth_manager.py#L589-L594\n\nto \n```\nif details and details.id:\n    # Check whether the user has permissions to access a specific DAG\n    resource_dag_name = permissions.resource_name(details.id, RESOURCE_DAG)\n    return self._is_authorized(method=method, resource_type=resource_dag_name, user=user)\n\nelse:\n    authorized_dags = self.get_authorized_dag_ids(user=user, method=method)\n    return len(authorized_dags) > 0\n```\n\nI patched over the code and this change seems to work. \n@willyyang nice, will you be submitting a PR?\nhttps://github.com/apache/airflow/pull/51462 @nailo2c @pierrejeambrun @jroachgolf84 \n\n", "commit_urls": ["https://github.com/apache/airflow/commit/f582722c2a1e3f301ccc3b37070f16fa15bb3384", "https://github.com/apache/airflow/commit/897957ecb8fb35baef615af1b75e553432919f73", "https://github.com/apache/airflow/commit/68aebf44c8b1317595a01b507494b6b929c90554"], "created_at": "2025-06-05T18:14:35Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 51509, "instance_id": "apache__airflow-51509", "issue_numbers": [51351], "base_commit": "46b8aebf3b8c8776cf4006101ec53bbc51ae7ab2", "patch": "diff --git a/airflow-core/src/airflow/api_fastapi/common/exceptions.py b/airflow-core/src/airflow/api_fastapi/common/exceptions.py\nindex 061eec55d3d84..39909b7a46395 100644\n--- a/airflow-core/src/airflow/api_fastapi/common/exceptions.py\n+++ b/airflow-core/src/airflow/api_fastapi/common/exceptions.py\n@@ -17,6 +17,8 @@\n \n from __future__ import annotations\n \n+import logging\n+import traceback\n from abc import ABC, abstractmethod\n from enum import Enum\n from typing import Generic, TypeVar\n@@ -24,8 +26,13 @@\n from fastapi import HTTPException, Request, status\n from sqlalchemy.exc import IntegrityError\n \n+from airflow.configuration import conf\n+from airflow.utils.strings import get_random_string\n+\n T = TypeVar(\"T\", bound=Exception)\n \n+log = logging.getLogger(__name__)\n+\n \n class BaseErrorHandler(Generic[T], ABC):\n     \"\"\"Base class for error handlers.\"\"\"\n@@ -61,12 +68,28 @@ def __init__(self):\n     def exception_handler(self, request: Request, exc: IntegrityError):\n         \"\"\"Handle IntegrityError exception.\"\"\"\n         if self._is_dialect_matched(exc):\n+            exception_id = get_random_string()\n+            stacktrace = \"\"\n+            for tb in traceback.format_tb(exc.__traceback__):\n+                stacktrace += tb\n+\n+            log_message = f\"Error with id {exception_id}\\n{stacktrace}\"\n+            log.error(log_message)\n+            if conf.get(\"api\", \"expose_stacktrace\") == \"True\":\n+                message = log_message\n+            else:\n+                message = (\n+                    \"Serious error when handling your request. Check logs for more details - \"\n+                    f\"you will find it in api server when you look for ID {exception_id}\"\n+                )\n+\n             raise HTTPException(\n                 status_code=status.HTTP_409_CONFLICT,\n                 detail={\n                     \"reason\": \"Unique constraint violation\",\n                     \"statement\": str(exc.statement),\n                     \"orig_error\": str(exc.orig),\n+                    \"message\": message,\n                 },\n             )\n \ndiff --git a/airflow-core/src/airflow/config_templates/config.yml b/airflow-core/src/airflow/config_templates/config.yml\nindex bda494eed5dcc..30c6037da4a92 100644\n--- a/airflow-core/src/airflow/config_templates/config.yml\n+++ b/airflow-core/src/airflow/config_templates/config.yml\n@@ -1321,6 +1321,12 @@ api:\n       type: string\n       example: ~\n       default: \"False\"\n+    expose_stacktrace:\n+      description: Expose stacktrace in the web server\n+      version_added: ~\n+      type: string\n+      example: ~\n+      default: \"False\"\n     base_url:\n       description: |\n         The base url of the API server. Airflow cannot guess what domain or CNAME you are using.\n", "test_patch": "diff --git a/airflow-core/tests/unit/api_fastapi/common/test_exceptions.py b/airflow-core/tests/unit/api_fastapi/common/test_exceptions.py\nindex bed77a67a2723..b5136310611e0 100644\n--- a/airflow-core/tests/unit/api_fastapi/common/test_exceptions.py\n+++ b/airflow-core/tests/unit/api_fastapi/common/test_exceptions.py\n@@ -16,6 +16,8 @@\n # under the License.\n from __future__ import annotations\n \n+from unittest.mock import patch\n+\n import pytest\n from fastapi import HTTPException, status\n from sqlalchemy.exc import IntegrityError\n@@ -26,6 +28,7 @@\n from airflow.utils.session import provide_session\n from airflow.utils.state import DagRunState\n \n+from tests_common.test_utils.config import conf_vars\n from tests_common.test_utils.db import clear_db_connections, clear_db_dags, clear_db_pools, clear_db_runs\n \n pytestmark = pytest.mark.db_test\n@@ -50,6 +53,11 @@\n         \"reason\": f\"Test for {_DatabaseDialect.POSTGRES.value} only\",\n     },\n ]\n+MOCKED_ID = \"TgVcT3QW\"\n+MESSAGE = (\n+    \"Serious error when handling your request. Check logs for more details - \"\n+    f\"you will find it in api server when you look for ID {MOCKED_ID}\"\n+)\n \n \n def generate_test_cases_parametrize(\n@@ -109,6 +117,7 @@ def teardown_method(self) -> None:\n                             \"reason\": \"Unique constraint violation\",\n                             \"statement\": \"INSERT INTO slot_pool (pool, slots, description, include_deferred) VALUES (?, ?, ?, ?)\",\n                             \"orig_error\": \"UNIQUE constraint failed: slot_pool.pool\",\n+                            \"message\": MESSAGE,\n                         },\n                     ),\n                     HTTPException(\n@@ -117,6 +126,7 @@ def teardown_method(self) -> None:\n                             \"reason\": \"Unique constraint violation\",\n                             \"statement\": \"INSERT INTO slot_pool (pool, slots, description, include_deferred) VALUES (%s, %s, %s, %s)\",\n                             \"orig_error\": \"(1062, \\\"Duplicate entry 'test_pool' for key 'slot_pool.slot_pool_pool_uq'\\\")\",\n+                            \"message\": MESSAGE,\n                         },\n                     ),\n                     HTTPException(\n@@ -125,6 +135,7 @@ def teardown_method(self) -> None:\n                             \"reason\": \"Unique constraint violation\",\n                             \"statement\": \"INSERT INTO slot_pool (pool, slots, description, include_deferred) VALUES (%(pool)s, %(slots)s, %(description)s, %(include_deferred)s) RETURNING slot_pool.id\",\n                             \"orig_error\": 'duplicate key value violates unique constraint \"slot_pool_pool_uq\"\\nDETAIL:  Key (pool)=(test_pool) already exists.\\n',\n+                            \"message\": MESSAGE,\n                         },\n                     ),\n                 ],\n@@ -135,6 +146,7 @@ def teardown_method(self) -> None:\n                             \"reason\": \"Unique constraint violation\",\n                             \"statement\": 'INSERT INTO variable (\"key\", val, description, is_encrypted) VALUES (?, ?, ?, ?)',\n                             \"orig_error\": \"UNIQUE constraint failed: variable.key\",\n+                            \"message\": MESSAGE,\n                         },\n                     ),\n                     HTTPException(\n@@ -143,6 +155,7 @@ def teardown_method(self) -> None:\n                             \"reason\": \"Unique constraint violation\",\n                             \"statement\": \"INSERT INTO variable (`key`, val, description, is_encrypted) VALUES (%s, %s, %s, %s)\",\n                             \"orig_error\": \"(1062, \\\"Duplicate entry 'test_key' for key 'variable.variable_key_uq'\\\")\",\n+                            \"message\": MESSAGE,\n                         },\n                     ),\n                     HTTPException(\n@@ -151,14 +164,23 @@ def teardown_method(self) -> None:\n                             \"reason\": \"Unique constraint violation\",\n                             \"statement\": \"INSERT INTO variable (key, val, description, is_encrypted) VALUES (%(key)s, %(val)s, %(description)s, %(is_encrypted)s) RETURNING variable.id\",\n                             \"orig_error\": 'duplicate key value violates unique constraint \"variable_key_uq\"\\nDETAIL:  Key (key)=(test_key) already exists.\\n',\n+                            \"message\": MESSAGE,\n                         },\n                     ),\n                 ],\n             ],\n         ),\n     )\n+    @patch(\"airflow.api_fastapi.common.exceptions.get_random_string\", return_value=MOCKED_ID)\n+    @conf_vars({(\"api\", \"expose_stacktrace\"): \"False\"})\n     @provide_session\n-    def test_handle_single_column_unique_constraint_error(self, session, table, expected_exception) -> None:\n+    def test_handle_single_column_unique_constraint_error(\n+        self,\n+        mock_get_random_string,\n+        session,\n+        table,\n+        expected_exception,\n+    ) -> None:\n         # Take Pool and Variable tables as test cases\n         if table == \"Pool\":\n             session.add(Pool(pool=TEST_POOL, slots=1, description=\"test pool\", include_deferred=False))\n@@ -188,6 +210,7 @@ def test_handle_single_column_unique_constraint_error(self, session, table, expe\n                             \"reason\": \"Unique constraint violation\",\n                             \"statement\": \"INSERT INTO dag_run (dag_id, queued_at, logical_date, start_date, end_date, state, run_id, creating_job_id, run_type, triggered_by, conf, data_interval_start, data_interval_end, run_after, last_scheduling_decision, log_template_id, updated_at, clear_number, backfill_id, bundle_version, scheduled_by_job_id, context_carrier, created_dag_version_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, (SELECT max(log_template.id) AS max_1 \\nFROM log_template), ?, ?, ?, ?, ?, ?, ?)\",\n                             \"orig_error\": \"UNIQUE constraint failed: dag_run.dag_id, dag_run.run_id\",\n+                            \"message\": MESSAGE,\n                         },\n                     ),\n                     HTTPException(\n@@ -196,6 +219,7 @@ def test_handle_single_column_unique_constraint_error(self, session, table, expe\n                             \"reason\": \"Unique constraint violation\",\n                             \"statement\": \"INSERT INTO dag_run (dag_id, queued_at, logical_date, start_date, end_date, state, run_id, creating_job_id, run_type, triggered_by, conf, data_interval_start, data_interval_end, run_after, last_scheduling_decision, log_template_id, updated_at, clear_number, backfill_id, bundle_version, scheduled_by_job_id, context_carrier, created_dag_version_id) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, (SELECT max(log_template.id) AS max_1 \\nFROM log_template), %s, %s, %s, %s, %s, %s, %s)\",\n                             \"orig_error\": \"(1062, \\\"Duplicate entry 'test_dag_id-test_run_id' for key 'dag_run.dag_run_dag_id_run_id_key'\\\")\",\n+                            \"message\": MESSAGE,\n                         },\n                     ),\n                     HTTPException(\n@@ -204,15 +228,22 @@ def test_handle_single_column_unique_constraint_error(self, session, table, expe\n                             \"reason\": \"Unique constraint violation\",\n                             \"statement\": \"INSERT INTO dag_run (dag_id, queued_at, logical_date, start_date, end_date, state, run_id, creating_job_id, run_type, triggered_by, conf, data_interval_start, data_interval_end, run_after, last_scheduling_decision, log_template_id, updated_at, clear_number, backfill_id, bundle_version, scheduled_by_job_id, context_carrier, created_dag_version_id) VALUES (%(dag_id)s, %(queued_at)s, %(logical_date)s, %(start_date)s, %(end_date)s, %(state)s, %(run_id)s, %(creating_job_id)s, %(run_type)s, %(triggered_by)s, %(conf)s, %(data_interval_start)s, %(data_interval_end)s, %(run_after)s, %(last_scheduling_decision)s, (SELECT max(log_template.id) AS max_1 \\nFROM log_template), %(updated_at)s, %(clear_number)s, %(backfill_id)s, %(bundle_version)s, %(scheduled_by_job_id)s, %(context_carrier)s, %(created_dag_version_id)s) RETURNING dag_run.id\",\n                             \"orig_error\": 'duplicate key value violates unique constraint \"dag_run_dag_id_run_id_key\"\\nDETAIL:  Key (dag_id, run_id)=(test_dag_id, test_run_id) already exists.\\n',\n+                            \"message\": MESSAGE,\n                         },\n                     ),\n                 ],\n             ],\n         ),\n     )\n+    @patch(\"airflow.api_fastapi.common.exceptions.get_random_string\", return_value=MOCKED_ID)\n+    @conf_vars({(\"api\", \"expose_stacktrace\"): \"False\"})\n     @provide_session\n     def test_handle_multiple_columns_unique_constraint_error(\n-        self, session, table, expected_exception\n+        self,\n+        mock_get_random_string,\n+        session,\n+        table,\n+        expected_exception,\n     ) -> None:\n         if table == \"DagRun\":\n             session.add(\ndiff --git a/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py b/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py\nindex 6ddf188935249..a4184e29e8da3 100644\n--- a/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py\n+++ b/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py\n@@ -306,7 +306,7 @@ def test_post_should_respond_already_exist(self, test_client, body):\n         assert response.status_code == 409\n         response_json = response.json()\n         assert \"detail\" in response_json\n-        assert list(response_json[\"detail\"].keys()) == [\"reason\", \"statement\", \"orig_error\"]\n+        assert list(response_json[\"detail\"].keys()) == [\"reason\", \"statement\", \"orig_error\", \"message\"]\n \n     @pytest.mark.enable_redact\n     @pytest.mark.parametrize(\ndiff --git a/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_dag_run.py b/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_dag_run.py\nindex baa7e503c83ed..718e76784bf88 100644\n--- a/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_dag_run.py\n+++ b/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_dag_run.py\n@@ -38,12 +38,7 @@\n from airflow.utils.types import DagRunTriggeredByType, DagRunType\n \n from tests_common.test_utils.api_fastapi import _check_dag_run_note, _check_last_log\n-from tests_common.test_utils.db import (\n-    clear_db_dags,\n-    clear_db_logs,\n-    clear_db_runs,\n-    clear_db_serialized_dags,\n-)\n+from tests_common.test_utils.db import clear_db_dags, clear_db_logs, clear_db_runs, clear_db_serialized_dags\n from tests_common.test_utils.format_datetime import from_datetime_to_zulu, from_datetime_to_zulu_without_ms\n \n if TYPE_CHECKING:\n@@ -1577,7 +1572,7 @@ def test_response_409(self, test_client):\n         assert response.status_code == 409\n         response_json = response.json()\n         assert \"detail\" in response_json\n-        assert list(response_json[\"detail\"].keys()) == [\"reason\", \"statement\", \"orig_error\"]\n+        assert list(response_json[\"detail\"].keys()) == [\"reason\", \"statement\", \"orig_error\", \"message\"]\n \n     @pytest.mark.usefixtures(\"configure_git_connection_for_dag_bundle\")\n     def test_should_respond_200_with_null_logical_date(self, test_client):\ndiff --git a/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_pools.py b/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_pools.py\nindex 8f6ed1a7019be..2d0700ea5d1be 100644\n--- a/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_pools.py\n+++ b/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_pools.py\n@@ -417,7 +417,7 @@ def test_should_response_409(\n         else:\n             response_json = response.json()\n             assert \"detail\" in response_json\n-            assert list(response_json[\"detail\"].keys()) == [\"reason\", \"statement\", \"orig_error\"]\n+            assert list(response_json[\"detail\"].keys()) == [\"reason\", \"statement\", \"orig_error\", \"message\"]\n \n         assert session.query(Pool).count() == n_pools + 1\n \n", "problem_statement": "API - Config setting to control exposing stacktraces\n### Body\n\nSimilarly to AF2 `expose_stacktrace` we need to implement the AF3 equivalent. Add an `api.expose_stacktrace` config parameter and update the fastapi exception handlers (mostly the DB ones because they are the one to return stacktraces at the moment, but other exception handlers should be checked to) to instead return a generic message such as \"DatabaseIntegrity error see more details in logs\" when stack traces are not exposed.\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.\n", "hints_text": "@pierrejeambrun do you want to work on this one?\nAdded the good first issue, I think it's a nice one to get started, is nobody grabs it I can work on it.\n@pierrejeambrun @amoghrajesh I\u2019d like to work on this issue! Can I get assigned?\n@kevinhongzl go ahead, you're assigned :)\n\n", "all_hints_text": "@pierrejeambrun do you want to work on this one?\nAdded the good first issue, I think it's a nice one to get started, is nobody grabs it I can work on it.\n@pierrejeambrun @amoghrajesh I\u2019d like to work on this issue! Can I get assigned?\n@kevinhongzl go ahead, you're assigned :)\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/ff0cb0d363b5262745f5fae06599ffab98e0e305", "https://github.com/apache/airflow/commit/4914db05585227637086945c8495941d3ed83ab8", "https://github.com/apache/airflow/commit/4507d5429d9c9d09f9d421affa5c41d7afd63e78", "https://github.com/apache/airflow/commit/6966a430bbef0ec0663cf885700b1b4f74133efe", "https://github.com/apache/airflow/commit/f9794464d5d4c28d12f762d0c8a5753114dde540", "https://github.com/apache/airflow/commit/9e7b902e5c3f56ce6042f5cf610b118ae46ceb49", "https://github.com/apache/airflow/commit/9b0bcdb46379d944322d1f0bb554940be5e7675a", "https://github.com/apache/airflow/commit/51d59bb84f4e2ca292b634a5d069e82c62ee99cc"], "created_at": "2025-06-07T22:09:23Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 51999, "instance_id": "apache__airflow-51999", "issue_numbers": [51944], "base_commit": "46385565a7f2589b53c6353f4f8a500a333d364d", "patch": "diff --git a/providers/microsoft/azure/docs/connections/wasb.rst b/providers/microsoft/azure/docs/connections/wasb.rst\nindex e7e2f78b596ad..057160b30f0e9 100644\n--- a/providers/microsoft/azure/docs/connections/wasb.rst\n+++ b/providers/microsoft/azure/docs/connections/wasb.rst\n@@ -27,7 +27,7 @@ The Microsoft Azure Blob Storage connection type enables the Azure Blob Storage\n Authenticating to Azure Blob Storage\n ------------------------------------\n \n-There are six ways to connect to Azure Blob Storage using Airflow.\n+There are seven ways to connect to Azure Blob Storage using Airflow.\n \n 1. Use `token credentials`_\n    i.e. add specific credentials (client_id, secret, tenant) and subscription id to the Airflow connection.\n@@ -37,8 +37,9 @@ There are six ways to connect to Azure Blob Storage using Airflow.\n    i.e. add a key config to ``sas_token`` in the Airflow connection.\n 4. Use a `Connection String`_\n    i.e. add connection string to ``connection_string`` in the Airflow connection.\n-5. Use managed identity by setting ``managed_identity_client_id``, ``workload_identity_tenant_id`` (under the hook, it uses DefaultAzureCredential_ with these arguments)\n-6. Fallback on DefaultAzureCredential_.\n+5. Use account key by setting ``account_key`` in the Airflow connection extra fields.\n+6. Use managed identity by setting ``managed_identity_client_id``, ``workload_identity_tenant_id`` (under the hook, it uses DefaultAzureCredential_ with these arguments)\n+7. Fallback on DefaultAzureCredential_.\n    This includes a mechanism to try different options to authenticate: Managed System Identity, environment variables, authentication through Azure CLI, etc.\n \n Only one authorization method can be used at a time. If you need to manage multiple credentials or keys then you should\n@@ -84,6 +85,7 @@ Extra (optional)\n     Specify the extra parameters (as json dictionary) that can be used in Azure connection.\n     The following parameters are all optional:\n \n+    * ``account_key``: Specify the account key for Azure Blob Storage authentication. This will be checked before falling back to DefaultAzureCredential_.\n     * ``client_secret_auth_config``: Extra config to pass while authenticating as a service principal using `ClientSecretCredential`_ It can be left out to fall back on DefaultAzureCredential_.\n     * ``managed_identity_client_id``:  The client ID of a user-assigned managed identity. If provided with `workload_identity_tenant_id`, they'll pass to ``DefaultAzureCredential``.\n     * ``workload_identity_tenant_id``: ID of the application's Microsoft Entra tenant. Also called its \"directory\" ID. If provided with `managed_identity_client_id`, they'll pass to ``DefaultAzureCredential``.\ndiff --git a/providers/microsoft/azure/docs/logging/index.rst b/providers/microsoft/azure/docs/logging/index.rst\nindex 16ed9b5c7fd07..04405eedf35b1 100644\n--- a/providers/microsoft/azure/docs/logging/index.rst\n+++ b/providers/microsoft/azure/docs/logging/index.rst\n@@ -58,7 +58,7 @@ Setup Steps:\n ''''''''''''''\n \n #. Install the provider package with ``pip install apache-airflow-providers-microsoft-azure``.\n-#. Ensure :ref:`connection <howto/connection:wasb>` is already setup with read and write access to Azure Blob Storage in the ``remote_wasb_log_container`` container and path ``remote_base_log_folder``.\n+#. Ensure :ref:`connection <howto/connection:wasb>` is already setup with read and write access to Azure Blob Storage in the ``remote_wasb_log_container`` container and path ``remote_base_log_folder``. The connection should be configured with appropriate authentication credentials (such as account key, shared access key, or managed identity). For account key authentication, you can add ``account_key`` to the connection's extra fields as a JSON dictionary: ``{\"account_key\": \"your_account_key\"}``.\n #. Setup the above configuration values. Please note that the container should already exist.\n #. Restart the Airflow webserver and scheduler, and trigger (or wait for) a new task execution.\n #. Verify that logs are showing up for newly executed tasks in the container at the specified base path you have defined.\ndiff --git a/providers/microsoft/azure/newsfragments/51944.bugfix.rst b/providers/microsoft/azure/newsfragments/51944.bugfix.rst\nnew file mode 100644\nindex 0000000000000..5706bcc088e17\n--- /dev/null\n+++ b/providers/microsoft/azure/newsfragments/51944.bugfix.rst\n@@ -0,0 +1,1 @@\n+Fix Azure Blob Storage authentication to check ``account_key`` field in connection extra before falling back to ``DefaultAzureCredential``\ndiff --git a/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/wasb.py b/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/wasb.py\nindex 8eb009592a15f..7ff704e0f5e45 100644\n--- a/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/wasb.py\n+++ b/providers/microsoft/azure/src/airflow/providers/microsoft/azure/hooks/wasb.py\n@@ -67,8 +67,9 @@ class WasbHook(BaseHook):\n     These parameters have to be passed in Airflow Data Base: account_name and account_key.\n \n     Additional options passed in the 'extra' field of the connection will be\n-    passed to the `BlockBlockService()` constructor. For example, authenticate\n-    using a SAS token by adding {\"sas_token\": \"YOUR_TOKEN\"}.\n+    passed to the `BlobServiceClient()` constructor. For example, authenticate\n+    using a SAS token by adding {\"sas_token\": \"YOUR_TOKEN\"} or using an account key\n+    by adding {\"account_key\": \"YOUR_ACCOUNT_KEY\"}.\n \n     If no authentication configuration is provided, DefaultAzureCredential will be used (applicable\n     when using Azure compute infrastructure).\n@@ -121,7 +122,7 @@ def get_ui_field_behaviour(cls) -> dict[str, Any]:\n                 \"tenant_id\": \"tenant\",\n                 \"shared_access_key\": \"shared access key\",\n                 \"sas_token\": \"account url or token\",\n-                \"extra\": \"additional options for use with ClientSecretCredential or DefaultAzureCredential\",\n+                \"extra\": \"additional options for use with ClientSecretCredential, DefaultAzureCredential, or account_key authentication\",\n             },\n         }\n \n@@ -198,13 +199,18 @@ def get_conn(self) -> BlobServiceClient:\n         # Fall back to old auth (password) or use managed identity if not provided.\n         credential = conn.password\n         if not credential:\n-            managed_identity_client_id = self._get_field(extra, \"managed_identity_client_id\")\n-            workload_identity_tenant_id = self._get_field(extra, \"workload_identity_tenant_id\")\n-            credential = get_sync_default_azure_credential(\n-                managed_identity_client_id=managed_identity_client_id,\n-                workload_identity_tenant_id=workload_identity_tenant_id,\n-            )\n-            self.log.info(\"Using DefaultAzureCredential as credential\")\n+            # Check for account_key in extra fields before falling back to DefaultAzureCredential\n+            account_key = self._get_field(extra, \"account_key\")\n+            if account_key:\n+                credential = account_key\n+            else:\n+                managed_identity_client_id = self._get_field(extra, \"managed_identity_client_id\")\n+                workload_identity_tenant_id = self._get_field(extra, \"workload_identity_tenant_id\")\n+                credential = get_sync_default_azure_credential(\n+                    managed_identity_client_id=managed_identity_client_id,\n+                    workload_identity_tenant_id=workload_identity_tenant_id,\n+                )\n+                self.log.info(\"Using DefaultAzureCredential as credential\")\n         return BlobServiceClient(\n             account_url=account_url,\n             credential=credential,\n@@ -646,13 +652,18 @@ async def get_async_conn(self) -> AsyncBlobServiceClient:\n         # Fall back to old auth (password) or use managed identity if not provided.\n         credential = conn.password\n         if not credential:\n-            managed_identity_client_id = self._get_field(extra, \"managed_identity_client_id\")\n-            workload_identity_tenant_id = self._get_field(extra, \"workload_identity_tenant_id\")\n-            credential = get_async_default_azure_credential(\n-                managed_identity_client_id=managed_identity_client_id,\n-                workload_identity_tenant_id=workload_identity_tenant_id,\n-            )\n-            self.log.info(\"Using DefaultAzureCredential as credential\")\n+            # Check for account_key in extra fields before falling back to DefaultAzureCredential\n+            account_key = self._get_field(extra, \"account_key\")\n+            if account_key:\n+                credential = account_key\n+            else:\n+                managed_identity_client_id = self._get_field(extra, \"managed_identity_client_id\")\n+                workload_identity_tenant_id = self._get_field(extra, \"workload_identity_tenant_id\")\n+                credential = get_async_default_azure_credential(\n+                    managed_identity_client_id=managed_identity_client_id,\n+                    workload_identity_tenant_id=workload_identity_tenant_id,\n+                )\n+                self.log.info(\"Using DefaultAzureCredential as credential\")\n         self.blob_service_client = AsyncBlobServiceClient(\n             account_url=account_url,\n             credential=credential,\n", "test_patch": "diff --git a/providers/microsoft/azure/tests/unit/microsoft/azure/hooks/test_wasb.py b/providers/microsoft/azure/tests/unit/microsoft/azure/hooks/test_wasb.py\nindex 55895879fac53..dffb2eeb4bb2d 100644\n--- a/providers/microsoft/azure/tests/unit/microsoft/azure/hooks/test_wasb.py\n+++ b/providers/microsoft/azure/tests/unit/microsoft/azure/hooks/test_wasb.py\n@@ -79,6 +79,7 @@ def setup_method(self, create_mock_connections):\n         self.public_read_conn_id = \"pub_read_id\"\n         self.public_read_conn_id_without_host = \"pub_read_id_without_host\"\n         self.managed_identity_conn_id = \"managed_identity_conn_id\"\n+        self.account_key_conn_id = \"account_key_conn_id\"\n         self.authority = \"https://test_authority.com\"\n \n         self.proxies = PROXIES\n@@ -135,6 +136,12 @@ def setup_method(self, create_mock_connections):\n                 conn_type=self.connection_type,\n                 extra={\"proxies\": self.proxies},\n             ),\n+            Connection(\n+                conn_id=self.account_key_conn_id,\n+                conn_type=self.connection_type,\n+                login=\"testaccount\",\n+                extra={\"account_key\": \"test_account_key\", \"proxies\": self.proxies},\n+            ),\n             Connection(\n                 conn_id=\"sas_conn_id\",\n                 conn_type=self.connection_type,\n@@ -223,6 +230,16 @@ def test_azure_directory_connection(self, mocked_client_secret_credential, mocke\n             proxies=self.proxies,\n         )\n \n+    def test_account_key_connection(self, mocked_blob_service_client):\n+        \"\"\"Test that account_key from extra is used when no password is provided.\"\"\"\n+        WasbHook(wasb_conn_id=self.account_key_conn_id).get_conn()\n+        mocked_blob_service_client.assert_called_once_with(\n+            account_url=\"https://testaccount.blob.core.windows.net/\",\n+            credential=\"test_account_key\",\n+            proxies=self.proxies,\n+            account_key=\"test_account_key\",\n+        )\n+\n     @pytest.mark.parametrize(\n         \"mocked_connection\",\n         [\n@@ -331,6 +348,7 @@ def test_sas_token_connection(self, conn_id_str, extra_key):\n             \"azure_shared_key_test\",\n             \"ad_conn_id\",\n             \"managed_identity_conn_id\",\n+            \"account_key_conn_id\",\n             \"sas_conn_id\",\n             \"extra__wasb__sas_conn_id\",\n             \"http_sas_conn_id\",\n@@ -659,6 +677,7 @@ def test_connection_failure(self, mocked_blob_service_client):\n             \"azure_shared_key_test\",\n             \"ad_conn_id\",\n             \"managed_identity_conn_id\",\n+            \"account_key_conn_id\",\n             \"sas_conn_id\",\n             \"extra__wasb__sas_conn_id\",\n             \"http_sas_conn_id\",\n", "problem_statement": "Azure BlobStorage for logging\n### Apache Airflow version\n\n3.0.2\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nInstalled Airflow 3.0.2 with helm but cannot configure Azure Blobstorage for logging.\n\n```logs\napi-server [2025-06-19T20:22:17.871+0000] {chained.py:218} WARNING - DefaultAzureCredential failed to retrieve a token from the included credentials.\napi-server Attempted credentials:\napi-server     EnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\napi-server Visit https://aka.ms/azsdk/python/identity/environmentcredential/troubleshoot to troubleshoot this issue.\napi-server     ManagedIdentityCredential: ManagedIdentityCredential authentication unavailable.\napi-server     SharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\napi-server     AzureCliCredential: Azure CLI not found on path\napi-server     AzurePowerShellCredential: PowerShell is not installed\napi-server     AzureDeveloperCliCredential: Azure Developer CLI could not be found. Please visit https://aka.ms/azure-dev for installation instructions and then,once installed, authentica\napi-server To mitigate this issue, please refer to the troubleshooting guidelines here at https://aka.ms/azsdk/python/identity/defaultazurecredential/troubleshoot.\n\n```\n\n### What you think should happen instead?\n\n_No response_\n\n### How to reproduce\n\nI set these values:\n\n```yaml\n# Environment variables for all airflow containers\nenv:\n  - name: AIRFLOW_VAR_ENVIRONMENT\n    value: dev\n  - name: AIRFLOW__LOGGING__REMOTE_LOGGING\n    value: \"True\"\n  - name: AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID\n    value: azure_blob\n  - name: AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER\n    value: \"wasb://<container-name>@<storage-account-name>.blob.core.windows.net/airflow/logs\"\n```\n\nand\n\n```yaml\nconfig:\n  logging:\n    remote_logging: \"True\"\n    remote_log_conn_id: \"azure_blob\"\n    remote_base_log_folder: \"wasb://<container-name>@<storage-account-name>.blob.core.windows.net/airflow/logs\"\n```\n\nCreated the azure connection with:\n\n```cmd\nairflow connections add 'azure_blob' \\\n  --conn-type wasb \\\n  --conn-extra '{\"account_name\": \"YOUR_ACCOUNT\", \"account_key\": \"YOUR_KEY\"}'\n```\n\n### Operating System\n\nLinux/Kubernetes\n\n### Versions of Apache Airflow Providers\n\nairflow@airflow-dev-api-server-9664568d6-5gkxx:/opt/airflow$ pip freeze | grep apache-airflow\napache-airflow==3.0.2\napache-airflow-core==3.0.2\napache-airflow-providers-amazon==9.8.0\napache-airflow-providers-celery==3.11.0\napache-airflow-providers-cncf-kubernetes==10.5.0\napache-airflow-providers-common-compat==1.7.0\napache-airflow-providers-common-io==1.6.0\napache-airflow-providers-common-messaging==1.0.2\napache-airflow-providers-common-sql==1.27.1\napache-airflow-providers-docker==4.4.0\napache-airflow-providers-elasticsearch==6.3.0\napache-airflow-providers-fab==2.2.0\napache-airflow-providers-ftp==3.13.0\napache-airflow-providers-git==0.0.2\napache-airflow-providers-google==15.1.0\napache-airflow-providers-grpc==3.8.0\napache-airflow-providers-hashicorp==4.2.0\napache-airflow-providers-http==5.3.0\napache-airflow-providers-microsoft-azure==12.4.0\napache-airflow-providers-mysql==6.3.0\napache-airflow-providers-odbc==4.10.0\napache-airflow-providers-openlineage==2.3.0\napache-airflow-providers-postgres==6.2.0\napache-airflow-providers-redis==4.1.0\napache-airflow-providers-sendgrid==4.1.0\napache-airflow-providers-sftp==5.3.0\napache-airflow-providers-slack==9.1.0\napache-airflow-providers-smtp==2.1.0\napache-airflow-providers-snowflake==6.3.1\napache-airflow-providers-ssh==4.1.0\napache-airflow-providers-standard==1.2.0\napache-airflow-task-sdk==1.0.2\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "hints_text": "Hi , plz assign to me.\nOnce this is figured out, these docs could definitely use some updating: https://airflow.apache.org/docs/apache-airflow-providers-microsoft-azure/stable/logging/index.html\nGood point! Once we\u2019ve confirmed the solution, I\u2019d be happy to help with updating the docs.\n\n", "all_hints_text": "Hi , plz assign to me.\nOnce this is figured out, these docs could definitely use some updating: https://airflow.apache.org/docs/apache-airflow-providers-microsoft-azure/stable/logging/index.html\nGood point! Once we\u2019ve confirmed the solution, I\u2019d be happy to help with updating the docs.\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/075c87ecf3f547edc6006e0d63f6d00f54b264a5"], "created_at": "2025-06-21T16:25:51Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 52295, "instance_id": "apache__airflow-52295", "issue_numbers": [52103], "base_commit": "5228dfb6993cb4b61974cc30985c08c7d1474223", "patch": "diff --git a/providers/fab/docs/auth-manager/token.rst b/providers/fab/docs/auth-manager/token.rst\nindex 35c9bb3a74a62..af610ab6ad496 100644\n--- a/providers/fab/docs/auth-manager/token.rst\n+++ b/providers/fab/docs/auth-manager/token.rst\n@@ -40,3 +40,6 @@ Example\n         }'\n \n This process will return a token that you can use in the Airflow public API requests.\n+\n+Only users from database (`AUTH_TYPE = AUTH_DB`) or from LDAP (`AUTH_TYPE = AUTH_LDAP`) can be used to generate a token.\n+See :doc:`Airflow public API <webserver-authentication>` for more details.\ndiff --git a/providers/fab/src/airflow/providers/fab/auth_manager/api_fastapi/services/login.py b/providers/fab/src/airflow/providers/fab/auth_manager/api_fastapi/services/login.py\nindex 0792efd18ce46..7024581137de3 100644\n--- a/providers/fab/src/airflow/providers/fab/auth_manager/api_fastapi/services/login.py\n+++ b/providers/fab/src/airflow/providers/fab/auth_manager/api_fastapi/services/login.py\n@@ -18,6 +18,7 @@\n \n from typing import TYPE_CHECKING, cast\n \n+from flask_appbuilder.const import AUTH_LDAP\n from starlette import status\n from starlette.exceptions import HTTPException\n \n@@ -44,14 +45,22 @@ def create_token(\n             )\n \n         auth_manager = cast(\"FabAuthManager\", get_auth_manager())\n-        user: User = auth_manager.security_manager.find_user(username=body.username)\n+        user: User | None = None\n+\n+        if auth_manager.security_manager.auth_type == AUTH_LDAP:\n+            user = auth_manager.security_manager.auth_user_ldap(\n+                body.username, body.password, rotate_session_id=False\n+            )\n+        if user is None:\n+            user = auth_manager.security_manager.auth_user_db(\n+                body.username, body.password, rotate_session_id=False\n+            )\n+\n         if not user:\n-            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid username\")\n+            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid credentials\")\n \n-        if auth_manager.security_manager.check_password(username=body.username, password=body.password):\n-            return LoginResponse(\n-                access_token=auth_manager.generate_jwt(\n-                    user=user, expiration_time_in_seconds=expiration_time_in_seconds\n-                )\n+        return LoginResponse(\n+            access_token=auth_manager.generate_jwt(\n+                user=user, expiration_time_in_seconds=expiration_time_in_seconds\n             )\n-        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid password\")\n+        )\ndiff --git a/providers/fab/src/airflow/providers/fab/auth_manager/security_manager/override.py b/providers/fab/src/airflow/providers/fab/auth_manager/security_manager/override.py\nindex 2a74e680b9151..8f0a98a832b35 100644\n--- a/providers/fab/src/airflow/providers/fab/auth_manager/security_manager/override.py\n+++ b/providers/fab/src/airflow/providers/fab/auth_manager/security_manager/override.py\n@@ -1723,7 +1723,7 @@ def get_user_roles(user=None):\n     --------------------\n     \"\"\"\n \n-    def auth_user_ldap(self, username, password):\n+    def auth_user_ldap(self, username, password, rotate_session_id=True):\n         \"\"\"\n         Authenticate user with LDAP.\n \n@@ -1890,7 +1890,8 @@ def auth_user_ldap(self, username, password):\n \n             # LOGIN SUCCESS (only if user is now registered)\n             if user:\n-                self._rotate_session_id()\n+                if rotate_session_id:\n+                    self._rotate_session_id()\n                 self.update_user_auth_stat(user)\n                 return user\n             return None\n@@ -1919,7 +1920,7 @@ def check_password(self, username, password) -> bool:\n             return False\n         return check_password_hash(user.password, password)\n \n-    def auth_user_db(self, username, password):\n+    def auth_user_db(self, username, password, rotate_session_id=True):\n         \"\"\"\n         Authenticate user, auth db style.\n \n@@ -1927,6 +1928,8 @@ def auth_user_db(self, username, password):\n             The username or registered email address\n         :param password:\n             The password, will be tested against hashed password on db\n+        :param rotate_session_id:\n+            Whether to rotate the session ID\n         \"\"\"\n         if username is None or username == \"\":\n             return None\n@@ -1942,7 +1945,8 @@ def auth_user_db(self, username, password):\n             log.info(LOGMSG_WAR_SEC_LOGIN_FAILED, username)\n             return None\n         if check_password_hash(user.password, password):\n-            self._rotate_session_id()\n+            if rotate_session_id:\n+                self._rotate_session_id()\n             self.update_user_auth_stat(user, True)\n             return user\n         self.update_user_auth_stat(user, False)\n", "test_patch": "diff --git a/providers/fab/tests/unit/fab/auth_manager/api_fastapi/services/test_login.py b/providers/fab/tests/unit/fab/auth_manager/api_fastapi/services/test_login.py\nindex 89dfd3e1ca481..dcc677d30da78 100644\n--- a/providers/fab/tests/unit/fab/auth_manager/api_fastapi/services/test_login.py\n+++ b/providers/fab/tests/unit/fab/auth_manager/api_fastapi/services/test_login.py\n@@ -17,16 +17,30 @@\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING\n-from unittest.mock import MagicMock, patch\n+from unittest.mock import ANY, MagicMock, patch\n \n import pytest\n+from flask_appbuilder.const import AUTH_DB, AUTH_LDAP, AUTH_OID\n from starlette.exceptions import HTTPException\n \n from airflow.providers.fab.auth_manager.api_fastapi.services.login import FABAuthManagerLogin\n \n-if TYPE_CHECKING:\n-    from airflow.providers.fab.auth_manager.api_fastapi.datamodels.login import LoginResponse\n+\n+@pytest.fixture\n+def auth_manager():\n+    return MagicMock()\n+\n+\n+@pytest.fixture\n+def security_manager():\n+    return MagicMock()\n+\n+\n+@pytest.fixture\n+def user():\n+    user = MagicMock()\n+    user.password = \"dummy\"\n+    return user\n \n \n @patch(\"airflow.providers.fab.auth_manager.api_fastapi.services.login.get_auth_manager\")\n@@ -34,71 +48,56 @@ class TestLogin:\n     def setup_method(\n         self,\n     ):\n-        self.dummy_auth_manager = MagicMock()\n-        self.dummy_app_builder = MagicMock()\n-        self.dummy_app = MagicMock()\n-        self.dummy_login_body = MagicMock()\n-        self.dummy_user = MagicMock()\n-        self.dummy_user.password = \"dummy\"\n-        self.dummy_security_manager = MagicMock()\n-        self.dummy_login_body.username = \"dummy\"\n-        self.dummy_login_body.password = \"dummy\"\n+        self.login_body = MagicMock()\n+        self.login_body.username = \"username\"\n+        self.login_body.password = \"password\"\n         self.dummy_token = \"DUMMY_TOKEN\"\n \n-    def test_create_token(self, get_auth_manager):\n-        get_auth_manager.return_value = self.dummy_auth_manager\n-        self.dummy_auth_manager.security_manager = self.dummy_security_manager\n-        self.dummy_security_manager.find_user.return_value = self.dummy_user\n-        self.dummy_auth_manager.generate_jwt.return_value = self.dummy_token\n-        self.dummy_security_manager.check_password.return_value = True\n+    @pytest.mark.parametrize(\n+        \"auth_type, method\",\n+        [\n+            [AUTH_DB, \"auth_user_db\"],\n+            [AUTH_LDAP, \"auth_user_ldap\"],\n+        ],\n+    )\n+    def test_create_token(self, get_auth_manager, auth_type, method, auth_manager, security_manager, user):\n+        security_manager.auth_type = auth_type\n+        getattr(security_manager, method).return_value = user\n \n-        login_response: LoginResponse = FABAuthManagerLogin.create_token(\n-            body=self.dummy_login_body,\n-            expiration_time_in_seconds=1,\n-        )\n-        assert login_response.access_token == self.dummy_token\n+        auth_manager.security_manager = security_manager\n+        auth_manager.generate_jwt.return_value = self.dummy_token\n \n-    def test_create_token_invalid_username(self, get_auth_manager):\n-        get_auth_manager.return_value = self.dummy_auth_manager\n-        self.dummy_auth_manager.security_manager = self.dummy_security_manager\n-        self.dummy_security_manager.find_user.return_value = None\n-        self.dummy_security_manager.check_password.return_value = False\n+        get_auth_manager.return_value = auth_manager\n \n-        with pytest.raises(HTTPException) as ex:\n-            FABAuthManagerLogin.create_token(\n-                body=self.dummy_login_body,\n-                expiration_time_in_seconds=1,\n-            )\n-        assert ex.value.status_code == 401\n-        assert ex.value.detail == \"Invalid username\"\n-\n-    def test_create_token_invalid_password(self, get_auth_manager):\n-        get_auth_manager.return_value = self.dummy_auth_manager\n-        self.dummy_auth_manager.security_manager = self.dummy_security_manager\n-        self.dummy_security_manager.find_user.return_value = self.dummy_user\n-        self.dummy_user.password = \"invalid_password\"\n-        self.dummy_security_manager.check_password.return_value = False\n+        result = FABAuthManagerLogin.create_token(\n+            body=self.login_body,\n+        )\n+        assert result.access_token == self.dummy_token\n+        getattr(security_manager, method).assert_called_once_with(\n+            self.login_body.username, self.login_body.password, rotate_session_id=False\n+        )\n+        auth_manager.generate_jwt.assert_called_once_with(user=user, expiration_time_in_seconds=ANY)\n \n-        with pytest.raises(HTTPException) as ex:\n-            FABAuthManagerLogin.create_token(\n-                body=self.dummy_login_body,\n-                expiration_time_in_seconds=1,\n-            )\n-        assert ex.value.status_code == 401\n-        assert ex.value.detail == \"Invalid password\"\n+    @pytest.mark.parametrize(\n+        \"auth_type, methods\",\n+        [\n+            [AUTH_DB, [\"auth_user_db\"]],\n+            [AUTH_LDAP, [\"auth_user_ldap\", \"auth_user_db\"]],\n+            [AUTH_OID, [\"auth_user_db\"]],\n+        ],\n+    )\n+    def test_create_token_no_user(\n+        self, get_auth_manager, auth_type, methods, auth_manager, security_manager, user\n+    ):\n+        security_manager.auth_type = auth_type\n+        for method in methods:\n+            getattr(security_manager, method).return_value = None\n \n-    def test_create_token_empty_user_password(self, get_auth_manager):\n-        get_auth_manager.return_value = self.dummy_auth_manager\n-        self.dummy_auth_manager.security_manager = self.dummy_security_manager\n-        self.dummy_security_manager.find_user.return_value = self.dummy_user\n-        self.dummy_login_body.username = \"\"\n-        self.dummy_login_body.password = \"\"\n-        self.dummy_security_manager.check_password.return_value = False\n+        auth_manager.security_manager = security_manager\n+        get_auth_manager.return_value = auth_manager\n \n         with pytest.raises(HTTPException) as ex:\n             FABAuthManagerLogin.create_token(\n-                body=self.dummy_login_body,\n-                expiration_time_in_seconds=1,\n+                body=self.login_body,\n             )\n-        assert ex.value.status_code == 400\n-        assert ex.value.detail == \"Username and password must be provided\"\n+        assert ex.value.status_code == 401\n", "problem_statement": "JWT Token Generation Fails for LDAP Users in Airflow 3.0 with FAB Auth Manager\n### Apache Airflow Provider(s)\n\nfab\n\n### Versions of Apache Airflow Providers\n\n`apache-airflow-providers-fab==2.2.0`\n\n### Apache Airflow version\n\n3.0.2\n\n### Operating System\n\nOS: RHEL UBI9, python3.12, Auth Manager: FAB; Authentication Backend: LDAP\n\n### Deployment\n\nOther Docker-based deployment\n\n### Deployment details\n\nCloudFormation and AWS ECS\n\n### What happened\n\nWe were using Airflow 2.9.2 with LDAP authentication. Other services use LDAP credentials to authenticate and call Airflow API to trigger DAG.\nNow we want to upgrade to Airflow 3.0.x with FAB auth manager and LDAP authentication, the JWT token generation endpoint `/auth/token` fails to authenticate LDAP users, even though these users can successfully log in through the web UI (token can be found in local storage).\n\n_Local Airflow users can generate JWT token successfully but we do not prefer using static credentials_  \n\n### What you think should happen instead\n\nLDAP users should be able to generate JWT tokens via the `/auth/token` endpoint for programmatic access to Airflow APIs\n\n### How to reproduce\n\n1. Configure Airflow 3.0 with FAB auth manager and LDAP authentication\n`\n[Environment]\nAIRFLOW__CORE__AUTH_MANAGER=airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager\nAIRFLOW__FAB__AUTH_BACKENDS=airflow.providers.fab.auth_manager.api.auth.backend.session, airflow.providers.fab.auth_manager.api.auth.backend.basic_auth\nAIRFLOW__DATABASE__EXTERNAL_DB_MANAGERS=airflow.providers.fab.auth_manager.models.db.FABDBManager\n`\n2. Create an LDAP user and verify webUI login works\n3. Attempt to generate JWT token via API `/auth/token` as describe [here](https://airflow.apache.org/docs/apache-airflow-providers-fab/stable/auth-manager/token.html)\n4. Error 401 Unauthorized\n\n\n\n### Anything else\n\nIf the bug is legit, will it be fixed soon? can you provide some estimation?\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\ncc @vincbeck \nCorrect, this is a known issue. Currently only users stored in DB can use the JWT token generation endpoint. The reason why I did not implement it is I found difficult (or lack of time during the Airflow 3 rush) to setup a LDAP server in order to test the implementation. But looking at the code, I am pretty sure this is relatively easy, I'll create a fix and potentially ask you (or someone else?) to test it?\nThanks @vincbeck for your confirmation\nAnd yes I can test it for you, we really need the fix.\nthanks @vincbeck I can test it too as soon as it is available, the exact same issue here\nI created #52295 which should fix the issue. Can you guys please try and test it? \nNice - > the change looks cool, would be indeed great to get confirmation :)\n\n", "all_hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\ncc @vincbeck \nCorrect, this is a known issue. Currently only users stored in DB can use the JWT token generation endpoint. The reason why I did not implement it is I found difficult (or lack of time during the Airflow 3 rush) to setup a LDAP server in order to test the implementation. But looking at the code, I am pretty sure this is relatively easy, I'll create a fix and potentially ask you (or someone else?) to test it?\nThanks @vincbeck for your confirmation\nAnd yes I can test it for you, we really need the fix.\nthanks @vincbeck I can test it too as soon as it is available, the exact same issue here\nI created #52295 which should fix the issue. Can you guys please try and test it? \nNice - > the change looks cool, would be indeed great to get confirmation :)\nI have tested the #52295 modifications on my Helm Chart deployed apache/airflow:3.0.2-python3.9\n image and post the result [there](https://github.com/apache/airflow/pull/52295#issuecomment-3011342756). Seems like rotate_session_id failed in auth_user_ldap.\nHi @vincbeck , I have added my test result to the https://github.com/apache/airflow/pull/52295, please have a look and let me know if there is anything else. I have attempted a fix for this but I don't quite understand how Airflow integrate with LDAP, so was quite stuck at the self. _rotate_session_id() session. Having the same warning issue.\n```\nRuntimeError: Working outside of request context.\nThis typically means that you attempted to use functionality that needed\nan active HTTP request. Consult the documentation on testing for\ninformation about how to avoid this problem.\n```\nMy understanding is that when airflow found the user in LDAP, it checks the database -> create the user if not exists -> then generate a session_id? If we generate token, we only check if the session_id of the user is still valid, but not rotating the session.\n\nMany thanks for your support @vincbeck ! \n\nHi @vincbeck the ldap auth and session_id fix in api token works fine for me\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/a17d236a55f4c3c9334a21dc131e3626966f7d91"], "created_at": "2025-06-26T14:46:07Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 52265, "instance_id": "apache__airflow-52265", "issue_numbers": [52106], "base_commit": "83348cda3aa95df1157bb09f7d88bdd574f4a5cb", "patch": "diff --git a/.dockerignore b/.dockerignore\nindex c50ed5ae24ee6..75e6291445ab6 100644\n--- a/.dockerignore\n+++ b/.dockerignore\n@@ -38,6 +38,7 @@\n !providers/\n !task-sdk/\n !airflow-ctl/\n+!go-sdk/\n \n # Add all \"test\" distributions\n !tests\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 15b5a3605d5ca..5a9adede382c6 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -213,7 +213,7 @@ repos:\n           ^scripts/ci/pre_commit/update_installers_and_pre_commit\\.py$\n         pass_filenames: false\n         require_serial: true\n-        additional_dependencies: ['pyyaml>=6.0.2', 'rich>=12.4.4', 'requests>=2.31.0']\n+        additional_dependencies: ['pyyaml>=6.0.2', 'rich>=12.4.4', 'requests>=2.31.0',\"packaging>=25\"]\n       - id: update-chart-dependencies\n         name: Update chart dependencies to latest (manual)\n         entry: ./scripts/ci/pre_commit/update_chart_dependencies.py\ndiff --git a/Dockerfile.ci b/Dockerfile.ci\nindex 6ba138121af1a..9e7c1afd6a2de 100644\n--- a/Dockerfile.ci\n+++ b/Dockerfile.ci\n@@ -16,13 +16,13 @@\n #\n # WARNING: THIS DOCKERFILE IS NOT INTENDED FOR PRODUCTION USE OR DEPLOYMENT.\n #\n-ARG PYTHON_BASE_IMAGE=\"python:3.10-slim-bookworm\"\n+ARG BASE_IMAGE=\"debian:bookworm-slim\"\n \n ##############################################################################################\n # This is the script image where we keep all inlined bash scripts needed in other segments\n-# We use PYTHON_BASE_IMAGE to make sure that the scripts are different for different platforms.\n+# We use BASE_IMAGE to make sure that the scripts are different for different platforms.\n ##############################################################################################\n-FROM ${PYTHON_BASE_IMAGE} as scripts\n+FROM ${BASE_IMAGE} as scripts\n \n ##############################################################################################\n # Please DO NOT modify the inlined scripts manually. The content of those files will be\n@@ -31,22 +31,27 @@ FROM ${PYTHON_BASE_IMAGE} as scripts\n # make the PROD Dockerfile standalone\n ##############################################################################################\n \n-# The content below is automatically copied from scripts/docker/install_os_dependencies.sh\n-COPY <<\"EOF\" /install_os_dependencies.sh\n+# The content below is automatically copied from scripts/docker/install_os_dependencies_ci.sh\n+COPY <<\"EOF\" /install_os_dependencies_ci.sh\n #!/usr/bin/env bash\n set -euo pipefail\n \n if [[ \"$#\" != 1 ]]; then\n-    echo \"ERROR! There should be 'runtime' or 'dev' parameter passed as argument.\".\n+    echo \"ERROR! There should be 'runtime', 'ci' or 'dev' parameter passed as argument.\".\n     exit 1\n fi\n \n+AIRFLOW_PYTHON_VERSION=${AIRFLOW_PYTHON_VERSION:-v3.10.10}\n+GOLANG_MAJOR_MINOR_VERSION=${GOLANG_MAJOR_MINOR_VERSION:-1.24.4}\n+\n if [[ \"${1}\" == \"runtime\" ]]; then\n     INSTALLATION_TYPE=\"RUNTIME\"\n elif   [[ \"${1}\" == \"dev\" ]]; then\n-    INSTALLATION_TYPE=\"dev\"\n+    INSTALLATION_TYPE=\"DEV\"\n+elif   [[ \"${1}\" == \"ci\" ]]; then\n+    INSTALLATION_TYPE=\"CI\"\n else\n-    echo \"ERROR! Wrong argument. Passed ${1} and it should be one of 'runtime' or 'dev'.\".\n+    echo \"ERROR! Wrong argument. Passed ${1} and it should be one of 'runtime', 'ci' or 'dev'.\".\n     exit 1\n fi\n \n@@ -56,7 +61,10 @@ function get_dev_apt_deps() {\n freetds-bin freetds-dev git graphviz graphviz-dev krb5-user ldap-utils libev4 libev-dev libffi-dev libgeos-dev \\\n libkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\\n libssl-dev libxmlsec1 libxmlsec1-dev locales lsb-release openssh-client pkgconf sasl2-bin \\\n-software-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev\"\n+software-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev \\\n+gdb lcov pkg-config libbz2-dev libgdbm-dev libgdbm-compat-dev liblzma-dev \\\n+libncurses5-dev libreadline6-dev libsqlite3-dev lzma lzma-dev tk-dev uuid-dev \\\n+libzstd-dev\"\n         export DEV_APT_DEPS\n     fi\n }\n@@ -143,14 +151,35 @@ function install_debian_runtime_dependencies() {\n     rm -rf /var/lib/apt/lists/* /var/log/*\n }\n \n+function install_python() {\n+    git clone --branch \"${AIRFLOW_PYTHON_VERSION}\" --depth 1 https://github.com/python/cpython.git\n+    cd cpython\n+    ./configure --enable-optimizations\n+    make -s -j \"$(nproc)\" install\n+    ln -s /usr/local/bin/python3 /usr/local/bin/python\n+    ln -s /usr/local/bin/pip3 /usr/local/bin/pip\n+    cd ..\n+    rm -rf cpython\n+}\n+\n+function install_golang() {\n+    curl \"https://dl.google.com/go/go${GOLANG_MAJOR_MINOR_VERSION}.linux-$(dpkg --print-architecture).tar.gz\" -o \"go${GOLANG_MAJOR_MINOR_VERSION}.linux.tar.gz\"\n+    rm -rf /usr/local/go && tar -C /usr/local -xzf go\"${GOLANG_MAJOR_MINOR_VERSION}\".linux.tar.gz\n+}\n+\n if [[ \"${INSTALLATION_TYPE}\" == \"RUNTIME\" ]]; then\n     get_runtime_apt_deps\n     install_debian_runtime_dependencies\n     install_docker_cli\n \n else\n+\n     get_dev_apt_deps\n     install_debian_dev_dependencies\n+    install_python\n+    if [[ \"${INSTALLATION_TYPE}\" == \"CI\" ]]; then\n+        install_golang\n+    fi\n     install_docker_cli\n fi\n EOF\n@@ -925,7 +954,7 @@ function environment_initialization() {\n     CI=${CI:=\"false\"}\n \n     # Added to have run-tests on path\n-    export PATH=${PATH}:${AIRFLOW_SOURCES}\n+    export PATH=${PATH}:${AIRFLOW_SOURCES}:/usr/local/go/bin/\n \n     mkdir -pv \"${AIRFLOW_HOME}/logs/\"\n \n@@ -1237,13 +1266,13 @@ COPY <<\"EOF\" /entrypoint_exec.sh\n exec /bin/bash \"${@}\"\n EOF\n \n-FROM ${PYTHON_BASE_IMAGE} as main\n+FROM ${BASE_IMAGE} as main\n \n # Nolog bash flag is currently ignored - but you can replace it with other flags (for example\n # xtrace - to show commands executed)\n SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"nounset\", \"-o\", \"nolog\", \"-c\"]\n \n-ARG PYTHON_BASE_IMAGE\n+ARG BASE_IMAGE\n ARG AIRFLOW_IMAGE_REPOSITORY=\"https://github.com/apache/airflow\"\n \n # By increasing this number we can do force build of all dependencies.\n@@ -1253,7 +1282,7 @@ ARG AIRFLOW_IMAGE_REPOSITORY=\"https://github.com/apache/airflow\"\n ARG DEPENDENCIES_EPOCH_NUMBER=\"15\"\n \n # Make sure noninteractive debian install is used and language variables set\n-ENV PYTHON_BASE_IMAGE=${PYTHON_BASE_IMAGE} \\\n+ENV BASE_IMAGE=${BASE_IMAGE} \\\n     DEBIAN_FRONTEND=noninteractive LANGUAGE=C.UTF-8 LANG=C.UTF-8 LC_ALL=C.UTF-8 \\\n     LC_CTYPE=C.UTF-8 LC_MESSAGES=C.UTF-8 \\\n     DEPENDENCIES_EPOCH_NUMBER=${DEPENDENCIES_EPOCH_NUMBER} \\\n@@ -1264,7 +1293,7 @@ ENV PYTHON_BASE_IMAGE=${PYTHON_BASE_IMAGE} \\\n     UV_CACHE_DIR=/root/.cache/uv\n \n \n-RUN echo \"Base image version: ${PYTHON_BASE_IMAGE}\"\n+RUN echo \"Base image version: ${BASE_IMAGE}\"\n \n ARG DEV_APT_COMMAND=\"\"\n ARG ADDITIONAL_DEV_APT_COMMAND=\"\"\n@@ -1279,8 +1308,12 @@ ENV DEV_APT_COMMAND=${DEV_APT_COMMAND} \\\n     ADDITIONAL_DEV_APT_DEPS=${ADDITIONAL_DEV_APT_DEPS} \\\n     ADDITIONAL_DEV_APT_COMMAND=${ADDITIONAL_DEV_APT_COMMAND}\n \n-COPY --from=scripts install_os_dependencies.sh /scripts/docker/\n-RUN bash /scripts/docker/install_os_dependencies.sh dev\n+ENV AIRFLOW_PYTHON_VERSION=v3.10.18\n+ENV GOLANG_MAJOR_MINOR_VERSION=1.24.4\n+\n+COPY --from=scripts install_os_dependencies_ci.sh /scripts/docker/\n+\n+RUN bash /scripts/docker/install_os_dependencies_ci.sh ci\n \n COPY --from=scripts common.sh /scripts/docker/\n \ndiff --git a/scripts/ci/pre_commit/update_installers_and_pre_commit.py b/scripts/ci/pre_commit/update_installers_and_pre_commit.py\nindex ef1b745f9fe5d..8613949f7b306 100755\n--- a/scripts/ci/pre_commit/update_installers_and_pre_commit.py\n+++ b/scripts/ci/pre_commit/update_installers_and_pre_commit.py\n@@ -24,6 +24,7 @@\n from pathlib import Path\n \n import requests\n+from packaging.version import Version\n \n sys.path.insert(0, str(Path(__file__).parent.resolve()))  # make sure common_precommit_utils is imported\n from common_precommit_utils import AIRFLOW_CORE_ROOT_PATH, AIRFLOW_ROOT_PATH, console\n@@ -65,6 +66,35 @@ def get_latest_pypi_version(package_name: str) -> str:\n     return latest_version\n \n \n+def get_latest_python_version(python_major_minor: str, github_token: str | None) -> str | None:\n+    latest_version = None\n+    # Matches versions of vA.B.C and vA.B where C can only be numeric and v is optional\n+    version_match = re.compile(rf\"^v?{python_major_minor}\\.?\\d*$\")\n+    headers = {\"User-Agent\": \"Python requests\"}\n+    if github_token:\n+        headers[\"Authorization\"] = f\"Bearer {github_token}\"\n+    for i in range(5):\n+        response = requests.get(\n+            f\"https://api.github.com/repos/python/cpython/tags?per_page=100&page={i + 1}\",\n+            headers=headers,\n+        )\n+        response.raise_for_status()  # Ensure we got a successful response\n+        data = response.json()\n+        versions = [str(tag[\"name\"]) for tag in data if version_match.match(tag.get(\"name\", \"\"))]\n+        if versions:\n+            latest_version = sorted(versions, key=Version, reverse=True)[0]\n+            break\n+    return latest_version\n+\n+\n+def get_latest_golang_version() -> str:\n+    response = requests.get(\"https://go.dev/dl/?mode=json\")\n+    response.raise_for_status()  # Ensure we got a successful response\n+    versions = response.json()\n+    stable_versions = [release[\"version\"].replace(\"go\", \"\") for release in versions if release[\"stable\"]]\n+    return sorted(stable_versions, key=Version, reverse=True)[0]\n+\n+\n def get_latest_lts_node_version() -> str:\n     response = requests.get(\"https://nodejs.org/dist/index.json\")\n     response.raise_for_status()  # Ensure we got a successful response\n@@ -92,6 +122,16 @@ class Quoting(Enum):\n     (re.compile(r\"(\\| *`AIRFLOW_PIP_VERSION` *\\| *)(`[0-9.]+`)( *\\|)\"), Quoting.REVERSE_SINGLE_QUOTED),\n ]\n \n+PYTHON_PATTERNS: list[tuple[re.Pattern, Quoting]] = [\n+    (re.compile(r\"(AIRFLOW_PYTHON_VERSION=)(v[0-9.]+)\"), Quoting.UNQUOTED),\n+    (re.compile(r\"(\\| *`AIRFLOW_PYTHON_VERSION` *\\| *)(`v[0-9.]+`)( *\\|)\"), Quoting.REVERSE_SINGLE_QUOTED),\n+]\n+\n+GOLANG_PATTERNS: list[tuple[re.Pattern, Quoting]] = [\n+    (re.compile(r\"(GOLANG_MAJOR_MINOR_VERSION=)([0-9.]+)\"), Quoting.UNQUOTED),\n+    (re.compile(r\"(\\| *`GOLANG_MAJOR_MINOR_VERSION` *\\| *)(`[0-9.]+`)( *\\|)\"), Quoting.REVERSE_SINGLE_QUOTED),\n+]\n+\n UV_PATTERNS: list[tuple[re.Pattern, Quoting]] = [\n     (re.compile(r\"(AIRFLOW_UV_VERSION=)([0-9.]+)\"), Quoting.UNQUOTED),\n     (re.compile(r\"(uv>=)([0-9.]+)\"), Quoting.UNQUOTED),\n@@ -167,10 +207,16 @@ def get_replacement(value: str, quoting: Quoting) -> str:\n \n UPGRADE_UV: bool = os.environ.get(\"UPGRADE_UV\", \"true\").lower() == \"true\"\n UPGRADE_PIP: bool = os.environ.get(\"UPGRADE_PIP\", \"true\").lower() == \"true\"\n+UPGRADE_PYTHON: bool = os.environ.get(\"UPGRADE_PYTHON\", \"true\").lower() == \"true\"\n+UPGRADE_GOLANG: bool = os.environ.get(\"UPGRADE_GOLANG\", \"true\").lower() == \"true\"\n UPGRADE_SETUPTOOLS: bool = os.environ.get(\"UPGRADE_SETUPTOOLS\", \"true\").lower() == \"true\"\n UPGRADE_PRE_COMMIT: bool = os.environ.get(\"UPGRADE_PRE_COMMIT\", \"true\").lower() == \"true\"\n UPGRADE_NODE_LTS: bool = os.environ.get(\"UPGRADE_NODE_LTS\", \"true\").lower() == \"true\"\n \n+PYTHON_VERSION: str = os.environ.get(\"PYTHON_VERSION\", \"3.10\")\n+\n+GITHUB_TOKEN: str | None = os.environ.get(\"GITHUB_TOKEN\")\n+\n \n def replace_version(pattern: re.Pattern[str], version: str, text: str, keep_total_length: bool = True) -> str:\n     # Assume that the pattern has up to 3 replacement groups:\n@@ -201,6 +247,8 @@ def replacer(match):\n \n if __name__ == \"__main__\":\n     changed = False\n+    python_version = get_latest_python_version(PYTHON_VERSION, GITHUB_TOKEN)\n+    golang_version = get_latest_golang_version()\n     pip_version = get_latest_pypi_version(\"pip\")\n     uv_version = get_latest_pypi_version(\"uv\")\n     setuptools_version = get_latest_pypi_version(\"setuptools\")\n@@ -217,6 +265,18 @@ def replacer(match):\n                 new_content = replace_version(\n                     line_pattern, get_replacement(pip_version, quoting), new_content, keep_length\n                 )\n+        if UPGRADE_PYTHON and python_version:\n+            console.print(f\"[bright_blue]Latest python {PYTHON_VERSION} version: {python_version}\")\n+            for line_pattern, quoting in PYTHON_PATTERNS:\n+                new_content = replace_version(\n+                    line_pattern, get_replacement(python_version, quoting), new_content, keep_length\n+                )\n+        if UPGRADE_GOLANG:\n+            console.print(f\"[bright_blue]Latest golang version: {golang_version}\")\n+            for line_pattern, quoting in GOLANG_PATTERNS:\n+                new_content = replace_version(\n+                    line_pattern, get_replacement(golang_version, quoting), new_content, keep_length\n+                )\n         if UPGRADE_SETUPTOOLS:\n             console.print(f\"[bright_blue]Latest setuptools version: {setuptools_version}\")\n             for line_pattern, quoting in SETUPTOOLS_PATTERNS:\ndiff --git a/scripts/docker/entrypoint_ci.sh b/scripts/docker/entrypoint_ci.sh\nindex e215ce642a0df..fef63aa88f068 100755\n--- a/scripts/docker/entrypoint_ci.sh\n+++ b/scripts/docker/entrypoint_ci.sh\n@@ -130,7 +130,7 @@ function environment_initialization() {\n     CI=${CI:=\"false\"}\n \n     # Added to have run-tests on path\n-    export PATH=${PATH}:${AIRFLOW_SOURCES}\n+    export PATH=${PATH}:${AIRFLOW_SOURCES}:/usr/local/go/bin/\n \n     mkdir -pv \"${AIRFLOW_HOME}/logs/\"\n \ndiff --git a/scripts/docker/install_os_dependencies_ci.sh b/scripts/docker/install_os_dependencies_ci.sh\nnew file mode 100644\nindex 0000000000000..b2e6294dbe07c\n--- /dev/null\n+++ b/scripts/docker/install_os_dependencies_ci.sh\n@@ -0,0 +1,166 @@\n+#!/usr/bin/env bash\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+# shellcheck shell=bash\n+set -euo pipefail\n+\n+if [[ \"$#\" != 1 ]]; then\n+    echo \"ERROR! There should be 'runtime', 'ci' or 'dev' parameter passed as argument.\".\n+    exit 1\n+fi\n+\n+AIRFLOW_PYTHON_VERSION=${AIRFLOW_PYTHON_VERSION:-v3.10.10}\n+GOLANG_MAJOR_MINOR_VERSION=${GOLANG_MAJOR_MINOR_VERSION:-1.24.4}\n+\n+if [[ \"${1}\" == \"runtime\" ]]; then\n+    INSTALLATION_TYPE=\"RUNTIME\"\n+elif   [[ \"${1}\" == \"dev\" ]]; then\n+    INSTALLATION_TYPE=\"DEV\"\n+elif   [[ \"${1}\" == \"ci\" ]]; then\n+    INSTALLATION_TYPE=\"CI\"\n+else\n+    echo \"ERROR! Wrong argument. Passed ${1} and it should be one of 'runtime', 'ci' or 'dev'.\".\n+    exit 1\n+fi\n+\n+function get_dev_apt_deps() {\n+    if [[ \"${DEV_APT_DEPS=}\" == \"\" ]]; then\n+        DEV_APT_DEPS=\"apt-transport-https apt-utils build-essential ca-certificates dirmngr \\\n+freetds-bin freetds-dev git graphviz graphviz-dev krb5-user ldap-utils libev4 libev-dev libffi-dev libgeos-dev \\\n+libkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\\n+libssl-dev libxmlsec1 libxmlsec1-dev locales lsb-release openssh-client pkgconf sasl2-bin \\\n+software-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev \\\n+gdb lcov pkg-config libbz2-dev libgdbm-dev libgdbm-compat-dev liblzma-dev \\\n+libncurses5-dev libreadline6-dev libsqlite3-dev lzma lzma-dev tk-dev uuid-dev \\\n+libzstd-dev\"\n+        export DEV_APT_DEPS\n+    fi\n+}\n+\n+function get_runtime_apt_deps() {\n+    local debian_version\n+    local debian_version_apt_deps\n+    # Get debian version without installing lsb_release\n+    # shellcheck disable=SC1091\n+    debian_version=$(. /etc/os-release;   printf '%s\\n' \"$VERSION_CODENAME\";)\n+    echo\n+    echo \"DEBIAN CODENAME: ${debian_version}\"\n+    echo\n+    debian_version_apt_deps=\"libffi8 libldap-2.5-0 libssl3 netcat-openbsd\"\n+    echo\n+    echo \"APPLIED INSTALLATION CONFIGURATION FOR DEBIAN VERSION: ${debian_version}\"\n+    echo\n+    if [[ \"${RUNTIME_APT_DEPS=}\" == \"\" ]]; then\n+        RUNTIME_APT_DEPS=\"apt-transport-https apt-utils ca-certificates \\\n+curl dumb-init freetds-bin git krb5-user libev4 libgeos-dev \\\n+ldap-utils libsasl2-2 libsasl2-modules libxmlsec1 locales ${debian_version_apt_deps} \\\n+lsb-release openssh-client python3-selinux rsync sasl2-bin sqlite3 sudo unixodbc\"\n+        export RUNTIME_APT_DEPS\n+    fi\n+}\n+\n+function install_docker_cli() {\n+    apt-get update\n+    apt-get install ca-certificates curl\n+    install -m 0755 -d /etc/apt/keyrings\n+    curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc\n+    chmod a+r /etc/apt/keyrings/docker.asc\n+    # shellcheck disable=SC1091\n+    echo \\\n+      \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \\\n+      $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n+      tee /etc/apt/sources.list.d/docker.list > /dev/null\n+    apt-get update\n+    apt-get install -y --no-install-recommends docker-ce-cli\n+}\n+\n+function install_debian_dev_dependencies() {\n+    apt-get update\n+    apt-get install -yqq --no-install-recommends apt-utils >/dev/null 2>&1\n+    apt-get install -y --no-install-recommends curl gnupg2 lsb-release\n+    # shellcheck disable=SC2086\n+    export ${ADDITIONAL_DEV_APT_ENV?}\n+    if [[ ${DEV_APT_COMMAND} != \"\" ]]; then\n+        bash -o pipefail -o errexit -o nounset -o nolog -c \"${DEV_APT_COMMAND}\"\n+    fi\n+    if [[ ${ADDITIONAL_DEV_APT_COMMAND} != \"\" ]]; then\n+        bash -o pipefail -o errexit -o nounset -o nolog -c \"${ADDITIONAL_DEV_APT_COMMAND}\"\n+    fi\n+    apt-get update\n+    local debian_version\n+    local debian_version_apt_deps\n+    # Get debian version without installing lsb_release\n+    # shellcheck disable=SC1091\n+    debian_version=$(. /etc/os-release;   printf '%s\\n' \"$VERSION_CODENAME\";)\n+    echo\n+    echo \"DEBIAN CODENAME: ${debian_version}\"\n+    echo\n+    # shellcheck disable=SC2086\n+    apt-get install -y --no-install-recommends ${DEV_APT_DEPS} ${ADDITIONAL_DEV_APT_DEPS}\n+}\n+\n+function install_debian_runtime_dependencies() {\n+    apt-get update\n+    apt-get install --no-install-recommends -yqq apt-utils >/dev/null 2>&1\n+    apt-get install -y --no-install-recommends curl gnupg2 lsb-release\n+    # shellcheck disable=SC2086\n+    export ${ADDITIONAL_RUNTIME_APT_ENV?}\n+    if [[ \"${RUNTIME_APT_COMMAND}\" != \"\" ]]; then\n+        bash -o pipefail -o errexit -o nounset -o nolog -c \"${RUNTIME_APT_COMMAND}\"\n+    fi\n+    if [[ \"${ADDITIONAL_RUNTIME_APT_COMMAND}\" != \"\" ]]; then\n+        bash -o pipefail -o errexit -o nounset -o nolog -c \"${ADDITIONAL_RUNTIME_APT_COMMAND}\"\n+    fi\n+    apt-get update\n+    # shellcheck disable=SC2086\n+    apt-get install -y --no-install-recommends ${RUNTIME_APT_DEPS} ${ADDITIONAL_RUNTIME_APT_DEPS}\n+    apt-get autoremove -yqq --purge\n+    apt-get clean\n+    rm -rf /var/lib/apt/lists/* /var/log/*\n+}\n+\n+function install_python() {\n+    git clone --branch \"${AIRFLOW_PYTHON_VERSION}\" --depth 1 https://github.com/python/cpython.git\n+    cd cpython\n+    ./configure --enable-optimizations\n+    make -s -j \"$(nproc)\" install\n+    ln -s /usr/local/bin/python3 /usr/local/bin/python\n+    ln -s /usr/local/bin/pip3 /usr/local/bin/pip\n+    cd ..\n+    rm -rf cpython\n+}\n+\n+function install_golang() {\n+    curl \"https://dl.google.com/go/go${GOLANG_MAJOR_MINOR_VERSION}.linux-$(dpkg --print-architecture).tar.gz\" -o \"go${GOLANG_MAJOR_MINOR_VERSION}.linux.tar.gz\"\n+    rm -rf /usr/local/go && tar -C /usr/local -xzf go\"${GOLANG_MAJOR_MINOR_VERSION}\".linux.tar.gz\n+}\n+\n+if [[ \"${INSTALLATION_TYPE}\" == \"RUNTIME\" ]]; then\n+    get_runtime_apt_deps\n+    install_debian_runtime_dependencies\n+    install_docker_cli\n+\n+else\n+\n+    get_dev_apt_deps\n+    install_debian_dev_dependencies\n+    install_python\n+    if [[ \"${INSTALLATION_TYPE}\" == \"CI\" ]]; then\n+        install_golang\n+    fi\n+    install_docker_cli\n+fi\n", "test_patch": "diff --git a/.github/workflows/basic-tests.yml b/.github/workflows/basic-tests.yml\nindex 161ba36d58043..5c395c3c30268 100644\n--- a/.github/workflows/basic-tests.yml\n+++ b/.github/workflows/basic-tests.yml\n@@ -292,6 +292,8 @@ jobs:\n         if: always()\n         env:\n           UPGRADE_UV: \"true\"\n+          UPGRADE_PYTHON: \"false\"\n+          UPGRADE_GOLANG: \"true\"\n           UPGRADE_PIP: \"false\"\n           UPGRADE_PRE_COMMIT: \"false\"\n           UPGRADE_NODE_LTS: \"false\"\n@@ -303,6 +305,8 @@ jobs:\n         if: always()\n         env:\n           UPGRADE_UV: \"false\"\n+          UPGRADE_PYTHON: \"true\"\n+          UPGRADE_GOLANG: \"false\"\n           UPGRADE_PIP: \"true\"\n           UPGRADE_PRE_COMMIT: \"true\"\n           UPGRADE_NODE_LTS: \"true\"\n", "problem_statement": "Change sources of our docker images\n### Body\n\nThe \"official\" Python images are far less official than we thought. They are built by the community and while they are somewhat supervised by the \"official Docker image program\" there is really little control over the build and release process.\n\nWe should change the way we build our images to install Python on base debian image from \"official\" releases rather than use \"python\" base images for it.\n\nThat has also some benefits - for example setuptools are quite old in those images and they tend to have security issues - if we decide when we rebuild the base images we can have better control over what is in the images. \n\nWe might also finally plug it in with automated snyk scanning and break releases if snyk does not pass.\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.\n", "hints_text": "Also we should add optional golang installation when we build the image.\nCurrently we are building our images by passing it image it should be build from\n\nThe command is similar to:\n\n```\ndocker buildx build --load --builder default --progress=auto --cache-from=ghcr.io/apache/airflow/v2-11-test/ci/python3.9:cache-linux-amd64 --pull --build-arg AIRFLOW_BRANCH=v2-11-test --build-arg AIRFLOW_CONSTRAINTS_MODE=constraints-source-providers --build-arg AIRFLOW_CONSTRAINTS_REFERENCE=constraints-2-11 --build-arg AIRFLOW_EXTRAS=devel-ci --build-arg AIRFLOW_IMAGE_DATE_CREATED=2025-06-23T18:48:03Z --build-arg AIRFLOW_IMAGE_REPOSITORY=https://github.com/apache/airflow --build-arg AIRFLOW_USE_UV=true --build-arg UV_HTTP_TIMEOUT=300 --build-arg AIRFLOW_VERSION=2.11.0.dev0 --build-arg BUILD_ID=0 --build-arg CONSTRAINTS_GITHUB_REPOSITORY=apache/airflow --build-arg PYTHON_BASE_IMAGE=python:3.9-slim-bookworm --build-arg BUILD_PROGRESS=auto --build-arg INSTALL_MYSQL_CLIENT_TYPE=mariadb --build-arg VERSION_SUFFIX_FOR_PYPI=dev0 -t ghcr.io/apache/airflow/v2-11-test/ci/python3.9 --target main . -f Dockerfile.ci --platform linux/amd64\n\n```\n\nThe importan thing is this arg:\n\n*  PYTHON_BASE_IMAGE=python:3.9-slim-bookworm\n\nRather than that, we should have BASE_IMAGE (`debian:slim-bookworm`) and pass PYTHON_MAJOR_MINOR_VERSION simplyh (this is the name we use internally and we automaticallly extract it in the images from installed python version.\n\nThen - one of the first step in our build should be to install python - ideally from official sources - but maybe we find other, faster way (building python is SLOW and requires build tools.\n\nWe should first start with CI images and apply the changes toi PROD after we test it for a while on CI.\n\n\n\n\n\n\n\n\n\n\n", "all_hints_text": "Also we should add optional golang installation when we build the image.\nCurrently we are building our images by passing it image it should be build from\n\nThe command is similar to:\n\n```\ndocker buildx build --load --builder default --progress=auto --cache-from=ghcr.io/apache/airflow/v2-11-test/ci/python3.9:cache-linux-amd64 --pull --build-arg AIRFLOW_BRANCH=v2-11-test --build-arg AIRFLOW_CONSTRAINTS_MODE=constraints-source-providers --build-arg AIRFLOW_CONSTRAINTS_REFERENCE=constraints-2-11 --build-arg AIRFLOW_EXTRAS=devel-ci --build-arg AIRFLOW_IMAGE_DATE_CREATED=2025-06-23T18:48:03Z --build-arg AIRFLOW_IMAGE_REPOSITORY=https://github.com/apache/airflow --build-arg AIRFLOW_USE_UV=true --build-arg UV_HTTP_TIMEOUT=300 --build-arg AIRFLOW_VERSION=2.11.0.dev0 --build-arg BUILD_ID=0 --build-arg CONSTRAINTS_GITHUB_REPOSITORY=apache/airflow --build-arg PYTHON_BASE_IMAGE=python:3.9-slim-bookworm --build-arg BUILD_PROGRESS=auto --build-arg INSTALL_MYSQL_CLIENT_TYPE=mariadb --build-arg VERSION_SUFFIX_FOR_PYPI=dev0 -t ghcr.io/apache/airflow/v2-11-test/ci/python3.9 --target main . -f Dockerfile.ci --platform linux/amd64\n\n```\n\nThe importan thing is this arg:\n\n*  PYTHON_BASE_IMAGE=python:3.9-slim-bookworm\n\nRather than that, we should have BASE_IMAGE (`debian:slim-bookworm`) and pass PYTHON_MAJOR_MINOR_VERSION simplyh (this is the name we use internally and we automaticallly extract it in the images from installed python version.\n\nThen - one of the first step in our build should be to install python - ideally from official sources - but maybe we find other, faster way (building python is SLOW and requires build tools.\n\nWe should first start with CI images and apply the changes toi PROD after we test it for a while on CI.\n\n\n\n\n\n\n\n\n\nThis one still needs PROD image update - reopening.\nLet's use the CI image for a week or so and then we apply it to PROD image.\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/b3ef993e2c08ea1abbd4a3ad59c97342d7d49b7e", "https://github.com/apache/airflow/commit/c656f94f1b86518fa263a3804f9616594bf45421", "https://github.com/apache/airflow/commit/437b5cb63b45c0dcf490306aa91e1cc7713e6696", "https://github.com/apache/airflow/commit/354a06530f8adfe5e00eb0644c320bb2b5c3c6ec", "https://github.com/apache/airflow/commit/3b5095c3be49a6025f5dd635a3d1006e53be93f0", "https://github.com/apache/airflow/commit/7810a2b193602b7d631e6048bc2696b19a04d1b0", "https://github.com/apache/airflow/commit/6bbe6e1051ff6c4134ff49f8a6eb26b37f2f8eab", "https://github.com/apache/airflow/commit/f00fbf0985a906f29af78602b034bccb4cd4b757", "https://github.com/apache/airflow/commit/f4bb5e5358e0a639ff6eb275910ce17282a86e4a", "https://github.com/apache/airflow/commit/ee190524d71bb319f98fdf388137889aa9a7b49d", "https://github.com/apache/airflow/commit/dcf109de7784dc63f18d0a1c192e7619e84f955d"], "created_at": "2025-06-25T18:49:30Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 53150, "instance_id": "apache__airflow-53150", "issue_numbers": [52106], "base_commit": "d3dbc28effaee0e02f0d419b150c11d708d53377", "patch": "diff --git a/.dockerignore b/.dockerignore\nindex c50ed5ae24ee6..75e6291445ab6 100644\n--- a/.dockerignore\n+++ b/.dockerignore\n@@ -38,6 +38,7 @@\n !providers/\n !task-sdk/\n !airflow-ctl/\n+!go-sdk/\n \n # Add all \"test\" distributions\n !tests\ndiff --git a/.github/workflows/additional-ci-image-checks.yml b/.github/workflows/additional-ci-image-checks.yml\nindex 024f4a4ea0a93..5709e658bfd7e 100644\n--- a/.github/workflows/additional-ci-image-checks.yml\n+++ b/.github/workflows/additional-ci-image-checks.yml\n@@ -150,4 +150,4 @@ jobs:\n           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         run: echo \"$GITHUB_TOKEN\" | docker login ghcr.io -u \"$actor\" --password-stdin\n       - name: \"Check that image builds quickly\"\n-        run: breeze shell --max-time 600 --platform \"${PLATFORM}\"\n+        run: breeze shell --max-time 900 --platform \"${PLATFORM}\"\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex c7c263a2752e2..fd869767d7555 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -213,7 +213,7 @@ repos:\n           ^scripts/ci/pre_commit/update_installers_and_pre_commit\\.py$\n         pass_filenames: false\n         require_serial: true\n-        additional_dependencies: ['pyyaml>=6.0.2', 'rich>=12.4.4', 'requests>=2.31.0']\n+        additional_dependencies: ['pyyaml>=6.0.2', 'rich>=12.4.4', 'requests>=2.31.0',\"packaging>=25\"]\n       - id: update-chart-dependencies\n         name: Update chart dependencies to latest (manual)\n         entry: ./scripts/ci/pre_commit/update_chart_dependencies.py\ndiff --git a/Dockerfile.ci b/Dockerfile.ci\nindex deee1b6d97cd1..a65fc4363ccd0 100644\n--- a/Dockerfile.ci\n+++ b/Dockerfile.ci\n@@ -16,13 +16,13 @@\n #\n # WARNING: THIS DOCKERFILE IS NOT INTENDED FOR PRODUCTION USE OR DEPLOYMENT.\n #\n-ARG PYTHON_BASE_IMAGE=\"python:3.10-slim-bookworm\"\n+ARG BASE_IMAGE=\"debian:bookworm-slim\"\n \n ##############################################################################################\n # This is the script image where we keep all inlined bash scripts needed in other segments\n-# We use PYTHON_BASE_IMAGE to make sure that the scripts are different for different platforms.\n+# We use BASE_IMAGE to make sure that the scripts are different for different platforms.\n ##############################################################################################\n-FROM ${PYTHON_BASE_IMAGE} as scripts\n+FROM ${BASE_IMAGE} as scripts\n \n ##############################################################################################\n # Please DO NOT modify the inlined scripts manually. The content of those files will be\n@@ -31,22 +31,27 @@ FROM ${PYTHON_BASE_IMAGE} as scripts\n # make the PROD Dockerfile standalone\n ##############################################################################################\n \n-# The content below is automatically copied from scripts/docker/install_os_dependencies.sh\n-COPY <<\"EOF\" /install_os_dependencies.sh\n+# The content below is automatically copied from scripts/docker/install_os_dependencies_ci.sh\n+COPY <<\"EOF\" /install_os_dependencies_ci.sh\n #!/usr/bin/env bash\n set -euo pipefail\n \n if [[ \"$#\" != 1 ]]; then\n-    echo \"ERROR! There should be 'runtime' or 'dev' parameter passed as argument.\".\n+    echo \"ERROR! There should be 'runtime', 'ci' or 'dev' parameter passed as argument.\".\n     exit 1\n fi\n \n+AIRFLOW_PYTHON_VERSION=${AIRFLOW_PYTHON_VERSION:-v3.10.10}\n+GOLANG_MAJOR_MINOR_VERSION=${GOLANG_MAJOR_MINOR_VERSION:-1.24.4}\n+\n if [[ \"${1}\" == \"runtime\" ]]; then\n     INSTALLATION_TYPE=\"RUNTIME\"\n elif   [[ \"${1}\" == \"dev\" ]]; then\n-    INSTALLATION_TYPE=\"dev\"\n+    INSTALLATION_TYPE=\"DEV\"\n+elif   [[ \"${1}\" == \"ci\" ]]; then\n+    INSTALLATION_TYPE=\"CI\"\n else\n-    echo \"ERROR! Wrong argument. Passed ${1} and it should be one of 'runtime' or 'dev'.\".\n+    echo \"ERROR! Wrong argument. Passed ${1} and it should be one of 'runtime', 'ci' or 'dev'.\".\n     exit 1\n fi\n \n@@ -56,7 +61,10 @@ function get_dev_apt_deps() {\n freetds-bin freetds-dev git graphviz graphviz-dev krb5-user ldap-utils libev4 libev-dev libffi-dev libgeos-dev \\\n libkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\\n libssl-dev libxmlsec1 libxmlsec1-dev locales lsb-release openssh-client pkgconf sasl2-bin \\\n-software-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev\"\n+software-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev \\\n+gdb lcov pkg-config libbz2-dev libgdbm-dev libgdbm-compat-dev liblzma-dev \\\n+libncurses5-dev libreadline6-dev libsqlite3-dev lzma lzma-dev tk-dev uuid-dev \\\n+libzstd-dev\"\n         export DEV_APT_DEPS\n     fi\n }\n@@ -143,14 +151,36 @@ function install_debian_runtime_dependencies() {\n     rm -rf /var/lib/apt/lists/* /var/log/*\n }\n \n+function install_python() {\n+    git clone --branch \"${AIRFLOW_PYTHON_VERSION}\" --depth 1 https://github.com/python/cpython.git\n+    cd cpython\n+    ./configure --enable-optimizations\n+    make -s -j \"$(nproc)\" all\n+    make -s -j \"$(nproc)\" install\n+    ln -s /usr/local/bin/python3 /usr/local/bin/python\n+    ln -s /usr/local/bin/pip3 /usr/local/bin/pip\n+    cd ..\n+    rm -rf cpython\n+}\n+\n+function install_golang() {\n+    curl \"https://dl.google.com/go/go${GOLANG_MAJOR_MINOR_VERSION}.linux-$(dpkg --print-architecture).tar.gz\" -o \"go${GOLANG_MAJOR_MINOR_VERSION}.linux.tar.gz\"\n+    rm -rf /usr/local/go && tar -C /usr/local -xzf go\"${GOLANG_MAJOR_MINOR_VERSION}\".linux.tar.gz\n+}\n+\n if [[ \"${INSTALLATION_TYPE}\" == \"RUNTIME\" ]]; then\n     get_runtime_apt_deps\n     install_debian_runtime_dependencies\n     install_docker_cli\n \n else\n+\n     get_dev_apt_deps\n     install_debian_dev_dependencies\n+    install_python\n+    if [[ \"${INSTALLATION_TYPE}\" == \"CI\" ]]; then\n+        install_golang\n+    fi\n     install_docker_cli\n fi\n EOF\n@@ -928,7 +958,7 @@ function environment_initialization() {\n     CI=${CI:=\"false\"}\n \n     # Added to have run-tests on path\n-    export PATH=${PATH}:${AIRFLOW_SOURCES}\n+    export PATH=${PATH}:${AIRFLOW_SOURCES}:/usr/local/go/bin/\n \n     mkdir -pv \"${AIRFLOW_HOME}/logs/\"\n \n@@ -1240,13 +1270,13 @@ COPY <<\"EOF\" /entrypoint_exec.sh\n exec /bin/bash \"${@}\"\n EOF\n \n-FROM ${PYTHON_BASE_IMAGE} as main\n+FROM ${BASE_IMAGE} as main\n \n # Nolog bash flag is currently ignored - but you can replace it with other flags (for example\n # xtrace - to show commands executed)\n SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"nounset\", \"-o\", \"nolog\", \"-c\"]\n \n-ARG PYTHON_BASE_IMAGE\n+ARG BASE_IMAGE\n ARG AIRFLOW_IMAGE_REPOSITORY=\"https://github.com/apache/airflow\"\n \n # By increasing this number we can do force build of all dependencies.\n@@ -1256,7 +1286,7 @@ ARG AIRFLOW_IMAGE_REPOSITORY=\"https://github.com/apache/airflow\"\n ARG DEPENDENCIES_EPOCH_NUMBER=\"15\"\n \n # Make sure noninteractive debian install is used and language variables set\n-ENV PYTHON_BASE_IMAGE=${PYTHON_BASE_IMAGE} \\\n+ENV BASE_IMAGE=${BASE_IMAGE} \\\n     DEBIAN_FRONTEND=noninteractive LANGUAGE=C.UTF-8 LANG=C.UTF-8 LC_ALL=C.UTF-8 \\\n     LC_CTYPE=C.UTF-8 LC_MESSAGES=C.UTF-8 \\\n     DEPENDENCIES_EPOCH_NUMBER=${DEPENDENCIES_EPOCH_NUMBER} \\\n@@ -1267,7 +1297,7 @@ ENV PYTHON_BASE_IMAGE=${PYTHON_BASE_IMAGE} \\\n     UV_CACHE_DIR=/root/.cache/uv\n \n \n-RUN echo \"Base image version: ${PYTHON_BASE_IMAGE}\"\n+RUN echo \"Base image version: ${BASE_IMAGE}\"\n \n ARG DEV_APT_COMMAND=\"\"\n ARG ADDITIONAL_DEV_APT_COMMAND=\"\"\n@@ -1282,8 +1312,13 @@ ENV DEV_APT_COMMAND=${DEV_APT_COMMAND} \\\n     ADDITIONAL_DEV_APT_DEPS=${ADDITIONAL_DEV_APT_DEPS} \\\n     ADDITIONAL_DEV_APT_COMMAND=${ADDITIONAL_DEV_APT_COMMAND}\n \n-COPY --from=scripts install_os_dependencies.sh /scripts/docker/\n-RUN bash /scripts/docker/install_os_dependencies.sh dev\n+ARG AIRFLOW_PYTHON_VERSION=v3.10.18\n+ENV AIRFLOW_PYTHON_VERSION=$AIRFLOW_PYTHON_VERSION\n+ENV GOLANG_MAJOR_MINOR_VERSION=1.24.4\n+\n+COPY --from=scripts install_os_dependencies_ci.sh /scripts/docker/\n+\n+RUN bash /scripts/docker/install_os_dependencies_ci.sh ci\n \n COPY --from=scripts common.sh /scripts/docker/\n \ndiff --git a/dev/breeze/src/airflow_breeze/global_constants.py b/dev/breeze/src/airflow_breeze/global_constants.py\nindex b742e300248ff..bc66a787e3ccd 100644\n--- a/dev/breeze/src/airflow_breeze/global_constants.py\n+++ b/dev/breeze/src/airflow_breeze/global_constants.py\n@@ -751,6 +751,16 @@ def generate_provider_dependencies_if_needed():\n     },\n ]\n \n+ALL_PYTHON_VERSION_TO_PATCH_VERSION: dict[str, str] = {\n+    \"3.6\": \"v3.6.1\",\n+    \"3.7\": \"v3.7.1\",\n+    \"3.8\": \"v3.8.1\",\n+    \"3.9\": \"v3.9.23\",\n+    \"3.10\": \"v3.10.18\",\n+    \"3.11\": \"v3.11.13\",\n+    \"3.12\": \"v3.12.11\",\n+}\n+\n # Number of slices for low dep tests\n NUMBER_OF_LOW_DEP_SLICES = 5\n \ndiff --git a/dev/breeze/src/airflow_breeze/params/build_ci_params.py b/dev/breeze/src/airflow_breeze/params/build_ci_params.py\nindex 65adcf6efde9d..b4c42c341ded0 100644\n--- a/dev/breeze/src/airflow_breeze/params/build_ci_params.py\n+++ b/dev/breeze/src/airflow_breeze/params/build_ci_params.py\n@@ -21,6 +21,7 @@\n from pathlib import Path\n \n from airflow_breeze.branch_defaults import DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH\n+from airflow_breeze.global_constants import ALL_PYTHON_VERSION_TO_PATCH_VERSION\n from airflow_breeze.params.common_build_params import CommonBuildParams\n from airflow_breeze.utils.path_utils import BUILD_CACHE_PATH\n \n@@ -68,6 +69,9 @@ def prepare_arguments_for_docker_build_command(self) -> list[str]:\n             self._opt_arg(\"UV_HTTP_TIMEOUT\", get_uv_timeout(self))\n         self._req_arg(\"AIRFLOW_VERSION\", self.airflow_version)\n         self._req_arg(\"PYTHON_BASE_IMAGE\", self.python_base_image)\n+        self._req_arg(\n+            \"AIRFLOW_PYTHON_VERSION\", ALL_PYTHON_VERSION_TO_PATCH_VERSION.get(self.python, self.python)\n+        )\n         if self.upgrade_to_newer_dependencies:\n             self._opt_arg(\"UPGRADE_RANDOM_INDICATOR_STRING\", f\"{random.randrange(2**32):x}\")\n         # optional build args\ndiff --git a/scripts/ci/pre_commit/update_installers_and_pre_commit.py b/scripts/ci/pre_commit/update_installers_and_pre_commit.py\nindex 612acdaaa2f2f..eed555e93fe40 100755\n--- a/scripts/ci/pre_commit/update_installers_and_pre_commit.py\n+++ b/scripts/ci/pre_commit/update_installers_and_pre_commit.py\n@@ -24,6 +24,7 @@\n from pathlib import Path\n \n import requests\n+from packaging.version import Version\n \n sys.path.insert(0, str(Path(__file__).parent.resolve()))  # make sure common_precommit_utils is imported\n from common_precommit_utils import AIRFLOW_CORE_ROOT_PATH, AIRFLOW_ROOT_PATH, console\n@@ -65,6 +66,35 @@ def get_latest_pypi_version(package_name: str) -> str:\n     return latest_version\n \n \n+def get_latest_python_version(python_major_minor: str, github_token: str | None) -> str | None:\n+    latest_version = None\n+    # Matches versions of vA.B.C and vA.B where C can only be numeric and v is optional\n+    version_match = re.compile(rf\"^v?{python_major_minor}\\.?\\d*$\")\n+    headers = {\"User-Agent\": \"Python requests\"}\n+    if github_token:\n+        headers[\"Authorization\"] = f\"Bearer {github_token}\"\n+    for i in range(5):\n+        response = requests.get(\n+            f\"https://api.github.com/repos/python/cpython/tags?per_page=100&page={i + 1}\",\n+            headers=headers,\n+        )\n+        response.raise_for_status()  # Ensure we got a successful response\n+        data = response.json()\n+        versions = [str(tag[\"name\"]) for tag in data if version_match.match(tag.get(\"name\", \"\"))]\n+        if versions:\n+            latest_version = sorted(versions, key=Version, reverse=True)[0]\n+            break\n+    return latest_version\n+\n+\n+def get_latest_golang_version() -> str:\n+    response = requests.get(\"https://go.dev/dl/?mode=json\")\n+    response.raise_for_status()  # Ensure we got a successful response\n+    versions = response.json()\n+    stable_versions = [release[\"version\"].replace(\"go\", \"\") for release in versions if release[\"stable\"]]\n+    return sorted(stable_versions, key=Version, reverse=True)[0]\n+\n+\n def get_latest_lts_node_version() -> str:\n     response = requests.get(\"https://nodejs.org/dist/index.json\")\n     response.raise_for_status()  # Ensure we got a successful response\n@@ -92,6 +122,15 @@ class Quoting(Enum):\n     (re.compile(r\"(\\| *`AIRFLOW_PIP_VERSION` *\\| *)(`[0-9.]+`)( *\\|)\"), Quoting.REVERSE_SINGLE_QUOTED),\n ]\n \n+PYTHON_PATTERNS: list[tuple[str, Quoting]] = [\n+    (r\"(\\\"{python_major_minor}\\\": \\\")(v[0-9.]+)(\\\")\", Quoting.UNQUOTED),\n+]\n+\n+GOLANG_PATTERNS: list[tuple[re.Pattern, Quoting]] = [\n+    (re.compile(r\"(GOLANG_MAJOR_MINOR_VERSION=)([0-9.]+)\"), Quoting.UNQUOTED),\n+    (re.compile(r\"(\\| *`GOLANG_MAJOR_MINOR_VERSION` *\\| *)(`[0-9.]+`)( *\\|)\"), Quoting.REVERSE_SINGLE_QUOTED),\n+]\n+\n UV_PATTERNS: list[tuple[re.Pattern, Quoting]] = [\n     (re.compile(r\"(AIRFLOW_UV_VERSION=)([0-9.]+)\"), Quoting.UNQUOTED),\n     (re.compile(r\"(uv>=)([0-9.]+)\"), Quoting.UNQUOTED),\n@@ -167,6 +206,8 @@ def get_replacement(value: str, quoting: Quoting) -> str:\n \n UPGRADE_UV: bool = os.environ.get(\"UPGRADE_UV\", \"true\").lower() == \"true\"\n UPGRADE_PIP: bool = os.environ.get(\"UPGRADE_PIP\", \"true\").lower() == \"true\"\n+UPGRADE_PYTHON: bool = os.environ.get(\"UPGRADE_PYTHON\", \"true\").lower() == \"true\"\n+UPGRADE_GOLANG: bool = os.environ.get(\"UPGRADE_GOLANG\", \"true\").lower() == \"true\"\n UPGRADE_SETUPTOOLS: bool = os.environ.get(\"UPGRADE_SETUPTOOLS\", \"true\").lower() == \"true\"\n UPGRADE_PRE_COMMIT: bool = os.environ.get(\"UPGRADE_PRE_COMMIT\", \"true\").lower() == \"true\"\n UPGRADE_NODE_LTS: bool = os.environ.get(\"UPGRADE_NODE_LTS\", \"true\").lower() == \"true\"\n@@ -175,6 +216,10 @@ def get_replacement(value: str, quoting: Quoting) -> str:\n UPGRADE_GITPYTHON: bool = os.environ.get(\"UPGRADE_GITPYTHON\", \"true\").lower() == \"true\"\n UPGRADE_RICH: bool = os.environ.get(\"UPGRADE_RICH\", \"true\").lower() == \"true\"\n \n+ALL_PYTHON_MAJOR_MINOR_VERSIONS = [\"3.6\", \"3.7\", \"3.8\", \"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n+\n+GITHUB_TOKEN: str | None = os.environ.get(\"GITHUB_TOKEN\")\n+\n \n def replace_version(pattern: re.Pattern[str], version: str, text: str, keep_total_length: bool = True) -> str:\n     # Assume that the pattern has up to 3 replacement groups:\n@@ -205,6 +250,7 @@ def replacer(match):\n \n if __name__ == \"__main__\":\n     changed = False\n+    golang_version = get_latest_golang_version()\n     pip_version = get_latest_pypi_version(\"pip\")\n     uv_version = get_latest_pypi_version(\"uv\")\n     setuptools_version = get_latest_pypi_version(\"setuptools\")\n@@ -225,6 +271,28 @@ def replacer(match):\n                 new_content = replace_version(\n                     line_pattern, get_replacement(pip_version, quoting), new_content, keep_length\n                 )\n+        if UPGRADE_PYTHON:\n+            for python_version in ALL_PYTHON_MAJOR_MINOR_VERSIONS:\n+                latest_python_version = get_latest_python_version(python_version, GITHUB_TOKEN)\n+                if latest_python_version:\n+                    console.print(\n+                        f\"[bright_blue]Latest python {python_version} version: {latest_python_version}\"\n+                    )\n+                    for line_format, quoting in PYTHON_PATTERNS:\n+                        line_pattern = re.compile(line_format.format(python_major_minor=python_version))\n+                        console.print(line_pattern)\n+                        new_content = replace_version(\n+                            line_pattern,\n+                            get_replacement(latest_python_version, quoting),\n+                            new_content,\n+                            keep_length,\n+                        )\n+        if UPGRADE_GOLANG:\n+            console.print(f\"[bright_blue]Latest golang version: {golang_version}\")\n+            for line_pattern, quoting in GOLANG_PATTERNS:\n+                new_content = replace_version(\n+                    line_pattern, get_replacement(golang_version, quoting), new_content, keep_length\n+                )\n         if UPGRADE_SETUPTOOLS:\n             console.print(f\"[bright_blue]Latest setuptools version: {setuptools_version}\")\n             for line_pattern, quoting in SETUPTOOLS_PATTERNS:\ndiff --git a/scripts/docker/entrypoint_ci.sh b/scripts/docker/entrypoint_ci.sh\nindex e215ce642a0df..fef63aa88f068 100755\n--- a/scripts/docker/entrypoint_ci.sh\n+++ b/scripts/docker/entrypoint_ci.sh\n@@ -130,7 +130,7 @@ function environment_initialization() {\n     CI=${CI:=\"false\"}\n \n     # Added to have run-tests on path\n-    export PATH=${PATH}:${AIRFLOW_SOURCES}\n+    export PATH=${PATH}:${AIRFLOW_SOURCES}:/usr/local/go/bin/\n \n     mkdir -pv \"${AIRFLOW_HOME}/logs/\"\n \ndiff --git a/scripts/docker/install_os_dependencies_ci.sh b/scripts/docker/install_os_dependencies_ci.sh\nnew file mode 100644\nindex 0000000000000..0873ac69c8c4f\n--- /dev/null\n+++ b/scripts/docker/install_os_dependencies_ci.sh\n@@ -0,0 +1,167 @@\n+#!/usr/bin/env bash\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+# shellcheck shell=bash\n+set -euo pipefail\n+\n+if [[ \"$#\" != 1 ]]; then\n+    echo \"ERROR! There should be 'runtime', 'ci' or 'dev' parameter passed as argument.\".\n+    exit 1\n+fi\n+\n+AIRFLOW_PYTHON_VERSION=${AIRFLOW_PYTHON_VERSION:-v3.10.10}\n+GOLANG_MAJOR_MINOR_VERSION=${GOLANG_MAJOR_MINOR_VERSION:-1.24.4}\n+\n+if [[ \"${1}\" == \"runtime\" ]]; then\n+    INSTALLATION_TYPE=\"RUNTIME\"\n+elif   [[ \"${1}\" == \"dev\" ]]; then\n+    INSTALLATION_TYPE=\"DEV\"\n+elif   [[ \"${1}\" == \"ci\" ]]; then\n+    INSTALLATION_TYPE=\"CI\"\n+else\n+    echo \"ERROR! Wrong argument. Passed ${1} and it should be one of 'runtime', 'ci' or 'dev'.\".\n+    exit 1\n+fi\n+\n+function get_dev_apt_deps() {\n+    if [[ \"${DEV_APT_DEPS=}\" == \"\" ]]; then\n+        DEV_APT_DEPS=\"apt-transport-https apt-utils build-essential ca-certificates dirmngr \\\n+freetds-bin freetds-dev git graphviz graphviz-dev krb5-user ldap-utils libev4 libev-dev libffi-dev libgeos-dev \\\n+libkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\\n+libssl-dev libxmlsec1 libxmlsec1-dev locales lsb-release openssh-client pkgconf sasl2-bin \\\n+software-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev \\\n+gdb lcov pkg-config libbz2-dev libgdbm-dev libgdbm-compat-dev liblzma-dev \\\n+libncurses5-dev libreadline6-dev libsqlite3-dev lzma lzma-dev tk-dev uuid-dev \\\n+libzstd-dev\"\n+        export DEV_APT_DEPS\n+    fi\n+}\n+\n+function get_runtime_apt_deps() {\n+    local debian_version\n+    local debian_version_apt_deps\n+    # Get debian version without installing lsb_release\n+    # shellcheck disable=SC1091\n+    debian_version=$(. /etc/os-release;   printf '%s\\n' \"$VERSION_CODENAME\";)\n+    echo\n+    echo \"DEBIAN CODENAME: ${debian_version}\"\n+    echo\n+    debian_version_apt_deps=\"libffi8 libldap-2.5-0 libssl3 netcat-openbsd\"\n+    echo\n+    echo \"APPLIED INSTALLATION CONFIGURATION FOR DEBIAN VERSION: ${debian_version}\"\n+    echo\n+    if [[ \"${RUNTIME_APT_DEPS=}\" == \"\" ]]; then\n+        RUNTIME_APT_DEPS=\"apt-transport-https apt-utils ca-certificates \\\n+curl dumb-init freetds-bin git krb5-user libev4 libgeos-dev \\\n+ldap-utils libsasl2-2 libsasl2-modules libxmlsec1 locales ${debian_version_apt_deps} \\\n+lsb-release openssh-client python3-selinux rsync sasl2-bin sqlite3 sudo unixodbc\"\n+        export RUNTIME_APT_DEPS\n+    fi\n+}\n+\n+function install_docker_cli() {\n+    apt-get update\n+    apt-get install ca-certificates curl\n+    install -m 0755 -d /etc/apt/keyrings\n+    curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc\n+    chmod a+r /etc/apt/keyrings/docker.asc\n+    # shellcheck disable=SC1091\n+    echo \\\n+      \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \\\n+      $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n+      tee /etc/apt/sources.list.d/docker.list > /dev/null\n+    apt-get update\n+    apt-get install -y --no-install-recommends docker-ce-cli\n+}\n+\n+function install_debian_dev_dependencies() {\n+    apt-get update\n+    apt-get install -yqq --no-install-recommends apt-utils >/dev/null 2>&1\n+    apt-get install -y --no-install-recommends curl gnupg2 lsb-release\n+    # shellcheck disable=SC2086\n+    export ${ADDITIONAL_DEV_APT_ENV?}\n+    if [[ ${DEV_APT_COMMAND} != \"\" ]]; then\n+        bash -o pipefail -o errexit -o nounset -o nolog -c \"${DEV_APT_COMMAND}\"\n+    fi\n+    if [[ ${ADDITIONAL_DEV_APT_COMMAND} != \"\" ]]; then\n+        bash -o pipefail -o errexit -o nounset -o nolog -c \"${ADDITIONAL_DEV_APT_COMMAND}\"\n+    fi\n+    apt-get update\n+    local debian_version\n+    local debian_version_apt_deps\n+    # Get debian version without installing lsb_release\n+    # shellcheck disable=SC1091\n+    debian_version=$(. /etc/os-release;   printf '%s\\n' \"$VERSION_CODENAME\";)\n+    echo\n+    echo \"DEBIAN CODENAME: ${debian_version}\"\n+    echo\n+    # shellcheck disable=SC2086\n+    apt-get install -y --no-install-recommends ${DEV_APT_DEPS} ${ADDITIONAL_DEV_APT_DEPS}\n+}\n+\n+function install_debian_runtime_dependencies() {\n+    apt-get update\n+    apt-get install --no-install-recommends -yqq apt-utils >/dev/null 2>&1\n+    apt-get install -y --no-install-recommends curl gnupg2 lsb-release\n+    # shellcheck disable=SC2086\n+    export ${ADDITIONAL_RUNTIME_APT_ENV?}\n+    if [[ \"${RUNTIME_APT_COMMAND}\" != \"\" ]]; then\n+        bash -o pipefail -o errexit -o nounset -o nolog -c \"${RUNTIME_APT_COMMAND}\"\n+    fi\n+    if [[ \"${ADDITIONAL_RUNTIME_APT_COMMAND}\" != \"\" ]]; then\n+        bash -o pipefail -o errexit -o nounset -o nolog -c \"${ADDITIONAL_RUNTIME_APT_COMMAND}\"\n+    fi\n+    apt-get update\n+    # shellcheck disable=SC2086\n+    apt-get install -y --no-install-recommends ${RUNTIME_APT_DEPS} ${ADDITIONAL_RUNTIME_APT_DEPS}\n+    apt-get autoremove -yqq --purge\n+    apt-get clean\n+    rm -rf /var/lib/apt/lists/* /var/log/*\n+}\n+\n+function install_python() {\n+    git clone --branch \"${AIRFLOW_PYTHON_VERSION}\" --depth 1 https://github.com/python/cpython.git\n+    cd cpython\n+    ./configure --enable-optimizations\n+    make -s -j \"$(nproc)\" all\n+    make -s -j \"$(nproc)\" install\n+    ln -s /usr/local/bin/python3 /usr/local/bin/python\n+    ln -s /usr/local/bin/pip3 /usr/local/bin/pip\n+    cd ..\n+    rm -rf cpython\n+}\n+\n+function install_golang() {\n+    curl \"https://dl.google.com/go/go${GOLANG_MAJOR_MINOR_VERSION}.linux-$(dpkg --print-architecture).tar.gz\" -o \"go${GOLANG_MAJOR_MINOR_VERSION}.linux.tar.gz\"\n+    rm -rf /usr/local/go && tar -C /usr/local -xzf go\"${GOLANG_MAJOR_MINOR_VERSION}\".linux.tar.gz\n+}\n+\n+if [[ \"${INSTALLATION_TYPE}\" == \"RUNTIME\" ]]; then\n+    get_runtime_apt_deps\n+    install_debian_runtime_dependencies\n+    install_docker_cli\n+\n+else\n+\n+    get_dev_apt_deps\n+    install_debian_dev_dependencies\n+    install_python\n+    if [[ \"${INSTALLATION_TYPE}\" == \"CI\" ]]; then\n+        install_golang\n+    fi\n+    install_docker_cli\n+fi\n", "test_patch": "diff --git a/.github/workflows/basic-tests.yml b/.github/workflows/basic-tests.yml\nindex bc23674f44285..24c0e8eb02e2a 100644\n--- a/.github/workflows/basic-tests.yml\n+++ b/.github/workflows/basic-tests.yml\n@@ -293,6 +293,9 @@ jobs:\n           --hook-stage manual update-installers-and-pre-commit || true\n         if: always()\n         env:\n+          UPGRADE_UV: \"true\"\n+          UPGRADE_PYTHON: \"false\"\n+          UPGRADE_GOLANG: \"true\"\n           UPGRADE_PIP: \"false\"\n           UPGRADE_PRE_COMMIT: \"false\"\n           UPGRADE_NODE_LTS: \"false\"\n@@ -303,6 +306,9 @@ jobs:\n           --hook-stage manual update-installers-and-pre-commit\n         if: always()\n         env:\n+          UPGRADE_UV: \"false\"\n+          UPGRADE_PYTHON: \"true\"\n+          UPGRADE_GOLANG: \"false\"\n           UPGRADE_PIP: \"true\"\n           UPGRADE_PRE_COMMIT: \"true\"\n           UPGRADE_NODE_LTS: \"true\"\n", "problem_statement": "Change sources of our docker images\n### Body\n\nThe \"official\" Python images are far less official than we thought. They are built by the community and while they are somewhat supervised by the \"official Docker image program\" there is really little control over the build and release process.\n\nWe should change the way we build our images to install Python on base debian image from \"official\" releases rather than use \"python\" base images for it.\n\nThat has also some benefits - for example setuptools are quite old in those images and they tend to have security issues - if we decide when we rebuild the base images we can have better control over what is in the images. \n\nWe might also finally plug it in with automated snyk scanning and break releases if snyk does not pass.\n\n### Committer\n\n- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.\n", "hints_text": "Also we should add optional golang installation when we build the image.\nCurrently we are building our images by passing it image it should be build from\n\nThe command is similar to:\n\n```\ndocker buildx build --load --builder default --progress=auto --cache-from=ghcr.io/apache/airflow/v2-11-test/ci/python3.9:cache-linux-amd64 --pull --build-arg AIRFLOW_BRANCH=v2-11-test --build-arg AIRFLOW_CONSTRAINTS_MODE=constraints-source-providers --build-arg AIRFLOW_CONSTRAINTS_REFERENCE=constraints-2-11 --build-arg AIRFLOW_EXTRAS=devel-ci --build-arg AIRFLOW_IMAGE_DATE_CREATED=2025-06-23T18:48:03Z --build-arg AIRFLOW_IMAGE_REPOSITORY=https://github.com/apache/airflow --build-arg AIRFLOW_USE_UV=true --build-arg UV_HTTP_TIMEOUT=300 --build-arg AIRFLOW_VERSION=2.11.0.dev0 --build-arg BUILD_ID=0 --build-arg CONSTRAINTS_GITHUB_REPOSITORY=apache/airflow --build-arg PYTHON_BASE_IMAGE=python:3.9-slim-bookworm --build-arg BUILD_PROGRESS=auto --build-arg INSTALL_MYSQL_CLIENT_TYPE=mariadb --build-arg VERSION_SUFFIX_FOR_PYPI=dev0 -t ghcr.io/apache/airflow/v2-11-test/ci/python3.9 --target main . -f Dockerfile.ci --platform linux/amd64\n\n```\n\nThe importan thing is this arg:\n\n*  PYTHON_BASE_IMAGE=python:3.9-slim-bookworm\n\nRather than that, we should have BASE_IMAGE (`debian:slim-bookworm`) and pass PYTHON_MAJOR_MINOR_VERSION simplyh (this is the name we use internally and we automaticallly extract it in the images from installed python version.\n\nThen - one of the first step in our build should be to install python - ideally from official sources - but maybe we find other, faster way (building python is SLOW and requires build tools.\n\nWe should first start with CI images and apply the changes toi PROD after we test it for a while on CI.\n\n\n\n\n\n\n\n\n\n\n", "all_hints_text": "Also we should add optional golang installation when we build the image.\nCurrently we are building our images by passing it image it should be build from\n\nThe command is similar to:\n\n```\ndocker buildx build --load --builder default --progress=auto --cache-from=ghcr.io/apache/airflow/v2-11-test/ci/python3.9:cache-linux-amd64 --pull --build-arg AIRFLOW_BRANCH=v2-11-test --build-arg AIRFLOW_CONSTRAINTS_MODE=constraints-source-providers --build-arg AIRFLOW_CONSTRAINTS_REFERENCE=constraints-2-11 --build-arg AIRFLOW_EXTRAS=devel-ci --build-arg AIRFLOW_IMAGE_DATE_CREATED=2025-06-23T18:48:03Z --build-arg AIRFLOW_IMAGE_REPOSITORY=https://github.com/apache/airflow --build-arg AIRFLOW_USE_UV=true --build-arg UV_HTTP_TIMEOUT=300 --build-arg AIRFLOW_VERSION=2.11.0.dev0 --build-arg BUILD_ID=0 --build-arg CONSTRAINTS_GITHUB_REPOSITORY=apache/airflow --build-arg PYTHON_BASE_IMAGE=python:3.9-slim-bookworm --build-arg BUILD_PROGRESS=auto --build-arg INSTALL_MYSQL_CLIENT_TYPE=mariadb --build-arg VERSION_SUFFIX_FOR_PYPI=dev0 -t ghcr.io/apache/airflow/v2-11-test/ci/python3.9 --target main . -f Dockerfile.ci --platform linux/amd64\n\n```\n\nThe importan thing is this arg:\n\n*  PYTHON_BASE_IMAGE=python:3.9-slim-bookworm\n\nRather than that, we should have BASE_IMAGE (`debian:slim-bookworm`) and pass PYTHON_MAJOR_MINOR_VERSION simplyh (this is the name we use internally and we automaticallly extract it in the images from installed python version.\n\nThen - one of the first step in our build should be to install python - ideally from official sources - but maybe we find other, faster way (building python is SLOW and requires build tools.\n\nWe should first start with CI images and apply the changes toi PROD after we test it for a while on CI.\n\n\n\n\n\n\n\n\n\nThis one still needs PROD image update - reopening.\nLet's use the CI image for a week or so and then we apply it to PROD image.\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/b3ef993e2c08ea1abbd4a3ad59c97342d7d49b7e", "https://github.com/apache/airflow/commit/c656f94f1b86518fa263a3804f9616594bf45421", "https://github.com/apache/airflow/commit/437b5cb63b45c0dcf490306aa91e1cc7713e6696", "https://github.com/apache/airflow/commit/354a06530f8adfe5e00eb0644c320bb2b5c3c6ec", "https://github.com/apache/airflow/commit/3b5095c3be49a6025f5dd635a3d1006e53be93f0", "https://github.com/apache/airflow/commit/7810a2b193602b7d631e6048bc2696b19a04d1b0", "https://github.com/apache/airflow/commit/6bbe6e1051ff6c4134ff49f8a6eb26b37f2f8eab", "https://github.com/apache/airflow/commit/f00fbf0985a906f29af78602b034bccb4cd4b757", "https://github.com/apache/airflow/commit/f4bb5e5358e0a639ff6eb275910ce17282a86e4a", "https://github.com/apache/airflow/commit/ee190524d71bb319f98fdf388137889aa9a7b49d", "https://github.com/apache/airflow/commit/fe05ee260cff2a2ddf174ffbde22a3e5cf04a864", "https://github.com/apache/airflow/commit/31e02378a1aaf3aa6e4eb9f3e31bdca454195127", "https://github.com/apache/airflow/commit/b054a475df6e5e2ed2b8b3ae11d53b51307e0598", "https://github.com/apache/airflow/commit/ec313523aa0fd50f98b680e04b2f03804724d486", "https://github.com/apache/airflow/commit/de17ae5a8dfc4e6aa91d908df73dc8ec9e6b65b0"], "created_at": "2025-07-10T16:41:54Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 52269, "instance_id": "apache__airflow-52269", "issue_numbers": [52267], "base_commit": "ee7f66729be312805efd7edba596cf4e5bb1cea0", "patch": "diff --git a/chart/templates/_helpers.yaml b/chart/templates/_helpers.yaml\nindex 2100e2d4267f2..8838b837dca0f 100644\n--- a/chart/templates/_helpers.yaml\n+++ b/chart/templates/_helpers.yaml\n@@ -91,13 +91,20 @@ If release name contains chart name it will be used as a full name.\n         name: {{ template \"airflow_metadata_secret\" . }}\n         key: kedaConnection\n   {{- end }}\n-  {{- if .Values.enableBuiltInSecretEnvVars.AIRFLOW__WEBSERVER__SECRET_KEY }}\n+  {{- if and (semverCompare \"<3.0.0\" .Values.airflowVersion) .Values.enableBuiltInSecretEnvVars.AIRFLOW__WEBSERVER__SECRET_KEY }}\n   - name: AIRFLOW__WEBSERVER__SECRET_KEY\n     valueFrom:\n       secretKeyRef:\n         name: {{ template \"webserver_secret_key_secret\" . }}\n         key: webserver-secret-key\n   {{- end }}\n+  {{- if and (semverCompare \">=3.0.0\" .Values.airflowVersion) .Values.enableBuiltInSecretEnvVars.AIRFLOW__API__SECRET_KEY }}\n+  - name: AIRFLOW__API__SECRET_KEY\n+    valueFrom:\n+      secretKeyRef:\n+        name: {{ template \"api_secret_key_secret\" . }}\n+        key: api-secret-key\n+  {{- end }}\n   {{- if and (semverCompare \">=3.0.0\" .Values.airflowVersion) .Values.enableBuiltInSecretEnvVars.AIRFLOW__API_AUTH__JWT_SECRET }}\n   - name: AIRFLOW__API_AUTH__JWT_SECRET\n     valueFrom:\n@@ -411,6 +418,10 @@ If release name contains chart name it will be used as a full name.\n   {{- default (printf \"%s-webserver-secret-key\" (include \"airflow.fullname\" .)) .Values.webserverSecretKeySecretName }}\n {{- end }}\n \n+{{- define \"api_secret_key_secret\" -}}\n+  {{- default (printf \"%s-api-secret-key\" (include \"airflow.fullname\" .)) .Values.apiSecretKeySecretName }}\n+{{- end }}\n+\n {{- define \"redis_password_secret\" -}}\n   {{- default (printf \"%s-redis-password\" .Release.Name) .Values.redis.passwordSecretName }}\n {{- end }}\ndiff --git a/chart/templates/secrets/api-secret-key-secret.yaml b/chart/templates/secrets/api-secret-key-secret.yaml\nnew file mode 100644\nindex 0000000000000..52e0fff689f44\n--- /dev/null\n+++ b/chart/templates/secrets/api-secret-key-secret.yaml\n@@ -0,0 +1,44 @@\n+{{/*\n+ Licensed to the Apache Software Foundation (ASF) under one\n+ or more contributor license agreements.  See the NOTICE file\n+ distributed with this work for additional information\n+ regarding copyright ownership.  The ASF licenses this file\n+ to you under the Apache License, Version 2.0 (the\n+ \"License\"); you may not use this file except in compliance\n+ with the License.  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+ Unless required by applicable law or agreed to in writing,\n+ software distributed under the License is distributed on an\n+ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ KIND, either express or implied.  See the License for the\n+ specific language governing permissions and limitations\n+ under the License.\n+*/}}\n+\n+############################################\n+## Airflow Api Flask Secret Key Secret\n+############################################\n+{{- if and (semverCompare \">=3.0.0\" .Values.airflowVersion) (not .Values.apiSecretKeySecretName) }}\n+apiVersion: v1\n+kind: Secret\n+metadata:\n+  name: {{ include \"airflow.fullname\" . }}-api-secret-key\n+  labels:\n+    tier: airflow\n+    component: api-server\n+    release: {{ .Release.Name }}\n+    chart: \"{{ .Chart.Name }}-{{ .Chart.Version }}\"\n+    heritage: {{ .Release.Service }}\n+    {{- with .Values.labels }}\n+      {{- toYaml . | nindent 4 }}\n+    {{- end }}\n+  {{- with .Values.apiSecretAnnotations }}\n+  annotations:\n+    {{- toYaml . | nindent 4 }}\n+  {{- end }}\n+type: Opaque\n+data:\n+  api-secret-key: {{ (.Values.apiSecretKey) | default (randAlphaNum 32) | b64enc | quote }}\n+{{- end }}\ndiff --git a/chart/templates/secrets/fernetkey-secret.yaml b/chart/templates/secrets/fernetkey-secret.yaml\nindex 5461b56f03c28..a9a1f6016938b 100644\n--- a/chart/templates/secrets/fernetkey-secret.yaml\n+++ b/chart/templates/secrets/fernetkey-secret.yaml\n@@ -21,7 +21,6 @@\n ## Airflow Fernet Key Secret\n #################################\n {{- if not .Values.fernetKeySecretName }}\n-{{- $generated_fernet_key := (randAlphaNum 32 | b64enc) }}\n apiVersion: v1\n kind: Secret\n metadata:\n@@ -43,5 +42,5 @@ metadata:\n     {{- end }}\n type: Opaque\n data:\n-  fernet-key: {{ (default $generated_fernet_key .Values.fernetKey) | b64enc | quote }}\n+  fernet-key: {{ (.Values.fernetKey) | default (randAlphaNum 32) | b64enc | quote }}\n {{- end }}\ndiff --git a/chart/templates/secrets/webserver-secret-key-secret.yaml b/chart/templates/secrets/webserver-secret-key-secret.yaml\nindex 5a9025236e7af..e7803c4d84033 100644\n--- a/chart/templates/secrets/webserver-secret-key-secret.yaml\n+++ b/chart/templates/secrets/webserver-secret-key-secret.yaml\n@@ -20,8 +20,7 @@\n ############################################\n ## Airflow Webserver Flask Secret Key Secret\n ############################################\n-{{- if not .Values.webserverSecretKeySecretName }}\n-{{ $generated_secret_key := (randAlphaNum 32 | b64enc) }}\n+{{- if and (semverCompare \"<3.0.0\" .Values.airflowVersion) .Values.webserver.enabled (not .Values.webserverSecretKeySecretName) }}\n apiVersion: v1\n kind: Secret\n metadata:\n@@ -41,5 +40,5 @@ metadata:\n   {{- end }}\n type: Opaque\n data:\n-  webserver-secret-key: {{ (default $generated_secret_key .Values.webserverSecretKey) | b64enc | quote }}\n+  webserver-secret-key: {{ (.Values.webserverSecretKey) | default (randAlphaNum 32) | b64enc | quote }}\n {{- end }}\ndiff --git a/chart/values.schema.json b/chart/values.schema.json\nindex 1bfe3c9757b69..b329b4a0dde76 100644\n--- a/chart/values.schema.json\n+++ b/chart/values.schema.json\n@@ -1064,6 +1064,11 @@\n                     \"type\": \"boolean\",\n                     \"default\": true\n                 },\n+                \"AIRFLOW__API__SECRET_KEY\": {\n+                    \"description\": \"Enable ``AIRFLOW__API__SECRET_KEY`` variable to be read from the Api Secret Key Secret\",\n+                    \"type\": \"boolean\",\n+                    \"default\": true\n+                },\n                 \"AIRFLOW__API_AUTH__JWT_SECRET\": {\n                     \"description\": \"Enable ``AIRFLOW__API_AUTH__JWT_SECRET`` variable to be read from the JWT Secret\",\n                     \"type\": \"boolean\",\n@@ -1464,6 +1469,33 @@\n                 \"type\": \"string\"\n             }\n         },\n+        \"apiSecretKey\": {\n+            \"description\": \"The Flask secret key for Airflow Api to encrypt browser session.\",\n+            \"type\": [\n+                \"string\",\n+                \"null\"\n+            ],\n+            \"x-docsSection\": \"Common\",\n+            \"default\": null\n+        },\n+        \"apiSecretAnnotations\": {\n+            \"description\": \"Annotations to add to the Api secret.\",\n+            \"type\": \"object\",\n+            \"x-docsSection\": \"Common\",\n+            \"default\": {},\n+            \"additionalProperties\": {\n+                \"type\": \"string\"\n+            }\n+        },\n+        \"apiSecretKeySecretName\": {\n+            \"description\": \"The Secret name containing Flask secret_key for the Api.\",\n+            \"type\": [\n+                \"string\",\n+                \"null\"\n+            ],\n+            \"x-docsSection\": \"Airflow\",\n+            \"default\": null\n+        },\n         \"jwtSecret\": {\n             \"description\": \"Secret key used to encode and decode JWTs to authenticate to public and private APIs (can only be set during install, not upgrade).\",\n             \"type\": [\ndiff --git a/chart/values.yaml b/chart/values.yaml\nindex 5d85210d419f1..cff157e6d50ae 100644\n--- a/chart/values.yaml\n+++ b/chart/values.yaml\n@@ -407,6 +407,7 @@ enableBuiltInSecretEnvVars:\n   AIRFLOW__CORE__SQL_ALCHEMY_CONN: true\n   AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: true\n   AIRFLOW_CONN_AIRFLOW_DB: true\n+  AIRFLOW__API__SECRET_KEY: true\n   AIRFLOW__API_AUTH__JWT_SECRET: true\n   AIRFLOW__WEBSERVER__SECRET_KEY: true\n   AIRFLOW__CELERY__CELERY_RESULT_BACKEND: true\n@@ -551,11 +552,11 @@ fernetKeySecretName: ~\n # Add custom annotations to the fernet key secret\n fernetKeySecretAnnotations: {}\n \n-# Flask secret key for Airflow Webserver: `[webserver] secret_key` in airflow.cfg\n-webserverSecretKey: ~\n-# Add custom annotations to the webserver secret\n-webserverSecretAnnotations: {}\n-webserverSecretKeySecretName: ~\n+# Flask secret key for Airflow 3+ Api: `[api] secret_key` in airflow.cfg\n+apiSecretKey: ~\n+# Add custom annotations to the api secret\n+apiSecretAnnotations: {}\n+apiSecretKeySecretName: ~\n \n # Secret key used to encode and decode JWTs: `[api_auth] jwt_secret` in airflow.cfg\n jwtSecret: ~\n@@ -563,6 +564,12 @@ jwtSecret: ~\n jwtSecretAnnotations: {}\n jwtSecretName: ~\n \n+# Flask secret key for Airflow <3 Webserver: `[webserver] secret_key` in airflow.cfg\n+webserverSecretKey: ~\n+# Add custom annotations to the webserver secret\n+webserverSecretAnnotations: {}\n+webserverSecretKeySecretName: ~\n+\n # In order to use kerberos you need to create secret containing the keytab file\n # The secret name should follow naming convention of the application where resources are\n # name {{ .Release-name }}-<POSTFIX>. In case of the keytab file, the postfix is \"kerberos-keytab\"\n", "test_patch": "diff --git a/helm-tests/tests/helm_tests/airflow_aux/test_airflow_common.py b/helm-tests/tests/helm_tests/airflow_aux/test_airflow_common.py\nindex e6b247afb1cd0..f4ad5b6e170e3 100644\n--- a/helm-tests/tests/helm_tests/airflow_aux/test_airflow_common.py\n+++ b/helm-tests/tests/helm_tests/airflow_aux/test_airflow_common.py\n@@ -323,6 +323,7 @@ def test_should_disable_some_variables(self):\n                 \"enableBuiltInSecretEnvVars\": {\n                     \"AIRFLOW__CORE__SQL_ALCHEMY_CONN\": False,\n                     \"AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\": False,\n+                    \"AIRFLOW__API__SECRET_KEY\": False,\n                     \"AIRFLOW__API_AUTH__JWT_SECRET\": False,\n                     \"AIRFLOW__WEBSERVER__SECRET_KEY\": False,\n                     # the following vars only appear if remote logging is set, so disabling them in this test is kind of a no-op\n@@ -370,7 +371,7 @@ def test_have_all_variables(self):\n             \"AIRFLOW__CORE__SQL_ALCHEMY_CONN\",\n             \"AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\",\n             \"AIRFLOW_CONN_AIRFLOW_DB\",\n-            \"AIRFLOW__WEBSERVER__SECRET_KEY\",\n+            \"AIRFLOW__API__SECRET_KEY\",\n             \"AIRFLOW__API_AUTH__JWT_SECRET\",\n             \"AIRFLOW__CELERY__BROKER_URL\",\n         ]\ndiff --git a/helm-tests/tests/helm_tests/airflow_aux/test_basic_helm_chart.py b/helm-tests/tests/helm_tests/airflow_aux/test_basic_helm_chart.py\nindex a3cc64b96688e..46f617943a884 100644\n--- a/helm-tests/tests/helm_tests/airflow_aux/test_basic_helm_chart.py\n+++ b/helm-tests/tests/helm_tests/airflow_aux/test_basic_helm_chart.py\n@@ -38,7 +38,6 @@\n     (\"Secret\", \"test-basic-airflow-metadata\"),\n     (\"Secret\", \"test-basic-broker-url\"),\n     (\"Secret\", \"test-basic-fernet-key\"),\n-    (\"Secret\", \"test-basic-airflow-webserver-secret-key\"),\n     (\"Secret\", \"test-basic-redis-password\"),\n     (\"Secret\", \"test-basic-postgresql\"),\n     (\"ConfigMap\", \"test-basic-airflow-config\"),\n@@ -71,6 +70,7 @@\n         (\"Deployment\", \"test-basic-airflow-dag-processor\"),\n         (\"ServiceAccount\", \"test-basic-airflow-api-server\"),\n         (\"ServiceAccount\", \"test-basic-airflow-dag-processor\"),\n+        (\"Secret\", \"test-basic-airflow-api-secret-key\"),\n         (\"Secret\", \"test-basic-jwt-secret\"),\n     }\n )\n@@ -82,6 +82,7 @@\n         (\"Service\", \"test-basic-airflow-webserver\"),\n         (\"Deployment\", \"test-basic-airflow-webserver\"),\n         (\"ServiceAccount\", \"test-basic-airflow-webserver\"),\n+        (\"Secret\", \"test-basic-airflow-webserver-secret-key\"),\n     }\n )\n \n@@ -137,7 +138,6 @@ def test_basic_deployments(self, version):\n             (\"Secret\", \"test-basic-metadata\"),\n             (\"Secret\", \"test-basic-broker-url\"),\n             (\"Secret\", \"test-basic-fernet-key\"),\n-            (\"Secret\", \"test-basic-webserver-secret-key\"),\n             (\"Secret\", \"test-basic-postgresql\"),\n             (\"Secret\", \"test-basic-redis-password\"),\n             (\"ConfigMap\", \"test-basic-config\"),\n@@ -171,6 +171,7 @@ def test_basic_deployments(self, version):\n                     (\"ServiceAccount\", \"test-basic-api-server\"),\n                     (\"ServiceAccount\", \"test-basic-dag-processor\"),\n                     (\"Service\", \"test-basic-triggerer\"),\n+                    (\"Secret\", \"test-basic-api-secret-key\"),\n                     (\"Secret\", \"test-basic-jwt-secret\"),\n                 )\n             )\n@@ -180,6 +181,7 @@ def test_basic_deployments(self, version):\n                     (\"Deployment\", \"test-basic-webserver\"),\n                     (\"Service\", \"test-basic-webserver\"),\n                     (\"ServiceAccount\", \"test-basic-webserver\"),\n+                    (\"Secret\", \"test-basic-webserver-secret-key\"),\n                 )\n             )\n         if version == \"default\":\n@@ -238,7 +240,6 @@ def test_basic_deployment_with_standalone_dag_processor(self, version):\n             (\"Secret\", \"test-basic-metadata\"),\n             (\"Secret\", \"test-basic-broker-url\"),\n             (\"Secret\", \"test-basic-fernet-key\"),\n-            (\"Secret\", \"test-basic-webserver-secret-key\"),\n             (\"Secret\", \"test-basic-postgresql\"),\n             (\"Secret\", \"test-basic-redis-password\"),\n             (\"ConfigMap\", \"test-basic-config\"),\n@@ -271,6 +272,7 @@ def test_basic_deployment_with_standalone_dag_processor(self, version):\n                     (\"Deployment\", \"test-basic-api-server\"),\n                     (\"Service\", \"test-basic-api-server\"),\n                     (\"ServiceAccount\", \"test-basic-api-server\"),\n+                    (\"Secret\", \"test-basic-api-secret-key\"),\n                     (\"Secret\", \"test-basic-jwt-secret\"),\n                 }\n             )\n@@ -280,6 +282,7 @@ def test_basic_deployment_with_standalone_dag_processor(self, version):\n                     (\"Service\", \"test-basic-webserver\"),\n                     (\"Deployment\", \"test-basic-webserver\"),\n                     (\"ServiceAccount\", \"test-basic-webserver\"),\n+                    (\"Secret\", \"test-basic-webserver-secret-key\"),\n                 }\n             )\n         assert list_of_kind_names_tuples == expected\n@@ -447,7 +450,6 @@ def test_labels_are_valid(self, airflow_version):\n             (f\"{release_name}-statsd\", \"Deployment\", \"statsd\"),\n             (f\"{release_name}-statsd\", \"Service\", \"statsd\"),\n             (f\"{release_name}-statsd-policy\", \"NetworkPolicy\", \"statsd-policy\"),\n-            (f\"{release_name}-webserver-secret-key\", \"Secret\", \"webserver\"),\n             (f\"{release_name}-worker\", \"Service\", \"worker\"),\n             (f\"{release_name}-worker\", \"StatefulSet\", \"worker\"),\n             (f\"{release_name}-worker-policy\", \"NetworkPolicy\", \"airflow-worker-policy\"),\n@@ -460,14 +462,17 @@ def test_labels_are_valid(self, airflow_version):\n         if self._is_airflow_3_or_above(airflow_version):\n             kind_names_tuples += [\n                 (f\"{release_name}-api-server\", \"Service\", \"api-server\"),\n-                (f\"{release_name}-api-server-policy\", \"NetworkPolicy\", \"airflow-api-server-policy\"),\n                 (f\"{release_name}-api-server\", \"Deployment\", \"api-server\"),\n+                (f\"{release_name}-airflow-api-server\", \"ServiceAccount\", \"api-server\"),\n+                (f\"{release_name}-api-secret-key\", \"Secret\", \"api-server\"),\n+                (f\"{release_name}-api-server-policy\", \"NetworkPolicy\", \"airflow-api-server-policy\"),\n             ]\n         else:\n             kind_names_tuples += [\n                 (f\"{release_name}-airflow-webserver\", \"ServiceAccount\", \"webserver\"),\n                 (f\"{release_name}-webserver\", \"Deployment\", \"webserver\"),\n                 (f\"{release_name}-webserver\", \"Service\", \"webserver\"),\n+                (f\"{release_name}-webserver-secret-key\", \"Secret\", \"webserver\"),\n                 (f\"{release_name}-webserver-policy\", \"NetworkPolicy\", \"airflow-webserver-policy\"),\n                 (f\"{release_name}-ingress\", \"Ingress\", \"airflow-ingress\"),\n             ]\ndiff --git a/helm-tests/tests/helm_tests/apiserver/test_apiserver.py b/helm-tests/tests/helm_tests/apiserver/test_apiserver.py\nindex 08796bde0c855..2db007234fed1 100644\n--- a/helm-tests/tests/helm_tests/apiserver/test_apiserver.py\n+++ b/helm-tests/tests/helm_tests/apiserver/test_apiserver.py\n@@ -90,3 +90,19 @@ def test_should_add_annotations_to_jwt_secret(self):\n \n         assert \"annotations\" in jmespath.search(\"metadata\", docs)\n         assert jmespath.search(\"metadata.annotations\", docs)[\"test_annotation\"] == \"test_annotation_value\"\n+\n+\n+class TestApiSecretKeySecret:\n+    \"\"\"Tests api secret key secret.\"\"\"\n+\n+    def test_should_add_annotations_to_api_secret_key_secret(self):\n+        docs = render_chart(\n+            values={\n+                \"airflowVersion\": \"3.0.0\",\n+                \"apiSecretAnnotations\": {\"test_annotation\": \"test_annotation_value\"},\n+            },\n+            show_only=[\"templates/secrets/api-secret-key-secret.yaml\"],\n+        )[0]\n+\n+        assert \"annotations\" in jmespath.search(\"metadata\", docs)\n+        assert jmespath.search(\"metadata.annotations\", docs)[\"test_annotation\"] == \"test_annotation_value\"\ndiff --git a/helm-tests/tests/helm_tests/security/test_rbac.py b/helm-tests/tests/helm_tests/security/test_rbac.py\nindex 3a4476644409a..aa93a91b691fe 100644\n--- a/helm-tests/tests/helm_tests/security/test_rbac.py\n+++ b/helm-tests/tests/helm_tests/security/test_rbac.py\n@@ -47,7 +47,6 @@\n     (\"Secret\", \"test-rbac-broker-url\"),\n     (\"Secret\", \"test-rbac-fernet-key\"),\n     (\"Secret\", \"test-rbac-redis-password\"),\n-    (\"Secret\", \"test-rbac-webserver-secret-key\"),\n     (\"Job\", \"test-rbac-create-user\"),\n     (\"Job\", \"test-rbac-run-airflow-migrations\"),\n     (\"CronJob\", \"test-rbac-cleanup\"),\n@@ -121,6 +120,7 @@ def _get_object_tuples(self, version, sa: bool = True):\n                     (\"Service\", \"test-rbac-api-server\"),\n                     (\"Deployment\", \"test-rbac-api-server\"),\n                     (\"Deployment\", \"test-rbac-dag-processor\"),\n+                    (\"Secret\", \"test-rbac-api-secret-key\"),\n                     (\"Secret\", \"test-rbac-jwt-secret\"),\n                 )\n             )\n@@ -132,6 +132,7 @@ def _get_object_tuples(self, version, sa: bool = True):\n                 (\n                     (\"Service\", \"test-rbac-webserver\"),\n                     (\"Deployment\", \"test-rbac-webserver\"),\n+                    (\"Secret\", \"test-rbac-webserver-secret-key\"),\n                 )\n             )\n             if sa:\ndiff --git a/helm-tests/tests/helm_tests/webserver/test_webserver.py b/helm-tests/tests/helm_tests/webserver/test_webserver.py\nindex d4b2a3d18873a..c70844555a2e3 100644\n--- a/helm-tests/tests/helm_tests/webserver/test_webserver.py\n+++ b/helm-tests/tests/helm_tests/webserver/test_webserver.py\n@@ -1258,6 +1258,7 @@ class TestWebserverSecretKeySecret:\n     def test_should_add_annotations_to_webserver_secret_key_secret(self):\n         docs = render_chart(\n             values={\n+                \"airflowVersion\": \"2.10.5\",\n                 \"webserverSecretAnnotations\": {\"test_annotation\": \"test_annotation_value\"},\n             },\n             show_only=[\"templates/secrets/webserver-secret-key-secret.yaml\"],\n", "problem_statement": "Missing api secret_key capabilities to replace webserver for 3.0+\n### Official Helm Chart version\n\n1.17.0 (latest released)\n\n### Apache Airflow version\n\n3.0.2\n\n### Kubernetes Version\n\n1.32.5\n\n### Helm Chart configuration\n\nHello,\nWhen I deploy helm chart, I have the following warning:\n```\n/home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py:858 DeprecationWarning: The secret_key option in [webserver] has been moved to the secret_key option in [api] - the old setting has been used, but please update your config.\n```\nBut we can't create an api one equivalent to webserver one.\n\n\n### Docker Image customizations\n\n_No response_\n\n### What happened\n\nJust the warning\n\n### What you think should happen instead\n\n_No response_\n\n### How to reproduce\n\nDeploy helm chart using the following values:\n```yaml\ncreateUserJob:\n  serviceAccount:\n    automountServiceAccountToken: false\n  useHelmHooks: false\n  applyCustomEnv: false\nmigrateDatabaseJob:\n  serviceAccount:\n    automountServiceAccountToken: false\n  useHelmHooks: false\n  applyCustomEnv: false\n  jobAnnotations:\n    argocd.argoproj.io/hook: Sync\nuseStandardNaming: true\ndefaultAirflowRepository: docker.io/apache/airflow\ndefaultAirflowTag: 3.0.2\nairflowVersion: 3.0.2\nregistry:\n  connection:\n    user: easi-app\n    pass: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n    host: airflow.youpi.fr\nexecutor: CeleryExecutor,KubernetesExecutor\nfernetKeySecretName: airflow-fernet-key\njwtSecretName: airflow-jwt-secret\nconfig:\n  core:\n    test_connection: Enabled\nlogs:\n  persistence:\n    enabled: false\ntriggerer:\n  serviceAccount:\n    automountServiceAccountToken: false\n  persistence:\n    enabled: false\n    size: 10Gi\n    storageClassName: gp2-retain-immediate\nstatsd:\n  serviceAccount:\n    automountServiceAccountToken: false\nredis:\n  serviceAccount:\n    automountServiceAccountToken: false\n  passwordSecretName: airflow-redis-password\n  persistence:\n    enabled: false\n    size: 1Gi\n    storageClassName: gp2-retain-immediate\n  terminationGracePeriodSeconds: 30\npostgresql:\n  enabled: false\npgbouncer:\n  enabled: false\ndata:\n  brokerUrlSecretName: airflow-broker-url\n  metadataSecretName: airflow-db-cnx\nworkers:\n  serviceAccount:\n    automountServiceAccountToken: true\n  safeToEvict: true\n  persistence:\n    enabled: true\n    size: 10Gi\n    storageClassName: gp2-retain-immediate\n  resources:\n    limits:\n      cpu: 1\n      ephemeral-storage: 2Gi\n      memory: 4Gi\n    requests:\n      cpu: 100m\n      ephemeral-storage: 100Mi\n      memory: 128Mi\nscheduler:\n  serviceAccount:\n    automountServiceAccountToken: true\ndags:\n  gitSync:\n    enabled: true\n    repo: https://airflow.youpi.fr/gitlab/dags/airflow.git\n    branch: main\n    rev: HEAD\n    credentialsSecret: git-credentials\n    subPath: dags\ningress:\n  apiServer:\n    enabled: true\n    annotations:\n      cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingressClassName: nginx\n    hosts:\n      - name: airflow.doca-easi-multicloud.fr\n        tls:\n          enabled: true\n          secretName: airflow-tls\napiServer:\n  serviceAccount:\n    automountServiceAccountToken: false\n  env:\n    - name: CLIENT_ID\n      valueFrom:\n        secretKeyRef:\n          name: airflow-api-keycloak\n          key: CLIENT_ID\n    - name: CLIENT_SECRET\n      valueFrom:\n        secretKeyRef:\n          name: airflow-api-keycloak\n          key: CLIENT_SECRET\n    - name: OIDC_ISSUER\n      valueFrom:\n        secretKeyRef:\n          name: airflow-api-keycloak\n          key: OIDC_ISSUER\n    - name: AIRFLOW__API__BASE_URL\n      valueFrom:\n        secretKeyRef:\n          name: airflow-api-keycloak\n          key: AIRFLOW__API__BASE_URL\n  apiServerConfig: |\n    from airflow.providers.fab.auth_manager.security_manager.override import FabAirflowSecurityManagerOverride\n    from flask_appbuilder.security.manager import AUTH_OAUTH\n    from base64 import b64decode\n    from cryptography.hazmat.primitives import serialization\n    from flask import redirect, session\n    from flask_appbuilder import expose\n    from flask_appbuilder.security.views import AuthOAuthView\n    import jwt\n    import logging\n    import os\n    import requests\n\n    log = logging.getLogger(__name__)\n    CSRF_ENABLED = True\n    AUTH_TYPE = AUTH_OAUTH\n    AUTH_USER_REGISTRATION = True\n    AUTH_ROLES_SYNC_AT_LOGIN = True\n    AUTH_USER_REGISTRATION_ROLE = 'Public'\n    PERMANENT_SESSION_LIFETIME = 43200\n\n    # Make sure you create these roles on Keycloak\n    AUTH_ROLES_MAPPING = {\n        'airflow_admin': ['Admin'],\n        'airflow_op': ['Op'],\n        'airflow_public': ['Public'],\n        'airflow_user': ['User'],\n        'airflow_viewer': ['Viewer'], \n    }\n    PROVIDER_NAME = 'keycloak'\n    CLIENT_ID = os.getenv('CLIENT_ID')\n    CLIENT_SECRET = os.getenv('CLIENT_SECRET')\n    AIRFLOW__API__BASE_URL = os.getenv('AIRFLOW__API__BASE_URL')\n    OIDC_ISSUER = os.getenv('OIDC_ISSUER')\n    OIDC_BASE_URL = f'{OIDC_ISSUER}/protocol/openid-connect'\n    OIDC_TOKEN_URL = f'{OIDC_BASE_URL}/token'\n    OIDC_AUTH_URL = f'{OIDC_BASE_URL}/auth'\n    OIDC_METADATA_URL = f'{OIDC_ISSUER}/.well-known/openid-configuration'\n    OAUTH_PROVIDERS = [{\n        'name': PROVIDER_NAME,\n        'token_key': 'access_token',\n        'icon': 'fa-circle-o',\n        'remote_app': {\n            'api_base_url': OIDC_BASE_URL,\n            'access_token_url': OIDC_TOKEN_URL,\n            'authorize_url': OIDC_AUTH_URL,\n            'server_metadata_url': OIDC_METADATA_URL,\n            'request_token_url': None,\n            'client_id': CLIENT_ID,\n            'client_secret': CLIENT_SECRET,\n            'client_kwargs': {\n                'scope': 'email profile',\n                'code_challenge_method': 'S256',\n                'response_type': 'code',\n            },\n        }\n    }]\n\n    # Fetch public key\n    req = requests.get(OIDC_ISSUER)\n    key_der_base64 = req.json()['public_key']\n    key_der = b64decode(key_der_base64.encode())\n    public_key = serialization.load_der_public_key(key_der)\n\n    class CustomOAuthView(AuthOAuthView):\n        @expose('/logout/', methods=['GET', 'POST'])\n        def logout(self):\n            session.clear()\n            return redirect(f'{OIDC_ISSUER}/protocol/openid-connect/logout?post_logout_redirect_uri={AIRFLOW__API__BASE_URL}&client_id={CLIENT_ID}')\n\n    class CustomSecurityManager(FabAirflowSecurityManagerOverride):\n        authoauthview = CustomOAuthView\n\n        def get_oauth_user_info(self, provider, response):\n            if provider == 'keycloak':\n                token = response['access_token']\n                me = jwt.decode(token, public_key, algorithms=['HS256', 'RS256'], audience=CLIENT_ID)\n\n                # Extract roles from resource access\n                groups = me.get('resource_access', {}).get(CLIENT_ID, {}).get('roles', [])\n\n                log.info(f'groups: {groups}')\n\n                if not groups:\n                    groups = ['Viewer']\n\n                userinfo = {\n                    'username': me.get('preferred_username'),\n                    'email': me.get('email'),\n                    'first_name': me.get('given_name'),\n                    'last_name': me.get('family_name'),\n                    'role_keys': groups,\n                }\n\n                log.info(f'user info: {userinfo}')\n\n                return userinfo\n            else:\n                return {}\n\n    # Make sure to replace this with your own implementation of AirflowSecurityManager class\n    SECURITY_MANAGER_CLASS = CustomSecurityManager\n```\n\n\n### Anything else\n\n_No response_\n\n### Are you willing to submit PR?\n\n- [x] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\n\n", "all_hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\nThere is the option to:\n* create a secret: AIRFLOW__API__SECRET_KEY for example in a secret workload \"airflow-secret\"\n* import it with\n```\n    extraEnvFrom: |\n      - secretRef:\n          name: 'airflow-secret'\n```\n* disable the generation of the previous one:\n```\n    enableBuiltInSecretEnvVars:\n      AIRFLOW__WEBSERVER__SECRET_KEY: false\n```\n\nI think it did the trick on my side.\n\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/58231a20490acfd874b37e549715f0f745a1ed2d", "https://github.com/apache/airflow/commit/4ef7550d0179096fbdfd2b5c13d200ffff799902", "https://github.com/apache/airflow/commit/9f0c911e0c6cd7963543406cb6e23dd6d8ebb448", "https://github.com/apache/airflow/commit/b4e1bb6ca7c007fe6498fcfb3ebff4cc79eee2ca", "https://github.com/apache/airflow/commit/87765a8619db48e1367cec593e5771bb457a0990", "https://github.com/apache/airflow/commit/b9d49ab7a06e7ed37fdc5e19dafdb92e5e888f2a", "https://github.com/apache/airflow/commit/9bd616854c76224cd0d4e49a67d3c065b50430d2", "https://github.com/apache/airflow/commit/4295c7ffc9f1fd687008ff0a862c3797606b49a9", "https://github.com/apache/airflow/commit/40b681d3a1d956c7986b668f5a825d502c004fe4", "https://github.com/apache/airflow/commit/19e27612159a18dcaca1ea040791f9c42f9ade98", "https://github.com/apache/airflow/commit/0870adf915e86da0acc5c22f6efe7b9829a20ead", "https://github.com/apache/airflow/commit/0ffea1db2f701a39dbab659d5c1c551becb48d60", "https://github.com/apache/airflow/commit/67bcc9189ee6621ae6ad68994438abbe8ff1bdc8", "https://github.com/apache/airflow/commit/14f816b391e90cba13c65eb6936c9f3d75e5a599", "https://github.com/apache/airflow/commit/5ee03922675c3abe19bc33e25b77759beeba39cd", "https://github.com/apache/airflow/commit/d37e174fff2355b37833f7d8603eaf414d16dac3", "https://github.com/apache/airflow/commit/0005c3b2600a3ef69c4d507453db85a091467309", "https://github.com/apache/airflow/commit/628fa5ad670db9465988bb829c2f2fbdab379b4b", "https://github.com/apache/airflow/commit/a36495b70337e9538cddbe02ca673eb4a6841566", "https://github.com/apache/airflow/commit/b16d0c8cec48441eaacb2d228be7ce3bceef1e46", "https://github.com/apache/airflow/commit/ac541f64d9a037af129c3d1c046b430d2a57e347", "https://github.com/apache/airflow/commit/cbb25fba9776b490c16dae4be613e9d62bf481fb", "https://github.com/apache/airflow/commit/6b5f8ccc0d3ae48f8d3bf39810a191acd5d35955", "https://github.com/apache/airflow/commit/a50a6a148f643ef9e3eb8ff7095b636d2cb6bf61", "https://github.com/apache/airflow/commit/624eb26149c24f2e67c084dc57eef7c3df977ed5", "https://github.com/apache/airflow/commit/774c6608cc9f6a73fb9137a58c9cd8de3e60acd5", "https://github.com/apache/airflow/commit/f2ea098dcd49b4aea88a4fc63051b725accedd71", "https://github.com/apache/airflow/commit/54f5b7d71dd4851db19d1f10c9b544f12cf309d2", "https://github.com/apache/airflow/commit/021f0850dafcdaab79e54c826fa551b0fdfe4711", "https://github.com/apache/airflow/commit/f5421196469bd4944d4e39b1ba6f494c5e8a86b4", "https://github.com/apache/airflow/commit/56567c15ee8f86028821efa8e383152472996d8a", "https://github.com/apache/airflow/commit/eced084cc5658f976321f7532ca8483a5ff2a318", "https://github.com/apache/airflow/commit/dc91b1d0b21962d6f57ddb4485a93955177c0ed3", "https://github.com/apache/airflow/commit/2e20cd78618c38b5bf5ba9b52bbb91431d852c64", "https://github.com/apache/airflow/commit/ba006dead0faafe2b61a43f9e01c288e2020b898", "https://github.com/apache/airflow/commit/b28d662cd8e73fb90ab1608b79abdb940d378383", "https://github.com/apache/airflow/commit/f4ad45b5225151c62554e46db45f7564a2559765", "https://github.com/apache/airflow/commit/5eab381ceec40ec49a7c4b7ec555325c95ebe42c", "https://github.com/apache/airflow/commit/867ec757e20fa50652b67790afe9dc66fbc6e329", "https://github.com/apache/airflow/commit/8cc458ed10f295d45e5b3a067a395a3fbe6648e7"], "created_at": "2025-06-25T22:35:56Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 52403, "instance_id": "apache__airflow-52403", "issue_numbers": [52301], "base_commit": "56c4701a15483990cd9ab1311269c9eb722b6699", "patch": "diff --git a/airflow-core/src/airflow/api_fastapi/core_api/datamodels/connections.py b/airflow-core/src/airflow/api_fastapi/core_api/datamodels/connections.py\nindex 2a6bd6bec8af2..f2ac1f7a9403a 100644\n--- a/airflow-core/src/airflow/api_fastapi/core_api/datamodels/connections.py\n+++ b/airflow-core/src/airflow/api_fastapi/core_api/datamodels/connections.py\n@@ -17,15 +17,12 @@\n \n from __future__ import annotations\n \n-import json\n from collections import abc\n from typing import Annotated\n \n-from pydantic import Field, field_validator\n-from pydantic_core.core_schema import ValidationInfo\n+from pydantic import Field\n \n from airflow.api_fastapi.core_api.base import BaseModel, StrictBaseModel\n-from airflow.sdk.execution_time.secrets_masker import redact\n \n \n # Response Models\n@@ -42,26 +39,6 @@ class ConnectionResponse(BaseModel):\n     password: str | None\n     extra: str | None\n \n-    @field_validator(\"password\", mode=\"after\")\n-    @classmethod\n-    def redact_password(cls, v: str | None, field_info: ValidationInfo) -> str | None:\n-        if v is None:\n-            return None\n-        return str(redact(v, field_info.field_name))\n-\n-    @field_validator(\"extra\", mode=\"before\")\n-    @classmethod\n-    def redact_extra(cls, v: str | None) -> str | None:\n-        if v is None:\n-            return None\n-        try:\n-            extra_dict = json.loads(v)\n-            redacted_dict = redact(extra_dict)\n-            return json.dumps(redacted_dict)\n-        except json.JSONDecodeError:\n-            # we can't redact fields in an unstructured `extra`\n-            return v\n-\n \n class ConnectionCollectionResponse(BaseModel):\n     \"\"\"Connection Collection serializer for responses.\"\"\"\ndiff --git a/airflow-core/src/airflow/ui/src/pages/Connections/ConnectionForm.tsx b/airflow-core/src/airflow/ui/src/pages/Connections/ConnectionForm.tsx\nindex bf921a1de3c99..9e4e38647eaf4 100644\n--- a/airflow-core/src/airflow/ui/src/pages/Connections/ConnectionForm.tsx\n+++ b/airflow-core/src/airflow/ui/src/pages/Connections/ConnectionForm.tsx\n@@ -57,7 +57,7 @@ const ConnectionForm = ({\n   const { conf: extra, setConf } = useParamStore();\n   const {\n     control,\n-    formState: { isValid },\n+    formState: { isDirty, isValid },\n     handleSubmit,\n     reset,\n     watch,\n@@ -94,6 +94,14 @@ const ConnectionForm = ({\n     mutateConnection(data);\n   };\n \n+  const hasChanges = () => {\n+    if (isDirty) {\n+      return true;\n+    }\n+\n+    return JSON.stringify(JSON.parse(extra)) !== JSON.stringify(JSON.parse(initialConnection.extra));\n+  };\n+\n   const validateAndPrettifyJson = (value: string) => {\n     try {\n       const parsedJson = JSON.parse(value) as JSON;\n@@ -234,7 +242,7 @@ const ConnectionForm = ({\n           <Spacer />\n           <Button\n             colorPalette=\"blue\"\n-            disabled={Boolean(errors.conf) || formErrors || isPending || !isValid}\n+            disabled={Boolean(errors.conf) || formErrors || isPending || !isValid || !hasChanges()}\n             onClick={() => void handleSubmit(onSubmit)()}\n           >\n             <FiSave /> {translate(\"formActions.save\")}\ndiff --git a/airflow-core/src/airflow/ui/src/queries/useEditConnection.tsx b/airflow-core/src/airflow/ui/src/queries/useEditConnection.tsx\nindex 60c813aa644dd..2392eda11d656 100644\n--- a/airflow-core/src/airflow/ui/src/queries/useEditConnection.tsx\n+++ b/airflow-core/src/airflow/ui/src/queries/useEditConnection.tsx\n@@ -65,7 +65,9 @@ export const useEditConnection = (\n   const editConnection = (requestBody: ConnectionBody) => {\n     const updateMask: Array<string> = [];\n \n-    if (requestBody.extra !== initialConnection.extra) {\n+    if (\n+      JSON.stringify(JSON.parse(requestBody.extra)) !== JSON.stringify(JSON.parse(initialConnection.extra))\n+    ) {\n       updateMask.push(\"extra\");\n     }\n     if (requestBody.conn_type !== initialConnection.conn_type) {\n", "test_patch": "diff --git a/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py b/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py\nindex 6c56232a68a92..eba9f6ddaf83e 100644\n--- a/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py\n+++ b/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py\n@@ -159,35 +159,6 @@ def test_get_should_respond_200_with_extra(self, test_client, session):\n         assert body[\"conn_type\"] == TEST_CONN_TYPE\n         assert body[\"extra\"] == '{\"extra_key\": \"extra_value\"}'\n \n-    @pytest.mark.enable_redact\n-    def test_get_should_respond_200_with_extra_redacted(self, test_client, session):\n-        self.create_connection()\n-        connection = session.query(Connection).first()\n-        connection.extra = '{\"password\": \"test-password\"}'\n-        session.commit()\n-        response = test_client.get(f\"/connections/{TEST_CONN_ID}\")\n-        assert response.status_code == 200\n-        body = response.json()\n-        assert body[\"connection_id\"] == TEST_CONN_ID\n-        assert body[\"conn_type\"] == TEST_CONN_TYPE\n-        assert body[\"extra\"] == '{\"password\": \"***\"}'\n-\n-    @pytest.mark.enable_redact\n-    def test_get_should_not_overmask_short_password_value_in_extra(self, test_client, session):\n-        connection = Connection(\n-            conn_id=TEST_CONN_ID, conn_type=\"generic\", login=\"a\", password=\"a\", extra='{\"key\": \"value\"}'\n-        )\n-        session.add(connection)\n-        session.commit()\n-\n-        response = test_client.get(f\"/connections/{TEST_CONN_ID}\")\n-        assert response.status_code == 200\n-        body = response.json()\n-        assert body[\"connection_id\"] == TEST_CONN_ID\n-        assert body[\"conn_type\"] == \"generic\"\n-        assert body[\"login\"] == \"a\"\n-        assert body[\"extra\"] == '{\"key\": \"value\"}'\n-\n \n class TestGetConnections(TestConnectionEndpoint):\n     @pytest.mark.parametrize(\n@@ -309,7 +280,6 @@ def test_post_should_respond_already_exist(self, test_client, body):\n         assert \"detail\" in response_json\n         assert list(response_json[\"detail\"].keys()) == [\"reason\", \"statement\", \"orig_error\", \"message\"]\n \n-    @pytest.mark.enable_redact\n     @pytest.mark.parametrize(\n         \"body, expected_response\",\n         [\n@@ -322,21 +292,7 @@ def test_post_should_respond_already_exist(self, test_client, body):\n                     \"extra\": None,\n                     \"host\": None,\n                     \"login\": None,\n-                    \"password\": \"***\",\n-                    \"port\": None,\n-                    \"schema\": None,\n-                },\n-            ),\n-            (\n-                {\"connection_id\": TEST_CONN_ID, \"conn_type\": TEST_CONN_TYPE, \"password\": \"?>@#+!_%()#\"},\n-                {\n-                    \"connection_id\": TEST_CONN_ID,\n-                    \"conn_type\": TEST_CONN_TYPE,\n-                    \"description\": None,\n-                    \"extra\": None,\n-                    \"host\": None,\n-                    \"login\": None,\n-                    \"password\": \"***\",\n+                    \"password\": \"test-password\",\n                     \"port\": None,\n                     \"schema\": None,\n                 },\n@@ -352,21 +308,23 @@ def test_post_should_respond_already_exist(self, test_client, body):\n                     \"connection_id\": TEST_CONN_ID,\n                     \"conn_type\": TEST_CONN_TYPE,\n                     \"description\": None,\n-                    \"extra\": '{\"password\": \"***\"}',\n+                    \"extra\": '{\"password\": \"test-password\"}',\n                     \"host\": None,\n                     \"login\": None,\n-                    \"password\": \"***\",\n+                    \"password\": \"A!rF|0wi$aw3s0m3\",\n                     \"port\": None,\n                     \"schema\": None,\n                 },\n             ),\n         ],\n     )\n-    def test_post_should_response_201_redacted_password(self, test_client, body, expected_response, session):\n+    def test_post_should_response_201_password_not_masked(\n+        self, test_client, body, expected_response, session\n+    ):\n         response = test_client.post(\"/connections\", json=body)\n         assert response.status_code == 201\n         assert response.json() == expected_response\n-        _check_last_log(session, dag_id=None, event=\"post_connection\", logical_date=None, check_masked=True)\n+        _check_last_log(session, dag_id=None, event=\"post_connection\", logical_date=None)\n \n \n class TestPatchConnection(TestConnectionEndpoint):\n@@ -776,22 +734,7 @@ def test_patch_should_respond_404(self, test_client, body):\n                     \"extra\": None,\n                     \"host\": \"some_host_a\",\n                     \"login\": \"some_login\",\n-                    \"password\": \"***\",\n-                    \"port\": 8080,\n-                    \"schema\": None,\n-                },\n-                {\"update_mask\": [\"password\"]},\n-            ),\n-            (\n-                {\"connection_id\": TEST_CONN_ID, \"conn_type\": TEST_CONN_TYPE, \"password\": \"?>@#+!_%()#\"},\n-                {\n-                    \"connection_id\": TEST_CONN_ID,\n-                    \"conn_type\": TEST_CONN_TYPE,\n-                    \"description\": \"some_description_a\",\n-                    \"extra\": None,\n-                    \"host\": \"some_host_a\",\n-                    \"login\": \"some_login\",\n-                    \"password\": \"***\",\n+                    \"password\": \"test-password\",\n                     \"port\": 8080,\n                     \"schema\": None,\n                 },\n@@ -808,10 +751,10 @@ def test_patch_should_respond_404(self, test_client, body):\n                     \"connection_id\": TEST_CONN_ID,\n                     \"conn_type\": TEST_CONN_TYPE,\n                     \"description\": \"some_description_a\",\n-                    \"extra\": '{\"password\": \"***\"}',\n+                    \"extra\": '{\"password\": \"test-password\"}',\n                     \"host\": \"some_host_a\",\n                     \"login\": \"some_login\",\n-                    \"password\": \"***\",\n+                    \"password\": \"A!rF|0wi$aw3s0m3\",\n                     \"port\": 8080,\n                     \"schema\": None,\n                 },\n@@ -819,14 +762,14 @@ def test_patch_should_respond_404(self, test_client, body):\n             ),\n         ],\n     )\n-    def test_patch_should_response_200_redacted_password(\n+    def test_patch_should_response_200_password_not_masked(\n         self, test_client, session, body, expected_response, update_mask\n     ):\n         self.create_connections()\n         response = test_client.patch(f\"/connections/{TEST_CONN_ID}\", json=body, params=update_mask)\n         assert response.status_code == 200\n         assert response.json() == expected_response\n-        _check_last_log(session, dag_id=None, event=\"patch_connection\", logical_date=None, check_masked=True)\n+        _check_last_log(session, dag_id=None, event=\"patch_connection\", logical_date=None)\n \n \n class TestConnection(TestConnectionEndpoint):\n", "problem_statement": "Editing connection with sensitive extra field saves literal asterisks\n### Apache Airflow version\n\n3.0.2 (actually 3.0.0+astro.2)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nIf an connection has an extra field with a name like \"token\" or other sensitive keyword, its value gets masked as \"***\" when displayed.  If you edit and save the connection (with or without any actual changes to any field), the extra field will get updated to a literal \"***\".\n\n### What you think should happen instead?\n\nExtra fields which are deemed sensitive should retain their original values when saving, if unmodified.\n\n### How to reproduce\n\n1. In the Airflow UI, add a new connection with any type, e.g. \"http\".\n2. In the \"Extra Fields JSON\" section, add a sensitive key/value pair such as `{\"token\": \"abcde\"}`.\n3. Save the connection.\n4. Confirm the value saved correctly in an unobfuscated way - for example `airflow connections export ...`.\n5. Click to edit the connection.  Note that the value for \"token\" in Extra Fields renders as \"***\".\n6. Save.\n7. Repeat step 4 and note that the value is now the literal \"***\".\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm)\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-http==5.3.0\napache-airflow-providers-standard==1.0.0\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\nHappens in Astronomer, as well as locally.\n\n### Anything else?\n\nHappens always\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\nCC @shubhamraj-git would you like to look at this?\n\n", "all_hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\nCC @shubhamraj-git would you like to look at this?\nFix will be reverted: https://github.com/apache/airflow/pull/53888\n\nThat is not how we should handle things.\nSince extra Fields are stored as a whole and keys are arbitrary, I don\u2019t think we can do anything at all \u201cfield level\u201d.\n\nWhat we can do though is check whether or not the frond-end input for extras is dirty, and use the field mask to not update the extra if it\u2019s not.\n\nIf it is dirty, look for redacted fields and warn the user about it before saving. \n\n", "commit_urls": ["https://github.com/apache/airflow/commit/6ac30af405a65f47ed77dc152a18f31a93092fc2", "https://github.com/apache/airflow/commit/ca4f141dd74fdb788863d6c333b8fd8e108c54a5", "https://github.com/apache/airflow/commit/b05e72e22ea87f4f51c41bc346e1a08fa2b7cbd3", "https://github.com/apache/airflow/commit/51f9666b1a5a7c903f820e73a93315d11a5df60c", "https://github.com/apache/airflow/commit/ef9a4931a4235204c8f785115b04acdf3587a948"], "created_at": "2025-06-28T10:35:58Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 53943, "instance_id": "apache__airflow-53943", "issue_numbers": [52301, 53753], "base_commit": "c4bc393b088e59c0a231e8b87d3b88155cbfa0d8", "patch": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 2f144e403de36..e6b19bf586e07 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -1636,6 +1636,7 @@ repos:\n           ^airflow-core/src/airflow/api/common/mark_tasks\\.py$|\n           ^airflow-core/src/airflow/api_fastapi/core_api/datamodels/assets\\.py$|\n           ^airflow-core/src/airflow/api_fastapi/core_api/datamodels/connections\\.py$|\n+          ^airflow-core/src/airflow/api_fastapi/core_api/services/public/connections\\.py$|\n           ^airflow-core/src/airflow/api_fastapi/core_api/datamodels/hitl\\.py$|\n           ^airflow-core/src/airflow/api_fastapi/core_api/datamodels/variables\\.py$|\n           ^airflow-core/src/airflow/api_fastapi/core_api/routes/ui/grid\\.py$|\ndiff --git a/airflow-core/src/airflow/api_fastapi/core_api/services/public/connections.py b/airflow-core/src/airflow/api_fastapi/core_api/services/public/connections.py\nindex 4c236cf9756be..05ab88ab90c1a 100644\n--- a/airflow-core/src/airflow/api_fastapi/core_api/services/public/connections.py\n+++ b/airflow-core/src/airflow/api_fastapi/core_api/services/public/connections.py\n@@ -17,6 +17,8 @@\n \n from __future__ import annotations\n \n+import json\n+\n from fastapi import HTTPException, status\n from pydantic import ValidationError\n from sqlalchemy import select\n@@ -32,6 +34,7 @@\n from airflow.api_fastapi.core_api.datamodels.connections import ConnectionBody\n from airflow.api_fastapi.core_api.services.public.common import BulkService\n from airflow.models.connection import Connection\n+from airflow.sdk.execution_time.secrets_masker import merge\n \n \n def update_orm_from_pydantic(\n@@ -56,11 +59,23 @@ def update_orm_from_pydantic(\n     if (not update_mask and \"password\" in pydantic_conn.model_fields_set) or (\n         update_mask and \"password\" in update_mask\n     ):\n-        orm_conn.set_password(pydantic_conn.password)\n+        if pydantic_conn.password is None:\n+            orm_conn.set_password(pydantic_conn.password)\n+        else:\n+            merged_password = merge(pydantic_conn.password, orm_conn.password, \"password\")\n+            orm_conn.set_password(merged_password)\n     if (not update_mask and \"extra\" in pydantic_conn.model_fields_set) or (\n         update_mask and \"extra\" in update_mask\n     ):\n-        orm_conn.set_extra(pydantic_conn.extra)\n+        if pydantic_conn.extra is None or orm_conn.extra is None:\n+            orm_conn.set_extra(pydantic_conn.extra)\n+            return\n+        try:\n+            merged_extra = merge(json.loads(pydantic_conn.extra), json.loads(orm_conn.extra))\n+            orm_conn.set_extra(json.dumps(merged_extra))\n+        except json.JSONDecodeError:\n+            # We can't merge fields in an unstructured `extra`\n+            orm_conn.set_extra(pydantic_conn.extra)\n \n \n class BulkConnectionService(BulkService[ConnectionBody]):\ndiff --git a/airflow-core/src/airflow/ui/public/i18n/locales/en/admin.json b/airflow-core/src/airflow/ui/public/i18n/locales/en/admin.json\nindex 1e90ba8266df3..0eb5df5b8a22d 100644\n--- a/airflow-core/src/airflow/ui/public/i18n/locales/en/admin.json\n+++ b/airflow-core/src/airflow/ui/public/i18n/locales/en/admin.json\n@@ -36,6 +36,7 @@\n       \"extraFields\": \"Extra Fields\",\n       \"extraFieldsJson\": \"Extra Fields JSON\",\n       \"helperText\": \"Connection type missing? Make sure you have installed the corresponding Airflow Providers Package.\",\n+      \"helperTextForRedactedFields\": \"Redacted fields ('***') will remain unchanged if not modified.\",\n       \"selectConnectionType\": \"Select Connection Type\",\n       \"standardFields\": \"Standard Fields\"\n     },\ndiff --git a/airflow-core/src/airflow/ui/src/components/FlexibleForm/FlexibleForm.tsx b/airflow-core/src/airflow/ui/src/components/FlexibleForm/FlexibleForm.tsx\nindex 4fea751e59960..cf9f2bf4b8518 100644\n--- a/airflow-core/src/airflow/ui/src/components/FlexibleForm/FlexibleForm.tsx\n+++ b/airflow-core/src/airflow/ui/src/components/FlexibleForm/FlexibleForm.tsx\n@@ -32,12 +32,14 @@ export type FlexibleFormProps = {\n   initialParamsDict: { paramsDict: ParamsSpec };\n   key?: string;\n   setError: (error: boolean) => void;\n+  subHeader?: string;\n };\n \n export const FlexibleForm = ({\n   flexibleFormDefaultSection,\n   initialParamsDict,\n   setError,\n+  subHeader,\n }: FlexibleFormProps) => {\n   const { paramsDict: params, setInitialParamDict, setParamsDict } = useParamStore();\n   const processedSections = new Map();\n@@ -126,6 +128,11 @@ export const FlexibleForm = ({\n \n               <Accordion.ItemContent pt={0}>\n                 <Accordion.ItemBody>\n+                  {Boolean(subHeader) ? (\n+                    <Text color=\"fg.muted\" fontSize=\"xs\" mb={2}>\n+                      {subHeader}\n+                    </Text>\n+                  ) : undefined}\n                   <Stack separator={<StackSeparator />}>\n                     {Object.entries(params)\n                       .filter(\ndiff --git a/airflow-core/src/airflow/ui/src/pages/Connections/ConnectionForm.tsx b/airflow-core/src/airflow/ui/src/pages/Connections/ConnectionForm.tsx\nindex bf921a1de3c99..65a2d9ed6dae0 100644\n--- a/airflow-core/src/airflow/ui/src/pages/Connections/ConnectionForm.tsx\n+++ b/airflow-core/src/airflow/ui/src/pages/Connections/ConnectionForm.tsx\n@@ -37,6 +37,7 @@ import type { ConnectionBody } from \"./Connections\";\n type AddConnectionFormProps = {\n   readonly error: unknown;\n   readonly initialConnection: ConnectionBody;\n+  readonly isEditMode?: boolean;\n   readonly isPending: boolean;\n   readonly mutateConnection: (requestBody: ConnectionBody) => void;\n };\n@@ -44,6 +45,7 @@ type AddConnectionFormProps = {\n const ConnectionForm = ({\n   error,\n   initialConnection,\n+  isEditMode = false,\n   isPending,\n   mutateConnection,\n }: AddConnectionFormProps) => {\n@@ -202,6 +204,7 @@ const ConnectionForm = ({\n               initialParamsDict={paramsDic}\n               key={selectedConnType}\n               setError={setFormErrors}\n+              subHeader={isEditMode ? translate(\"connections.form.helperTextForRedactedFields\") : undefined}\n             />\n             <Accordion.Item key=\"extraJson\" value=\"extraJson\">\n               <Accordion.ItemTrigger cursor=\"button\">\n@@ -220,6 +223,11 @@ const ConnectionForm = ({\n                         }}\n                       />\n                       {Boolean(errors.conf) ? <Field.ErrorText>{errors.conf}</Field.ErrorText> : undefined}\n+                      {isEditMode ? (\n+                        <Field.HelperText>\n+                          {translate(\"connections.form.helperTextForRedactedFields\")}\n+                        </Field.HelperText>\n+                      ) : undefined}\n                     </Field.Root>\n                   )}\n                 />\ndiff --git a/airflow-core/src/airflow/ui/src/pages/Connections/EditConnectionButton.tsx b/airflow-core/src/airflow/ui/src/pages/Connections/EditConnectionButton.tsx\nindex 1e3b7eccfabeb..f7c8c204ed0e7 100644\n--- a/airflow-core/src/airflow/ui/src/pages/Connections/EditConnectionButton.tsx\n+++ b/airflow-core/src/airflow/ui/src/pages/Connections/EditConnectionButton.tsx\n@@ -81,6 +81,7 @@ const EditConnectionButton = ({ connection, disabled }: Props) => {\n             <ConnectionForm\n               error={error}\n               initialConnection={initialConnectionValue}\n+              isEditMode\n               isPending={isPending}\n               mutateConnection={editConnection}\n             />\ndiff --git a/task-sdk/src/airflow/sdk/execution_time/secrets_masker.py b/task-sdk/src/airflow/sdk/execution_time/secrets_masker.py\nindex b6a7b359b3af4..1080d2f2d5d97 100644\n--- a/task-sdk/src/airflow/sdk/execution_time/secrets_masker.py\n+++ b/task-sdk/src/airflow/sdk/execution_time/secrets_masker.py\n@@ -27,7 +27,7 @@\n from enum import Enum\n from functools import cache, cached_property\n from re import Pattern\n-from typing import Any, TextIO, TypeAlias, TypeVar\n+from typing import Any, TextIO, TypeAlias, TypeVar, overload\n \n from airflow import settings\n \n@@ -116,6 +116,27 @@ def redact(value: Redactable, name: str | None = None, max_depth: int | None = N\n     return _secrets_masker().redact(value, name, max_depth)\n \n \n+@overload\n+def merge(new_value: str, old_value: str, name: str | None = None, max_depth: int | None = None) -> str: ...\n+\n+\n+@overload\n+def merge(new_value: dict, old_value: dict, name: str | None = None, max_depth: int | None = None) -> str: ...\n+\n+\n+def merge(\n+    new_value: Redacted, old_value: Redactable, name: str | None = None, max_depth: int | None = None\n+) -> Redacted:\n+    \"\"\"\n+    Merge a redacted value with its original unredacted counterpart.\n+\n+    Takes a user-modified redacted value and merges it with the original unredacted value.\n+    For sensitive fields that still contain \"***\" (unchanged), the original value is restored.\n+    For fields that have been updated by the user, the new value is preserved.\n+    \"\"\"\n+    return _secrets_masker().merge(new_value, old_value, name, max_depth)\n+\n+\n @cache\n def _secrets_masker() -> SecretsMasker:\n     for flt in logging.getLogger(\"airflow.task\").filters:\n@@ -292,6 +313,83 @@ def _redact(self, item: Redactable, name: str | None, depth: int, max_depth: int\n             )\n             return item\n \n+    def _merge(\n+        self,\n+        new_item: Redacted,\n+        old_item: Redactable,\n+        name: str | None,\n+        depth: int,\n+        max_depth: int,\n+        force_sensitive: bool = False,\n+    ) -> Redacted:\n+        \"\"\"Merge a redacted item with its original unredacted counterpart.\"\"\"\n+        if depth > max_depth:\n+            if isinstance(new_item, str) and new_item == \"***\":\n+                return old_item\n+            return new_item\n+\n+        try:\n+            # Determine if we should treat this as sensitive\n+            is_sensitive = force_sensitive or (name is not None and should_hide_value_for_key(name))\n+\n+            if isinstance(new_item, dict) and isinstance(old_item, dict):\n+                merged = {}\n+                for key in new_item.keys():\n+                    if key in old_item:\n+                        # For dicts, pass the key as name unless we're in sensitive mode\n+                        child_name = None if is_sensitive else key\n+                        merged[key] = self._merge(\n+                            new_item[key],\n+                            old_item[key],\n+                            name=child_name,\n+                            depth=depth + 1,\n+                            max_depth=max_depth,\n+                            force_sensitive=is_sensitive,\n+                        )\n+                    else:\n+                        merged[key] = new_item[key]\n+                return merged\n+\n+            if isinstance(new_item, (list, tuple)) and type(old_item) is type(new_item):\n+                merged_list = []\n+                for i in range(len(new_item)):\n+                    if i < len(old_item):\n+                        # In sensitive mode, check if individual item is redacted\n+                        if is_sensitive and isinstance(new_item[i], str) and new_item[i] == \"***\":\n+                            merged_list.append(old_item[i])\n+                        else:\n+                            merged_list.append(\n+                                self._merge(\n+                                    new_item[i],\n+                                    old_item[i],\n+                                    name=None,\n+                                    depth=depth + 1,\n+                                    max_depth=max_depth,\n+                                    force_sensitive=is_sensitive,\n+                                )\n+                            )\n+                    else:\n+                        merged_list.append(new_item[i])\n+\n+                if isinstance(new_item, list):\n+                    return list(merged_list)\n+                return tuple(merged_list)\n+\n+            if isinstance(new_item, set) and isinstance(old_item, set):\n+                # Sets are unordered, we cannot restore original items.\n+                return new_item\n+\n+            if _is_v1_env_var(new_item) and _is_v1_env_var(old_item):\n+                # TODO: Handle Kubernetes V1EnvVar objects if needed\n+                return new_item\n+\n+            if is_sensitive and isinstance(new_item, str) and new_item == \"***\":\n+                return old_item\n+            return new_item\n+\n+        except (TypeError, AttributeError, ValueError):\n+            return new_item\n+\n     def redact(self, item: Redactable, name: str | None = None, max_depth: int | None = None) -> Redacted:\n         \"\"\"\n         Redact an any secrets found in ``item``, if it is a string.\n@@ -302,6 +400,25 @@ def redact(self, item: Redactable, name: str | None = None, max_depth: int | Non\n         \"\"\"\n         return self._redact(item, name, depth=0, max_depth=max_depth or self.MAX_RECURSION_DEPTH)\n \n+    def merge(\n+        self, new_item: Redacted, old_item: Redactable, name: str | None = None, max_depth: int | None = None\n+    ) -> Redacted:\n+        \"\"\"\n+        Merge a redacted item with its original unredacted counterpart.\n+\n+        Takes a user-modified redacted item and merges it with the original unredacted item.\n+        For sensitive fields that still contain \"***\" (unchanged), the original value is restored.\n+        For fields that have been updated, the new value is preserved.\n+        \"\"\"\n+        return self._merge(\n+            new_item,\n+            old_item,\n+            name=name,\n+            depth=0,\n+            max_depth=max_depth or self.MAX_RECURSION_DEPTH,\n+            force_sensitive=False,\n+        )\n+\n     @cached_property\n     def _mask_adapter(self) -> None | Callable:\n         \"\"\"\n", "test_patch": "diff --git a/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py b/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py\nindex 6c56232a68a92..33acdef6f413e 100644\n--- a/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py\n+++ b/airflow-core/tests/unit/api_fastapi/core_api/routes/public/test_connections.py\n@@ -468,6 +468,27 @@ class TestPatchConnection(TestConnectionEndpoint):\n                     \"schema\": None,\n                 },\n             ),\n+            (\n+                # Sensitive \"***\" should be ignored.\n+                {\n+                    \"connection_id\": TEST_CONN_ID,\n+                    \"conn_type\": TEST_CONN_TYPE,\n+                    \"port\": 80,\n+                    \"login\": \"test_login_patch\",\n+                    \"password\": \"***\",\n+                },\n+                {\n+                    \"conn_type\": TEST_CONN_TYPE,\n+                    \"connection_id\": TEST_CONN_ID,\n+                    \"description\": TEST_CONN_DESCRIPTION,\n+                    \"extra\": None,\n+                    \"host\": TEST_CONN_HOST,\n+                    \"login\": \"test_login_patch\",\n+                    \"password\": None,\n+                    \"port\": 80,\n+                    \"schema\": None,\n+                },\n+            ),\n             (\n                 {\n                     \"connection_id\": TEST_CONN_ID,\ndiff --git a/task-sdk/tests/task_sdk/definitions/test_secrets_masker.py b/task-sdk/tests/task_sdk/definitions/test_secrets_masker.py\nindex ca20cf2c9f4a6..4d3c3e7d5769a 100644\n--- a/task-sdk/tests/task_sdk/definitions/test_secrets_masker.py\n+++ b/task-sdk/tests/task_sdk/definitions/test_secrets_masker.py\n@@ -34,6 +34,7 @@\n     RedactedIO,\n     SecretsMasker,\n     mask_secret,\n+    merge,\n     redact,\n     reset_secrets_masker,\n     should_hide_value_for_key,\n@@ -53,6 +54,7 @@ def lineno():\n \n class MyEnum(str, Enum):\n     testname = \"testvalue\"\n+    testname2 = \"testvalue2\"\n \n \n @pytest.fixture\n@@ -729,3 +731,305 @@ def test_mixed_structured_unstructured_data(self):\n             assert \"***\" in redacted_data[\"description\"]\n             assert redacted_data[\"nested\"][\"token\"] == \"***\"\n             assert redacted_data[\"nested\"][\"info\"] == \"No secrets here\"\n+\n+\n+class TestSecretsMaskerMerge:\n+    \"\"\"Test the merge functionality for restoring original values from redacted data.\"\"\"\n+\n+    @pytest.mark.parametrize(\n+        (\"new_value\", \"old_value\", \"name\", \"expected\"),\n+        [\n+            (\"***\", \"original_secret\", \"password\", \"original_secret\"),\n+            (\"new_secret\", \"original_secret\", \"password\", \"new_secret\"),\n+            (\"***\", \"original_value\", \"normal_field\", \"***\"),\n+            (\"new_value\", \"original_value\", \"normal_field\", \"new_value\"),\n+            (\"***\", \"original_value\", None, \"***\"),\n+            (\"new_value\", \"original_value\", None, \"new_value\"),\n+        ],\n+    )\n+    @pytest.mark.usefixtures(\"patched_secrets_masker\")\n+    def test_merge_simple_strings(self, new_value, old_value, name, expected):\n+        result = merge(new_value, old_value, name)\n+        assert result == expected\n+\n+    @pytest.mark.parametrize(\n+        (\"old_data\", \"new_data\", \"expected\"),\n+        [\n+            (\n+                {\n+                    \"password\": \"original_password\",\n+                    \"api_key\": \"original_api_key\",\n+                    \"normal_field\": \"original_normal\",\n+                },\n+                {\n+                    \"password\": \"***\",\n+                    \"api_key\": \"new_api_key\",\n+                    \"normal_field\": \"new_normal\",\n+                },\n+                {\n+                    \"password\": \"original_password\",\n+                    \"api_key\": \"new_api_key\",\n+                    \"normal_field\": \"new_normal\",\n+                },\n+            ),\n+            (\n+                {\n+                    \"config\": {\"password\": \"original_password\", \"host\": \"original_host\"},\n+                    \"credentials\": {\"api_key\": \"original_api_key\", \"username\": \"original_user\"},\n+                },\n+                {\n+                    \"config\": {\n+                        \"password\": \"***\",\n+                        \"host\": \"new_host\",\n+                    },\n+                    \"credentials\": {\n+                        \"api_key\": \"new_api_key\",\n+                        \"username\": \"new_user\",\n+                    },\n+                },\n+                {\n+                    \"config\": {\n+                        \"password\": \"original_password\",\n+                        \"host\": \"new_host\",\n+                    },\n+                    \"credentials\": {\n+                        \"api_key\": \"new_api_key\",\n+                        \"username\": \"new_user\",\n+                    },\n+                },\n+            ),\n+        ],\n+    )\n+    @pytest.mark.usefixtures(\"patched_secrets_masker\")\n+    def test_merge_dictionaries(self, old_data, new_data, expected):\n+        result = merge(new_data, old_data)\n+        assert result == expected\n+\n+    @pytest.mark.parametrize(\n+        (\"old_data\", \"new_data\", \"name\", \"expected\"),\n+        [\n+            # Lists\n+            (\n+                [\"original_item1\", \"original_item2\", \"original_item3\"],\n+                [\"new_item1\", \"new_item2\"],\n+                None,\n+                [\"new_item1\", \"new_item2\"],\n+            ),\n+            (\n+                [\"original_item1\", \"original_item2\"],\n+                [\"new_item1\", \"new_item2\", \"new_item3\", \"new_item4\"],\n+                None,\n+                [\"new_item1\", \"new_item2\", \"new_item3\", \"new_item4\"],\n+            ),\n+            (\n+                [\"secret1\", \"secret2\", \"secret3\"],\n+                [\"***\", \"new_secret2\", \"***\"],\n+                \"password\",\n+                [\"secret1\", \"new_secret2\", \"secret3\"],\n+            ),\n+            (\n+                [\"value1\", \"value2\", \"value3\"],\n+                [\"***\", \"new_value2\", \"***\"],\n+                \"normal_list\",\n+                [\"***\", \"new_value2\", \"***\"],\n+            ),\n+            # Tuples\n+            (\n+                (\"original_item1\", \"original_item2\", \"original_item3\"),\n+                (\"new_item1\", \"new_item2\"),\n+                None,\n+                (\"new_item1\", \"new_item2\"),\n+            ),\n+            (\n+                (\"original_item1\", \"original_item2\"),\n+                (\"new_item1\", \"new_item2\", \"new_item3\", \"new_item4\"),\n+                None,\n+                (\"new_item1\", \"new_item2\", \"new_item3\", \"new_item4\"),\n+            ),\n+            (\n+                (\"secret1\", \"secret2\", \"secret3\"),\n+                (\"***\", \"new_secret2\", \"***\"),\n+                \"password\",\n+                (\"secret1\", \"new_secret2\", \"secret3\"),\n+            ),\n+            (\n+                (\"value1\", \"value2\", \"value3\"),\n+                (\"***\", \"new_value2\", \"***\"),\n+                \"normal_tuple\",\n+                (\"***\", \"new_value2\", \"***\"),\n+            ),\n+            # Sets\n+            (\n+                {\"original_item1\", \"original_item2\", \"original_item3\"},\n+                {\"new_item1\", \"new_item2\"},\n+                None,\n+                {\"new_item1\", \"new_item2\"},\n+            ),\n+            (\n+                {\"original_item1\", \"original_item2\"},\n+                {\"new_item1\", \"new_item2\", \"new_item3\", \"new_item4\"},\n+                None,\n+                {\"new_item1\", \"new_item2\", \"new_item3\", \"new_item4\"},\n+            ),\n+            (\n+                {\"secret1\", \"secret2\", \"secret3\"},\n+                {\"***\", \"new_secret2\", \"***\"},\n+                \"password\",\n+                {\"***\", \"new_secret2\", \"***\"},\n+            ),\n+            (\n+                {\"value1\", \"value2\", \"value3\"},\n+                {\"***\", \"new_value2\", \"***\"},\n+                \"normal_tuple\",\n+                {\"***\", \"new_value2\", \"***\"},\n+            ),\n+        ],\n+    )\n+    @pytest.mark.usefixtures(\"patched_secrets_masker\")\n+    def test_merge_collections(self, old_data, new_data, name, expected):\n+        result = merge(new_data, old_data, name)\n+        assert result == expected\n+\n+    @pytest.mark.usefixtures(\"patched_secrets_masker\")\n+    def test_merge_mismatched_types(self):\n+        old_data = {\"key\": \"value\"}\n+        new_data = \"some_string\"  # Different type\n+\n+        # When types don't match, prefer the new item\n+        expected = \"some_string\"\n+\n+        result = merge(new_data, old_data)\n+        assert result == expected\n+\n+    @pytest.mark.usefixtures(\"patched_secrets_masker\")\n+    def test_merge_with_missing_keys(self):\n+        old_data = {\"password\": \"original_password\", \"old_only_key\": \"old_value\", \"common_key\": \"old_common\"}\n+\n+        new_data = {\n+            \"password\": \"***\",\n+            \"new_only_key\": \"new_value\",\n+            \"common_key\": \"new_common\",\n+        }\n+\n+        expected = {\n+            \"password\": \"original_password\",\n+            \"new_only_key\": \"new_value\",\n+            \"common_key\": \"new_common\",\n+        }\n+\n+        result = merge(new_data, old_data)\n+        assert result == expected\n+\n+    @pytest.mark.usefixtures(\"patched_secrets_masker\")\n+    def test_merge_complex_redacted_structures(self):\n+        old_data = {\n+            \"some_config\": {\n+                \"nested_password\": \"original_nested_password\",\n+                \"passwords\": [\"item1\", \"item2\"],\n+            },\n+            \"normal_field\": \"normal_value\",\n+        }\n+\n+        new_data = {\n+            \"some_config\": {\"nested_password\": \"***\", \"passwords\": [\"***\", \"new_item2\"]},\n+            \"normal_field\": \"new_normal_value\",\n+        }\n+\n+        result = merge(new_data, old_data)\n+        expected = {\n+            \"some_config\": {\n+                \"nested_password\": \"original_nested_password\",\n+                \"passwords\": [\"item1\", \"new_item2\"],\n+            },\n+            \"normal_field\": \"new_normal_value\",\n+        }\n+        assert result == expected\n+\n+    @pytest.mark.usefixtures(\"patched_secrets_masker\")\n+    def test_merge_partially_redacted_structures(self):\n+        old_data = {\n+            \"config\": {\n+                \"password\": \"original_password\",\n+                \"host\": \"original_host\",\n+                \"nested\": {\"api_key\": \"original_api_key\", \"timeout\": 30},\n+            }\n+        }\n+\n+        new_data = {\n+            \"config\": {\n+                \"password\": \"***\",\n+                \"host\": \"new_host\",\n+                \"nested\": {\n+                    \"api_key\": \"***\",\n+                    \"timeout\": 60,\n+                },\n+            }\n+        }\n+\n+        expected = {\n+            \"config\": {\n+                \"password\": \"original_password\",\n+                \"host\": \"new_host\",\n+                \"nested\": {\n+                    \"api_key\": \"original_api_key\",\n+                    \"timeout\": 60,\n+                },\n+            }\n+        }\n+\n+        result = merge(new_data, old_data)\n+        assert result == expected\n+\n+    @pytest.mark.usefixtures(\"patched_secrets_masker\")\n+    def test_merge_max_depth(self):\n+        old_data = {\"level1\": {\"level2\": {\"level3\": {\"password\": \"original_password\"}}}}\n+        new_data = {\"level1\": {\"level2\": {\"level3\": {\"password\": \"***\"}}}}\n+\n+        result = merge(new_data, old_data, max_depth=1)\n+        assert result == new_data\n+\n+        result = merge(new_data, old_data, max_depth=10)\n+        assert result[\"level1\"][\"level2\"][\"level3\"][\"password\"] == \"original_password\"\n+\n+    @pytest.mark.usefixtures(\"patched_secrets_masker\")\n+    def test_merge_enum_values(self):\n+        old_enum = MyEnum.testname\n+        new_enum = MyEnum.testname2\n+\n+        result = merge(new_enum, old_enum)\n+        assert result == new_enum\n+        assert isinstance(result, MyEnum)\n+\n+    @pytest.mark.usefixtures(\"patched_secrets_masker\")\n+    def test_merge_round_trip(self):\n+        # Original data with sensitive information\n+        original_config = {\n+            \"database\": {\"host\": \"db.example.com\", \"password\": \"super_secret_password\", \"username\": \"admin\"},\n+            \"api\": {\"api_key\": \"secret_api_key_12345\", \"endpoint\": \"https://api.example.com\", \"timeout\": 30},\n+            \"app_name\": \"my_application\",\n+        }\n+\n+        # Step 1: Redact the original data\n+        redacted_dict = redact(original_config)\n+\n+        # Verify sensitive fields are redacted\n+        assert redacted_dict[\"database\"][\"password\"] == \"***\"\n+        assert redacted_dict[\"api\"][\"api_key\"] == \"***\"\n+        assert redacted_dict[\"database\"][\"host\"] == \"db.example.com\"\n+\n+        # Step 2: User modifies some fields\n+        updated_dict = redacted_dict.copy()\n+        updated_dict[\"database\"][\"host\"] = \"new-db.example.com\"\n+        updated_dict[\"api\"][\"timeout\"] = 60\n+        updated_dict[\"api\"][\"api_key\"] = \"new_api_key_67890\"\n+        # User left password as \"***\" (unchanged)\n+\n+        # Step 3: Merge to restore unchanged sensitive values\n+        final_dict = merge(updated_dict, original_config)\n+\n+        # Verify the results\n+        assert final_dict[\"database\"][\"password\"] == \"super_secret_password\"  # Restored\n+        assert final_dict[\"database\"][\"host\"] == \"new-db.example.com\"  # User modification kept\n+        assert final_dict[\"api\"][\"api_key\"] == \"new_api_key_67890\"  # User modification kept\n+        assert final_dict[\"api\"][\"timeout\"] == 60  # User modification kept\n+        assert final_dict[\"app_name\"] == \"my_application\"  # Unchanged\n", "problem_statement": "Editing connection with sensitive extra field saves literal asterisks\n### Apache Airflow version\n\n3.0.2 (actually 3.0.0+astro.2)\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nIf an connection has an extra field with a name like \"token\" or other sensitive keyword, its value gets masked as \"***\" when displayed.  If you edit and save the connection (with or without any actual changes to any field), the extra field will get updated to a literal \"***\".\n\n### What you think should happen instead?\n\nExtra fields which are deemed sensitive should retain their original values when saving, if unmodified.\n\n### How to reproduce\n\n1. In the Airflow UI, add a new connection with any type, e.g. \"http\".\n2. In the \"Extra Fields JSON\" section, add a sensitive key/value pair such as `{\"token\": \"abcde\"}`.\n3. Save the connection.\n4. Confirm the value saved correctly in an unobfuscated way - for example `airflow connections export ...`.\n5. Click to edit the connection.  Note that the value for \"token\" in Extra Fields renders as \"***\".\n6. Save.\n7. Repeat step 4 and note that the value is now the literal \"***\".\n\n### Operating System\n\nDebian GNU/Linux 12 (bookworm)\n\n### Versions of Apache Airflow Providers\n\napache-airflow-providers-http==5.3.0\napache-airflow-providers-standard==1.0.0\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\nHappens in Astronomer, as well as locally.\n\n### Anything else?\n\nHappens always\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\nConnection passwords visible in UI\n### Apache Airflow version\n\n3.0.3\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nIn airflow 3.0.2 the `AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS` is set to True and you are not allowed to see passwords, secrets ect, as they are hidden with `***`. However when updating to airflow 3.0.3 the connections are accessible through the UI.\n\n### What you think should happen instead?\n\nI would keep the passwords hashed and hidden from the UI. Seen in this image it should be like:\n\n\n<img width=\"861\" height=\"373\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ede6e247-6117-485f-bff1-3d94c7833077\" />\n\nand not like this where someone can go grab it \n\n<img width=\"861\" height=\"449\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ec99421d-8d7c-41bf-9555-2d0962de1345\" />\n\n### How to reproduce\n\nMake sure that you have airflow 3.0.2 installed and go make a connection. You will see the passwords are not filtered and show raw on the frontend UI.\n\n### Operating System\n\nWe tried on linux x64 and on arm64\n\n### Versions of Apache Airflow Providers\n\n```\napache-airflow-providers-amazon==9.9.0\napache-airflow-providers-celery==3.12.1\napache-airflow-providers-cncf-kubernetes==10.6.1\napache-airflow-providers-common-compat==1.7.2\napache-airflow-providers-common-io==1.6.1\napache-airflow-providers-common-messaging==1.0.4\napache-airflow-providers-common-sql==1.27.3\napache-airflow-providers-docker==4.4.1\napache-airflow-providers-elasticsearch==6.3.1\napache-airflow-providers-fab==2.3.0\napache-airflow-providers-ftp==3.13.1\napache-airflow-providers-git==0.0.4\napache-airflow-providers-google==16.1.0\napache-airflow-providers-grpc==3.8.1\napache-airflow-providers-hashicorp==4.3.1\napache-airflow-providers-http==5.3.2\napache-airflow-providers-microsoft-azure==12.5.0\napache-airflow-providers-mysql==6.3.2\napache-airflow-providers-odbc==4.10.1\napache-airflow-providers-openlineage==2.5.0\napache-airflow-providers-postgres==6.2.1\napache-airflow-providers-redis==4.1.1\napache-airflow-providers-sendgrid==4.1.2\napache-airflow-providers-sftp==5.3.2\napache-airflow-providers-slack==9.1.2\napache-airflow-providers-smtp==2.1.1\napache-airflow-providers-snowflake==6.5.0\napache-airflow-providers-ssh==4.1.1\napache-airflow-providers-standard==1.4.1\n```\n\n### Deployment\n\nOfficial Apache Airflow Helm Chart\n\n### Deployment details\n\nUsing k8s we deployed using the helm chart on the arm64 machine and used argo to deploy on the linux machine.\n\n### Anything else?\n\nThis bug happens every time and I suspect its not a big issue which can be resolved for next release.\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\nCC @shubhamraj-git would you like to look at this?\nFix will be reverted: https://github.com/apache/airflow/pull/53888\n\nThat is not how we should handle things.\nSince extra Fields are stored as a whole and keys are arbitrary, I don\u2019t think we can do anything at all \u201cfield level\u201d.\n\nWhat we can do though is check whether or not the frond-end input for extras is dirty, and use the field mask to not update the extra if it\u2019s not.\n\nIf it is dirty, look for redacted fields and warn the user about it before saving. \n\nThanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\n> Make sure that you have airflow 3.0.2 installed and go make a connection. You will see the passwords are not filtered and show raw on the frontend UI.\n\nJust to validate, this is for Airflow 3.0.2, or 3.0.3?\ncc: @vatsrahul1001 \nVerified this is a new bug introduced in 3.0.3. This works fine with 3.0.2\ncc: @pierrejeambrun \nThis change seems to be deliberate, introduced in https://github.com/apache/airflow/pull/52403 and I really do not understand the motivation there. That seems plain wrong but I might be missing some context. (Basically you can query the API and retrieve sensitive fields unmasked with that, which completely defeat the purpose of `AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS`)\n@pierrejeambrun Agreed, That change needs reverting and 3.0.4 released immediately. The UI should be updated to not try to send the password then the form is submitted. \nThere were other reasons discussed on [thread](https://apache-airflow.slack.com/archives/C0809U4S1Q9/p1751108215738019), which saw potential in removing that. But if we have `AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS` to configure that accordingly then it makes complete sense to have it. Thanks @pierrejeambrun \n\n", "all_hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\nCC @shubhamraj-git would you like to look at this?\nFix will be reverted: https://github.com/apache/airflow/pull/53888\n\nThat is not how we should handle things.\nSince extra Fields are stored as a whole and keys are arbitrary, I don\u2019t think we can do anything at all \u201cfield level\u201d.\n\nWhat we can do though is check whether or not the frond-end input for extras is dirty, and use the field mask to not update the extra if it\u2019s not.\n\nIf it is dirty, look for redacted fields and warn the user about it before saving. \n\nThanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\n> Make sure that you have airflow 3.0.2 installed and go make a connection. You will see the passwords are not filtered and show raw on the frontend UI.\n\nJust to validate, this is for Airflow 3.0.2, or 3.0.3?\ncc: @vatsrahul1001 \nVerified this is a new bug introduced in 3.0.3. This works fine with 3.0.2\ncc: @pierrejeambrun \nThis change seems to be deliberate, introduced in https://github.com/apache/airflow/pull/52403 and I really do not understand the motivation there. That seems plain wrong but I might be missing some context. (Basically you can query the API and retrieve sensitive fields unmasked with that, which completely defeat the purpose of `AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS`)\n@pierrejeambrun Agreed, That change needs reverting and 3.0.4 released immediately. The UI should be updated to not try to send the password then the form is submitted. \nThere were other reasons discussed on [thread](https://apache-airflow.slack.com/archives/C0809U4S1Q9/p1751108215738019), which saw potential in removing that. But if we have `AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS` to configure that accordingly then it makes complete sense to have it. Thanks @pierrejeambrun \n\n", "commit_urls": ["https://github.com/apache/airflow/commit/1ec59efdff5930fbb2f2304618302476ea2e73ae", "https://github.com/apache/airflow/commit/f53ba11878f5a054edbac1045fc5aeaca0f0a58f", "https://github.com/apache/airflow/commit/a98ffe064e346230ab25c19b5035092fc71586b3", "https://github.com/apache/airflow/commit/a6058fed7f1f78b519e61e8a8de5e949da838c7c", "https://github.com/apache/airflow/commit/1499d2900ed2096ee691f71d4cf619258d4b43ac"], "created_at": "2025-07-30T17:39:50Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 52897, "instance_id": "apache__airflow-52897", "issue_numbers": [52332], "base_commit": "a9bcc9b2e33075fcafeee356940892aa6c0f7b1a", "patch": "diff --git a/providers/git/src/airflow/providers/git/bundles/git.py b/providers/git/src/airflow/providers/git/bundles/git.py\nindex 1b4084f1db2f9..18a393749163c 100644\n--- a/providers/git/src/airflow/providers/git/bundles/git.py\n+++ b/providers/git/src/airflow/providers/git/bundles/git.py\n@@ -80,17 +80,14 @@ def __init__(\n \n         self._log.debug(\"bundle configured\")\n         self.hook: GitHook | None = None\n-        if not repo_url:\n-            if not git_conn_id:\n-                self._log.debug(\"Neither git_conn_id nor repo_url provided; loading 'git_default'\")\n-                git_conn_id = \"git_default\"\n-            try:\n-                self.hook = GitHook(git_conn_id=git_conn_id)\n-            except AirflowException as e:\n-                self._log.warning(\"Could not create GitHook\", conn_id=git_conn_id, exc=e)\n-            else:\n-                self.repo_url = self.hook.repo_url\n-                self._log.debug(\"repo_url updated from hook\")\n+        try:\n+            self.hook = GitHook(git_conn_id=git_conn_id or \"git_default\")\n+        except AirflowException as e:\n+            self._log.warning(\"Could not create GitHook\", conn_id=git_conn_id, exc=e)\n+\n+        if not repo_url and self.hook and self.hook.repo_url:\n+            self.repo_url = self.hook.repo_url\n+            self._log.debug(\"repo_url updated from hook\")\n \n     def _initialize(self):\n         with self.lock():\n", "test_patch": "diff --git a/providers/git/tests/unit/git/bundles/test_git.py b/providers/git/tests/unit/git/bundles/test_git.py\nindex b6392d58533e5..1a97c726c8773 100644\n--- a/providers/git/tests/unit/git/bundles/test_git.py\n+++ b/providers/git/tests/unit/git/bundles/test_git.py\n@@ -20,6 +20,7 @@\n import json\n import os\n import re\n+import types\n from unittest import mock\n from unittest.mock import patch\n \n@@ -602,3 +603,29 @@ def test_repo_url_precedence(self, conn_json, repo_url, expected):\n                 repo_url=repo_url,\n             )\n             assert bundle.repo_url == expected\n+\n+    @mock.patch(\"airflow.providers.git.bundles.git.Repo\")\n+    def test_clone_passes_env_from_githook(self, mock_gitRepo):\n+        def _fake_clone_from(*_, **kwargs):\n+            if \"env\" not in kwargs:\n+                raise GitCommandError(\"git\", 128, \"Permission denied\")\n+            return types.SimpleNamespace()\n+\n+        EXPECTED_ENV = {\"GIT_SSH_COMMAND\": \"ssh -i /id_rsa -o StrictHostKeyChecking=no\"}\n+\n+        mock_gitRepo.clone_from.side_effect = _fake_clone_from\n+        mock_gitRepo.return_value = types.SimpleNamespace()\n+\n+        with mock.patch(\"airflow.providers.git.bundles.git.GitHook\") as mock_githook:\n+            mock_githook.return_value.repo_url = \"git@github.com:apache/airflow.git\"\n+            mock_githook.return_value.env = EXPECTED_ENV\n+\n+            bundle = GitDagBundle(\n+                name=\"my_repo\",\n+                git_conn_id=\"git_default\",\n+                repo_url=\"git@github.com:apache/airflow.git\",\n+                tracking_ref=\"main\",\n+            )\n+            bundle._clone_bare_repo_if_required()\n+            _, kwargs = mock_gitRepo.clone_from.call_args\n+            assert kwargs[\"env\"] == EXPECTED_ENV\n", "problem_statement": "Repo URLs provided in the git bundle config prevent git hooks executing\n### Apache Airflow version\n\n3.0.2\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nIf the `repo_url` is provided in the `kwargs` of the `GitDagBundle` config, a `GitHook` is never created due to this if condition: https://github.com/apache/airflow/blob/3c1568ae7734153691201aa9a4996a9e4793450e/providers/git/src/airflow/providers/git/bundles/git.py#L83. \n\nThis means authentication (key file or private key) and environment configuration does not happen on clone repo commands (see https://github.com/apache/airflow/blob/3c1568ae7734153691201aa9a4996a9e4793450e/providers/git/src/airflow/providers/git/bundles/git.py#L144 for example where hook is None). \n\nA simple workaround is to not provide `repo_url` in the config and use `host` in the connection definition but the git bundle should cope with both scenarios.\n\n### What you think should happen instead?\n\nThe GitDagBundle should configure the GitHook even if the repo_url is provided in the bundle config.\n\n### How to reproduce\n\nCreate a private git repository, and configure the git bundle config list something like the following:\n```\n[{\"name\": \"dags-folder\", \"classpath\": \"airflow.providers.git.bundles.git.GitDagBundle\", \"kwargs\": {\"repo_url\": \"git@github.com:myorg/my-repo.git\"    , \"tracking_ref\": \"main\", \"subdir\": \"dags\"}}]\n```\n\nIn addition configure a git connection; it could be like:\n```\n{\"conn_type\":\"git\",\"extra\":{\"private_key\": \"-----BEGIN OPENSSH PRIVATE KEY-----\\nb3...REDACTED...lu\\n-----END OPENSSH PRIVATE KEY-----\\n\", \"strict_host_key_checking\": \"no\"},\"host\":\"git@github.com:myorg/my-repo.git\"}\n```\n\n\n### Operating System\n\nDebian GNU/Linux\n\n### Versions of Apache Airflow Providers\n\napache_airflow_providers_git-0.0.2\n\n### Deployment\n\nAstronomer\n\n### Deployment details\n\n_No response_\n\n### Anything else?\n\nEvery time if repo_url is configured.\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "hints_text": "\n\n", "all_hints_text": "\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/b9faf5f6ebbe994be6961c0bedfe0008fb6e5cb7", "https://github.com/apache/airflow/commit/bbdb95bd225a17d0f5c2acc869cbc1d349ef2f21", "https://github.com/apache/airflow/commit/fd4cc29f6bcec0fe0b52fdcf264b8ec231030de2", "https://github.com/apache/airflow/commit/29b65da74877074ff0b1705073fb532c8803b254", "https://github.com/apache/airflow/commit/dfb9778b396ea2c076bebd4b6c9ede4fe0e9e39f"], "created_at": "2025-07-04T23:54:22Z", "classification": "Security"}
{"repo": "apache/airflow", "pull_number": 53574, "instance_id": "apache__airflow-53574", "issue_numbers": [53493], "base_commit": "67596f9432ecebf19710b8a72b67e1e76e8cd8be", "patch": "diff --git a/airflow-core/src/airflow/config_templates/config.yml b/airflow-core/src/airflow/config_templates/config.yml\nindex 748139694c0e5..3a824fbcda6d7 100644\n--- a/airflow-core/src/airflow/config_templates/config.yml\n+++ b/airflow-core/src/airflow/config_templates/config.yml\n@@ -1408,6 +1408,8 @@ api:\n       description: |\n         Paths to the SSL certificate and key for the api server. When both are\n         provided SSL will be enabled. This does not change the api server port.\n+        The same SSL certificate will also be loaded into the worker to enable\n+        it to be trusted when a self-signed certificate is used.\n       version_added: ~\n       type: string\n       example: ~\ndiff --git a/task-sdk/src/airflow/sdk/api/client.py b/task-sdk/src/airflow/sdk/api/client.py\nindex 0822045852900..6ec13cd070242 100644\n--- a/task-sdk/src/airflow/sdk/api/client.py\n+++ b/task-sdk/src/airflow/sdk/api/client.py\n@@ -18,11 +18,13 @@\n from __future__ import annotations\n \n import logging\n+import ssl\n import sys\n import uuid\n from http import HTTPStatus\n from typing import TYPE_CHECKING, Any, TypeVar\n \n+import certifi\n import httpx\n import msgspec\n import structlog\n@@ -747,6 +749,7 @@ def noop_handler(request: httpx.Request) -> httpx.Response:\n API_RETRIES = conf.getint(\"workers\", \"execution_api_retries\")\n API_RETRY_WAIT_MIN = conf.getfloat(\"workers\", \"execution_api_retry_wait_min\")\n API_RETRY_WAIT_MAX = conf.getfloat(\"workers\", \"execution_api_retry_wait_max\")\n+API_SSL_CERT_PATH = conf.get(\"api\", \"ssl_cert\")\n \n \n class Client(httpx.Client):\n@@ -762,6 +765,10 @@ def __init__(self, *, base_url: str | None, dry_run: bool = False, token: str, *\n             kwargs.setdefault(\"base_url\", \"dry-run://server\")\n         else:\n             kwargs[\"base_url\"] = base_url\n+            ctx = ssl.create_default_context(cafile=certifi.where())\n+            if API_SSL_CERT_PATH:\n+                ctx.load_verify_locations(API_SSL_CERT_PATH)\n+            kwargs[\"verify\"] = ctx\n         pyver = f\"{'.'.join(map(str, sys.version_info[:3]))}\"\n         super().__init__(\n             auth=auth,\n", "test_patch": "diff --git a/task-sdk/tests/task_sdk/api/test_client.py b/task-sdk/tests/task_sdk/api/test_client.py\nindex 30cfd2fd7d616..b1a2ef63926aa 100644\n--- a/task-sdk/tests/task_sdk/api/test_client.py\n+++ b/task-sdk/tests/task_sdk/api/test_client.py\n@@ -88,6 +88,16 @@ def test_dry_run(self, path, json_response):\n         assert resp.status_code == 200\n         assert resp.json() == json_response\n \n+    @mock.patch(\"airflow.sdk.api.client.API_SSL_CERT_PATH\", \"/capath/does/not/exist/\")\n+    def test_add_capath(self):\n+        def handle_request(request: httpx.Request) -> httpx.Response:\n+            return httpx.Response(status_code=200)\n+\n+        with pytest.raises(FileNotFoundError) as err:\n+            make_client(httpx.MockTransport(handle_request))\n+\n+        assert isinstance(err.value, FileNotFoundError)\n+\n     def test_error_parsing(self):\n         responses = [\n             httpx.Response(422, json={\"detail\": [{\"loc\": [\"#0\"], \"msg\": \"err\", \"type\": \"required\"}]})\n", "problem_statement": "Unable to use a self-signed certificate due to API failing on verify\n### Apache Airflow version\n\n3.0.3\n\n### If \"Other Airflow 2 version\" selected, which one?\n\n_No response_\n\n### What happened?\n\nWhen using Docker with a self-signed certificate all appears to initially be well from the front end but DAGS will never run and eventually get marked as failed. The message in the run log is:\n\n```\nERROR - Executor reports task instance <...> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?\n```\n\nChecking container logs shows that the worker is unable to communicate with the API server due to an SSL certificate verification error:\n\n```\nairflow-worker-1 | 2025-07-18 09:21:32.543021 [error] Task execute_workload[73ed6338-5c59-450c-8e6a-284d52e0cf60] raised unexpected: ConnectError('[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1010)') [celery.app.trace]\n```\n\n#50726 mentions this and a possible workaround by assigning alternative names localhost and airflow-apiserver to the self-signed certificate then adding it into the container. I tried this and was unable to get it to work with the same error message.\n\n### What you think should happen instead?\n\nI think not working immediately is sensible but there should be a config parameter to disable certificate verification with suitable warning against doing so.\n\nThe setup should still work without needing to import the self-signed certificate to get it verified.\n\n### How to reproduce\n\nSetup Airflow as per https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html then add SSL with:\n\n```\nAIRFLOW__WEBSERVER__WEB_SERVER_SSL_CERT: <cert>\nAIRFLOW__WEBSERVER__WEB_SERVER_SSL_KEY: <key>\n```\n\nOr their equivalent new API versions. Set the below to HTTPS and add -k flag for healthcheck:\n\n```\n        AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'https://airflow-apiserver:8080/execution/'\n\n            test: [\"CMD\", \"curl\", \"--fail\", \"-k\", \"https://localhost:8080/api/v2/version\"]\n```\n\nStart up Airflow, try to run a DAG and check airflow-worker-1 logs.\n\n### Operating System\n\nUbuntu 22.04.5 LTS\n\n### Versions of Apache Airflow Providers\n\nNot relevant.\n\n### Deployment\n\nDocker-Compose\n\n### Deployment details\n\nCompose file as provided with amendments already give. Docker version 28.3.2, build 578ccf6. Nothing else relevant.\n\n### Anything else?\n\nNothing else to add.\n\n### Are you willing to submit PR?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\n", "hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\nHi, i would like to work on this issue.\nI took a look at this and see that httpx is used. This issue on httpx is relevant:\n\nhttps://github.com/encode/httpx/issues/302\n\nAs it stands there is no way to alter anything via environment variables as httpx uses certifi. It looks like a cert can be added using the verify arguement which would involve code change to the API SDK.\n\nI managed to get a self-signed certificate working by customising the image and appending it to the certifi bundle.\n\nI think a good solution would be to have the certificate used for the api-server added to the trust store. This could be done regardless of whether it is self-signed or not and would require no extra config options.\n> Hi, i would like to work on this issue.\n\nAssigned you @Subham-KRLX . \n\n> I think a good solution would be to have the certificate used for the api-server added to the trust store. This could be done regardless of whether it is self-signed or not and would require no extra config options.\n\nYep. An easy way to add the actual self-signed certificate you use would be the best approach. Any setting to \"disable\" security is a bad idea, and explicitly marking \"this certificate is secure\" is way better from security point of view.\n\nI am currently working on this issue and looking into the best way to address the certificate verification problem. I will need a bit more time to properly investigate and identify the best solution. Once I pinpoint the root cause i will raise a pr here.\n@Subham-KRLX appologies if I am stepping on your toes. I did not initially think I would be able to fix this otherwise I would have indicated I could do so.\n\nI think both methods would work but I have gone with a way that it more succinct and fetches only existing config options.\nNo problem at all @danieldean! Happy to collaborate or combine ideas if needed. \n\n", "all_hints_text": "Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\nHi, i would like to work on this issue.\nI took a look at this and see that httpx is used. This issue on httpx is relevant:\n\nhttps://github.com/encode/httpx/issues/302\n\nAs it stands there is no way to alter anything via environment variables as httpx uses certifi. It looks like a cert can be added using the verify arguement which would involve code change to the API SDK.\n\nI managed to get a self-signed certificate working by customising the image and appending it to the certifi bundle.\n\nI think a good solution would be to have the certificate used for the api-server added to the trust store. This could be done regardless of whether it is self-signed or not and would require no extra config options.\n> Hi, i would like to work on this issue.\n\nAssigned you @Subham-KRLX . \n\n> I think a good solution would be to have the certificate used for the api-server added to the trust store. This could be done regardless of whether it is self-signed or not and would require no extra config options.\n\nYep. An easy way to add the actual self-signed certificate you use would be the best approach. Any setting to \"disable\" security is a bad idea, and explicitly marking \"this certificate is secure\" is way better from security point of view.\n\nI am currently working on this issue and looking into the best way to address the certificate verification problem. I will need a bit more time to properly investigate and identify the best solution. Once I pinpoint the root cause i will raise a pr here.\n@Subham-KRLX appologies if I am stepping on your toes. I did not initially think I would be able to fix this otherwise I would have indicated I could do so.\n\nI think both methods would work but I have gone with a way that it more succinct and fetches only existing config options.\nNo problem at all @danieldean! Happy to collaborate or combine ideas if needed. \n@Subham-KRLX as suggested you could do a unit test for the PR? I am unsure how to do this.\nHi @danieldean I had like to help with this issue and assist in adding unit tests for your PR. Let me know how I can contribute!\nI am facing a persistent pre-commit failure in PR #53589 during UI build with error: \u201cpackages field missing or empty\u201d when running pnpm config set store-dir .pnpm-store in airflow-core/src/airflow/ui. I have removed the workspaces field from package.json, cleaned node_modules and lockfiles, reinstalled deps, and approved build scripts, but the issue remains locally and in CI. Could you please advise on any other workspace or pnpm configs I should adjust to fix this?\n\n\n> I am facing a persistent pre-commit failure in PR [#53589](https://github.com/apache/airflow/pull/53589) during UI build with error: \u201cpackages field missing or empty\u201d when running pnpm config set store-dir .pnpm-store in airflow-core/src/airflow/ui. I have removed the workspaces field from package.json, cleaned node_modules and lockfiles, reinstalled deps, and approved build scripts, but the issue remains locally and in CI. Could you please advise on any other workspace or pnpm configs I should adjust to fix this?\n\n```\nbreeze compile-ui-assets --force-clean\n```\n\n", "commit_urls": ["https://github.com/apache/airflow/commit/d078e30ff34b4350114cb62b893af8aff2f40b7b", "https://github.com/apache/airflow/commit/1a7b32acf16e76583b1b25ccd0c55dba2be0e0c5", "https://github.com/apache/airflow/commit/af8334fd7868e5a26c4a2630e7a3d4a91cbd17f9", "https://github.com/apache/airflow/commit/5dd389f91efeba051f92f17d751e0c730115ac20", "https://github.com/apache/airflow/commit/41a89e2148277f69529fbcc0178057e37f88fdb5"], "created_at": "2025-07-20T19:34:08Z", "classification": "Security"}
{"repo": "mem0ai/mem0", "pull_number": 2937, "instance_id": "mem0ai__mem0-2937", "issue_numbers": [2636], "base_commit": "7484eed4b230d05306d5d0eb89610d5b496c9ccc", "patch": "diff --git a/evaluation/metrics/llm_judge.py b/evaluation/metrics/llm_judge.py\nindex 20acc4afdd..55c946a0d0 100644\n--- a/evaluation/metrics/llm_judge.py\n+++ b/evaluation/metrics/llm_judge.py\n@@ -4,6 +4,7 @@\n \n import numpy as np\n from openai import OpenAI\n+\n from mem0.memory.utils import extract_json\n \n client = OpenAI()\ndiff --git a/examples/misc/diet_assistant_voice_cartesia.py b/examples/misc/diet_assistant_voice_cartesia.py\nindex 62b2bf35b8..2fb2f6c1a5 100644\n--- a/examples/misc/diet_assistant_voice_cartesia.py\n+++ b/examples/misc/diet_assistant_voice_cartesia.py\n@@ -8,10 +8,12 @@\n \"\"\"\n \n from textwrap import dedent\n+\n from agno.agent import Agent\n from agno.models.openai import OpenAIChat\n from agno.tools.cartesia import CartesiaTools\n from agno.utils.audio import write_audio_to_file\n+\n from mem0 import MemoryClient\n \n memory_client = MemoryClient()\ndiff --git a/mem0/llms/aws_bedrock.py b/mem0/llms/aws_bedrock.py\nindex b170f2983c..8266beef8e 100644\n--- a/mem0/llms/aws_bedrock.py\n+++ b/mem0/llms/aws_bedrock.py\n@@ -11,7 +11,6 @@\n from mem0.configs.llms.base import BaseLlmConfig\n from mem0.llms.base import LLMBase\n \n-\n PROVIDERS = [\"ai21\", \"amazon\", \"anthropic\", \"cohere\", \"meta\", \"mistral\", \"stability\", \"writer\"]\n \n \ndiff --git a/mem0/llms/azure_openai.py b/mem0/llms/azure_openai.py\nindex 1dcb5f3fbc..9a04a8046d 100644\n--- a/mem0/llms/azure_openai.py\n+++ b/mem0/llms/azure_openai.py\n@@ -82,6 +82,12 @@ def generate_response(\n             str: The generated response.\n         \"\"\"\n \n+        user_prompt = messages[-1]['content']\n+\n+        user_prompt = user_prompt.replace(\"assistant\", \"ai\")\n+\n+        messages[-1]['content'] = user_prompt\n+\n         common_params = {\n             \"model\": self.config.model,\n             \"messages\": messages,\ndiff --git a/mem0/llms/azure_openai_structured.py b/mem0/llms/azure_openai_structured.py\nindex 1a746fb2c6..a9361fc585 100644\n--- a/mem0/llms/azure_openai_structured.py\n+++ b/mem0/llms/azure_openai_structured.py\n@@ -48,6 +48,13 @@ def generate_response(\n         Returns:\n             str: The generated response.\n         \"\"\"\n+\n+        user_prompt = messages[-1]['content']\n+\n+        user_prompt = user_prompt.replace(\"assistant\", \"ai\")\n+\n+        messages[-1]['content'] = user_prompt\n+\n         params = {\n             \"model\": self.config.model,\n             \"messages\": messages,\ndiff --git a/mem0/llms/sarvam.py b/mem0/llms/sarvam.py\nindex b3389c94dd..6ef836ed68 100644\n--- a/mem0/llms/sarvam.py\n+++ b/mem0/llms/sarvam.py\n@@ -1,6 +1,8 @@\n import os\n-import requests\n from typing import Dict, List, Optional\n+\n+import requests\n+\n from mem0.configs.llms.base import BaseLlmConfig\n from mem0.llms.base import LLMBase\n \ndiff --git a/mem0/llms/vllm.py b/mem0/llms/vllm.py\nindex 52abf427d6..6aa13addfa 100644\n--- a/mem0/llms/vllm.py\n+++ b/mem0/llms/vllm.py\n@@ -1,8 +1,8 @@\n import json\n import os\n from typing import Dict, List, Optional\n-from openai import OpenAI\n \n+from openai import OpenAI\n \n from mem0.configs.llms.base import BaseLlmConfig\n from mem0.llms.base import LLMBase\ndiff --git a/mem0/memory/utils.py b/mem0/memory/utils.py\nindex 9018d54615..00a0a36b19 100644\n--- a/mem0/memory/utils.py\n+++ b/mem0/memory/utils.py\n@@ -1,5 +1,5 @@\n-import re\n import hashlib\n+import re\n \n from mem0.configs.prompts import FACT_RETRIEVAL_PROMPT\n \ndiff --git a/mem0/vector_stores/azure_ai_search.py b/mem0/vector_stores/azure_ai_search.py\nindex 7e06cff71f..b6ebde3713 100644\n--- a/mem0/vector_stores/azure_ai_search.py\n+++ b/mem0/vector_stores/azure_ai_search.py\n@@ -5,8 +5,8 @@\n \n from pydantic import BaseModel\n \n-from mem0.vector_stores.base import VectorStoreBase\n from mem0.memory.utils import extract_json\n+from mem0.vector_stores.base import VectorStoreBase\n \n try:\n     from azure.core.credentials import AzureKeyCredential\ndiff --git a/mem0/vector_stores/baidu.py b/mem0/vector_stores/baidu.py\nindex 0a3ed1398e..2c211abe9f 100644\n--- a/mem0/vector_stores/baidu.py\n+++ b/mem0/vector_stores/baidu.py\n@@ -8,12 +8,31 @@\n \n try:\n     import pymochow\n-    from pymochow.configuration import Configuration\n     from pymochow.auth.bce_credentials import BceCredentials\n-    from pymochow.model.enum import FieldType, MetricType, IndexType, TableState, ServerErrCode\n-    from pymochow.model.schema import Field, Schema, VectorIndex, FilteringIndex, HNSWParams, AutoBuildRowCountIncrement\n-    from pymochow.model.table import Partition, Row, VectorSearchConfig, VectorTopkSearchRequest, FloatVector\n+    from pymochow.configuration import Configuration\n     from pymochow.exception import ServerError\n+    from pymochow.model.enum import (\n+        FieldType,\n+        IndexType,\n+        MetricType,\n+        ServerErrCode,\n+        TableState,\n+    )\n+    from pymochow.model.schema import (\n+        AutoBuildRowCountIncrement,\n+        Field,\n+        FilteringIndex,\n+        HNSWParams,\n+        Schema,\n+        VectorIndex,\n+    )\n+    from pymochow.model.table import (\n+        FloatVector,\n+        Partition,\n+        Row,\n+        VectorSearchConfig,\n+        VectorTopkSearchRequest,\n+    )\n except ImportError:\n     raise ImportError(\"The 'pymochow' library is required. Please install it using 'pip install pymochow'.\")\n \ndiff --git a/mem0/vector_stores/mongodb.py b/mem0/vector_stores/mongodb.py\nindex 5b212eedc9..ef82c6e60e 100644\n--- a/mem0/vector_stores/mongodb.py\n+++ b/mem0/vector_stores/mongodb.py\n@@ -1,12 +1,12 @@\n import logging\n-from typing import List, Optional, Dict, Any\n+from typing import Any, Dict, List, Optional\n \n from pydantic import BaseModel\n \n try:\n     from pymongo import MongoClient\n-    from pymongo.operations import SearchIndexModel\n     from pymongo.errors import PyMongoError\n+    from pymongo.operations import SearchIndexModel\n except ImportError:\n     raise ImportError(\"The 'pymongo' library is required. Please install it using 'pip install pymongo'.\")\n \ndiff --git a/mem0/vector_stores/redis.py b/mem0/vector_stores/redis.py\nindex 8e85055f1c..7fb1ada9e8 100644\n--- a/mem0/vector_stores/redis.py\n+++ b/mem0/vector_stores/redis.py\n@@ -11,8 +11,8 @@\n from redisvl.query import VectorQuery\n from redisvl.query.filter import Tag\n \n-from mem0.vector_stores.base import VectorStoreBase\n from mem0.memory.utils import extract_json\n+from mem0.vector_stores.base import VectorStoreBase\n \n logger = logging.getLogger(__name__)\n \ndiff --git a/openmemory/api/alembic/env.py b/openmemory/api/alembic/env.py\nindex b4295b69fa..278cc65fdd 100644\n--- a/openmemory/api/alembic/env.py\n+++ b/openmemory/api/alembic/env.py\n@@ -1,13 +1,10 @@\n+import os\n+import sys\n from logging.config import fileConfig\n \n-from sqlalchemy import engine_from_config\n-from sqlalchemy import pool\n-\n from alembic import context\n-\n-import os\n-import sys\n from dotenv import load_dotenv\n+from sqlalchemy import engine_from_config, pool\n \n # Add the parent directory to the Python path\n sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n@@ -15,9 +12,8 @@\n # Load environment variables\n load_dotenv()\n \n-# Import your models here\n-from app.database import Base\n-from app.models import *  # Import all your models\n+# Import your models here - moved after path setup\n+from app.database import Base  # noqa: E402\n \n # this is the Alembic Config object, which provides\n # access to the values within the .ini file in use.\ndiff --git a/openmemory/api/alembic/versions/0b53c747049a_initial_migration.py b/openmemory/api/alembic/versions/0b53c747049a_initial_migration.py\nindex fb834314a9..6bbfbccab3 100644\n--- a/openmemory/api/alembic/versions/0b53c747049a_initial_migration.py\n+++ b/openmemory/api/alembic/versions/0b53c747049a_initial_migration.py\n@@ -7,9 +7,8 @@\n \"\"\"\n from typing import Sequence, Union\n \n-from alembic import op\n import sqlalchemy as sa\n-\n+from alembic import op\n \n # revision identifiers, used by Alembic.\n revision: str = '0b53c747049a'\ndiff --git a/openmemory/api/alembic/versions/add_config_table.py b/openmemory/api/alembic/versions/add_config_table.py\nindex cc7c826821..b53488f9b7 100644\n--- a/openmemory/api/alembic/versions/add_config_table.py\n+++ b/openmemory/api/alembic/versions/add_config_table.py\n@@ -5,11 +5,11 @@\n Create Date: 2023-06-01 10:00:00.000000\n \n \"\"\"\n-from alembic import op\n-import sqlalchemy as sa\n-from sqlalchemy.dialects import postgresql\n import uuid\n \n+import sqlalchemy as sa\n+from alembic import op\n+\n # revision identifiers, used by Alembic.\n revision = 'add_config_table'\n down_revision = '0b53c747049a'\ndiff --git a/openmemory/api/alembic/versions/afd00efbd06b_add_unique_user_id_constraints.py b/openmemory/api/alembic/versions/afd00efbd06b_add_unique_user_id_constraints.py\nindex a685f4e249..bec325c3b9 100644\n--- a/openmemory/api/alembic/versions/afd00efbd06b_add_unique_user_id_constraints.py\n+++ b/openmemory/api/alembic/versions/afd00efbd06b_add_unique_user_id_constraints.py\n@@ -8,8 +8,6 @@\n from typing import Sequence, Union\n \n from alembic import op\n-import sqlalchemy as sa\n-\n \n # revision identifiers, used by Alembic.\n revision: str = 'afd00efbd06b'\ndiff --git a/openmemory/api/app/database.py b/openmemory/api/app/database.py\nindex 6404ce700e..4ab4eaaa57 100644\n--- a/openmemory/api/app/database.py\n+++ b/openmemory/api/app/database.py\n@@ -1,7 +1,8 @@\n import os\n+\n+from dotenv import load_dotenv\n from sqlalchemy import create_engine\n from sqlalchemy.orm import declarative_base, sessionmaker\n-from dotenv import load_dotenv\n \n # load .env file (make sure you have DATABASE_URL set)\n load_dotenv()\ndiff --git a/openmemory/api/app/mcp_server.py b/openmemory/api/app/mcp_server.py\nindex 16bcf6c539..f4759628b5 100644\n--- a/openmemory/api/app/mcp_server.py\n+++ b/openmemory/api/app/mcp_server.py\n@@ -15,22 +15,22 @@\n - Environment variable parsing for API keys\n \"\"\"\n \n-import logging\n-import json\n-from mcp.server.fastmcp import FastMCP\n-from mcp.server.sse import SseServerTransport\n-from app.utils.memory import get_memory_client\n-from fastapi import FastAPI, Request\n-from fastapi.routing import APIRouter\n import contextvars\n-import os\n-from dotenv import load_dotenv\n+import datetime\n+import json\n+import logging\n+import uuid\n+\n from app.database import SessionLocal\n-from app.models import Memory, MemoryState, MemoryStatusHistory, MemoryAccessLog\n+from app.models import Memory, MemoryAccessLog, MemoryState, MemoryStatusHistory\n from app.utils.db import get_user_and_app\n-import uuid\n-import datetime\n+from app.utils.memory import get_memory_client\n from app.utils.permissions import check_memory_access_permissions\n+from dotenv import load_dotenv\n+from fastapi import FastAPI, Request\n+from fastapi.routing import APIRouter\n+from mcp.server.fastmcp import FastMCP\n+from mcp.server.sse import SseServerTransport\n from qdrant_client import models as qdrant_models\n \n # Load environment variables\n@@ -410,32 +410,32 @@ async def handle_get_message(request: Request):\n async def handle_post_message(request: Request):\n     return await handle_post_message(request)\n \n-async def handle_post_message(request: Request):\n-    \"\"\"Handle POST messages for SSE\"\"\"\n-    try:\n-        body = await request.body()\n+# async def handle_post_message(request: Request):\n+#     \"\"\"Handle POST messages for SSE\"\"\"\n+#     try:\n+#         body = await request.body()\n \n-        # Create a simple receive function that returns the body\n-        async def receive():\n-            return {\"type\": \"http.request\", \"body\": body, \"more_body\": False}\n+#         # Create a simple receive function that returns the body\n+#         async def receive():\n+#             return {\"type\": \"http.request\", \"body\": body, \"more_body\": False}\n \n-        # Create a simple send function that does nothing\n-        async def send(message):\n-            return {}\n+#         # Create a simple send function that does nothing\n+#         async def send(message):\n+#             return {}\n \n-        # Call handle_post_message with the correct arguments\n-        await sse.handle_post_message(request.scope, receive, send)\n+#         # Call handle_post_message with the correct arguments\n+#         await sse.handle_post_message(request.scope, receive, send)\n \n-        # Return a success response\n-        return {\"status\": \"ok\"}\n-    finally:\n-        pass\n-        # Clean up context variable\n-        # client_name_var.reset(client_token)\n+#         # Return a success response\n+#         return {\"status\": \"ok\"}\n+#     finally:\n+#         pass\n+#         # Clean up context variable\n+#         # client_name_var.reset(client_token)\n \n def setup_mcp_server(app: FastAPI):\n     \"\"\"Setup MCP server with the FastAPI application\"\"\"\n-    mcp._mcp_server.name = f\"mem0-mcp-server\"\n+    mcp._mcp_server.name = \"mem0-mcp-server\"\n \n     # Include MCP router in the FastAPI app\n     app.include_router(mcp_router)\ndiff --git a/openmemory/api/app/models.py b/openmemory/api/app/models.py\nindex 7bc3546dae..66541013b7 100644\n--- a/openmemory/api/app/models.py\n+++ b/openmemory/api/app/models.py\n@@ -1,15 +1,25 @@\n+import datetime\n import enum\n import uuid\n-import datetime\n+\n import sqlalchemy as sa\n-from sqlalchemy import (\n-    Column, String, Boolean, ForeignKey, Enum, Table,\n-    DateTime, JSON, Integer, UUID, Index, event\n-)\n-from sqlalchemy.orm import relationship\n from app.database import Base\n-from sqlalchemy.orm import Session\n from app.utils.categorization import get_categories_for_memory\n+from sqlalchemy import (\n+    JSON,\n+    UUID,\n+    Boolean,\n+    Column,\n+    DateTime,\n+    Enum,\n+    ForeignKey,\n+    Index,\n+    Integer,\n+    String,\n+    Table,\n+    event,\n+)\n+from sqlalchemy.orm import Session, relationship\n \n \n def get_current_utc_time():\ndiff --git a/openmemory/api/app/routers/__init__.py b/openmemory/api/app/routers/__init__.py\nindex 4210577c7a..519e7edded 100644\n--- a/openmemory/api/app/routers/__init__.py\n+++ b/openmemory/api/app/routers/__init__.py\n@@ -1,6 +1,6 @@\n-from .memories import router as memories_router\n from .apps import router as apps_router\n-from .stats import router as stats_router\n from .config import router as config_router\n+from .memories import router as memories_router\n+from .stats import router as stats_router\n \n-__all__ = [\"memories_router\", \"apps_router\", \"stats_router\", \"config_router\"]\n\\ No newline at end of file\n+__all__ = [\"memories_router\", \"apps_router\", \"stats_router\", \"config_router\"]\ndiff --git a/openmemory/api/app/routers/apps.py b/openmemory/api/app/routers/apps.py\nindex 36584f92b0..97f0dc898b 100644\n--- a/openmemory/api/app/routers/apps.py\n+++ b/openmemory/api/app/routers/apps.py\n@@ -1,11 +1,11 @@\n from typing import Optional\n from uuid import UUID\n-from fastapi import APIRouter, Depends, HTTPException, Query\n-from sqlalchemy.orm import Session, joinedload\n-from sqlalchemy import func, desc\n \n from app.database import get_db\n from app.models import App, Memory, MemoryAccessLog, MemoryState\n+from fastapi import APIRouter, Depends, HTTPException, Query\n+from sqlalchemy import desc, func\n+from sqlalchemy.orm import Session, joinedload\n \n router = APIRouter(prefix=\"/api/v1/apps\", tags=[\"apps\"])\n \ndiff --git a/openmemory/api/app/routers/config.py b/openmemory/api/app/routers/config.py\nindex cab9630a07..7eaae4bfa3 100644\n--- a/openmemory/api/app/routers/config.py\n+++ b/openmemory/api/app/routers/config.py\n@@ -1,12 +1,11 @@\n-import os\n-import json\n-from typing import Dict, Any, Optional\n-from fastapi import APIRouter, HTTPException, Depends\n-from pydantic import BaseModel, Field\n-from sqlalchemy.orm import Session\n+from typing import Any, Dict, Optional\n+\n from app.database import get_db\n from app.models import Config as ConfigModel\n from app.utils.memory import reset_memory_client\n+from fastapi import APIRouter, Depends, HTTPException\n+from pydantic import BaseModel, Field\n+from sqlalchemy.orm import Session\n \n router = APIRouter(prefix=\"/api/v1/config\", tags=[\"config\"])\n \ndiff --git a/openmemory/api/app/routers/memories.py b/openmemory/api/app/routers/memories.py\nindex ff7d3c4be1..f73aa702b7 100644\n--- a/openmemory/api/app/routers/memories.py\n+++ b/openmemory/api/app/routers/memories.py\n@@ -1,23 +1,28 @@\n-from datetime import datetime, UTC\n-from typing import List, Optional, Set\n-from uuid import UUID, uuid4\n import logging\n-import os\n-from fastapi import APIRouter, Depends, HTTPException, Query\n-from sqlalchemy.orm import Session, joinedload\n-from fastapi_pagination import Page, Params\n-from fastapi_pagination.ext.sqlalchemy import paginate as sqlalchemy_paginate\n-from pydantic import BaseModel\n-from sqlalchemy import or_, func\n-from app.utils.memory import get_memory_client\n+from datetime import UTC, datetime\n+from typing import List, Optional, Set\n+from uuid import UUID\n \n from app.database import get_db\n from app.models import (\n-    Memory, MemoryState, MemoryAccessLog, App,\n-    MemoryStatusHistory, User, Category, AccessControl, Config as ConfigModel\n+    AccessControl,\n+    App,\n+    Category,\n+    Memory,\n+    MemoryAccessLog,\n+    MemoryState,\n+    MemoryStatusHistory,\n+    User,\n )\n-from app.schemas import MemoryResponse, PaginatedMemoryResponse\n+from app.schemas import MemoryResponse\n+from app.utils.memory import get_memory_client\n from app.utils.permissions import check_memory_access_permissions\n+from fastapi import APIRouter, Depends, HTTPException, Query\n+from fastapi_pagination import Page, Params\n+from fastapi_pagination.ext.sqlalchemy import paginate as sqlalchemy_paginate\n+from pydantic import BaseModel\n+from sqlalchemy import func\n+from sqlalchemy.orm import Session, joinedload\n \n router = APIRouter(prefix=\"/api/v1/memories\", tags=[\"memories\"])\n \n@@ -412,7 +417,7 @@ async def pause_memories(\n         ).all()\n         for memory in memories:\n             update_memory_state(db, memory.id, state, user_id)\n-        return {\"message\": f\"Successfully paused all memories\"}\n+        return {\"message\": \"Successfully paused all memories\"}\n \n     if memory_ids:\n         # Pause specific memories\ndiff --git a/openmemory/api/app/routers/stats.py b/openmemory/api/app/routers/stats.py\nindex 047721f1c3..c609d3726e 100644\n--- a/openmemory/api/app/routers/stats.py\n+++ b/openmemory/api/app/routers/stats.py\n@@ -1,8 +1,7 @@\n+from app.database import get_db\n+from app.models import App, Memory, MemoryState, User\n from fastapi import APIRouter, Depends, HTTPException\n from sqlalchemy.orm import Session\n-from app.database import get_db\n-from app.models import User, Memory, App, MemoryState\n-\n \n router = APIRouter(prefix=\"/api/v1/stats\", tags=[\"stats\"])\n \ndiff --git a/openmemory/api/app/schemas.py b/openmemory/api/app/schemas.py\nindex 35a512f69d..f5462e7f66 100644\n--- a/openmemory/api/app/schemas.py\n+++ b/openmemory/api/app/schemas.py\n@@ -1,8 +1,10 @@\n from datetime import datetime\n-from typing import Optional, List\n+from typing import List, Optional\n from uuid import UUID\n+\n from pydantic import BaseModel, Field, validator\n \n+\n class MemoryBase(BaseModel):\n     content: str\n     metadata_: Optional[dict] = Field(default_factory=dict)\ndiff --git a/openmemory/api/app/utils/categorization.py b/openmemory/api/app/utils/categorization.py\nindex d355469158..e20c400526 100644\n--- a/openmemory/api/app/utils/categorization.py\n+++ b/openmemory/api/app/utils/categorization.py\n@@ -1,11 +1,11 @@\n import logging\n from typing import List\n \n+from app.utils.prompts import MEMORY_CATEGORIZATION_PROMPT\n from dotenv import load_dotenv\n from openai import OpenAI\n from pydantic import BaseModel\n from tenacity import retry, stop_after_attempt, wait_exponential\n-from app.utils.prompts import MEMORY_CATEGORIZATION_PROMPT\n \n load_dotenv()\n openai_client = OpenAI()\ndiff --git a/openmemory/api/app/utils/db.py b/openmemory/api/app/utils/db.py\nindex bf2fcf9cfe..50a90f6a9a 100644\n--- a/openmemory/api/app/utils/db.py\n+++ b/openmemory/api/app/utils/db.py\n@@ -1,7 +1,8 @@\n-from sqlalchemy.orm import Session\n-from app.models import User, App\n from typing import Tuple\n \n+from app.models import App, User\n+from sqlalchemy.orm import Session\n+\n \n def get_or_create_user(db: Session, user_id: str) -> User:\n     \"\"\"Get or create a user with the given user_id\"\"\"\ndiff --git a/openmemory/api/app/utils/memory.py b/openmemory/api/app/utils/memory.py\nindex a7319b9435..921e4dffba 100644\n--- a/openmemory/api/app/utils/memory.py\n+++ b/openmemory/api/app/utils/memory.py\n@@ -27,16 +27,15 @@\n }\n \"\"\"\n \n-import os\n-import json\n import hashlib\n+import json\n+import os\n import socket\n-import platform\n \n-from mem0 import Memory\n from app.database import SessionLocal\n from app.models import Config as ConfigModel\n \n+from mem0 import Memory\n \n _memory_client = None\n _config_hash = None\ndiff --git a/openmemory/api/app/utils/permissions.py b/openmemory/api/app/utils/permissions.py\nindex 4b8a04edd8..060caf962c 100644\n--- a/openmemory/api/app/utils/permissions.py\n+++ b/openmemory/api/app/utils/permissions.py\n@@ -1,7 +1,8 @@\n from typing import Optional\n from uuid import UUID\n+\n+from app.models import App, Memory, MemoryState\n from sqlalchemy.orm import Session\n-from app.models import Memory, App, MemoryState\n \n \n def check_memory_access_permissions(\ndiff --git a/openmemory/api/main.py b/openmemory/api/main.py\nindex 049d9e51ea..923ba8cfd2 100644\n--- a/openmemory/api/main.py\n+++ b/openmemory/api/main.py\n@@ -1,13 +1,14 @@\n import datetime\n-from fastapi import FastAPI\n-from app.database import engine, Base, SessionLocal\n+from uuid import uuid4\n+\n+from app.config import DEFAULT_APP_ID, USER_ID\n+from app.database import Base, SessionLocal, engine\n from app.mcp_server import setup_mcp_server\n-from app.routers import memories_router, apps_router, stats_router, config_router\n-from fastapi_pagination import add_pagination\n+from app.models import App, User\n+from app.routers import apps_router, config_router, memories_router, stats_router\n+from fastapi import FastAPI\n from fastapi.middleware.cors import CORSMiddleware\n-from app.models import User, App\n-from uuid import uuid4\n-from app.config import USER_ID, DEFAULT_APP_ID\n+from fastapi_pagination import add_pagination\n \n app = FastAPI(title=\"OpenMemory API\")\n \n", "test_patch": "diff --git a/examples/misc/test.py b/examples/misc/test.py\nindex 06c73a9d53..ec21f6ac6c 100644\n--- a/examples/misc/test.py\n+++ b/examples/misc/test.py\n@@ -1,4 +1,4 @@\n-from agents import Agent, Runner, function_tool, enable_verbose_stdout_logging\n+from agents import Agent, Runner, enable_verbose_stdout_logging, function_tool\n from dotenv import load_dotenv\n \n from mem0 import MemoryClient\ndiff --git a/tests/embeddings/test_gemini_emeddings.py b/tests/embeddings/test_gemini_emeddings.py\nindex e3254702d6..dff834dfa6 100644\n--- a/tests/embeddings/test_gemini_emeddings.py\n+++ b/tests/embeddings/test_gemini_emeddings.py\n@@ -1,4 +1,4 @@\n-from unittest.mock import patch, ANY\n+from unittest.mock import ANY, patch\n \n import pytest\n \ndiff --git a/tests/test_memory_integration.py b/tests/test_memory_integration.py\nnew file mode 100644\nindex 0000000000..899eb76d5a\n--- /dev/null\n+++ b/tests/test_memory_integration.py\n@@ -0,0 +1,175 @@\n+from unittest.mock import MagicMock, patch\n+\n+from mem0.memory.main import Memory\n+\n+\n+def test_memory_configuration_without_env_vars():\n+    \"\"\"Test Memory configuration with mock config instead of environment variables\"\"\"\n+    \n+    # Mock configuration without relying on environment variables\n+    mock_config = {\n+        \"llm\": {\n+            \"provider\": \"openai\",\n+            \"config\": {\n+                \"model\": \"gpt-4\",\n+                \"temperature\": 0.1,\n+                \"max_tokens\": 1500,\n+            }\n+        },\n+        \"vector_store\": {\n+            \"provider\": \"chroma\",\n+            \"config\": {\n+                \"collection_name\": \"test_collection\",\n+                \"path\": \"./test_db\",\n+            }\n+        },\n+        \"embedder\": {\n+            \"provider\": \"openai\",\n+            \"config\": {\n+                \"model\": \"text-embedding-ada-002\",\n+            }\n+        }\n+    }\n+    \n+    # Test messages similar to the main.py file\n+    test_messages = [\n+        {\"role\": \"user\", \"content\": \"Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts.\"},\n+        {\"role\": \"assistant\", \"content\": \"Hello Alex! I've noted that you're a vegetarian and have a nut allergy. I'll keep this in mind for any food-related recommendations or discussions.\"}\n+    ]\n+    \n+    # Mock the Memory class methods to avoid actual API calls\n+    with patch.object(Memory, '__init__', return_value=None):\n+        with patch.object(Memory, 'from_config') as mock_from_config:\n+            with patch.object(Memory, 'add') as mock_add:\n+                with patch.object(Memory, 'get_all') as mock_get_all:\n+                    \n+                    # Configure mocks\n+                    mock_memory_instance = MagicMock()\n+                    mock_from_config.return_value = mock_memory_instance\n+                    \n+                    mock_add.return_value = {\n+                        \"results\": [\n+                            {\"id\": \"1\", \"text\": \"Alex is a vegetarian\"},\n+                            {\"id\": \"2\", \"text\": \"Alex is allergic to nuts\"}\n+                        ]\n+                    }\n+                    \n+                    mock_get_all.return_value = [\n+                        {\"id\": \"1\", \"text\": \"Alex is a vegetarian\", \"metadata\": {\"category\": \"dietary_preferences\"}},\n+                        {\"id\": \"2\", \"text\": \"Alex is allergic to nuts\", \"metadata\": {\"category\": \"allergies\"}}\n+                    ]\n+                    \n+                    # Test the workflow\n+                    mem = Memory.from_config(config_dict=mock_config)\n+                    assert mem is not None\n+                    \n+                    # Test adding memories\n+                    result = mock_add(test_messages, user_id=\"alice\", metadata={\"category\": \"book_recommendations\"})\n+                    assert \"results\" in result\n+                    assert len(result[\"results\"]) == 2\n+                    \n+                    # Test retrieving memories\n+                    all_memories = mock_get_all(user_id=\"alice\")\n+                    assert len(all_memories) == 2\n+                    assert any(\"vegetarian\" in memory[\"text\"] for memory in all_memories)\n+                    assert any(\"allergic to nuts\" in memory[\"text\"] for memory in all_memories)\n+\n+\n+def test_azure_config_structure():\n+    \"\"\"Test that Azure configuration structure is properly formatted\"\"\"\n+    \n+    # Test Azure configuration structure (without actual credentials)\n+    azure_config = {\n+        \"llm\": {\n+            \"provider\": \"azure_openai\",\n+            \"config\": {\n+                \"model\": \"gpt-4\",\n+                \"temperature\": 0.1,\n+                \"max_tokens\": 1500,\n+                \"azure_kwargs\": {\n+                    \"azure_deployment\": \"test-deployment\",\n+                    \"api_version\": \"2023-12-01-preview\",\n+                    \"azure_endpoint\": \"https://test.openai.azure.com/\",\n+                    \"api_key\": \"test-key\",\n+                }\n+            }\n+        },\n+        \"vector_store\": {\n+            \"provider\": \"azure_ai_search\",\n+            \"config\": {\n+                \"service_name\": \"test-service\",\n+                \"api_key\": \"test-key\",\n+                \"collection_name\": \"test-collection\",\n+                \"embedding_model_dims\": 1536,\n+            }\n+        },\n+        \"embedder\": {\n+            \"provider\": \"azure_openai\",\n+            \"config\": {\n+                \"model\": \"text-embedding-ada-002\",\n+                \"api_key\": \"test-key\",\n+                \"azure_kwargs\": {\n+                    \"api_version\": \"2023-12-01-preview\",\n+                    \"azure_deployment\": \"test-embedding-deployment\",\n+                    \"azure_endpoint\": \"https://test.openai.azure.com/\",\n+                    \"api_key\": \"test-key\",\n+                }\n+            }\n+        }\n+    }\n+    \n+    # Validate configuration structure\n+    assert \"llm\" in azure_config\n+    assert \"vector_store\" in azure_config\n+    assert \"embedder\" in azure_config\n+    \n+    # Validate Azure-specific configurations\n+    assert azure_config[\"llm\"][\"provider\"] == \"azure_openai\"\n+    assert \"azure_kwargs\" in azure_config[\"llm\"][\"config\"]\n+    assert \"azure_deployment\" in azure_config[\"llm\"][\"config\"][\"azure_kwargs\"]\n+    \n+    assert azure_config[\"vector_store\"][\"provider\"] == \"azure_ai_search\"\n+    assert \"service_name\" in azure_config[\"vector_store\"][\"config\"]\n+    \n+    assert azure_config[\"embedder\"][\"provider\"] == \"azure_openai\"\n+    assert \"azure_kwargs\" in azure_config[\"embedder\"][\"config\"]\n+\n+\n+def test_memory_messages_format():\n+    \"\"\"Test that memory messages are properly formatted\"\"\"\n+    \n+    # Test message format from main.py\n+    messages = [\n+        {\"role\": \"user\", \"content\": \"Hi, I'm Alex. I'm a vegetarian and I'm allergic to nuts.\"},\n+        {\"role\": \"assistant\", \"content\": \"Hello Alex! I've noted that you're a vegetarian and have a nut allergy. I'll keep this in mind for any food-related recommendations or discussions.\"}\n+    ]\n+    \n+    # Validate message structure\n+    assert len(messages) == 2\n+    assert all(\"role\" in msg for msg in messages)\n+    assert all(\"content\" in msg for msg in messages)\n+    \n+    # Validate roles\n+    assert messages[0][\"role\"] == \"user\"\n+    assert messages[1][\"role\"] == \"assistant\"\n+    \n+    # Validate content\n+    assert \"vegetarian\" in messages[0][\"content\"].lower()\n+    assert \"allergic to nuts\" in messages[0][\"content\"].lower()\n+    assert \"vegetarian\" in messages[1][\"content\"].lower()\n+    assert \"nut allergy\" in messages[1][\"content\"].lower()\n+\n+\n+def test_safe_update_prompt_constant():\n+    \"\"\"Test the SAFE_UPDATE_PROMPT constant from main.py\"\"\"\n+    \n+    SAFE_UPDATE_PROMPT = \"\"\"\n+Based on the user's latest messages, what new preference can be inferred?\n+Reply only in this json_object format:\n+\"\"\"\n+    \n+    # Validate prompt structure\n+    assert isinstance(SAFE_UPDATE_PROMPT, str)\n+    assert \"user's latest messages\" in SAFE_UPDATE_PROMPT\n+    assert \"json_object format\" in SAFE_UPDATE_PROMPT\n+    assert len(SAFE_UPDATE_PROMPT.strip()) > 0\ndiff --git a/tests/vector_stores/test_baidu.py b/tests/vector_stores/test_baidu.py\nindex 987c298bac..981c7790ae 100644\n--- a/tests/vector_stores/test_baidu.py\n+++ b/tests/vector_stores/test_baidu.py\n@@ -1,11 +1,16 @@\n-from unittest.mock import Mock, patch, PropertyMock\n+from unittest.mock import Mock, PropertyMock, patch\n \n import pytest\n+from pymochow.exception import ServerError\n+from pymochow.model.enum import ServerErrCode, TableState\n+from pymochow.model.table import (\n+    FloatVector,\n+    Table,\n+    VectorSearchConfig,\n+    VectorTopkSearchRequest,\n+)\n \n from mem0.vector_stores.baidu import BaiduDB\n-from pymochow.model.enum import TableState, ServerErrCode\n-from pymochow.model.table import VectorSearchConfig, VectorTopkSearchRequest, FloatVector, Table\n-from pymochow.exception import ServerError\n \n \n @pytest.fixture\ndiff --git a/tests/vector_stores/test_mongodb.py b/tests/vector_stores/test_mongodb.py\nindex 812abfc279..abb299e01b 100644\n--- a/tests/vector_stores/test_mongodb.py\n+++ b/tests/vector_stores/test_mongodb.py\n@@ -1,5 +1,7 @@\n-import pytest\n from unittest.mock import MagicMock, patch\n+\n+import pytest\n+\n from mem0.vector_stores.mongodb import MongoDB\n \n \n", "problem_statement": "Memory.add triggering Azure OpenAI's content management policy\n### \ud83d\udc1b Describe the bug\n\n# Description\n\nWhile following the mem0 documentation, I attempted to test the mem0 module using Azure OpenAI LLM, Vector Store, and Embedding Model. However, I encountered an error related to the content filter. I tried to resolve this by adding a safe prompt in the prompt parameter and slightly changing the messages, but my efforts were unsuccessful.\n\n**Possible Causes**\n\nI suspect that the example provided in the [DOCS](https://docs.mem0.ai/open-source/python-quickstart#store-a-memory) is triggering a content filter type labeled **Indirect Attacks**. You can find more information about this filter type in the [Microsoft documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cuser-prompt%2Cpython-new#content-filter-types).\n\nI kindly request to investigate whether this error originates from the module itself or from the code provided below. Additionally, I would appreciate any solutions to this issue.\n\n# Python Code\n```python\nfrom dotenv import load_dotenv\nimport os\n\nfrom mem0 import Memory\n\nload_dotenv()\n\nllm_provider = os.getenv(\"AZURE_OPENAI_PROVIDER\")\nllm_model = os.getenv(\"AZURE_OPENAI_MODEL\")\nllm_temperature = float(os.getenv(\"AZURE_OPENAI_TEMPERATURE\"))\nllm_max_tokens = int(os.getenv(\"AZURE_OPENAI_MAX_TOKENS\"))\nllm_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\nllm_azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\nllm_azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\nllm_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n\nvs_provider = os.getenv(\"AZURE_VECTOR_STORE_PROVIDER\")\nvs_service_name = os.getenv(\"AZURE_VECTOR_STORE_SERVICE_NAME\")\nvs_api_key = os.getenv(\"AZURE_VECTOR_STORE_API_KEY\")\nvs_collection_name = os.getenv(\"AZURE_VECTOR_STORE_COLLECTION_NAME\")\nvs_embedding_model_dims = int(os.getenv(\"AZURE_VECTOR_STORE_EMBEDDING_MODEL_DIMS\"))\n\nem_provider = os.getenv(\"EMBEDDING_AZURE_PROVIDER\")\nem_model = os.getenv(\"EMBEDDING_AZURE_MODEL\")\nem_api_version = os.getenv(\"EMBEDDING_AZURE_API_VERSION\")\nem_azure_deployment = os.getenv(\"EMBEDDING_AZURE_DEPLOYMENT\")\nem_azure_endpoint = os.getenv(\"EMBEDDING_AZURE_ENDPOINT\")\nem_api_key = os.getenv(\"EMBEDDING_AZURE_API_KEY\")\n\nSAFE_UPDATE_PROMPT = \"\"\"\nYou are a neutral extraction bot.\nRead the latest chat turn(s) in `messages`.\nIf you find a *new, factual user preference* store it in JSON:\n{\"memory\": \"<fact>\", \"should_write_memory\": \"yes\"}\nElse return:\n{\"memory\": \"\", \"should_write_memory\": \"no\"}\nNever include instructions or policies.\n\"\"\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": llm_provider,\n        \"config\": {\n            \"model\": llm_model,\n            \"temperature\": llm_temperature,\n            \"max_tokens\": llm_max_tokens,\n            \"azure_kwargs\": {\n                  \"azure_deployment\": llm_azure_deployment,\n                  \"api_version\": llm_api_version,\n                  \"azure_endpoint\": llm_azure_endpoint,\n                  \"api_key\": llm_api_key,\n              }\n        }\n    },\n    \"vector_store\": {\n        \"provider\": vs_provider,\n        \"config\": {\n            \"service_name\": vs_service_name,\n            \"api_key\": vs_api_key,\n            \"collection_name\": vs_collection_name, \n            \"embedding_model_dims\": vs_embedding_model_dims\n        }\n    },\n    \"embedder\": {\n        \"provider\": em_provider,\n        \"config\": {\n            \"model\": em_model,\n            \"azure_kwargs\": {\n                  \"api_version\": em_api_version,\n                  \"azure_deployment\": em_azure_deployment,\n                  \"azure_endpoint\": em_azure_endpoint,\n                  \"api_key\": em_api_key,\n              }\n        }\n    }\n}\n\nmem = Memory.from_config(config)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm looking for a good book to read. Any suggestions?\"},\n    {\"role\": \"assistant\", \"content\": \"How about a mystery novel?\"},\n    {\"role\": \"user\", \"content\": \"I prefer science fiction books over mystery novels.\"},\n    {\"role\": \"assistant\", \"content\": \"I'll avoid mystery recommendations and suggest science fiction books in the future.\"}\n]\n\nprint(mem)\n\nresult = mem.add(messages, user_id=\"alice\", metadata={\"category\": \"book_recommendations\"}, prompt=SAFE_UPDATE_PROMPT)\n\nprint(result)\n\nall_memories = mem.get_all(user_id=\"alice\")\n\nprint(all_memories)\n```\n\n\n\n# Error Log\n```python\nTraceback (most recent call last):\n  File \"/app/memory/memops.py\", line 80, in <module>\n    result = mem.add(messages, user_id=\"alice\", metadata={\"category\": \"book_recommendations\"})\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/mem0/memory/main.py\", line 182, in add\n    vector_store_result = future1.result()\n                          ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/usr/local/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/mem0/memory/main.py\", line 221, in _add_to_vector_store\n    response = self.llm.generate_response(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/mem0/llms/azure_openai.py\", line 104, in generate_response\n    response = self.client.chat.completions.create(**params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1239, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/openai/_base_client.py\", line 1034, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n```\n", "hints_text": "**New Insights into the Bug**\n\nIn the following lines of `mem0.memory.main`:\n\n- 226\n- 268\n- 959\n- 1013\n\nThe `response_format` parameter is configured as `json_object`.\n\nThe underlying cause of this bug is related to the `json_object` setting.\n\nI think if the `response_format` configured to `json_schema`, then it should fix the bug.\n\n@prateekchhikara What is the status of this issue? Any progress on the draft PR?\n@V-Silpin can you please share the PR link?\n@prateekchhikara Hi, sorry for the late reply.\n\nI will share the PR link in few hours\n\nTill then, I just want to know how do I attach a test script in the PR\nHere is the PR link\n\nhttps://github.com/mem0ai/mem0/pull/2900\n@prateekchhikara @deshraj \n\nI have fixed the issue. Just changed the keyword 'assisstant' to 'secretary' at line no. 19 in function parse_messages. \n\nFile Path : `/mem0/memory/utils.py`\n\n", "all_hints_text": "**New Insights into the Bug**\n\nIn the following lines of `mem0.memory.main`:\n\n- 226\n- 268\n- 959\n- 1013\n\nThe `response_format` parameter is configured as `json_object`.\n\nThe underlying cause of this bug is related to the `json_object` setting.\n\nI think if the `response_format` configured to `json_schema`, then it should fix the bug.\n\n@prateekchhikara What is the status of this issue? Any progress on the draft PR?\n@V-Silpin can you please share the PR link?\n@prateekchhikara Hi, sorry for the late reply.\n\nI will share the PR link in few hours\n\nTill then, I just want to know how do I attach a test script in the PR\nHere is the PR link\n\nhttps://github.com/mem0ai/mem0/pull/2900\n@prateekchhikara @deshraj \n\nI have fixed the issue. Just changed the keyword 'assisstant' to 'secretary' at line no. 19 in function parse_messages. \n\nFile Path : `/mem0/memory/utils.py`\n\n", "commit_urls": ["https://github.com/mem0ai/mem0/commit/19e717eb21d8e9fc8f8efe132cc796f55459161a", "https://github.com/mem0ai/mem0/commit/8750aa5fbe7ddba42ee4354f378cc6b34da85b2a", "https://github.com/mem0ai/mem0/commit/972fd7b2c6786649af4286513e40f6115ae55cd7", "https://github.com/mem0ai/mem0/commit/87d51b7e180305f9476e9f9bf7aec5ce15ef018d", "https://github.com/mem0ai/mem0/commit/51774645ca196087c1ade9552310d76d478a40e1", "https://github.com/mem0ai/mem0/commit/1c1366b911897c74277b70ce9cb8125a5250a3fe", "https://github.com/mem0ai/mem0/commit/2a4f7ac7159f4babc05071b030bb5fe98dc1694f", "https://github.com/mem0ai/mem0/commit/6381ce93569d28b594b0972b64ee949bc631837e", "https://github.com/mem0ai/mem0/commit/40ef464f6bf3788adbaebeed76c3292e1103e931", "https://github.com/mem0ai/mem0/commit/966d68d92c26aef915ef8f95cf1d62215c28ac92"], "created_at": "2025-06-10T11:05:51Z", "classification": "Security"}
{"repo": "certbot/certbot", "pull_number": 10319, "instance_id": "certbot__certbot-10319", "issue_numbers": [10281], "base_commit": "372813175d229f36cc739a44bb93ec556d146fef", "patch": "diff --git a/acme/src/acme/messages.py b/acme/src/acme/messages.py\nindex 0ed1f5c78e5..6c11041d06f 100644\n--- a/acme/src/acme/messages.py\n+++ b/acme/src/acme/messages.py\n@@ -304,15 +304,26 @@ class ExternalAccountBinding:\n \n     @classmethod\n     def from_data(cls, account_public_key: jose.JWK, kid: str, hmac_key: str,\n-                  directory: Directory) -> Dict[str, Any]:\n+                  directory: Directory, hmac_alg: str = \"HS256\") -> Dict[str, Any]:\n         \"\"\"Create External Account Binding Resource from contact details, kid and hmac.\"\"\"\n \n         key_json = json.dumps(account_public_key.to_partial_json()).encode()\n         decoded_hmac_key = jose.b64.b64decode(hmac_key)\n         url = directory[\"newAccount\"]\n \n+        hmac_alg_map = {\n+            \"HS256\": jose.jwa.HS256,\n+            \"HS384\": jose.jwa.HS384,\n+            \"HS512\": jose.jwa.HS512,\n+        }\n+        alg = hmac_alg_map.get(hmac_alg)\n+        if alg is None:\n+            supported = \", \".join(hmac_alg_map.keys())\n+            raise ValueError(f\"Invalid value for hmac_alg: {hmac_alg}. \"\n+                             f\"Expected one of: {supported}.\")\n+\n         eab = jws.JWS.sign(key_json, jose.jwk.JWKOct(key=decoded_hmac_key),\n-                           jose.jwa.HS256, None,\n+                           alg, None,\n                            url, kid)\n \n         return eab.to_partial_json()\ndiff --git a/certbot/CHANGELOG.md b/certbot/CHANGELOG.md\nindex 1514ae5a546..f3d0b959d17 100644\n--- a/certbot/CHANGELOG.md\n+++ b/certbot/CHANGELOG.md\n@@ -23,6 +23,12 @@ Certbot adheres to [Semantic Versioning](https://semver.org/).\n \n More details about these changes can be found on our GitHub repo.\n \n+## 4.2.0\n+\n+### Added\n+\n+* Added `--eab-hmac-alg` parameter to support custom HMAC algorithm for External Account Binding.\n+\n ## 4.1.1 - 2025-06-12\n \n ### Fixed\ndiff --git a/certbot/src/certbot/_internal/cli/__init__.py b/certbot/src/certbot/_internal/cli/__init__.py\nindex 755df451909..63f95115020 100644\n--- a/certbot/src/certbot/_internal/cli/__init__.py\n+++ b/certbot/src/certbot/_internal/cli/__init__.py\n@@ -134,6 +134,13 @@ def prepare_and_parse_args(plugins: plugins_disco.PluginsRegistry, args: List[st\n         metavar=\"EAB_HMAC_KEY\",\n         help=\"HMAC key for External Account Binding\"\n     )\n+    helpful.add(\n+        [None, \"run\", \"certonly\", \"register\"],\n+        \"--eab-hmac-alg\", dest=\"eab_hmac_alg\",\n+        metavar=\"EAB_HMAC_ALG\",\n+        default=flag_default(\"eab_hmac_alg\"),\n+        help=\"HMAC algorithm for External Account Binding\"\n+    )\n     helpful.add(\n         [None, \"run\", \"certonly\", \"manage\", \"delete\", \"certificates\",\n          \"renew\", \"enhance\", \"reconfigure\"], \"--cert-name\", dest=\"certname\",\ndiff --git a/certbot/src/certbot/_internal/client.py b/certbot/src/certbot/_internal/client.py\nindex 9eb7906c4a6..0bc4cae04fb 100644\n--- a/certbot/src/certbot/_internal/client.py\n+++ b/certbot/src/certbot/_internal/client.py\n@@ -238,7 +238,8 @@ def perform_registration(acme: acme_client.ClientV2, config: configuration.Names\n         eab = messages.ExternalAccountBinding.from_data(account_public_key=account_public_key,\n                                                         kid=config.eab_kid,\n                                                         hmac_key=config.eab_hmac_key,\n-                                                        directory=acme.directory)\n+                                                        directory=acme.directory,\n+                                                        hmac_alg=config.eab_hmac_alg)\n     else:\n         eab = None\n \ndiff --git a/certbot/src/certbot/_internal/constants.py b/certbot/src/certbot/_internal/constants.py\nindex ebddd484e4e..14f4d059ab0 100644\n--- a/certbot/src/certbot/_internal/constants.py\n+++ b/certbot/src/certbot/_internal/constants.py\n@@ -82,6 +82,7 @@\n     random_sleep_on_renew=True,\n     eab_hmac_key=None,\n     eab_kid=None,\n+    eab_hmac_alg=\"HS256\",\n     issuance_timeout=90,\n     run_deploy_hooks=False,\n \n", "test_patch": "diff --git a/acme/src/acme/_internal/tests/messages_test.py b/acme/src/acme/_internal/tests/messages_test.py\nindex ae656bd0ce4..156666ccc73 100644\n--- a/acme/src/acme/_internal/tests/messages_test.py\n+++ b/acme/src/acme/_internal/tests/messages_test.py\n@@ -1,5 +1,6 @@\n \"\"\"Tests for acme.messages.\"\"\"\n import sys\n+import json\n from typing import Dict\n import unittest\n from unittest import mock\n@@ -218,17 +219,43 @@ def setUp(self):\n         self.key = jose.jwk.JWKRSA(key=KEY.public_key())\n         self.kid = \"kid-for-testing\"\n         self.hmac_key = \"hmac-key-for-testing\"\n+        self.hmac_alg = \"HS256\"\n         self.dir = Directory({\n             'newAccount': 'http://url/acme/new-account',\n         })\n \n     def test_from_data(self):\n         from acme.messages import ExternalAccountBinding\n-        eab = ExternalAccountBinding.from_data(self.key, self.kid, self.hmac_key, self.dir)\n+        eab = ExternalAccountBinding.from_data(self.key, self.kid, self.hmac_key, self.dir, self.hmac_alg)\n \n         assert len(eab) == 3\n         assert sorted(eab.keys()) == sorted(['protected', 'payload', 'signature'])\n \n+    def test_from_data_invalid_hmac_alg(self):\n+        from acme.messages import ExternalAccountBinding\n+        invalid_alg = \"HS9999\"\n+        with pytest.raises(ValueError) as info:\n+            ExternalAccountBinding.from_data(self.key, self.kid, self.hmac_key, self.dir, invalid_alg)\n+\n+        assert \"Invalid value for hmac_alg\" in str(info.value)\n+\n+    def test_from_data_default_hmac_alg(self):\n+        from acme.messages import ExternalAccountBinding\n+        eab_default = ExternalAccountBinding.from_data(self.key, self.kid, self.hmac_key, self.dir)\n+\n+        assert len(eab_default) == 3\n+        assert sorted(eab_default.keys()) == sorted(['protected', 'payload', 'signature'])\n+\n+        eab_explicit = ExternalAccountBinding.from_data(\n+            self.key, self.kid, self.hmac_key, self.dir, \"HS256\"\n+        )\n+\n+        assert eab_default == eab_explicit\n+\n+        protected_default = json.loads(\n+            jose.b64.b64decode(eab_default['protected']).decode()\n+        )\n+        assert protected_default['alg'] == 'HS256'\n \n class RegistrationTest(unittest.TestCase):\n     \"\"\"Tests for acme.messages.Registration.\"\"\"\n@@ -268,10 +295,11 @@ def test_new_registration_from_data_with_eab(self):\n         key = jose.jwk.JWKRSA(key=KEY.public_key())\n         kid = \"kid-for-testing\"\n         hmac_key = \"hmac-key-for-testing\"\n+        hmac_alg = \"HS256\"\n         directory = Directory({\n             'newAccount': 'http://url/acme/new-account',\n         })\n-        eab = ExternalAccountBinding.from_data(key, kid, hmac_key, directory)\n+        eab = ExternalAccountBinding.from_data(key, kid, hmac_key, directory, hmac_alg)\n         reg = NewRegistration.from_data(email='admin@foo.com', external_account_binding=eab)\n         assert reg.contact == (\n             'mailto:admin@foo.com',\n", "problem_statement": "[Feature Request]: Support for HMAC-SHA-512 in External Account Binding (EAB) Signatures\n### What problem does this feature solve or what does it enhance?\n\nCertbot currently hardcodes HMAC-SHA-256 as the only supported algorithm for generating External Account Binding (EAB) signatures as part of the ACME protocol. This implementation limits flexibility for ACME servers that may require stronger or alternate HMAC algorithms, such as HMAC-SHA-512, either for compliance, security policy, or future-proofing against evolving cryptographic threats. \n\n```\nmessages.py:\n\nclass ExternalAccountBinding:\n    \"\"\"ACME External Account Binding\"\"\"\n\n    @classmethod\n    def from_data(cls, account_public_key: jose.JWK, kid: str, hmac_key: str,\n                  directory: Directory) -> Dict[str, Any]:\n        \"\"\"Create External Account Binding Resource from contact details, kid and hmac.\"\"\"\n\n        key_json = json.dumps(account_public_key.to_partial_json()).encode()\n        decoded_hmac_key = jose.b64.b64decode(hmac_key)\n        url = directory[\"newAccount\"]\n\n        eab = jws.JWS.sign(key_json, jose.jwk.JWKOct(key=decoded_hmac_key),\n                           jose.jwa.HS256, None, url, kid)\n\n        return eab.to_partial_json()\n```\n\n### Proposed Solution\n\nIntroduce support for configurable HMAC algorithms in the EAB signature generation process, with HMAC-SHA-512 as a supported alternative.\n\n### Alternatives Considered\n\n_No response_\n", "hints_text": "If you could manage a PR around this we'd be glad to take a look into it, otherwise I labeled it to call out to the general community as well.\n\n", "all_hints_text": "If you could manage a PR around this we'd be glad to take a look into it, otherwise I labeled it to call out to the general community as well.\n\n", "commit_urls": ["https://github.com/certbot/certbot/commit/6aca3c3559039b60524388c512f827ee9a9ab375", "https://github.com/certbot/certbot/commit/7bf160118ce8d7b60d5c0b47999ff6f79c5fafe2", "https://github.com/certbot/certbot/commit/41cd6c782f36cee0dc3aead642abcdf434103f31"], "created_at": "2025-06-07T14:50:29Z", "classification": "Security"}
{"repo": "certbot/certbot", "pull_number": 10317, "instance_id": "certbot__certbot-10317", "issue_numbers": [10308], "base_commit": "48f34938c6d4aa8c1456602152deba102db3c5a8", "patch": "diff --git a/acme/src/acme/client.py b/acme/src/acme/client.py\nindex eea184619da..675a09fa070 100644\n--- a/acme/src/acme/client.py\n+++ b/acme/src/acme/client.py\n@@ -319,6 +319,10 @@ def renewal_time(self, cert_pem: bytes\n         \"\"\"Return an appropriate time to attempt renewal of the certificate,\n         and the next time to ask the ACME server for renewal info.\n \n+        If the certificate has already expired, renewal info isn't checked.\n+        Instead, the certificate's notAfter time is returned and the certificate\n+        should be immediately renewed.\n+\n         If the ACME directory has a \"renewalInfo\" field, the response will be\n         based on a fetch of the renewal info resource for the certificate\n         (https://www.ietf.org/archive/id/draft-ietf-acme-ari-08.html).\n@@ -339,6 +343,14 @@ def renewal_time(self, cert_pem: bytes\n \n         cert = x509.load_pem_x509_certificate(cert_pem)\n \n+        # from https://www.ietf.org/archive/id/draft-ietf-acme-ari-08.html#section-4.3, \"Clients\n+        # MUST NOT check a certificate's RenewalInfo after the certificate has expired.\"\n+        #\n+        # we call datetime.datetime.now here with the UTC argument to create a timezone aware\n+        # datetime object that can be compared with the UTC notAfter from cryptography\n+        if cert.not_valid_after_utc < datetime.datetime.now(datetime.timezone.utc):\n+            return cert.not_valid_after_utc, now + default_retry_after\n+\n         try:\n             renewal_info_base_url = self.directory['renewalInfo']\n         except KeyError:\n", "test_patch": "diff --git a/acme/src/acme/_internal/tests/client_test.py b/acme/src/acme/_internal/tests/client_test.py\nindex 42b910f1a02..521427b9c68 100644\n--- a/acme/src/acme/_internal/tests/client_test.py\n+++ b/acme/src/acme/_internal/tests/client_test.py\n@@ -9,6 +9,8 @@\n import unittest\n from unittest import mock\n \n+from cryptography import x509\n+\n import josepy as jose\n import pytest\n import requests\n@@ -462,7 +464,24 @@ def test_get_directory(self):\n         assert DIRECTORY_V2.to_partial_json() == \\\n             ClientV2.get_directory('https://example.com/dir', self.net).to_partial_json()\n \n-    def test_renewal_time_no_renewal_info(self):\n+    @mock.patch('acme.client.datetime')\n+    def test_renewal_time_expired_cert(self, dt_mock):\n+        utc_now = datetime.datetime(2026, 1, 1, tzinfo=datetime.timezone.utc)\n+        dt_mock.datetime.now.return_value = utc_now\n+\n+        cert_pem = make_cert_for_renewal(\n+            not_before=datetime.datetime(2025, 3, 12, 00, 00, 00),\n+            not_after=datetime.datetime(2025, 3, 20, 00, 00, 00),\n+        )\n+        cert = x509.load_pem_x509_certificate(cert_pem)\n+\n+        t, _ = self.client.renewal_time(cert_pem)\n+        assert t == cert.not_valid_after_utc\n+\n+    @mock.patch('acme.client.datetime')\n+    def test_renewal_time_no_renewal_info(self, dt_mock):\n+        utc_now = datetime.datetime(2025, 3, 15, tzinfo=datetime.timezone.utc)\n+        dt_mock.datetime.now.return_value = utc_now\n         # A directory with no 'renewalInfo' should result in None.\n         self.client.directory = messages.Directory({})\n         cert_pem = make_cert_for_renewal(\n@@ -479,9 +498,14 @@ def test_renewal_time_no_renewal_info(self):\n         t, _ = self.client.renewal_time(cert_pem)\n         assert t == None\n \n-    def test_renewal_time_with_renewal_info(self):\n+    @mock.patch('acme.client.datetime')\n+    def test_renewal_time_with_renewal_info(self, dt_mock):\n         from cryptography import x509\n         from acme.client import _renewal_info_path_component\n+        utc_now = datetime.datetime(2025, 3, 15, tzinfo=datetime.timezone.utc)\n+        dt_mock.datetime.now.return_value = utc_now\n+        dt_mock.timedelta = datetime.timedelta\n+\n         cert_pem = make_cert_for_renewal(\n             not_before=datetime.datetime(2025, 3, 12, 00, 00, 00),\n             not_after=datetime.datetime(2025, 3, 20, 00, 00, 00),\n@@ -522,7 +546,10 @@ def test_renewal_time_with_renewal_info(self):\n         assert t >= datetime.datetime(2025, 3, 16, 1, 1, 1, tzinfo=datetime.timezone.utc)\n         assert t <= datetime.datetime(2025, 3, 17, 1, 1, 1, tzinfo=datetime.timezone.utc)\n \n-    def test_renewal_time_renewal_info_errors(self):\n+    @mock.patch('acme.client.datetime')\n+    def test_renewal_time_renewal_info_errors(self, dt_mock):\n+        utc_now = datetime.datetime(2025, 3, 15, tzinfo=datetime.timezone.utc)\n+        dt_mock.datetime.now.return_value = utc_now\n         self.client.directory = messages.Directory({\n             'renewalInfo': 'https://www.letsencrypt-demo.org/acme/renewal-info',\n         })\n@@ -545,8 +572,11 @@ def test_renewal_time_renewal_info_errors(self):\n \n     @mock.patch('acme.client.datetime')\n     def test_renewal_time_returns_retry_after(self, dt_mock):\n-        dt_mock.datetime.now.return_value = datetime.datetime(2025, 5, 12, 0, 0, 0)\n+        def now(tzinfo=None):\n+            return datetime.datetime(2025, 3, 15, tzinfo=tzinfo)\n+        dt_mock.datetime.now.side_effect = now\n         dt_mock.timedelta = datetime.timedelta\n+        dt_mock.timezone = datetime.timezone\n \n         self.client.directory = messages.Directory({\n             'renewalInfo': 'https://www.letsencrypt-demo.org/acme/renewal-info',\n@@ -565,12 +595,12 @@ def test_renewal_time_returns_retry_after(self, dt_mock):\n \n         # With no explicit Retry-After in header, default to six hours\n         _, retry_after = self.client.renewal_time(cert_pem)\n-        assert retry_after == datetime.datetime(2025, 5, 12, 6, 0, 0)\n+        assert retry_after == datetime.datetime(2025, 3, 15, 6, 0, 0)\n \n         # With an explicit Retry-After in header, use that\n         self.response.headers['Retry-After'] = '100'\n         _, retry_after = self.client.renewal_time(cert_pem)\n-        assert retry_after == datetime.datetime(2025, 5, 12, 00, 1, 40)\n+        assert retry_after == datetime.datetime(2025, 3, 15, 00, 1, 40)\n \n def test_renewal_info_path_component():\n     from cryptography import x509\n", "problem_statement": "[Task]: client MUST NOT check renewalinfo after expiration\n### Task description\n\ni just read thru the ARI spec and came across the line:\n\n> Clients MUST NOT check a certificate's RenewalInfo after the certificate has expired.\n\nfrom https://www.ietf.org/archive/id/draft-ietf-acme-ari-08.html#section-4.3\n\nwe don't currently have code that prevents this do we @jsha? should we?\n", "hints_text": "Yeah, I think we do not have such code. Should be simple to add - if certificate is already expired, that takes precedence over everything, renew before doing any checks (OCSP or ARI).\n\n", "all_hints_text": "Yeah, I think we do not have such code. Should be simple to add - if certificate is already expired, that takes precedence over everything, renew before doing any checks (OCSP or ARI).\n\n", "commit_urls": ["https://github.com/certbot/certbot/commit/967db56b38f490c47a6a6ccb04818078c2ac0f61"], "created_at": "2025-06-06T17:22:06Z", "classification": "Security"}
{"repo": "huggingface/diffusers", "pull_number": 11750, "instance_id": "huggingface__diffusers-11750", "issue_numbers": [11745], "base_commit": "195926bbdc69d20d442179b2fe96023edc751212", "patch": "diff --git a/src/diffusers/pipelines/pipeline_loading_utils.py b/src/diffusers/pipelines/pipeline_loading_utils.py\nindex 7132e9521f79..d1c2c2adb4c3 100644\n--- a/src/diffusers/pipelines/pipeline_loading_utils.py\n+++ b/src/diffusers/pipelines/pipeline_loading_utils.py\n@@ -1131,3 +1131,26 @@ def _maybe_raise_error_for_incorrect_transformers(config_dict):\n                 break\n     if has_transformers_component and not is_transformers_version(\">\", \"4.47.1\"):\n         raise ValueError(\"Please upgrade your `transformers` installation to the latest version to use DDUF.\")\n+\n+\n+def _maybe_warn_for_wrong_component_in_quant_config(pipe_init_dict, quant_config):\n+    if quant_config is None:\n+        return\n+\n+    actual_pipe_components = set(pipe_init_dict.keys())\n+    missing = \"\"\n+    quant_components = None\n+    if getattr(quant_config, \"components_to_quantize\", None) is not None:\n+        quant_components = set(quant_config.components_to_quantize)\n+    elif getattr(quant_config, \"quant_mapping\", None) is not None and isinstance(quant_config.quant_mapping, dict):\n+        quant_components = set(quant_config.quant_mapping.keys())\n+\n+    if quant_components and not quant_components.issubset(actual_pipe_components):\n+        missing = quant_components - actual_pipe_components\n+\n+    if missing:\n+        logger.warning(\n+            f\"The following components in the quantization config {missing} will be ignored \"\n+            \"as they do not belong to the underlying pipeline. Acceptable values for the pipeline \"\n+            f\"components are: {', '.join(actual_pipe_components)}.\"\n+        )\ndiff --git a/src/diffusers/pipelines/pipeline_utils.py b/src/diffusers/pipelines/pipeline_utils.py\nindex efeb085a723b..4492d314a308 100644\n--- a/src/diffusers/pipelines/pipeline_utils.py\n+++ b/src/diffusers/pipelines/pipeline_utils.py\n@@ -88,6 +88,7 @@\n     _identify_model_variants,\n     _maybe_raise_error_for_incorrect_transformers,\n     _maybe_raise_warning_for_inpainting,\n+    _maybe_warn_for_wrong_component_in_quant_config,\n     _resolve_custom_pipeline_and_cls,\n     _unwrap_model,\n     _update_init_kwargs_with_connected_pipeline,\n@@ -984,6 +985,7 @@ def load_module(name, value):\n \n         # 7. Load each module in the pipeline\n         current_device_map = None\n+        _maybe_warn_for_wrong_component_in_quant_config(init_dict, quantization_config)\n         for name, (library_name, class_name) in logging.tqdm(init_dict.items(), desc=\"Loading pipeline components...\"):\n             # 7.1 device_map shenanigans\n             if final_device_map is not None and len(final_device_map) > 0:\n", "test_patch": "diff --git a/tests/quantization/test_pipeline_level_quantization.py b/tests/quantization/test_pipeline_level_quantization.py\nindex f9803c0ede10..5a724df5c3ca 100644\n--- a/tests/quantization/test_pipeline_level_quantization.py\n+++ b/tests/quantization/test_pipeline_level_quantization.py\n@@ -16,10 +16,13 @@\n import unittest\n \n import torch\n+from parameterized import parameterized\n \n from diffusers import DiffusionPipeline, QuantoConfig\n from diffusers.quantizers import PipelineQuantizationConfig\n+from diffusers.utils import logging\n from diffusers.utils.testing_utils import (\n+    CaptureLogger,\n     is_transformers_available,\n     require_accelerate,\n     require_bitsandbytes_version_greater,\n@@ -188,3 +191,55 @@ def test_saving_loading(self):\n         output_2 = loaded_pipe(**pipe_inputs, generator=torch.manual_seed(self.seed)).images\n \n         self.assertTrue(torch.allclose(output_1, output_2))\n+\n+    @parameterized.expand([\"quant_kwargs\", \"quant_mapping\"])\n+    def test_warn_invalid_component(self, method):\n+        invalid_component = \"foo\"\n+        if method == \"quant_kwargs\":\n+            components_to_quantize = [\"transformer\", invalid_component]\n+            quant_config = PipelineQuantizationConfig(\n+                quant_backend=\"bitsandbytes_8bit\",\n+                quant_kwargs={\"load_in_8bit\": True},\n+                components_to_quantize=components_to_quantize,\n+            )\n+        else:\n+            quant_config = PipelineQuantizationConfig(\n+                quant_mapping={\n+                    \"transformer\": QuantoConfig(\"int8\"),\n+                    invalid_component: TranBitsAndBytesConfig(load_in_8bit=True),\n+                }\n+            )\n+\n+        logger = logging.get_logger(\"diffusers.pipelines.pipeline_loading_utils\")\n+        logger.setLevel(logging.WARNING)\n+        with CaptureLogger(logger) as cap_logger:\n+            _ = DiffusionPipeline.from_pretrained(\n+                self.model_name,\n+                quantization_config=quant_config,\n+                torch_dtype=torch.bfloat16,\n+            )\n+        self.assertTrue(invalid_component in cap_logger.out)\n+\n+    @parameterized.expand([\"quant_kwargs\", \"quant_mapping\"])\n+    def test_no_quantization_for_all_invalid_components(self, method):\n+        invalid_component = \"foo\"\n+        if method == \"quant_kwargs\":\n+            components_to_quantize = [invalid_component]\n+            quant_config = PipelineQuantizationConfig(\n+                quant_backend=\"bitsandbytes_8bit\",\n+                quant_kwargs={\"load_in_8bit\": True},\n+                components_to_quantize=components_to_quantize,\n+            )\n+        else:\n+            quant_config = PipelineQuantizationConfig(\n+                quant_mapping={invalid_component: TranBitsAndBytesConfig(load_in_8bit=True)}\n+            )\n+\n+        pipe = DiffusionPipeline.from_pretrained(\n+            self.model_name,\n+            quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n+        )\n+        for name, component in pipe.components.items():\n+            if isinstance(component, torch.nn.Module):\n+                self.assertTrue(not hasattr(component.config, \"quantization_config\"))\n", "problem_statement": "Pipeline Quantization silent ignore of components not present in pipeline\n### Describe the bug\n\nif you add a component that doesn't exist in the pipeline to the `PipelineQuantizationConfig`, the quantization works for the components present but it doesn't throw a warning that one of them doesn't exist, this could lead to wrong assumptions that it worked.\n\n### Reproduction\n\n```python\nrepo_id = \"imnotednamode/Chroma-v36-dc-diffusers\"\n\npipeline_quant_config = PipelineQuantizationConfig(\n    quant_backend=\"bitsandbytes_4bit\",\n    quant_kwargs={\n        \"load_in_4bit\": True,\n        \"bnb_4bit_quant_type\": \"nf4\",\n        \"bnb_4bit_compute_dtype\": dtype,\n        \"llm_int8_skip_modules\": [\"distilled_guidance_layer\"],\n    },\n    components_to_quantize=[\"transformer\", \"text_encoder_2\"],\n)\n\npipe = ChromaPipeline.from_pretrained(\n    \"imnotednamode/Chroma-v36-dc-diffusers\",\n    quantization_config=pipeline_quant_config,\n    torch_dtype=dtype,\n)\npipe.enable_model_cpu_offload()\n\nprompt = 'Ultra-realistic, high-quality photo of an anthropomorphic capybara with a tough, streetwise attitude, wearing a worn black leather jacket, dark sunglasses, and ripped jeans. The capybara is leaning casually against a gritty urban wall covered in vibrant graffiti. Behind it, in bold, dripping yellow spray paint, the word \"HuggingFace\" is scrawled in large street-art style letters. The scene is set in a dimly lit alleyway with moody lighting, scattered trash, and an edgy, rebellious vibe \u2014 like a character straight out of an underground comic book.'\nnegative = \"low quality, bad anatomy, extra digits, missing digits, extra limbs, missing limbs\"\n\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative,\n    num_inference_steps=30,\n    guidance_scale=4.0,\n    width=1024,\n    height=1024,\n    generator=torch.Generator().manual_seed(42),\n).images[0]\n```\n\nChroma only has one text encoder: `text_encoder` and not `text_encoder_2` but the code runs without any issue and produces the image but the user won't have the benefit of the quantization.\n\n### System Info\n\nDiffusers version: 0.34.0.dev0\n\n### Who can help?\n\n@sayakpaul \n", "hints_text": "cc @sayakpaul \n\nshould be an easy fix here\nrelated and more broadly, it would be really helpful to add quantisation info into components manager `__repr__` \nYes, I will open a PR to fix this.\n\n> related and more broadly, it would be really helpful to add quantisation info into components manager __repr__\n\n\n@yiyixuxu I can take that up. Okay with you?\n\n", "all_hints_text": "cc @sayakpaul \n\nshould be an easy fix here\nrelated and more broadly, it would be really helpful to add quantisation info into components manager `__repr__` \nYes, I will open a PR to fix this.\n\n> related and more broadly, it would be really helpful to add quantisation info into components manager __repr__\n\n\n@yiyixuxu I can take that up. Okay with you?\n\n", "commit_urls": ["https://github.com/huggingface/diffusers/commit/0a5a860f92d2d8adb5e18adfc0b21ad8396b360e", "https://github.com/huggingface/diffusers/commit/e1084afb729f94a28397cf10625f8ce4266cf3ee", "https://github.com/huggingface/diffusers/commit/69f24bf33aade5fce1bd00a32600ca3c1cf97cc9"], "created_at": "2025-06-19T03:52:00Z", "classification": "Security"}
{"repo": "deepset-ai/haystack", "pull_number": 9582, "instance_id": "deepset-ai__haystack-9582", "issue_numbers": [9581], "base_commit": "848115c65edb98fe600d71cb398f8a5e4c874f76", "patch": "diff --git a/haystack/dataclasses/streaming_chunk.py b/haystack/dataclasses/streaming_chunk.py\nindex 7cd30e466f..107a0d55b2 100644\n--- a/haystack/dataclasses/streaming_chunk.py\n+++ b/haystack/dataclasses/streaming_chunk.py\n@@ -30,12 +30,6 @@ class ToolCallDelta:\n     arguments: Optional[str] = field(default=None)\n     id: Optional[str] = field(default=None)  # noqa: A003\n \n-    def __post_init__(self):\n-        # NOTE: We allow for name and arguments to both be present because some providers like Mistral provide the\n-        # name and full arguments in one chunk\n-        if self.tool_name is None and self.arguments is None:\n-            raise ValueError(\"At least one of tool_name or arguments must be provided.\")\n-\n \n @dataclass\n class ComponentInfo:\ndiff --git a/releasenotes/notes/relax-tool-call-delta-a9e1f3e9c753cdf4.yaml b/releasenotes/notes/relax-tool-call-delta-a9e1f3e9c753cdf4.yaml\nnew file mode 100644\nindex 0000000000..4789e696fe\n--- /dev/null\n+++ b/releasenotes/notes/relax-tool-call-delta-a9e1f3e9c753cdf4.yaml\n@@ -0,0 +1,4 @@\n+---\n+enhancements:\n+  - |\n+    We relaxed the requirement that in ToolCallDelta (introduced in Haystack 2.15) which required the parameters arguments or name to be populated to be able to create a ToolCallDelta dataclass. We remove this requirement to be more in line with OpenAI's SDK and since this was causing errors for some hosted versions of open source models following OpenAI's SDK specification.\n", "test_patch": "diff --git a/test/components/generators/chat/test_openai.py b/test/components/generators/chat/test_openai.py\nindex dfbb2a39d5..9d051749fd 100644\n--- a/test/components/generators/chat/test_openai.py\n+++ b/test/components/generators/chat/test_openai.py\n@@ -1177,6 +1177,32 @@ def test_convert_chat_completion_chunk_to_streaming_chunk(self, chat_completion_\n             assert stream_chunk == haystack_chunk\n             previous_chunks.append(stream_chunk)\n \n+    def test_convert_chat_completion_chunk_with_empty_tool_calls(self):\n+        # This can happen with some LLM providers where tool calls are not present but the pydantic models are still\n+        # initialized.\n+        chunk = ChatCompletionChunk(\n+            id=\"chatcmpl-BC1y4wqIhe17R8sv3lgLcWlB4tXCw\",\n+            choices=[\n+                chat_completion_chunk.Choice(\n+                    delta=chat_completion_chunk.ChoiceDelta(\n+                        tool_calls=[ChoiceDeltaToolCall(index=0, function=ChoiceDeltaToolCallFunction())]\n+                    ),\n+                    index=0,\n+                )\n+            ],\n+            created=1742207200,\n+            model=\"gpt-4o-mini-2024-07-18\",\n+            object=\"chat.completion.chunk\",\n+        )\n+        result = _convert_chat_completion_chunk_to_streaming_chunk(chunk=chunk, previous_chunks=[])\n+        assert result.content == \"\"\n+        assert result.start is False\n+        assert result.tool_calls == [ToolCallDelta(index=0)]\n+        assert result.tool_call_result is None\n+        assert result.index == 0\n+        assert result.meta[\"model\"] == \"gpt-4o-mini-2024-07-18\"\n+        assert result.meta[\"received_at\"] is not None\n+\n     def test_handle_stream_response(self, chat_completion_chunks):\n         openai_chunks = chat_completion_chunks\n         comp = OpenAIChatGenerator(api_key=Secret.from_token(\"test-api-key\"))\ndiff --git a/test/components/generators/test_utils.py b/test/components/generators/test_utils.py\nindex 80d5372908..b428067224 100644\n--- a/test/components/generators/test_utils.py\n+++ b/test/components/generators/test_utils.py\n@@ -388,6 +388,123 @@ def test_convert_streaming_chunk_to_chat_message_two_tool_calls_in_same_chunk():\n     assert result.tool_calls[1].arguments == {\"city\": \"Berlin\"}\n \n \n+def test_convert_streaming_chunk_to_chat_message_empty_tool_call_delta():\n+    chunks = [\n+        StreamingChunk(\n+            content=\"\",\n+            meta={\n+                \"model\": \"gpt-4o-mini-2024-07-18\",\n+                \"index\": 0,\n+                \"tool_calls\": None,\n+                \"finish_reason\": None,\n+                \"received_at\": \"2025-02-19T16:02:55.910076\",\n+            },\n+            component_info=ComponentInfo(name=\"test\", type=\"test\"),\n+        ),\n+        StreamingChunk(\n+            content=\"\",\n+            meta={\n+                \"model\": \"gpt-4o-mini-2024-07-18\",\n+                \"index\": 0,\n+                \"tool_calls\": [\n+                    chat_completion_chunk.ChoiceDeltaToolCall(\n+                        index=0,\n+                        id=\"call_ZOj5l67zhZOx6jqjg7ATQwb6\",\n+                        function=chat_completion_chunk.ChoiceDeltaToolCallFunction(\n+                            arguments='{\"query\":', name=\"rag_pipeline_tool\"\n+                        ),\n+                        type=\"function\",\n+                    )\n+                ],\n+                \"finish_reason\": None,\n+                \"received_at\": \"2025-02-19T16:02:55.913919\",\n+            },\n+            component_info=ComponentInfo(name=\"test\", type=\"test\"),\n+            index=0,\n+            start=True,\n+            tool_calls=[\n+                ToolCallDelta(\n+                    id=\"call_ZOj5l67zhZOx6jqjg7ATQwb6\", tool_name=\"rag_pipeline_tool\", arguments='{\"query\":', index=0\n+                )\n+            ],\n+        ),\n+        StreamingChunk(\n+            content=\"\",\n+            meta={\n+                \"model\": \"gpt-4o-mini-2024-07-18\",\n+                \"index\": 0,\n+                \"tool_calls\": [\n+                    chat_completion_chunk.ChoiceDeltaToolCall(\n+                        index=0,\n+                        function=chat_completion_chunk.ChoiceDeltaToolCallFunction(\n+                            arguments=' \"Where does Mark live?\"}'\n+                        ),\n+                    )\n+                ],\n+                \"finish_reason\": None,\n+                \"received_at\": \"2025-02-19T16:02:55.924420\",\n+            },\n+            component_info=ComponentInfo(name=\"test\", type=\"test\"),\n+            index=0,\n+            tool_calls=[ToolCallDelta(arguments=' \"Where does Mark live?\"}', index=0)],\n+        ),\n+        StreamingChunk(\n+            content=\"\",\n+            meta={\n+                \"model\": \"gpt-4o-mini-2024-07-18\",\n+                \"index\": 0,\n+                \"tool_calls\": [\n+                    chat_completion_chunk.ChoiceDeltaToolCall(\n+                        index=0, function=chat_completion_chunk.ChoiceDeltaToolCallFunction()\n+                    )\n+                ],\n+                \"finish_reason\": \"tool_calls\",\n+                \"received_at\": \"2025-02-19T16:02:55.948772\",\n+            },\n+            tool_calls=[ToolCallDelta(index=0)],\n+            component_info=ComponentInfo(name=\"test\", type=\"test\"),\n+            finish_reason=\"tool_calls\",\n+            index=0,\n+        ),\n+        StreamingChunk(\n+            content=\"\",\n+            meta={\n+                \"model\": \"gpt-4o-mini-2024-07-18\",\n+                \"index\": 0,\n+                \"tool_calls\": None,\n+                \"finish_reason\": None,\n+                \"received_at\": \"2025-02-19T16:02:55.948772\",\n+                \"usage\": {\n+                    \"completion_tokens\": 42,\n+                    \"prompt_tokens\": 282,\n+                    \"total_tokens\": 324,\n+                    \"completion_tokens_details\": {\n+                        \"accepted_prediction_tokens\": 0,\n+                        \"audio_tokens\": 0,\n+                        \"reasoning_tokens\": 0,\n+                        \"rejected_prediction_tokens\": 0,\n+                    },\n+                    \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0},\n+                },\n+            },\n+            component_info=ComponentInfo(name=\"test\", type=\"test\"),\n+        ),\n+    ]\n+\n+    # Convert chunks to a chat message\n+    result = _convert_streaming_chunks_to_chat_message(chunks=chunks)\n+\n+    assert not result.texts\n+    assert not result.text\n+\n+    # Verify both tool calls were found and processed\n+    assert len(result.tool_calls) == 1\n+    assert result.tool_calls[0].id == \"call_ZOj5l67zhZOx6jqjg7ATQwb6\"\n+    assert result.tool_calls[0].tool_name == \"rag_pipeline_tool\"\n+    assert result.tool_calls[0].arguments == {\"query\": \"Where does Mark live?\"}\n+    assert result.meta[\"finish_reason\"] == \"tool_calls\"\n+\n+\n def test_print_streaming_chunk_content_only():\n     chunk = StreamingChunk(\n         content=\"Hello, world!\",\ndiff --git a/test/dataclasses/test_streaming_chunk.py b/test/dataclasses/test_streaming_chunk.py\nindex 1d53633026..af9fd8011f 100644\n--- a/test/dataclasses/test_streaming_chunk.py\n+++ b/test/dataclasses/test_streaming_chunk.py\n@@ -99,11 +99,6 @@ def test_tool_call_delta():\n     assert tool_call.index == 0\n \n \n-def test_tool_call_delta_with_missing_fields():\n-    with pytest.raises(ValueError):\n-        _ = ToolCallDelta(id=\"123\", index=0)\n-\n-\n def test_create_chunk_with_finish_reason():\n     \"\"\"Test creating a chunk with the new finish_reason field.\"\"\"\n     chunk = StreamingChunk(content=\"Test content\", finish_reason=\"stop\")\n", "problem_statement": "ValueError when using streaming with `qwen-plus-latest` and `dashscope`\n**Describe the bug**\nCreated based on this PR https://github.com/deepset-ai/haystack/pull/9565\n\n> This pull request addresses a `ValueError` that occurs in `OpenAIChatGenerator` when processing a streaming response that includes tool calls. The issue arises because some LLM providers, like qwen-plus, can occasionally send a `tool_calls` delta chunk where the `function` object has both `name` and `arguments` as `None`.\n>\n> This empty delta causes a `ValueError` during the instantiation of the `ToolCallDelta` dataclass, as its `__post_init__` validation requires at least `tool_name` or `arguments` to be present. The application crashes with the following error:\n\nhttps://github.com/deepset-ai/haystack/blob/c54a68ab631ba9af23b37a5cda2478eca3353bf3/haystack/dataclasses/streaming_chunk.py#L33-L37\n\n```\nValueError: At least one of tool_name or arguments must be provided.\n```\n\n**Expected behavior**\nFor streaming to work with LLMs that return ChatCompletionChunks with empty ChoiceDeltaToolCalls.\n\n**Additional context**\nThis was identified to occur when using the provider https://dashscope.aliyuncs.com/compatible-mode/v1 and the model `qwen-plus-latest`.\n\n**To Reproduce**\nReproducing from the original PR\nYou can use the simplest Agent Tools sample to test.\n\nPlease customize the api_base_url of OpenAIChatGenerator to https://dashscope.aliyuncs.com/compatible-mode/v1. This is the compatible endpoint of OpenAI.\n\n```python\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.dataclasses import ChatMessage\nfrom haystack.tools.tool import Tool\nfrom haystack.components.agents import Agent\nfrom typing import List\nfrom haystack.components.generators.utils import print_streaming_chunk\nfrom haystack.utils import Secret\nfrom haystack import component\nfrom haystack.tools import ComponentTool\n\n@component\nclass rephraser:\n\n    def __init__(self):\n        self.result = \"\"\n\n    @component.output_types(result=str)\n    def run(self, query: str) -> dict:\n        \"\"\"\n        Rephrase the query.\n\n        Args:\n            query: The query to rephrase.\n\n        Returns:\n            The rephrased query.\n        \"\"\"\n        try:\n            result = \"Json had lunch?\"\n            return {\"result\": result}\n        except Exception as e:\n            return {\"error\": str(e)}\n\n# Tool Definition\nrephraser_tool = ComponentTool(\n    component=rephraser(),\n    name=\"rephraser\",\n    description=\"Rephrase the query.\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\"type\": \"string\", \"description\": \"Query to rephrase\"}\n        },\n        \"required\": [\"query\"]\n    },\n)\n\n# Agent Setup\nagent = Agent(\n    chat_generator=OpenAIChatGenerator(\n        api_base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n        api_key=Secret.from_token(\"xxxxxxxxxx\"),\n        model=\"qwen-plus-latest\",\n        streaming_callback=print_streaming_chunk\n    ),\n    system_prompt=\"You are a helpful assistant. You can chat with the user and use tools to answer questions. Always start by using the 'rephraser' tool to rephrase the user's question. You can use the 'rephraser' tool to rephrase the user's question as many times as you want.\",\n    tools=[rephraser_tool],\n)\n\n# Run the Agent\nagent.warm_up()\nresponse = agent.run(messages=[ChatMessage.from_user(\"He had lunch?\")])\n\n# Output\nprint(response[\"messages\"])\n```\n\nYou will most likely see this error message. When you switch to deepseek or other advanced models, this problem is unlikely to occur.\n\n**System:**\n - Haystack version (commit or version number): 2.15\n\n", "hints_text": "\n\n", "all_hints_text": "\n\n", "commit_urls": ["https://github.com/deepset-ai/haystack/commit/907745da16f1d5ae21ab0f74a7af24ed5a2a3b36", "https://github.com/deepset-ai/haystack/commit/d2e2826b9465a7108869ba4b0977209385dbe9bb", "https://github.com/deepset-ai/haystack/commit/75d201b25c9390bb00c70e976360fed94d53e77c"], "created_at": "2025-07-02T07:05:42Z", "classification": "Security"}
{"repo": "BerriAI/litellm", "pull_number": 12661, "instance_id": "BerriAI__litellm-12661", "issue_numbers": [9863], "base_commit": "6e426c8b7c1438d14f6a83156281ca618ce1293d", "patch": "diff --git a/litellm/llms/vertex_ai/vertex_llm_base.py b/litellm/llms/vertex_ai/vertex_llm_base.py\nindex 22f119dab2f7..f50a1d236e56 100644\n--- a/litellm/llms/vertex_ai/vertex_llm_base.py\n+++ b/litellm/llms/vertex_ai/vertex_llm_base.py\n@@ -405,10 +405,20 @@ def get_access_token(\n             verbose_logger.debug(\n                 f\"Cached credentials found for project_id: {project_id}.\"\n             )\n-            _credentials = self._credentials_project_mapping[credential_cache_key]\n-            verbose_logger.debug(\"Using cached credentials\")\n-            credential_project_id = _credentials.quota_project_id or getattr(\n-                _credentials, \"project_id\", None\n+            # Retrieve both credentials and cached project_id\n+            cached_entry = self._credentials_project_mapping[credential_cache_key]\n+            verbose_logger.debug(\"cached_entry: %s\", cached_entry)\n+            if isinstance(cached_entry, tuple):\n+                _credentials, credential_project_id = cached_entry\n+            else:\n+                # Backward compatibility with old cache format\n+                _credentials = cached_entry\n+                credential_project_id = _credentials.quota_project_id or getattr(\n+                    _credentials, \"project_id\", None\n+                )\n+            verbose_logger.debug(\n+                \"Using cached credentials for project_id: %s\",\n+                credential_project_id,\n             )\n \n         else:\n@@ -432,8 +442,11 @@ def get_access_token(\n                         project_id\n                     )\n                 )\n-\n-            self._credentials_project_mapping[credential_cache_key] = _credentials\n+            # Cache the project_id and credentials from load_auth result (resolved project_id)\n+            self._credentials_project_mapping[credential_cache_key] = (\n+                _credentials,\n+                credential_project_id,\n+            )\n \n         ## VALIDATE CREDENTIALS\n         verbose_logger.debug(f\"Validating credentials for project_id: {project_id}\")\n@@ -443,9 +456,23 @@ def get_access_token(\n             and isinstance(credential_project_id, str)\n         ):\n             project_id = credential_project_id\n+            # Update cache with resolved project_id for future lookups\n+            resolved_cache_key = (cache_credentials, project_id)\n+            if resolved_cache_key not in self._credentials_project_mapping:\n+                self._credentials_project_mapping[resolved_cache_key] = (\n+                    _credentials,\n+                    credential_project_id,\n+                )\n \n         if _credentials.expired:\n+            verbose_logger.debug(\n+                f\"Credentials expired, refreshing for project_id: {project_id}\"\n+            )\n             self.refresh_auth(_credentials)\n+            self._credentials_project_mapping[credential_cache_key] = (\n+                _credentials,\n+                credential_project_id,\n+            )\n \n         ## VALIDATION STEP\n         if _credentials.token is None or not isinstance(_credentials.token, str):\n", "test_patch": "diff --git a/tests/test_litellm/llms/vertex_ai/test_vertex_llm_base.py b/tests/test_litellm/llms/vertex_ai/test_vertex_llm_base.py\nindex 350a90f0d94b..c1cefa41ae54 100644\n--- a/tests/test_litellm/llms/vertex_ai/test_vertex_llm_base.py\n+++ b/tests/test_litellm/llms/vertex_ai/test_vertex_llm_base.py\n@@ -1,3 +1,4 @@\n+import json\n import os\n import sys\n from unittest.mock import MagicMock, call, patch\n@@ -334,6 +335,341 @@ def mock_refresh_impl(creds):\n             assert mock_credentials_from_identity_pool_with_aws.called\n             assert token == \"refreshed-token\"\n \n+    @pytest.mark.parametrize(\"is_async\", [True, False], ids=[\"async\", \"sync\"])\n+    @pytest.mark.asyncio\n+    async def test_new_cache_format_tuple_storage(self, is_async):\n+        \"\"\"Test that new cache format stores (credentials, project_id) tuples\"\"\"\n+        vertex_base = VertexBase()\n+\n+        mock_creds = MagicMock()\n+        mock_creds.token = \"token-1\"\n+        mock_creds.expired = False\n+        mock_creds.project_id = \"project-1\"\n+        mock_creds.quota_project_id = \"project-1\"\n+\n+        credentials = {\"type\": \"service_account\", \"project_id\": \"project-1\"}\n+\n+        with patch.object(\n+            vertex_base, \"load_auth\", return_value=(mock_creds, \"project-1\")\n+        ):\n+            if is_async:\n+                token, project = await vertex_base._ensure_access_token_async(\n+                    credentials=credentials,\n+                    project_id=\"project-1\",\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+            else:\n+                token, project = vertex_base._ensure_access_token(\n+                    credentials=credentials,\n+                    project_id=\"project-1\",\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+\n+            assert token == \"token-1\"\n+            assert project == \"project-1\"\n+\n+            # Verify cache stores tuple format\n+            cache_key = (json.dumps(credentials), \"project-1\")\n+            assert cache_key in vertex_base._credentials_project_mapping\n+            cached_entry = vertex_base._credentials_project_mapping[cache_key]\n+            assert isinstance(cached_entry, tuple)\n+            assert len(cached_entry) == 2\n+            cached_creds, cached_project = cached_entry\n+            assert cached_creds == mock_creds\n+            assert cached_project == \"project-1\"\n+\n+    @pytest.mark.parametrize(\"is_async\", [True, False], ids=[\"async\", \"sync\"])\n+    @pytest.mark.asyncio\n+    async def test_backward_compatibility_old_cache_format(self, is_async):\n+        \"\"\"Test backward compatibility with old cache format (just credentials)\"\"\"\n+        vertex_base = VertexBase()\n+\n+        mock_creds = MagicMock()\n+        mock_creds.token = \"token-1\"\n+        mock_creds.expired = False\n+        mock_creds.project_id = \"project-1\"\n+        mock_creds.quota_project_id = \"project-1\"\n+\n+        credentials = {\"type\": \"service_account\", \"project_id\": \"project-1\"}\n+\n+        # Simulate old cache format by manually adding just credentials (not tuple)\n+        cache_key = (json.dumps(credentials), \"project-1\")\n+        vertex_base._credentials_project_mapping[cache_key] = mock_creds\n+\n+        # Should handle old format gracefully\n+        if is_async:\n+            token, project = await vertex_base._ensure_access_token_async(\n+                credentials=credentials,\n+                project_id=\"project-1\",\n+                custom_llm_provider=\"vertex_ai\",\n+            )\n+        else:\n+            token, project = vertex_base._ensure_access_token(\n+                credentials=credentials,\n+                project_id=\"project-1\",\n+                custom_llm_provider=\"vertex_ai\",\n+            )\n+\n+        assert token == \"token-1\"\n+        assert project == \"project-1\"\n+\n+    @pytest.mark.parametrize(\"is_async\", [True, False], ids=[\"async\", \"sync\"])\n+    @pytest.mark.asyncio\n+    async def test_resolved_project_id_cache_optimization(self, is_async):\n+        \"\"\"Test that resolved project_id creates additional cache entries for optimization\"\"\"\n+        vertex_base = VertexBase()\n+\n+        mock_creds = MagicMock()\n+        mock_creds.token = \"token-1\"\n+        mock_creds.expired = False\n+        mock_creds.project_id = \"resolved-project\"\n+        mock_creds.quota_project_id = \"resolved-project\"\n+\n+        credentials = {\"type\": \"service_account\"}\n+\n+        with patch.object(\n+            vertex_base, \"load_auth\", return_value=(mock_creds, \"resolved-project\")\n+        ):\n+            # Call without project_id, should use resolved project from credentials\n+            if is_async:\n+                token, project = await vertex_base._ensure_access_token_async(\n+                    credentials=credentials,\n+                    project_id=None,\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+            else:\n+                token, project = vertex_base._ensure_access_token(\n+                    credentials=credentials,\n+                    project_id=None,\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+\n+            assert token == \"token-1\"\n+            assert project == \"resolved-project\"\n+\n+                        # Verify both cache entries exist\n+            original_cache_key = (json.dumps(credentials), None)\n+            resolved_cache_key = (json.dumps(credentials), \"resolved-project\")\n+\n+            assert original_cache_key in vertex_base._credentials_project_mapping\n+            assert resolved_cache_key in vertex_base._credentials_project_mapping\n+\n+            # Both should contain the same tuple\n+            original_entry = vertex_base._credentials_project_mapping[original_cache_key]\n+            resolved_entry = vertex_base._credentials_project_mapping[resolved_cache_key]\n+\n+            assert isinstance(original_entry, tuple)\n+            assert isinstance(resolved_entry, tuple)\n+            assert original_entry[0] == mock_creds\n+            assert original_entry[1] == \"resolved-project\"\n+            assert resolved_entry[0] == mock_creds\n+            assert resolved_entry[1] == \"resolved-project\"\n+\n+    @pytest.mark.parametrize(\"is_async\", [True, False], ids=[\"async\", \"sync\"])\n+    @pytest.mark.asyncio\n+    async def test_cache_update_on_credential_refresh(self, is_async):\n+        \"\"\"Test that cache is updated when credentials are refreshed\"\"\"\n+        vertex_base = VertexBase()\n+\n+        mock_creds = MagicMock()\n+        mock_creds.token = \"original-token\"\n+        mock_creds.expired = True  # Start with expired credentials\n+        mock_creds.project_id = \"project-1\"\n+        mock_creds.quota_project_id = \"project-1\"\n+\n+        credentials = {\"type\": \"service_account\", \"project_id\": \"project-1\"}\n+\n+        with patch.object(\n+            vertex_base, \"load_auth\", return_value=(mock_creds, \"project-1\")\n+        ), patch.object(vertex_base, \"refresh_auth\") as mock_refresh:\n+\n+            def mock_refresh_impl(creds):\n+                creds.token = \"refreshed-token\"\n+                creds.expired = False\n+\n+            mock_refresh.side_effect = mock_refresh_impl\n+\n+            if is_async:\n+                token, project = await vertex_base._ensure_access_token_async(\n+                    credentials=credentials,\n+                    project_id=\"project-1\",\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+            else:\n+                token, project = vertex_base._ensure_access_token(\n+                    credentials=credentials,\n+                    project_id=\"project-1\",\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+\n+            assert mock_refresh.called\n+            assert token == \"refreshed-token\"\n+            assert project == \"project-1\"\n+\n+            # Verify cache was updated with refreshed credentials\n+            cache_key = (json.dumps(credentials), \"project-1\")\n+            assert cache_key in vertex_base._credentials_project_mapping\n+            cached_entry = vertex_base._credentials_project_mapping[cache_key]\n+            assert isinstance(cached_entry, tuple)\n+            cached_creds, cached_project = cached_entry\n+            assert cached_creds.token == \"refreshed-token\"\n+            assert not cached_creds.expired\n+            assert cached_project == \"project-1\"\n+\n+    @pytest.mark.parametrize(\"is_async\", [True, False], ids=[\"async\", \"sync\"])\n+    @pytest.mark.asyncio\n+    async def test_cache_with_different_project_id_combinations(self, is_async):\n+        \"\"\"Test caching behavior with different project_id parameter combinations\"\"\"\n+        vertex_base = VertexBase()\n+\n+        mock_creds = MagicMock()\n+        mock_creds.token = \"token-1\"\n+        mock_creds.expired = False\n+        mock_creds.project_id = \"cred-project\"\n+        mock_creds.quota_project_id = \"cred-project\"\n+\n+        credentials = {\"type\": \"service_account\", \"project_id\": \"cred-project\"}\n+\n+        with patch.object(\n+            vertex_base, \"load_auth\", return_value=(mock_creds, \"cred-project\")\n+        ):\n+            # First call with explicit project_id\n+            if is_async:\n+                token1, project1 = await vertex_base._ensure_access_token_async(\n+                    credentials=credentials,\n+                    project_id=\"explicit-project\",\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+            else:\n+                token1, project1 = vertex_base._ensure_access_token(\n+                    credentials=credentials,\n+                    project_id=\"explicit-project\",\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+\n+            # Second call with None project_id (should use credential project)\n+            if is_async:\n+                token2, project2 = await vertex_base._ensure_access_token_async(\n+                    credentials=credentials,\n+                    project_id=None,\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+            else:\n+                token2, project2 = vertex_base._ensure_access_token(\n+                    credentials=credentials,\n+                    project_id=None,\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+\n+            assert token1 == \"token-1\"\n+            assert project1 == \"explicit-project\"  # Should use explicit project_id\n+            assert token2 == \"token-1\"\n+            assert project2 == \"cred-project\"  # Should use credential project_id\n+\n+            # Verify separate cache entries\n+            explicit_cache_key = (json.dumps(credentials), \"explicit-project\")\n+            none_cache_key = (json.dumps(credentials), None)\n+            resolved_cache_key = (json.dumps(credentials), \"cred-project\")\n+\n+            assert explicit_cache_key in vertex_base._credentials_project_mapping\n+            assert none_cache_key in vertex_base._credentials_project_mapping\n+            assert resolved_cache_key in vertex_base._credentials_project_mapping\n+\n+    @pytest.mark.parametrize(\"is_async\", [True, False], ids=[\"async\", \"sync\"])\n+    @pytest.mark.asyncio\n+    async def test_project_id_resolution_and_caching_core_issue(self, is_async):\n+        \"\"\"\n+        When user doesn't provide project_id, system should resolve it from credentials\n+        and cache the resolved project_id for future calls without calling load_auth again.\n+        \"\"\"\n+        vertex_base = VertexBase()\n+\n+        mock_creds = MagicMock()\n+        mock_creds.token = \"token-from-creds\"\n+        mock_creds.expired = False\n+        mock_creds.project_id = \"resolved-from-credentials\"\n+        mock_creds.quota_project_id = \"resolved-from-credentials\"\n+\n+        # User provides credentials but NO project_id (this is the key scenario)\n+        credentials = {\"type\": \"service_account\"}\n+\n+        with patch.object(\n+            vertex_base, \"load_auth\", return_value=(mock_creds, \"resolved-from-credentials\")\n+        ) as mock_load_auth:\n+\n+            # First call: User provides NO project_id, should resolve from credentials\n+            if is_async:\n+                token1, project1 = await vertex_base._ensure_access_token_async(\n+                    credentials=credentials,\n+                    project_id=None,  # Key: user doesn't provide project_id\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+            else:\n+                token1, project1 = vertex_base._ensure_access_token(\n+                    credentials=credentials,\n+                    project_id=None,  # Key: user doesn't provide project_id\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+\n+            # Should have called load_auth once to resolve project_id\n+            assert mock_load_auth.call_count == 1\n+            assert token1 == \"token-from-creds\"\n+            assert project1 == \"resolved-from-credentials\"\n+\n+            # Verify cache contains both the original key and resolved key\n+            original_cache_key = (json.dumps(credentials), None)\n+            resolved_cache_key = (json.dumps(credentials), \"resolved-from-credentials\")\n+\n+            assert original_cache_key in vertex_base._credentials_project_mapping\n+            assert resolved_cache_key in vertex_base._credentials_project_mapping\n+\n+            # Both should contain the tuple with resolved project_id\n+            original_entry = vertex_base._credentials_project_mapping[original_cache_key]\n+            resolved_entry = vertex_base._credentials_project_mapping[resolved_cache_key]\n+\n+            assert isinstance(original_entry, tuple)\n+            assert isinstance(resolved_entry, tuple)\n+            assert original_entry[1] == \"resolved-from-credentials\"\n+            assert resolved_entry[1] == \"resolved-from-credentials\"\n+\n+            # Second call: Same scenario - should use cache and NOT call load_auth again\n+            if is_async:\n+                token2, project2 = await vertex_base._ensure_access_token_async(\n+                    credentials=credentials,\n+                    project_id=None,  # Still no project_id provided\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+            else:\n+                token2, project2 = vertex_base._ensure_access_token(\n+                    credentials=credentials,\n+                    project_id=None,  # Still no project_id provided\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+\n+            # Should NOT have called load_auth again (still 1 call total)\n+            assert mock_load_auth.call_count == 1\n+            assert token2 == \"token-from-creds\"\n+            assert project2 == \"resolved-from-credentials\"\n+\n+            # Third call: Now user provides the resolved project_id explicitly\n+            # This should also use cache (the resolved_cache_key)\n+            if is_async:\n+                token3, project3 = await vertex_base._ensure_access_token_async(\n+                    credentials=credentials,\n+                    project_id=\"resolved-from-credentials\",  # Explicit resolved project_id\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+            else:\n+                token3, project3 = vertex_base._ensure_access_token(\n+                    credentials=credentials,\n+                    project_id=\"resolved-from-credentials\",  # Explicit resolved project_id\n+                    custom_llm_provider=\"vertex_ai\",\n+                )\n+\n+            # Should still NOT have called load_auth again (cache hit)\n+            assert mock_load_auth.call_count == 1\n+            assert token3 == \"token-from-creds\"\n+            assert project3 == \"resolved-from-credentials\"\n \n     @pytest.mark.parametrize(\n         \"api_base, vertex_location, expected\",\n@@ -362,7 +698,6 @@ def mock_refresh_impl(creds):\n             ),\n         ],\n     )\n-\n     def test_get_api_base(self, api_base, vertex_location, expected):\n         vertex_base = VertexBase()\n         assert (\n", "problem_statement": "[Bug]: GCP Service Account Credentials not refreshing appropriately when set via env var\n### What happened?\n\nI believe that after the token expiration, LiteLLM is not refreshing the access token as needed. My application is running properly for some amount of time, but at some point I receive the errors below. I'm assuming the token has a lifetime of 12hrs so this is hard to pin down since it occurs only overnight. The error is manifesting as cannot resolve `project_id`, it only happens during the refresh cycle. A brief glance at the code, the project_id is optional and I guess the initial access token is fine without it but refresh is not.\n\nThis specifically related to VertexAI on GCP via built in Service Account. Not a service account file, ADC, etc although those may have similar issues. In this scenario, providing a specific project is not necessary since I'm retrieving and refreshing a token within the same project. Perhaps in this scenario, project_id should be set once the initial token is received based on the metadata.\n\nOther notes:\n\n1. versions\n* `1.65.4.post1` - issue occurred\n* `1.65.3` - in testing\n1. I'm 8+ hours into a fresh deployment and the issue has not yet occurred but I assume will around the 12hr mark. UPDATE: I am 24 hours in and haven't seen the issue but I did downgrade the version as well. I will keep an eye out and may try upgrading again.\n1. I presume this will show up on ADC based credentials as well but haven't tested with this length of time. \n1. When using the official GCP libraries and running on GCP, these token issues should automatically handled. \n1. My app runs in Cloud Run with a service account and has no issues initially but eventually hits this without a redeployment.\n1. Hard to tell if #8086, #8424, #8771 are all related\n\n\n[1] https://cloud.google.com/docs/authentication/token-types#at-lifetime\n\nPotential Mitigation:\n\nI'm going to try to see if this fixes the issue:\n\n```\n    response = completion(\n        model=\"gemini-2.0-flash-001\",\n        vertex_project=\"my_project_id\",\n...\n```\n\n\n### Relevant log output\n\n```shell\nERROR 2025-04-09T17:50:26.228856Z Traceback (most recent call last): File \"/usr/local/lib/python3.11/site-packages/litellm/main.py\", line 2449, in completion model_response = vertex_chat_completion.completion( # type: ignore\nDEFAULT 2025-04-09T17:50:26.228858Z ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nDEFAULT 2025-04-09T17:50:26.228861Z File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1292, in completion\nDEFAULT 2025-04-09T17:50:26.228864Z _auth_header, vertex_project = self._ensure_access_token(\nDEFAULT 2025-04-09T17:50:26.228867Z ^^^^^^^^^^^^^^^^^^^^^^^^^^\nDEFAULT 2025-04-09T17:50:26.228870Z File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 136, in _ensure_access_token\nDEFAULT 2025-04-09T17:50:26.228873Z return self.get_access_token(\nDEFAULT 2025-04-09T17:50:26.228875Z ^^^^^^^^^^^^^^^^^^^^^^\nDEFAULT 2025-04-09T17:50:26.228878Z File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 336, in get_access_token\nDEFAULT 2025-04-09T17:50:26.228882Z raise ValueError(\"Could not resolve project_id\")\nDEFAULT 2025-04-09T17:50:26.228887Z ValueError: Could not resolve project_id\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\n1.65.4.post1\n\n### Twitter / LinkedIn details\n\n_No response_\n", "hints_text": "I am 24 hours in and haven't seen the issue but I did downgrade the version as well. I will keep an eye out and may try upgrading again over the weekend.\nHi @ryanc4ai do you still see this? \n@krrishdholakia I've not had issues with it on `1.65.3` but havent' been able to test any higher version.\nHad the same issue with `1.66.1`, downgraded to `1.65.3` and seems like the issue is gone. \n\nError when using `1.66.1`:\n\n```\nTraceback (most recent call last):\n  File \"/Users/chuck/Documents/CWS/SubCat/venv/lib/python3.11/site-packages/litellm/main.py\", line 2435, in completion\n    model_response = vertex_chat_completion.completion(  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chuck/Documents/CWS/SubCat/venv/lib/python3.11/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1301, in completion\n    _auth_header, vertex_project = self._ensure_access_token(\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chuck/Documents/CWS/SubCat/venv/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 135, in _ensure_access_token\n    return self.get_access_token(\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chuck/Documents/CWS/SubCat/venv/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 335, in get_access_token\n    raise ValueError(\"Could not resolve project_id\")\nValueError: Could not resolve project_id\n```\nI see the same error `1.66.0`. What is the new version that could work fine?\n```\nlitellm.APIConnectionError: Could not resolve project_id\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/litellm/main.py\", line 477, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1039, in async_streaming\n    _auth_header, vertex_project = await self._ensure_access_token_async(\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 359, in _ensure_access_token_async\n    raise e\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 354, in _ensure_access_token_async\n    return await asyncify(self.get_access_token)(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/asyncify.py\", line 57, in wrapper\n    return await anyio.to_thread.run_sync(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 335, in get_access_token\n    raise ValueError(\"Could not resolve project_id\")\nValueError: Could not resolve project_id\n```\n> I see the same error `1.66.0`. What is the new version that could work fine?\n\n`1.65.3` seems to work fine. You can try adding `vertex_project=\"my_project_id\"` as a potential unproven mitigation:\n\n```\n    response = completion(\n        model=\"gemini-2.0-flash-001\",\n        vertex_project=\"my_project_id\",\n...\n```\n\nI've just started a test on `litellm==1.67.2` with and without the potential mitigation and will let it run for at least 24 hours.\nUpdate: on`1.67.2` I saw the failure within 2 hours.\n\nBut good news, the mitigation mentioned appears to work. The script with `vertex_project=` included is still working:\n\n```\n    response = completion(\n        model=\"gemini-2.0-flash-001\",\n        vertex_project=\"my_project_id\",\n...\n```\n\nOk, so for solutions, either `vertex_project` should be required, or better, during a token refresh, litellm should use the project_id available from the metadata.\nHey @ryanc4ai we do check for project id in the metadata - https://github.com/BerriAI/litellm/blob/56d00c43f7921d5d9cac1cab39b98913667fdd8d/litellm/llms/vertex_ai/vertex_llm_base.py#L93\n\n\nThe only change i can see across versions is the context caching fix - https://github.com/BerriAI/litellm/pull/9756\n\nHow were you initially calling litellm ? (trying to see what we missed) \n@krrishdholakia , I tried to figure out where it was breaking down but only got as far as seeing that the following is Optional\n\nhttps://github.com/BerriAI/litellm/blob/6ba3c4a4f84f57a00c4f1a10e6857a7b0d8c8f19/litellm/llms/vertex_ai/vertex_llm_base.py#L193\n\nBut that the code needs a project_id:\n\nhttps://github.com/BerriAI/litellm/blob/6ba3c4a4f84f57a00c4f1a10e6857a7b0d8c8f19/litellm/llms/vertex_ai/vertex_llm_base.py#L334C12-L334C22\n\nI can see the logic trying to set it `project_id` but it's breaking down somewhere. \n\nSo  this causes the error\n\n```python\n    response = completion(\n        model=\"gemini-2.0-flash-001\",\n    ...\n```\n\nand this does not\n```python\n    response = completion(\n        model=\"gemini-2.0-flash-001\",\n        vertex_project=\"my_project_id\",\n    ...\n```\n\nI've got a VM on GCP with a test script so happy to play around with it if you have any proposals.\nSame issue - I think the current workaround would be to downgrade to `1.65.3` - @ryanc4ai solution of setting the vertex_project explicitly also works but it's less than ideal.\nHas anybody tried setting vertex_project with latest version 1.68.2? Is it working? I am running into permission issues.\nWhat permission issue do you see? @tanvithakur94 \n> What permission issue do you see? [@tanvithakur94](https://github.com/tanvithakur94)\n@krrishdholakia I was able to successfully run version 1.65.3....and no version after that works for me. I am seeing error:\n`{\n'Unable to acquire impersonated credentials', '{\\\\n  \\\"error\\\": {\\\\n    \\\"code\\\": 403,\\\\n    \\\"message\\\": \\\"Caller does not have required permission to use project xxx. Grant the caller the roles/serviceusage.serviceUsageConsumer role, or a custom role with the serviceusage.services.use permission, by visiting https://console.developers.google.com/iam-admin/iam/project?project=xxx and then retry. Propagation of the new permission may take a few minutes.\\\",\\\\n    \\\"status\\\": \\\"PERMISSION_DENIED\\\",\\\\n    \\\"details\\\": [\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.ErrorInfo\\\",\\\\n        \\\"reason\\\": \\\"USER_PROJECT_DENIED\\\",\\\\n        \\\"domain\\\": \\\"googleapis.com\\\",\\\\n        \\\"metadata\\\": {\\\\n          \\\"containerInfo\\\": \\\"xxxx\\\",\\\\n          \\\"consumer\\\": \\\"projects/xx\\\",\\\\n          \\\"service\\\": \\\"iamcredentials.googleapis.com\\\"\\\\n        }\\\\n      },\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.LocalizedMessage\\\",\\\\n        \\\"locale\\\": \\\"en-US\\\",\\\\n        \\\"message\\\": \\\"Caller does not have required permission to use project xxxxx. Grant the caller the roles/serviceusage.serviceUsageConsumer role, or a custom role with the serviceusage.services.use permission, by visiting https://console.developers.google.com/iam-admin/iam/project?project=xxxx and then retry. Propagation of the new permission may take a few minutes.\\\"\\\\n      },\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.Help\\\",\\\\n        \\\"links\\\": [\\\\n          {\\\\n            \\\"description\\\": \\\"Google developer console IAM admin\\\",\\\\n            \\\"url\\\": \\\"https://console.developers.google.com/iam-admin/iam/project?project=xxxxx\\\"\\\\n          }\\\\n        ]\\\\n      }\\\\n    ]\\\\n  }\\\\n}\\\\n')\"\n}`\n> > What permission issue do you see? [@tanvithakur94](https://github.com/tanvithakur94)\n> > [@krrishdholakia](https://github.com/krrishdholakia) I was able to successfully run version 1.65.3....and no version after that works for me. I am seeing error:\n> > `{ 'Unable to acquire impersonated credentials', '{\\\\n  \\\"error\\\": {\\\\n    \\\"code\\\": 403,\\\\n    \\\"message\\\": \\\"Caller does not have required permission to use project xxx. Grant the caller the roles/serviceusage.serviceUsageConsumer role, or a custom role with the serviceusage.services.use permission, by visiting https://console.developers.google.com/iam-admin/iam/project?project=xxx and then retry. Propagation of the new permission may take a few minutes.\\\",\\\\n    \\\"status\\\": \\\"PERMISSION_DENIED\\\",\\\\n    \\\"details\\\": [\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.ErrorInfo\\\",\\\\n        \\\"reason\\\": \\\"USER_PROJECT_DENIED\\\",\\\\n        \\\"domain\\\": \\\"googleapis.com\\\",\\\\n        \\\"metadata\\\": {\\\\n          \\\"containerInfo\\\": \\\"xxxx\\\",\\\\n          \\\"consumer\\\": \\\"projects/xx\\\",\\\\n          \\\"service\\\": \\\"iamcredentials.googleapis.com\\\"\\\\n        }\\\\n      },\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.LocalizedMessage\\\",\\\\n        \\\"locale\\\": \\\"en-US\\\",\\\\n        \\\"message\\\": \\\"Caller does not have required permission to use project xxxxx. Grant the caller the roles/serviceusage.serviceUsageConsumer role, or a custom role with the serviceusage.services.use permission, by visiting https://console.developers.google.com/iam-admin/iam/project?project=xxxx and then retry. Propagation of the new permission may take a few minutes.\\\"\\\\n      },\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.Help\\\",\\\\n        \\\"links\\\": [\\\\n          {\\\\n            \\\"description\\\": \\\"Google developer console IAM admin\\\",\\\\n            \\\"url\\\": \\\"https://console.developers.google.com/iam-admin/iam/project?project=xxxxx\\\"\\\\n          }\\\\n        ]\\\\n      }\\\\n    ]\\\\n  }\\\\n}\\\\n')\" }`\n\n@krrishdholakia Could this error be related to recent changes made for getting the access_token?  https://github.com/BerriAI/litellm/commit/e1f7bcb47dbc6d830ac8a81a8866469f07695ee4\n@krrishdholakia Did you get a chance to look at the error above?\n@krrishdholakia we are using workload identity federation when calling GCP models and that seems to be not working for any version above 1.65.3 with the error shown above. Can  you check at your end because @vaghelarahul94 pointed, there were some changes made regarding access tokens.\nHi team, acknowledging this - i see 2 issues - credential caller and project id not resolved, let's scope this ticket to the project id not resolved issue - i believe this occurs when the refreshed credentials don't contain a project id. \n\nThis is the function is the place the fix needs to happen - https://github.com/BerriAI/litellm/blob/b9460dfe37dfac0cdc07a29b5aa4609462804e3c/litellm/llms/vertex_ai/vertex_llm_base.py#L244\n\nCan somebody please enable `litellm._turn_on_debug()` for sdk or `--detailed_debug` for the proxy and share what those logs look like - specifically trying to trace the path for the project id not resolved issue. \n> Can somebody please enable `litellm._turn_on_debug()` for sdk or `--detailed_debug` for the proxy and share what those logs look like - specifically trying to trace the path for the project id not resolved issue.\n\nOn it @krrishdholakia . just started one. Also updated to `litellm==1.69.3`.\n> [@krrishdholakia](https://github.com/krrishdholakia) we are using workload identity federation when calling GCP models and that seems to be not working for any version above 1.65.3 with the error shown above. Can you check at your end because [@vaghelarahul94](https://github.com/vaghelarahul94) pointed, there were some changes made regarding access tokens.\n\n@krrishdholakia If we remove the `quota_project_id` parameter from `google_auth.default` https://github.com/BerriAI/litellm/blob/main/litellm/llms/vertex_ai/vertex_llm_base.py#L96, then we are able to fetch the creds successfully. Here is the script that can be used to reproduce.\n\nSuccessful script: -\n```\nimport os\nimport google.auth\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"workload_identity_federation_file_path\" # replace your json file path\n\ncreds, creds_project_id = google.auth.default(\n    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n)\n\nprint(f\"creds: {creds}\")\nprint(f\"creds_project_id: {creds_project_id}\")\n```\n\nUnsuccessful script: -\n\n```\nimport os\nimport google.auth\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"workload_identity_federation_file_path\" # replace your json file path\n\ncreds, creds_project_id = google.auth.default(\n    quota_project_id=project_id,\n    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n)\n\nprint(f\"creds: {creds}\")\nprint(f\"creds_project_id: {creds_project_id}\")\n```\n@krrishdholakia here are the debug logs:\n\n```\nDEBUG:LiteLLM:\nLiteLLM: Non-Default params passed to completion() {}\nDEBUG:LiteLLM:Final returned optional params: {}\nDEBUG:LiteLLM:self.optional_params: {}\nDEBUG:LiteLLM:Checking cached credentials for project_id: None\nDEBUG:LiteLLM:Cached credentials found for project_id: None.\nDEBUG:LiteLLM:Using cached credentials\nDEBUG:LiteLLM:Validating credentials for project_id: None\nDEBUG:google.auth.transport.requests:Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/?recursive=true\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): metadata.google.internal:80\nDEBUG:urllib3.connectionpool:http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/?recursive=true HTTP/1.1\" 200 143\nDEBUG:google.auth.transport.requests:Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform\nDEBUG:urllib3.connectionpool:http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform HTTP/1.1\" 200 1083\nDEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n```\n\nI can attach the whole file if needed \nThanks @vaghelarahul94 \n\nPushed a fix with your changes, aiming to have it out in today's release. I suspect this was a pre-existing error which was only surfaced once we stopped storing the self.project_id\n> Thanks [@vaghelarahul94](https://github.com/vaghelarahul94)\n> \n> Pushed a fix with your changes, aiming to have it out in today's release. I suspect this was a pre-existing error which was only surfaced once we stopped storing the self.project_id\n\n@krrishdholakia Thanks for checking this. Can you point us to the PR wherein this is fixed?\n@tanvithakur94 https://github.com/BerriAI/litellm/commit/b4d54c16780e716fa5034d8ee2c3e9f353237774\nHello @krrishdholakia another problem also is when using common GCP cross-project service account pattern:\n\n- You have service account credentials from Project A.\n\n- You want to use those credentials to access Vertex AI in Project B (where Vertex AI is enabled).\n\n- Your service account from Project A has been granted the necessary roles on Project B.\n\n[litellm/litellm/llms/vertex_ai/vertex_llm_base.py](https://github.com/BerriAI/litellm/blob/b9460dfe37dfac0cdc07a29b5aa4609462804e3c/litellm/llms/vertex_ai/vertex_llm_base.py#L306-L315)\n\n`credential_project_id` is the project in the service account file (Project A).\n`project_id` is the target Vertex project (Project B).\n\nthis won't work because the two projects `credential_project_id` and `project_id` won't match in this case\nGot it - is the suggestion to not have that validation check since it is possible for that credential to call another project? \nyes, no need for that i think.\n@krrishdholakia I see the build is still failing after the recent fix. Could you please take a look when you get a chance \nhttps://app.circleci.com/pipelines/github/BerriAI/litellm/32344/workflows/85af9df8-fe8f-4cc5-b113-33241c1348fa/jobs/411005/tests\nThis is still happening in v1.71.1\nHey @anatolec i can see @vaghelarahul94's fix is live\n\nhttps://github.com/BerriAI/litellm/blob/e74ff23b458dd153a442307bf118e34ff165ef64/litellm/llms/vertex_ai/vertex_llm_base.py#L95\n\n\nCan i see how you're configuring the vertex model on litellm? \n\n@vaghelarahul94 do you see the same issue on v1.71.1? \n> Can i see how you're configuring the vertex model on litellm?\n\nWhat do you need to see exactly @krrishdholakia ?\n\nI'm using the `acompletion` method and I consistently get that error when our app is deployed in GCP Cloud Run. Being on Cloud Run, my app automatically gets a service account, so I don't expect to have to do anything to authenticate.\n\nI can't reproduce the problem locally.\n\nI tried to add the `vertex_project` parameter to `acompletion`, but then I get a different error (this [one](https://github.com/BerriAI/litellm/issues/9638)), which makes it hard to progress.\n\nI'm available for a pairing session if that helps.\nI've started a test on `litellm-1.72.0`. This has, in the past, usually failed within 2 or 3 hours. I have a test VM as well if sharing would access would be helpful.\n@krrishdholakia unfortunately it's still failing ( I started the test prior to my last comment so it still faliled in that 2-ish hours window):\n\n```bash\nDEBUG:LiteLLM:self.optional_params: {}\nDEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\nDEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash', 'custom_llm_provider': 'vertex_ai'}\nINFO:LiteLLM:\nLiteLLM completion() model= gemini-2.0-flash-001; provider = vertex_ai\nDEBUG:LiteLLM:\nLiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-001', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'vertex_ai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nfollow thse insrtucions:\\n                    '}, {'role': 'user', 'content': '\\n                    say pong if I say ping\\n'}], 'thinking': None, 'web_search_options': None}\nDEBUG:LiteLLM:\nLiteLLM: Non-Default params passed to completion() {}\nDEBUG:LiteLLM:Final returned optional params: {}\nDEBUG:LiteLLM:self.optional_params: {}\nDEBUG:LiteLLM:Checking cached credentials for project_id: None\nDEBUG:LiteLLM:Cached credentials found for project_id: None.\nDEBUG:LiteLLM:Using cached credentials\nDEBUG:LiteLLM:Validating credentials for project_id: None\nDEBUG:google.auth.transport.requests:Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/?recursive=true\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): metadata.google.internal:80\nDEBUG:urllib3.connectionpool:http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/?recursive=true HTTP/1.1\" 200 143\nDEBUG:google.auth.transport.requests:Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform\nDEBUG:urllib3.connectionpool:http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform HTTP/1.1\" 200 1083\nDEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n```\nHey @krrishdholakia ,\nShall we reopen this bug then ?\nLet me know if I can help with debugging.\n@krrishdholakia can we reopen this bug. It still presists as per last check on Jun 4th. I'm running into the issue where 3rd party libraries which rely on Litellm are hitting it (crewai) and passing the in the workaround isn't always straightforward or working.\nUnable to reproduce this issue. If someone is able to reproduce this, a PR for the fix would be appreciated! \n@krrishdholakia I've got a fix I'm testing overnight and hoping to open a PR in the morning.\n\n", "all_hints_text": "I am 24 hours in and haven't seen the issue but I did downgrade the version as well. I will keep an eye out and may try upgrading again over the weekend.\nHi @ryanc4ai do you still see this? \n@krrishdholakia I've not had issues with it on `1.65.3` but havent' been able to test any higher version.\nHad the same issue with `1.66.1`, downgraded to `1.65.3` and seems like the issue is gone. \n\nError when using `1.66.1`:\n\n```\nTraceback (most recent call last):\n  File \"/Users/chuck/Documents/CWS/SubCat/venv/lib/python3.11/site-packages/litellm/main.py\", line 2435, in completion\n    model_response = vertex_chat_completion.completion(  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chuck/Documents/CWS/SubCat/venv/lib/python3.11/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1301, in completion\n    _auth_header, vertex_project = self._ensure_access_token(\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chuck/Documents/CWS/SubCat/venv/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 135, in _ensure_access_token\n    return self.get_access_token(\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/chuck/Documents/CWS/SubCat/venv/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 335, in get_access_token\n    raise ValueError(\"Could not resolve project_id\")\nValueError: Could not resolve project_id\n```\nI see the same error `1.66.0`. What is the new version that could work fine?\n```\nlitellm.APIConnectionError: Could not resolve project_id\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/litellm/main.py\", line 477, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 1039, in async_streaming\n    _auth_header, vertex_project = await self._ensure_access_token_async(\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 359, in _ensure_access_token_async\n    raise e\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 354, in _ensure_access_token_async\n    return await asyncify(self.get_access_token)(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/asyncify.py\", line 57, in wrapper\n    return await anyio.to_thread.run_sync(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 335, in get_access_token\n    raise ValueError(\"Could not resolve project_id\")\nValueError: Could not resolve project_id\n```\n> I see the same error `1.66.0`. What is the new version that could work fine?\n\n`1.65.3` seems to work fine. You can try adding `vertex_project=\"my_project_id\"` as a potential unproven mitigation:\n\n```\n    response = completion(\n        model=\"gemini-2.0-flash-001\",\n        vertex_project=\"my_project_id\",\n...\n```\n\nI've just started a test on `litellm==1.67.2` with and without the potential mitigation and will let it run for at least 24 hours.\nUpdate: on`1.67.2` I saw the failure within 2 hours.\n\nBut good news, the mitigation mentioned appears to work. The script with `vertex_project=` included is still working:\n\n```\n    response = completion(\n        model=\"gemini-2.0-flash-001\",\n        vertex_project=\"my_project_id\",\n...\n```\n\nOk, so for solutions, either `vertex_project` should be required, or better, during a token refresh, litellm should use the project_id available from the metadata.\nHey @ryanc4ai we do check for project id in the metadata - https://github.com/BerriAI/litellm/blob/56d00c43f7921d5d9cac1cab39b98913667fdd8d/litellm/llms/vertex_ai/vertex_llm_base.py#L93\n\n\nThe only change i can see across versions is the context caching fix - https://github.com/BerriAI/litellm/pull/9756\n\nHow were you initially calling litellm ? (trying to see what we missed) \n@krrishdholakia , I tried to figure out where it was breaking down but only got as far as seeing that the following is Optional\n\nhttps://github.com/BerriAI/litellm/blob/6ba3c4a4f84f57a00c4f1a10e6857a7b0d8c8f19/litellm/llms/vertex_ai/vertex_llm_base.py#L193\n\nBut that the code needs a project_id:\n\nhttps://github.com/BerriAI/litellm/blob/6ba3c4a4f84f57a00c4f1a10e6857a7b0d8c8f19/litellm/llms/vertex_ai/vertex_llm_base.py#L334C12-L334C22\n\nI can see the logic trying to set it `project_id` but it's breaking down somewhere. \n\nSo  this causes the error\n\n```python\n    response = completion(\n        model=\"gemini-2.0-flash-001\",\n    ...\n```\n\nand this does not\n```python\n    response = completion(\n        model=\"gemini-2.0-flash-001\",\n        vertex_project=\"my_project_id\",\n    ...\n```\n\nI've got a VM on GCP with a test script so happy to play around with it if you have any proposals.\nSame issue - I think the current workaround would be to downgrade to `1.65.3` - @ryanc4ai solution of setting the vertex_project explicitly also works but it's less than ideal.\nHas anybody tried setting vertex_project with latest version 1.68.2? Is it working? I am running into permission issues.\nWhat permission issue do you see? @tanvithakur94 \n> What permission issue do you see? [@tanvithakur94](https://github.com/tanvithakur94)\n@krrishdholakia I was able to successfully run version 1.65.3....and no version after that works for me. I am seeing error:\n`{\n'Unable to acquire impersonated credentials', '{\\\\n  \\\"error\\\": {\\\\n    \\\"code\\\": 403,\\\\n    \\\"message\\\": \\\"Caller does not have required permission to use project xxx. Grant the caller the roles/serviceusage.serviceUsageConsumer role, or a custom role with the serviceusage.services.use permission, by visiting https://console.developers.google.com/iam-admin/iam/project?project=xxx and then retry. Propagation of the new permission may take a few minutes.\\\",\\\\n    \\\"status\\\": \\\"PERMISSION_DENIED\\\",\\\\n    \\\"details\\\": [\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.ErrorInfo\\\",\\\\n        \\\"reason\\\": \\\"USER_PROJECT_DENIED\\\",\\\\n        \\\"domain\\\": \\\"googleapis.com\\\",\\\\n        \\\"metadata\\\": {\\\\n          \\\"containerInfo\\\": \\\"xxxx\\\",\\\\n          \\\"consumer\\\": \\\"projects/xx\\\",\\\\n          \\\"service\\\": \\\"iamcredentials.googleapis.com\\\"\\\\n        }\\\\n      },\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.LocalizedMessage\\\",\\\\n        \\\"locale\\\": \\\"en-US\\\",\\\\n        \\\"message\\\": \\\"Caller does not have required permission to use project xxxxx. Grant the caller the roles/serviceusage.serviceUsageConsumer role, or a custom role with the serviceusage.services.use permission, by visiting https://console.developers.google.com/iam-admin/iam/project?project=xxxx and then retry. Propagation of the new permission may take a few minutes.\\\"\\\\n      },\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.Help\\\",\\\\n        \\\"links\\\": [\\\\n          {\\\\n            \\\"description\\\": \\\"Google developer console IAM admin\\\",\\\\n            \\\"url\\\": \\\"https://console.developers.google.com/iam-admin/iam/project?project=xxxxx\\\"\\\\n          }\\\\n        ]\\\\n      }\\\\n    ]\\\\n  }\\\\n}\\\\n')\"\n}`\n> > What permission issue do you see? [@tanvithakur94](https://github.com/tanvithakur94)\n> > [@krrishdholakia](https://github.com/krrishdholakia) I was able to successfully run version 1.65.3....and no version after that works for me. I am seeing error:\n> > `{ 'Unable to acquire impersonated credentials', '{\\\\n  \\\"error\\\": {\\\\n    \\\"code\\\": 403,\\\\n    \\\"message\\\": \\\"Caller does not have required permission to use project xxx. Grant the caller the roles/serviceusage.serviceUsageConsumer role, or a custom role with the serviceusage.services.use permission, by visiting https://console.developers.google.com/iam-admin/iam/project?project=xxx and then retry. Propagation of the new permission may take a few minutes.\\\",\\\\n    \\\"status\\\": \\\"PERMISSION_DENIED\\\",\\\\n    \\\"details\\\": [\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.ErrorInfo\\\",\\\\n        \\\"reason\\\": \\\"USER_PROJECT_DENIED\\\",\\\\n        \\\"domain\\\": \\\"googleapis.com\\\",\\\\n        \\\"metadata\\\": {\\\\n          \\\"containerInfo\\\": \\\"xxxx\\\",\\\\n          \\\"consumer\\\": \\\"projects/xx\\\",\\\\n          \\\"service\\\": \\\"iamcredentials.googleapis.com\\\"\\\\n        }\\\\n      },\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.LocalizedMessage\\\",\\\\n        \\\"locale\\\": \\\"en-US\\\",\\\\n        \\\"message\\\": \\\"Caller does not have required permission to use project xxxxx. Grant the caller the roles/serviceusage.serviceUsageConsumer role, or a custom role with the serviceusage.services.use permission, by visiting https://console.developers.google.com/iam-admin/iam/project?project=xxxx and then retry. Propagation of the new permission may take a few minutes.\\\"\\\\n      },\\\\n      {\\\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.Help\\\",\\\\n        \\\"links\\\": [\\\\n          {\\\\n            \\\"description\\\": \\\"Google developer console IAM admin\\\",\\\\n            \\\"url\\\": \\\"https://console.developers.google.com/iam-admin/iam/project?project=xxxxx\\\"\\\\n          }\\\\n        ]\\\\n      }\\\\n    ]\\\\n  }\\\\n}\\\\n')\" }`\n\n@krrishdholakia Could this error be related to recent changes made for getting the access_token?  https://github.com/BerriAI/litellm/commit/e1f7bcb47dbc6d830ac8a81a8866469f07695ee4\n@krrishdholakia Did you get a chance to look at the error above?\n@krrishdholakia we are using workload identity federation when calling GCP models and that seems to be not working for any version above 1.65.3 with the error shown above. Can  you check at your end because @vaghelarahul94 pointed, there were some changes made regarding access tokens.\nHi team, acknowledging this - i see 2 issues - credential caller and project id not resolved, let's scope this ticket to the project id not resolved issue - i believe this occurs when the refreshed credentials don't contain a project id. \n\nThis is the function is the place the fix needs to happen - https://github.com/BerriAI/litellm/blob/b9460dfe37dfac0cdc07a29b5aa4609462804e3c/litellm/llms/vertex_ai/vertex_llm_base.py#L244\n\nCan somebody please enable `litellm._turn_on_debug()` for sdk or `--detailed_debug` for the proxy and share what those logs look like - specifically trying to trace the path for the project id not resolved issue. \n> Can somebody please enable `litellm._turn_on_debug()` for sdk or `--detailed_debug` for the proxy and share what those logs look like - specifically trying to trace the path for the project id not resolved issue.\n\nOn it @krrishdholakia . just started one. Also updated to `litellm==1.69.3`.\n> [@krrishdholakia](https://github.com/krrishdholakia) we are using workload identity federation when calling GCP models and that seems to be not working for any version above 1.65.3 with the error shown above. Can you check at your end because [@vaghelarahul94](https://github.com/vaghelarahul94) pointed, there were some changes made regarding access tokens.\n\n@krrishdholakia If we remove the `quota_project_id` parameter from `google_auth.default` https://github.com/BerriAI/litellm/blob/main/litellm/llms/vertex_ai/vertex_llm_base.py#L96, then we are able to fetch the creds successfully. Here is the script that can be used to reproduce.\n\nSuccessful script: -\n```\nimport os\nimport google.auth\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"workload_identity_federation_file_path\" # replace your json file path\n\ncreds, creds_project_id = google.auth.default(\n    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n)\n\nprint(f\"creds: {creds}\")\nprint(f\"creds_project_id: {creds_project_id}\")\n```\n\nUnsuccessful script: -\n\n```\nimport os\nimport google.auth\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"workload_identity_federation_file_path\" # replace your json file path\n\ncreds, creds_project_id = google.auth.default(\n    quota_project_id=project_id,\n    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n)\n\nprint(f\"creds: {creds}\")\nprint(f\"creds_project_id: {creds_project_id}\")\n```\n@krrishdholakia here are the debug logs:\n\n```\nDEBUG:LiteLLM:\nLiteLLM: Non-Default params passed to completion() {}\nDEBUG:LiteLLM:Final returned optional params: {}\nDEBUG:LiteLLM:self.optional_params: {}\nDEBUG:LiteLLM:Checking cached credentials for project_id: None\nDEBUG:LiteLLM:Cached credentials found for project_id: None.\nDEBUG:LiteLLM:Using cached credentials\nDEBUG:LiteLLM:Validating credentials for project_id: None\nDEBUG:google.auth.transport.requests:Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/?recursive=true\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): metadata.google.internal:80\nDEBUG:urllib3.connectionpool:http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/?recursive=true HTTP/1.1\" 200 143\nDEBUG:google.auth.transport.requests:Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform\nDEBUG:urllib3.connectionpool:http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform HTTP/1.1\" 200 1083\nDEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n```\n\nI can attach the whole file if needed \nThanks @vaghelarahul94 \n\nPushed a fix with your changes, aiming to have it out in today's release. I suspect this was a pre-existing error which was only surfaced once we stopped storing the self.project_id\n> Thanks [@vaghelarahul94](https://github.com/vaghelarahul94)\n> \n> Pushed a fix with your changes, aiming to have it out in today's release. I suspect this was a pre-existing error which was only surfaced once we stopped storing the self.project_id\n\n@krrishdholakia Thanks for checking this. Can you point us to the PR wherein this is fixed?\n@tanvithakur94 https://github.com/BerriAI/litellm/commit/b4d54c16780e716fa5034d8ee2c3e9f353237774\nHello @krrishdholakia another problem also is when using common GCP cross-project service account pattern:\n\n- You have service account credentials from Project A.\n\n- You want to use those credentials to access Vertex AI in Project B (where Vertex AI is enabled).\n\n- Your service account from Project A has been granted the necessary roles on Project B.\n\n[litellm/litellm/llms/vertex_ai/vertex_llm_base.py](https://github.com/BerriAI/litellm/blob/b9460dfe37dfac0cdc07a29b5aa4609462804e3c/litellm/llms/vertex_ai/vertex_llm_base.py#L306-L315)\n\n`credential_project_id` is the project in the service account file (Project A).\n`project_id` is the target Vertex project (Project B).\n\nthis won't work because the two projects `credential_project_id` and `project_id` won't match in this case\nGot it - is the suggestion to not have that validation check since it is possible for that credential to call another project? \nyes, no need for that i think.\n@krrishdholakia I see the build is still failing after the recent fix. Could you please take a look when you get a chance \nhttps://app.circleci.com/pipelines/github/BerriAI/litellm/32344/workflows/85af9df8-fe8f-4cc5-b113-33241c1348fa/jobs/411005/tests\nThis is still happening in v1.71.1\nHey @anatolec i can see @vaghelarahul94's fix is live\n\nhttps://github.com/BerriAI/litellm/blob/e74ff23b458dd153a442307bf118e34ff165ef64/litellm/llms/vertex_ai/vertex_llm_base.py#L95\n\n\nCan i see how you're configuring the vertex model on litellm? \n\n@vaghelarahul94 do you see the same issue on v1.71.1? \n> Can i see how you're configuring the vertex model on litellm?\n\nWhat do you need to see exactly @krrishdholakia ?\n\nI'm using the `acompletion` method and I consistently get that error when our app is deployed in GCP Cloud Run. Being on Cloud Run, my app automatically gets a service account, so I don't expect to have to do anything to authenticate.\n\nI can't reproduce the problem locally.\n\nI tried to add the `vertex_project` parameter to `acompletion`, but then I get a different error (this [one](https://github.com/BerriAI/litellm/issues/9638)), which makes it hard to progress.\n\nI'm available for a pairing session if that helps.\nI've started a test on `litellm-1.72.0`. This has, in the past, usually failed within 2 or 3 hours. I have a test VM as well if sharing would access would be helpful.\n@krrishdholakia unfortunately it's still failing ( I started the test prior to my last comment so it still faliled in that 2-ish hours window):\n\n```bash\nDEBUG:LiteLLM:self.optional_params: {}\nDEBUG:LiteLLM:SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\nDEBUG:LiteLLM:checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'vertex_ai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-2.0-flash', 'custom_llm_provider': 'vertex_ai'}\nINFO:LiteLLM:\nLiteLLM completion() model= gemini-2.0-flash-001; provider = vertex_ai\nDEBUG:LiteLLM:\nLiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-001', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'vertex_ai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nfollow thse insrtucions:\\n                    '}, {'role': 'user', 'content': '\\n                    say pong if I say ping\\n'}], 'thinking': None, 'web_search_options': None}\nDEBUG:LiteLLM:\nLiteLLM: Non-Default params passed to completion() {}\nDEBUG:LiteLLM:Final returned optional params: {}\nDEBUG:LiteLLM:self.optional_params: {}\nDEBUG:LiteLLM:Checking cached credentials for project_id: None\nDEBUG:LiteLLM:Cached credentials found for project_id: None.\nDEBUG:LiteLLM:Using cached credentials\nDEBUG:LiteLLM:Validating credentials for project_id: None\nDEBUG:google.auth.transport.requests:Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/?recursive=true\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): metadata.google.internal:80\nDEBUG:urllib3.connectionpool:http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/?recursive=true HTTP/1.1\" 200 143\nDEBUG:google.auth.transport.requests:Making request: GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform\nDEBUG:urllib3.connectionpool:http://metadata.google.internal:80 \"GET /computeMetadata/v1/instance/service-accounts/test-service@test-litellm.iam.gserviceaccount.com/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform HTTP/1.1\" 200 1083\nDEBUG:LiteLLM:Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG:LiteLLM:Logging Details LiteLLM-Failure Call: []\n```\nHey @krrishdholakia ,\nShall we reopen this bug then ?\nLet me know if I can help with debugging.\n@krrishdholakia can we reopen this bug. It still presists as per last check on Jun 4th. I'm running into the issue where 3rd party libraries which rely on Litellm are hitting it (crewai) and passing the in the workaround isn't always straightforward or working.\nUnable to reproduce this issue. If someone is able to reproduce this, a PR for the fix would be appreciated! \n@krrishdholakia I've got a fix I'm testing overnight and hoping to open a PR in the morning.\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/84e68d9889b2257db6e32886ea638cb94e5c789f"], "created_at": "2025-07-16T19:12:28Z", "classification": "Security"}
{"repo": "BerriAI/litellm", "pull_number": 12441, "instance_id": "BerriAI__litellm-12441", "issue_numbers": [11009], "base_commit": "d720b3d369eca916c87cd044acc6d2b192b8519f", "patch": "diff --git a/litellm/proxy/_experimental/out/onboarding.html b/litellm/proxy/_experimental/out/onboarding.html\ndeleted file mode 100644\nindex 061a432ac306..000000000000\n--- a/litellm/proxy/_experimental/out/onboarding.html\n+++ /dev/null\n@@ -1,1 +0,0 @@\n-<!DOCTYPE html><html id=\"__next_error__\"><head><meta charSet=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/><link rel=\"preload\" as=\"script\" fetchPriority=\"low\" href=\"/litellm-asset-prefix/_next/static/chunks/webpack-a426aae3231a8df1.js\"/><script src=\"/litellm-asset-prefix/_next/static/chunks/fd9d1056-205af899b895cbac.js\" async=\"\"></script><script src=\"/litellm-asset-prefix/_next/static/chunks/117-a0da667066d322b6.js\" async=\"\"></script><script src=\"/litellm-asset-prefix/_next/static/chunks/main-app-475d6efe4080647d.js\" async=\"\"></script><title>LiteLLM Dashboard</title><meta name=\"description\" content=\"LiteLLM Proxy Admin UI\"/><link rel=\"icon\" href=\"/favicon.ico\" type=\"image/x-icon\" sizes=\"16x16\"/><link rel=\"icon\" href=\"./favicon.ico\"/><meta name=\"next-size-adjust\"/><script src=\"/litellm-asset-prefix/_next/static/chunks/polyfills-42372ed130431b0a.js\" noModule=\"\"></script></head><body><script src=\"/litellm-asset-prefix/_next/static/chunks/webpack-a426aae3231a8df1.js\" async=\"\"></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,\"1:HL[\\\"/litellm-asset-prefix/_next/static/media/e4af272ccee01ff0-s.p.woff2\\\",\\\"font\\\",{\\\"crossOrigin\\\":\\\"\\\",\\\"type\\\":\\\"font/woff2\\\"}]\\n2:HL[\\\"/litellm-asset-prefix/_next/static/css/31b7f215e119031e.css\\\",\\\"style\\\"]\\n3:HL[\\\"/litellm-asset-prefix/_next/static/css/d7889674c16b55a5.css\\\",\\\"style\\\"]\\n\"])</script><script>self.__next_f.push([1,\"4:I[12846,[],\\\"\\\"]\\n6:I[19107,[],\\\"ClientPageRoot\\\"]\\n7:I[12011,[\\\"665\\\",\\\"static/chunks/3014691f-b7b79b78e27792f3.js\\\",\\\"402\\\",\\\"static/chunks/402-d418d0a7bf158a22.js\\\",\\\"899\\\",\\\"static/chunks/899-3dc1e1a0832876e3.js\\\",\\\"250\\\",\\\"static/chunks/250-ef2b37352f059268.js\\\",\\\"461\\\",\\\"static/chunks/app/onboarding/page-14caffd05bb17048.js\\\"],\\\"default\\\",1]\\n8:I[4707,[],\\\"\\\"]\\n9:I[36423,[],\\\"\\\"]\\nb:I[61060,[],\\\"\\\"]\\nc:[]\\n\"])</script><script>self.__next_f.push([1,\"0:[\\\"$\\\",\\\"$L4\\\",null,{\\\"buildId\\\":\\\"MMdp1-El_-0lE4vSV9loU\\\",\\\"assetPrefix\\\":\\\"/litellm-asset-prefix\\\",\\\"urlParts\\\":[\\\"\\\",\\\"onboarding\\\"],\\\"initialTree\\\":[\\\"\\\",{\\\"children\\\":[\\\"onboarding\\\",{\\\"children\\\":[\\\"__PAGE__\\\",{}]}]},\\\"$undefined\\\",\\\"$undefined\\\",true],\\\"initialSeedData\\\":[\\\"\\\",{\\\"children\\\":[\\\"onboarding\\\",{\\\"children\\\":[\\\"__PAGE__\\\",{},[[\\\"$L5\\\",[\\\"$\\\",\\\"$L6\\\",null,{\\\"props\\\":{\\\"params\\\":{},\\\"searchParams\\\":{}},\\\"Component\\\":\\\"$7\\\"}],null],null],null]},[null,[\\\"$\\\",\\\"$L8\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"segmentPath\\\":[\\\"children\\\",\\\"onboarding\\\",\\\"children\\\"],\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"errorScripts\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L9\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"templateScripts\\\":\\\"$undefined\\\",\\\"notFound\\\":\\\"$undefined\\\",\\\"notFoundStyles\\\":\\\"$undefined\\\"}]],null]},[[[[\\\"$\\\",\\\"link\\\",\\\"0\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/litellm-asset-prefix/_next/static/css/31b7f215e119031e.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}],[\\\"$\\\",\\\"link\\\",\\\"1\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/litellm-asset-prefix/_next/static/css/d7889674c16b55a5.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\"}]],[\\\"$\\\",\\\"html\\\",null,{\\\"lang\\\":\\\"en\\\",\\\"children\\\":[\\\"$\\\",\\\"body\\\",null,{\\\"className\\\":\\\"__className_b0dd8a\\\",\\\"children\\\":[\\\"$\\\",\\\"$L8\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"segmentPath\\\":[\\\"children\\\"],\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"errorScripts\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L9\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"templateScripts\\\":\\\"$undefined\\\",\\\"notFound\\\":[[\\\"$\\\",\\\"title\\\",null,{\\\"children\\\":\\\"404: This page could not be found.\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontFamily\\\":\\\"system-ui,\\\\\\\"Segoe UI\\\\\\\",Roboto,Helvetica,Arial,sans-serif,\\\\\\\"Apple Color Emoji\\\\\\\",\\\\\\\"Segoe UI Emoji\\\\\\\"\\\",\\\"height\\\":\\\"100vh\\\",\\\"textAlign\\\":\\\"center\\\",\\\"display\\\":\\\"flex\\\",\\\"flexDirection\\\":\\\"column\\\",\\\"alignItems\\\":\\\"center\\\",\\\"justifyContent\\\":\\\"center\\\"},\\\"children\\\":[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"style\\\",null,{\\\"dangerouslySetInnerHTML\\\":{\\\"__html\\\":\\\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\\\"}}],[\\\"$\\\",\\\"h1\\\",null,{\\\"className\\\":\\\"next-error-h1\\\",\\\"style\\\":{\\\"display\\\":\\\"inline-block\\\",\\\"margin\\\":\\\"0 20px 0 0\\\",\\\"padding\\\":\\\"0 23px 0 0\\\",\\\"fontSize\\\":24,\\\"fontWeight\\\":500,\\\"verticalAlign\\\":\\\"top\\\",\\\"lineHeight\\\":\\\"49px\\\"},\\\"children\\\":\\\"404\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"display\\\":\\\"inline-block\\\"},\\\"children\\\":[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"fontSize\\\":14,\\\"fontWeight\\\":400,\\\"lineHeight\\\":\\\"49px\\\",\\\"margin\\\":0},\\\"children\\\":\\\"This page could not be found.\\\"}]}]]}]}]],\\\"notFoundStyles\\\":[]}]}]}]],null],null],\\\"couldBeIntercepted\\\":false,\\\"initialHead\\\":[null,\\\"$La\\\"],\\\"globalErrorComponent\\\":\\\"$b\\\",\\\"missingSlots\\\":\\\"$Wc\\\"}]\\n\"])</script><script>self.__next_f.push([1,\"a:[[\\\"$\\\",\\\"meta\\\",\\\"0\\\",{\\\"name\\\":\\\"viewport\\\",\\\"content\\\":\\\"width=device-width, initial-scale=1\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"1\\\",{\\\"charSet\\\":\\\"utf-8\\\"}],[\\\"$\\\",\\\"title\\\",\\\"2\\\",{\\\"children\\\":\\\"LiteLLM Dashboard\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"3\\\",{\\\"name\\\":\\\"description\\\",\\\"content\\\":\\\"LiteLLM Proxy Admin UI\\\"}],[\\\"$\\\",\\\"link\\\",\\\"4\\\",{\\\"rel\\\":\\\"icon\\\",\\\"href\\\":\\\"/favicon.ico\\\",\\\"type\\\":\\\"image/x-icon\\\",\\\"sizes\\\":\\\"16x16\\\"}],[\\\"$\\\",\\\"link\\\",\\\"5\\\",{\\\"rel\\\":\\\"icon\\\",\\\"href\\\":\\\"./favicon.ico\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"6\\\",{\\\"name\\\":\\\"next-size-adjust\\\"}]]\\n5:null\\n\"])</script></body></html>\n\\ No newline at end of file\ndiff --git a/litellm/proxy/auth/route_checks.py b/litellm/proxy/auth/route_checks.py\nindex 22490919e01a..083a93641e45 100644\n--- a/litellm/proxy/auth/route_checks.py\n+++ b/litellm/proxy/auth/route_checks.py\n@@ -99,6 +99,7 @@ def non_proxy_admin_allowed_routes_check(\n         ):\n             pass\n         elif _user_role == LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY.value:\n+\n             if RouteChecks.is_llm_api_route(route=route):\n                 raise HTTPException(\n                     status_code=status.HTTP_403_FORBIDDEN,\n@@ -107,6 +108,7 @@ def non_proxy_admin_allowed_routes_check(\n             if RouteChecks.check_route_access(\n                 route=route, allowed_routes=LiteLLMRoutes.management_routes.value\n             ):\n+\n                 # the Admin Viewer is only allowed to call /user/update for their own user_id and can only update\n                 if route == \"/user/update\":\n                     # Check the Request params are valid for PROXY_ADMIN_VIEW_ONLY\n@@ -123,6 +125,11 @@ def non_proxy_admin_allowed_routes_check(\n                         status_code=status.HTTP_403_FORBIDDEN,\n                         detail=f\"user not allowed to access this route, role= {_user_role}. Trying to access: {route}\",\n                     )\n+            else:\n+                raise HTTPException(\n+                    status_code=status.HTTP_403_FORBIDDEN,\n+                    detail=f\"user not allowed to access this route, role= {_user_role}. Trying to access: {route}\",\n+                )\n \n         elif (\n             _user_role == LitellmUserRoles.INTERNAL_USER.value\n@@ -196,8 +203,7 @@ def is_llm_api_route(route: str) -> bool:\n             return True\n \n         if RouteChecks.check_route_access(\n-            route=route, \n-            allowed_routes=LiteLLMRoutes.mcp_routes.value\n+            route=route, allowed_routes=LiteLLMRoutes.mcp_routes.value\n         ):\n             return True\n \ndiff --git a/litellm/proxy/types_utils/utils.py b/litellm/proxy/types_utils/utils.py\nindex f3dbfda6b263..e159da495498 100644\n--- a/litellm/proxy/types_utils/utils.py\n+++ b/litellm/proxy/types_utils/utils.py\n@@ -14,9 +14,6 @@ def get_instance_fn(value: str, config_file_path: Optional[str] = None) -> Any:\n         module_name = \".\".join(parts[:-1])\n         instance_name = parts[-1]\n \n-        # Security: Check if the module name contains any dangerous modules that can execute arbitrary code\n-        security_checks(module_name=module_name)\n-\n         # If config_file_path is provided, use it to determine the module spec and load the module\n         if config_file_path is not None:\n             directory = os.path.dirname(config_file_path)\n@@ -50,37 +47,8 @@ def get_instance_fn(value: str, config_file_path: Optional[str] = None) -> Any:\n         raise e\n \n \n-def security_checks(\n-    module_name: str,\n-):\n-    \"\"\"\n-    This function checks if the module name contains any dangerous modules that can execute arbitrary code.\n-\n-    Reference: https://huntr.com/bounties/1d98bebb-6cf4-46c9-87c3-d3b1972973b5\n-    \"\"\"\n-    DANGEROUS_MODULES = [\n-        \"os\",\n-        \"sys\",\n-        \"subprocess\",\n-        \"shutil\",\n-        \"socket\",\n-        \"multiprocessing\",\n-        \"threading\",\n-        \"ctypes\",\n-        \"pickle\",\n-        \"marshal\",\n-        \"builtins\",\n-        \"__builtin__\",\n-    ]\n-    # Security: Check if the module name contains any dangerous modules\n-    if any(dangerous in module_name.lower() for dangerous in DANGEROUS_MODULES):\n-        raise ImportError(\n-            f\"Importing from module {module_name} is not allowed for security reasons\"\n-        )\n-\n-\n def validate_custom_validate_return_type(\n-    fn: Optional[Callable[..., Any]]\n+    fn: Optional[Callable[..., Any]],\n ) -> Optional[Callable[..., Literal[True]]]:\n     if fn is None:\n         return None\n", "test_patch": "diff --git a/tests/test_litellm/proxy/auth/test_route_checks.py b/tests/test_litellm/proxy/auth/test_route_checks.py\nnew file mode 100644\nindex 000000000000..a6358e73c361\n--- /dev/null\n+++ b/tests/test_litellm/proxy/auth/test_route_checks.py\n@@ -0,0 +1,91 @@\n+import asyncio\n+import os\n+import sys\n+from unittest.mock import MagicMock\n+\n+sys.path.insert(\n+    0, os.path.abspath(\"../../..\")\n+)  # Adds the parent directory to the system path\n+\n+import pytest\n+from fastapi import HTTPException, Request\n+\n+from litellm.proxy._types import LiteLLM_UserTable, LitellmUserRoles, UserAPIKeyAuth\n+from litellm.proxy.auth.route_checks import RouteChecks\n+\n+\n+def test_non_admin_config_update_route_rejected():\n+    \"\"\"Test that non-admin users are rejected when trying to call /config/update\"\"\"\n+\n+    # Create a non-admin user object\n+    user_obj = LiteLLM_UserTable(\n+        user_id=\"test_user\",\n+        user_email=\"test@example.com\",\n+        user_role=LitellmUserRoles.INTERNAL_USER.value,  # Non-admin role\n+    )\n+\n+    # Create a non-admin user API key auth\n+    valid_token = UserAPIKeyAuth(\n+        user_id=\"test_user\",\n+        user_role=LitellmUserRoles.INTERNAL_USER.value,  # Non-admin role\n+    )\n+\n+    # Create a mock request\n+    request = MagicMock(spec=Request)\n+    request.query_params = {}\n+\n+    # Test that calling /config/update route raises HTTPException with 403 status\n+    with pytest.raises(Exception) as exc_info:\n+        RouteChecks.non_proxy_admin_allowed_routes_check(\n+            user_obj=user_obj,\n+            _user_role=LitellmUserRoles.INTERNAL_USER.value,\n+            route=\"/config/update\",\n+            request=request,\n+            valid_token=valid_token,\n+            request_data={},\n+        )\n+\n+    # Verify the exception is raised with the correct message\n+    assert (\n+        \"Only proxy admin can be used to generate, delete, update info for new keys/users/teams\"\n+        in str(exc_info.value)\n+    )\n+    assert \"Route=/config/update\" in str(exc_info.value)\n+    assert \"Your role=internal_user\" in str(exc_info.value)\n+\n+\n+def test_proxy_admin_viewer_config_update_route_rejected():\n+    \"\"\"Test that proxy admin viewer users are rejected when trying to call /config/update\"\"\"\n+\n+    # Create a proxy admin viewer user object (read-only admin)\n+    user_obj = LiteLLM_UserTable(\n+        user_id=\"viewer_user\",\n+        user_email=\"viewer@example.com\",\n+        user_role=LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY.value,\n+    )\n+\n+    # Create a proxy admin viewer user API key auth\n+    valid_token = UserAPIKeyAuth(\n+        user_id=\"viewer_user\",\n+        user_role=LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY.value,\n+    )\n+\n+    # Create a mock request\n+    request = MagicMock(spec=Request)\n+    request.query_params = {}\n+\n+    # Test that calling /config/update route raises HTTPException with 403 status\n+    with pytest.raises(HTTPException) as exc_info:\n+        RouteChecks.non_proxy_admin_allowed_routes_check(\n+            user_obj=user_obj,\n+            _user_role=LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY.value,\n+            route=\"/config/update\",\n+            request=request,\n+            valid_token=valid_token,\n+            request_data={},\n+        )\n+\n+    # Verify the exception is HTTPException with 403 status\n+    assert exc_info.value.status_code == 403\n+    assert \"user not allowed to access this route\" in str(exc_info.value.detail)\n+    assert \"role= proxy_admin_viewer\" in str(exc_info.value.detail)\ndiff --git a/tests/test_litellm/proxy/types_utils/test_litellm_proxy_types_utils.py b/tests/test_litellm/proxy/types_utils/test_litellm_proxy_types_utils.py\ndeleted file mode 100644\nindex 5685489bfce0..000000000000\n--- a/tests/test_litellm/proxy/types_utils/test_litellm_proxy_types_utils.py\n+++ /dev/null\n@@ -1,72 +0,0 @@\n-import json\n-import os\n-import sys\n-\n-import pytest\n-from fastapi.testclient import TestClient\n-\n-from litellm.proxy.types_utils.utils import security_checks\n-\n-sys.path.insert(\n-    0, os.path.abspath(\"../../..\")\n-)  # Adds the parent directory to the system path\n-\n-\n-def test_security_checks_blocks_dangerous_modules():\n-    \"\"\"\n-    Resolves: https://huntr.com/bounties/1d98bebb-6cf4-46c9-87c3-d3b1972973b5\n-\n-    This test checks if the security_checks function correctly blocks the import of dangerous modules.\n-    \"\"\"\n-    dangerous_module = \"/usr/lib/python3/os.system\"\n-    with pytest.raises(ImportError) as exc_info:\n-        security_checks(dangerous_module)\n-\n-    assert \"not allowed for security reasons\" in str(exc_info.value)\n-    assert dangerous_module in str(exc_info.value)\n-\n-\n-def test_security_checks_various_dangerous_modules():\n-    dangerous_modules = [\n-        \"subprocess.run\",\n-        \"socket.socket\",\n-        \"pickle.loads\",\n-        \"marshal.loads\",\n-        \"ctypes.CDLL\",\n-        \"builtins.eval\",\n-        \"__builtin__.exec\",\n-        \"shutil.rmtree\",\n-        \"multiprocessing.Process\",\n-        \"threading.Thread\",\n-    ]\n-\n-    for module in dangerous_modules:\n-        with pytest.raises(ImportError) as exc_info:\n-            security_checks(module)\n-        assert \"not allowed for security reasons\" in str(exc_info.value)\n-        assert module in str(exc_info.value)\n-\n-\n-def test_security_checks_case_insensitive():\n-    # Test that the check is case-insensitive\n-    variations = [\"OS.system\", \"os.System\", \"Os.SyStEm\", \"SUBPROCESS.run\"]\n-\n-    for module in variations:\n-        with pytest.raises(ImportError) as exc_info:\n-            security_checks(module)\n-        assert \"not allowed for security reasons\" in str(exc_info.value)\n-\n-\n-def test_security_checks_nested_paths():\n-    # Test nested paths that contain dangerous modules\n-    nested_paths = [\n-        \"some/path/to/os/system\",\n-        \"myproject/utils/subprocess_wrapper\",\n-        \"lib/helpers/socket_utils\",\n-        \"../../../system/os.py\",\n-    ]\n-\n-    for path in nested_paths:\n-        with pytest.raises(ImportError) as exc_info:\n-            security_checks(path)\n-        assert \"not allowed for security reasons\" in str(exc_info.value)\n", "problem_statement": "[Bug]: Security checks too strict for custom handlers\n### What happened?\n\nIf you pass the path to a custom handler and it contains the substring `os` anywhere, the handler will fail to load \"for security reasons\". This is very prone to false positives, e.g. having your handler in a directory tree containing `git_repos` will trigger it. \n\nExample for having a directory in the tree called `tempos` below:\n\n```\nlitellm_settings:\n  custom_provider_map:\n  - custom_handler: /Users/jpaton/temp/tempos/test-litellm-proxy/my_llm.my_llm\n```\nyields\n```\nERROR:    Traceback (most recent call last):\n  File \"/Users/jpaton/temp/tempos/test-litellm-proxy/.venv/lib/python3.12/site-packages/litellm/proxy/types_utils/utils.py\", line 18, in get_instance_fn\n    security_checks(module_name=module_name)\n  File \"/Users/jpaton/temp/tempos/test-litellm-proxy/.venv/lib/python3.12/site-packages/litellm/proxy/types_utils/utils.py\", line 77, in security_checks\n    raise ImportError(\nImportError: Importing from module /Users/jpaton/temp/tempos/test-litellm-proxy/my_llm is not allowed for security reasons\n```\n\nThe checks were introduced in https://github.com/BerriAI/litellm/commit/441c7275ed2715f47650a7c2e525055c804073a9 \n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\n1.69.2\n\n### Twitter / LinkedIn details\n\n_No response_\n", "hints_text": "How would you propose checking this @JohnPaton ? \nThe first thing you need to check is whether the [`POST /config/update` route](https://github.com/BerriAI/litellm/blob/aa11ea3a9f8ad73d6830ccdfc1de1636ca33e825/litellm/proxy/proxy_server.py#L7566-L7572) is properly protected or not? Based on what I understand it's intended to be used for admins via the Admin UI and it's a dangerous API, since there may be many ways to exploit it. Either way it's either already a correctly protected endpoint or it is not.\n\n* If the `POST /config/update` route **is** already properly protected and **only accessible to admins** but not regular users, the original vulnerability report may have been a false positive (not actually a security issue), since they already had admin rights and they used the admin rights to reconfigure the LiteLLM proxy. That's not really a privilege escalation.\n* If the `POST /config/update` route **is not** already properly protected and it's **also accessible to regular users** and not just admins, then it's a more serious issue with the LiteLLM proxy. It would mean that any user can reconfigure the LiteLLM proxy, and you have only partially fixed one of the many way in which regular users can do harm by reconfiguring the proxy.\n\nEither way, you need to make sure only admins can reconfigure LiteLLM and regular (or anonymous, unauthenticated) users can not. Once that's done, I think the changes in 441c7275ed2715f47650a7c2e525055c804073a9 can be safely reverted since no one will be able to trigger the reported vulnerability in LiteLLM, only admins. But admins are already expected to have full privileges, so arbitrary code execution is something they can already do, it would not be an issue, not a privilege escalation.\nJust to make this explicit, blacklisting a small number of \"dangerous\" modules doesn't actually fix the arbitrary code execution vulnerability anyway. E.g., hackers could still call other dangerous modules you weren't aware of, including 3rd party modules. This type of check is akin to banning `<script>` in HTML form submissions in web apps, it prevents only the most trivial exploits, but the real fix is to sanitize whatever gets inserted into HTML pages.\n\nAgain in LiteLLM's case the key is making sure that only admins can reconfigure the LiteLLM proxy at all. Once that's implemented, it doesn't matter if they can reconfigure it to execute arbitrary code with `post_call_hooks` or whatever since they are admins they can already do whatever they want.\ncc @wagnerjt high priority issue ^ \n+1. This is a textbook case of security by obscurity. I even had to add the missing `o` renaming `post_call_rules.ensure_non_empty` to `pst_call_rules.ensure_non_empty` just to get past the filter. Blocking any module name that contains `os` is the wrong approach; security checks belong in the API layer, not in a module\u2010name blocklist that's trivial to bypass (as it was mentioned before).\nHad another user report this as well \nwill investigate today\nConfirmed - `/config/update` **is** a protected endpoint. I agree with @palotasb and @nightshiba - if this is an admin only endpoint, then i do not view the admin executing arbitrary code as a vulnerability. We should look to roll back these changes given the false flags it raises. \n\n<img width=\"1226\" height=\"737\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/23eb3b8f-de3b-49be-a059-4b40b35554b0\" />\n\n", "all_hints_text": "How would you propose checking this @JohnPaton ? \nThe first thing you need to check is whether the [`POST /config/update` route](https://github.com/BerriAI/litellm/blob/aa11ea3a9f8ad73d6830ccdfc1de1636ca33e825/litellm/proxy/proxy_server.py#L7566-L7572) is properly protected or not? Based on what I understand it's intended to be used for admins via the Admin UI and it's a dangerous API, since there may be many ways to exploit it. Either way it's either already a correctly protected endpoint or it is not.\n\n* If the `POST /config/update` route **is** already properly protected and **only accessible to admins** but not regular users, the original vulnerability report may have been a false positive (not actually a security issue), since they already had admin rights and they used the admin rights to reconfigure the LiteLLM proxy. That's not really a privilege escalation.\n* If the `POST /config/update` route **is not** already properly protected and it's **also accessible to regular users** and not just admins, then it's a more serious issue with the LiteLLM proxy. It would mean that any user can reconfigure the LiteLLM proxy, and you have only partially fixed one of the many way in which regular users can do harm by reconfiguring the proxy.\n\nEither way, you need to make sure only admins can reconfigure LiteLLM and regular (or anonymous, unauthenticated) users can not. Once that's done, I think the changes in 441c7275ed2715f47650a7c2e525055c804073a9 can be safely reverted since no one will be able to trigger the reported vulnerability in LiteLLM, only admins. But admins are already expected to have full privileges, so arbitrary code execution is something they can already do, it would not be an issue, not a privilege escalation.\nJust to make this explicit, blacklisting a small number of \"dangerous\" modules doesn't actually fix the arbitrary code execution vulnerability anyway. E.g., hackers could still call other dangerous modules you weren't aware of, including 3rd party modules. This type of check is akin to banning `<script>` in HTML form submissions in web apps, it prevents only the most trivial exploits, but the real fix is to sanitize whatever gets inserted into HTML pages.\n\nAgain in LiteLLM's case the key is making sure that only admins can reconfigure the LiteLLM proxy at all. Once that's implemented, it doesn't matter if they can reconfigure it to execute arbitrary code with `post_call_hooks` or whatever since they are admins they can already do whatever they want.\ncc @wagnerjt high priority issue ^ \n+1. This is a textbook case of security by obscurity. I even had to add the missing `o` renaming `post_call_rules.ensure_non_empty` to `pst_call_rules.ensure_non_empty` just to get past the filter. Blocking any module name that contains `os` is the wrong approach; security checks belong in the API layer, not in a module\u2010name blocklist that's trivial to bypass (as it was mentioned before).\nHad another user report this as well \nwill investigate today\nConfirmed - `/config/update` **is** a protected endpoint. I agree with @palotasb and @nightshiba - if this is an admin only endpoint, then i do not view the admin executing arbitrary code as a vulnerability. We should look to roll back these changes given the false flags it raises. \n\n<img width=\"1226\" height=\"737\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/23eb3b8f-de3b-49be-a059-4b40b35554b0\" />\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/9767e436ace7e55c11aed1a247b3ec9a33a3fe68", "https://github.com/BerriAI/litellm/commit/f1be0dc06af17b1e902566d499c12d6b65a12a18"], "created_at": "2025-07-09T02:21:42Z", "classification": "Security"}
{"repo": "BerriAI/litellm", "pull_number": 12177, "instance_id": "BerriAI__litellm-12177", "issue_numbers": [12174], "base_commit": "d727d63a817a4aa2d325e040e947f4eb145e837b", "patch": "diff --git a/litellm/google_genai/adapters/handler.py b/litellm/google_genai/adapters/handler.py\nindex 651a6413cad9..ee7ddd0f2c63 100644\n--- a/litellm/google_genai/adapters/handler.py\n+++ b/litellm/google_genai/adapters/handler.py\n@@ -1,6 +1,7 @@\n from typing import Any, AsyncIterator, Coroutine, Dict, List, Optional, Union, cast\n \n import litellm\n+from litellm.types.router import GenericLiteLLMParams\n from litellm.types.utils import ModelResponse\n \n from .transformation import GoogleGenAIAdapter\n@@ -18,6 +19,7 @@ def _prepare_completion_kwargs(\n         contents: Union[List[Dict[str, Any]], Dict[str, Any]],\n         config: Optional[Dict[str, Any]] = None,\n         stream: bool = False,\n+        litellm_params: Optional[GenericLiteLLMParams] = None,\n         extra_kwargs: Optional[Dict[str, Any]] = None,\n     ) -> Dict[str, Any]:\n         \"\"\"Prepare kwargs for litellm.completion/acompletion\"\"\"\n@@ -27,6 +29,7 @@ def _prepare_completion_kwargs(\n             model=model,\n             contents=contents,\n             config=config,\n+            litellm_params=litellm_params,\n             **(extra_kwargs or {})\n         )\n         \n@@ -41,6 +44,7 @@ def _prepare_completion_kwargs(\n     async def async_generate_content_handler(\n         model: str,\n         contents: Union[List[Dict[str, Any]], Dict[str, Any]],\n+        litellm_params: GenericLiteLLMParams,\n         config: Optional[Dict[str, Any]] = None,\n         stream: bool = False,\n         **kwargs,\n@@ -52,6 +56,7 @@ async def async_generate_content_handler(\n             contents=contents,\n             config=config,\n             stream=stream,\n+            litellm_params=litellm_params,\n             extra_kwargs=kwargs,\n         )\n         \n@@ -82,6 +87,7 @@ async def async_generate_content_handler(\n     def generate_content_handler(\n         model: str,\n         contents: Union[List[Dict[str, Any]], Dict[str, Any]],\n+        litellm_params: GenericLiteLLMParams,\n         config: Optional[Dict[str, Any]] = None,\n         stream: bool = False,\n         _is_async: bool = False,\n@@ -95,6 +101,7 @@ def generate_content_handler(\n                 contents=contents,\n                 config=config,\n                 stream=stream,\n+                litellm_params=litellm_params,\n                 **kwargs,\n             )\n             \n@@ -103,6 +110,7 @@ def generate_content_handler(\n             contents=contents,\n             config=config,\n             stream=stream,\n+            litellm_params=litellm_params,\n             extra_kwargs=kwargs,\n         )\n         \ndiff --git a/litellm/google_genai/adapters/transformation.py b/litellm/google_genai/adapters/transformation.py\nindex e80da47d5ea3..7d6fdaf6e02b 100644\n--- a/litellm/google_genai/adapters/transformation.py\n+++ b/litellm/google_genai/adapters/transformation.py\n@@ -13,6 +13,7 @@\n     ChatCompletionToolParam,\n     ChatCompletionUserMessage,\n )\n+from litellm.types.router import GenericLiteLLMParams\n from litellm.types.utils import (\n     AdapterCompletionStreamWrapper,\n     Choices,\n@@ -107,8 +108,9 @@ def translate_generate_content_to_completion(\n         model: str,\n         contents: Union[List[Dict[str, Any]], Dict[str, Any]],\n         config: Optional[Dict[str, Any]] = None,\n+        litellm_params: Optional[GenericLiteLLMParams] = None,\n         **kwargs,\n-    ) -> ChatCompletionRequest:\n+    ) -> Dict[str, Any]:\n         \"\"\"\n         Transform generate_content request to litellm completion format\n \n@@ -119,7 +121,7 @@ def translate_generate_content_to_completion(\n             **kwargs: Additional parameters\n \n         Returns:\n-            ChatCompletionRequest in OpenAI format\n+            Dict in OpenAI format\n         \"\"\"\n \n         # Normalize contents to list format\n@@ -131,11 +133,11 @@ def translate_generate_content_to_completion(\n         # Transform contents to OpenAI messages format\n         messages = self._transform_contents_to_messages(contents_list)\n \n-        # Create base request\n-        completion_request: ChatCompletionRequest = ChatCompletionRequest(\n-            model=model,\n-            messages=messages,\n-        )\n+        # Create base request as dict (which is compatible with ChatCompletionRequest)\n+        completion_request: ChatCompletionRequest = {\n+            \"model\": model,\n+            \"messages\": messages,\n+        }\n \n         #########################################################\n         # Supported OpenAI chat completion params\n@@ -182,8 +184,38 @@ def translate_generate_content_to_completion(\n             )\n             if tool_choice:\n                 completion_request[\"tool_choice\"] = tool_choice\n+        \n+        #########################################################\n+        # forward any litellm specific params\n+        #########################################################\n+        completion_request_dict = dict(completion_request)\n+        if litellm_params:\n+            completion_request_dict = self._add_generic_litellm_params_to_request(\n+                completion_request_dict=completion_request_dict,\n+                litellm_params=litellm_params\n+            )\n \n-        return completion_request\n+        return completion_request_dict\n+    \n+    def _add_generic_litellm_params_to_request(\n+        self, \n+        completion_request_dict: Dict[str, Any], \n+        litellm_params: Optional[GenericLiteLLMParams] = None\n+    ) -> dict:\n+        \"\"\"Add generic litellm params to request. e.g add api_base, api_key, api_version, etc.\n+\n+        Args:\n+            completion_request_dict: Dict[str, Any]\n+            litellm_params: GenericLiteLLMParams\n+\n+        Returns:\n+            Dict[str, Any]\n+        \"\"\"\n+        if litellm_params:\n+            litellm_dict = litellm_params.model_dump(exclude_none=True)\n+            for key, value in litellm_dict.items():\n+                completion_request_dict[key] = value\n+        return completion_request_dict\n \n     def translate_completion_output_params_streaming(\n         self, completion_stream: Any\ndiff --git a/litellm/google_genai/main.py b/litellm/google_genai/main.py\nindex f1847057db95..5bf85bfc55df 100644\n--- a/litellm/google_genai/main.py\n+++ b/litellm/google_genai/main.py\n@@ -300,6 +300,7 @@ def generate_content(\n                 config=setup_result.generate_content_config_dict,\n                 stream=False,\n                 _is_async=_is_async,\n+                litellm_params=setup_result.litellm_params,\n                 **kwargs\n             )\n \n@@ -377,6 +378,7 @@ async def agenerate_content_stream(\n                 model=setup_result.model,\n                 contents=contents,  # type: ignore\n                 config=setup_result.generate_content_config_dict,\n+                litellm_params=setup_result.litellm_params,\n                 stream=True,\n                 **kwargs\n             )\n@@ -452,6 +454,7 @@ def generate_content_stream(\n                 config=setup_result.generate_content_config_dict,\n                 stream=True,\n                 _is_async=_is_async,\n+                litellm_params=setup_result.litellm_params,\n                 **kwargs\n             )\n \n", "test_patch": "diff --git a/tests/test_litellm/google_genai/test_google_genai_adapter.py b/tests/test_litellm/google_genai/test_google_genai_adapter.py\nindex 45fa9375234e..3e79434f2930 100644\n--- a/tests/test_litellm/google_genai/test_google_genai_adapter.py\n+++ b/tests/test_litellm/google_genai/test_google_genai_adapter.py\n@@ -18,6 +18,8 @@\n \n import pytest\n \n+import litellm\n+\n \n def test_adapter_import():\n     \"\"\"Test that the adapter can be imported successfully\"\"\"\n@@ -869,6 +871,104 @@ def test_handler_parameter_exclusion():\n     assert \"temperature\" in completion_kwargs\n     assert completion_kwargs[\"temperature\"] == 0.7\n \n+@pytest.mark.parametrize(\"function_name,is_async,is_stream\", [\n+    (\"generate_content\", False, False),\n+    (\"agenerate_content\", True, False),\n+    (\"generate_content_stream\", False, True),\n+    (\"agenerate_content_stream\", True, True),\n+])\n+def test_api_base_and_api_key_passthrough(function_name, is_async, is_stream):\n+    \"\"\"Test that api_base and api_key parameters are passed through to litellm.completion/acompletion when using generate_content\"\"\"\n+    import asyncio\n+    import unittest.mock\n+    \n+    litellm._turn_on_debug()\n+\n+    # Import the specific function being tested\n+    if function_name == \"generate_content\":\n+        from litellm.google_genai.main import generate_content as test_function\n+    elif function_name == \"agenerate_content\":\n+        from litellm.google_genai.main import agenerate_content as test_function\n+    elif function_name == \"generate_content_stream\":\n+        from litellm.google_genai.main import generate_content_stream as test_function\n+    elif function_name == \"agenerate_content_stream\":\n+        from litellm.google_genai.main import agenerate_content_stream as test_function\n+\n+    # Test input parameters\n+    model = \"gpt-3.5-turbo\"\n+    test_api_base = \"https://test-api.example.com\"\n+    test_api_key = \"test-api-key-123\"\n+    \n+    # Mock the appropriate litellm function (completion vs acompletion)\n+    mock_target = 'litellm.acompletion' if is_async else 'litellm.completion'\n+    \n+    with unittest.mock.patch(mock_target) as mock_completion:\n+        # Mock return value\n+        mock_return = unittest.mock.MagicMock()\n+        if is_async:\n+            # For async functions, return a coroutine that resolves to the mock\n+            async def mock_async_return():\n+                return mock_return\n+            mock_completion.return_value = mock_async_return()\n+        else:\n+            mock_completion.return_value = mock_return\n+        \n+        # Define the test call\n+        def make_test_call():\n+            return test_function(\n+                model=model,\n+                contents={\n+                    \"role\": \"user\",\n+                    \"parts\": [{\"text\": \"Hello, world!\"}]\n+                },\n+                config={\n+                    \"temperature\": 0.7,\n+                },\n+                api_base=test_api_base,\n+                api_key=test_api_key\n+            )\n+        \n+        # Call the handler with api_base and api_key\n+        try:\n+            if is_async:\n+                # Run the async function\n+                async def run_async_test():\n+                    return await make_test_call()\n+                \n+                asyncio.run(run_async_test())\n+            else:\n+                make_test_call()\n+        except Exception:\n+            # Ignore any errors from the mock response processing\n+            pass\n+        \n+        # Verify that the appropriate litellm function was called\n+        mock_completion.assert_called_once()\n+        \n+        # Get the arguments passed to litellm.completion/acompletion\n+        call_args, call_kwargs = mock_completion.call_args\n+        \n+        # Verify that api_base and api_key were passed through\n+        assert \"api_base\" in call_kwargs, f\"api_base not found in completion kwargs: {call_kwargs.keys()}\"\n+        assert call_kwargs[\"api_base\"] == test_api_base, f\"Expected api_base {test_api_base}, got {call_kwargs['api_base']}\"\n+        \n+        assert \"api_key\" in call_kwargs, f\"api_key not found in completion kwargs: {call_kwargs.keys()}\"\n+        assert call_kwargs[\"api_key\"] == test_api_key, f\"Expected api_key {test_api_key}, got {call_kwargs['api_key']}\"\n+        \n+        # Verify other expected parameters\n+        assert call_kwargs[\"model\"] == model\n+        assert len(call_kwargs[\"messages\"]) == 1\n+        assert call_kwargs[\"messages\"][0][\"role\"] == \"user\"\n+        assert call_kwargs[\"messages\"][0][\"content\"] == \"Hello, world!\"\n+        assert call_kwargs[\"temperature\"] == 0.7\n+        \n+        # Verify stream parameter for streaming functions\n+        if is_stream:\n+            assert call_kwargs.get(\"stream\") is True, f\"Expected stream=True for {function_name}\"\n+        else:\n+            # For non-streaming, stream should be False or not present\n+            assert call_kwargs.get(\"stream\") is not True, f\"Expected stream not True for {function_name}\"\n+\n def test_shared_schema_normalization_utilities():\n     \"\"\"Test the shared schema normalization utility functions work correctly\"\"\"\n     from litellm.litellm_core_utils.json_validation_rule import (\n", "problem_statement": "[Bug]: When requesting Gemini `/v1beta/` endpoints, `api_base` and `api_key` are ignored\n### What happened?\n\nReproduce:\n\n1. create a `config.yaml`\n\n```yaml\nmodel_list:\n  - model_name: gemini-2.5-pro\n    litellm_params:\n      model: openai/o4-mini\n      api_base: \"https://api.not-openai.com/v1\"\n      api_key: \"sk-not-openai-key\"\n```\n\n2. `litellm --config config.yaml`\n3. create a request to `AstreamGenerateContent` of `/v1beta` Gemini API\n4. litellm still trys to send requests to `https://api.openai.com/v1` and not sending api key correctly (empty api key finally results in error: \n\n```\nlitellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - Error calling litellm.acompletion for generate_content: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable LiteLLM Retried: 2 times\n```\n\nUsing environment variables still works though.\n\n\n\n### Relevant log output\n\n```shell\n01:15:52 - LiteLLM Proxy:ERROR: common_request_processing.py:628 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.AuthenticationError: AuthenticationError: OpenAIException - Error calling litellm.acompletion for generate_content: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable LiteLLM Retried: 2 times\nTraceback (most recent call last):\n  File \"[...]/site-packages/litellm/llms/openai/openai.py\", line 947, in async_streaming\n    openai_aclient: AsyncOpenAI = self._get_openai_client(  # type: ignore\n                                  ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n        is_async=True,\n        ^^^^^^^^^^^^^^\n    ...<6 lines>...\n        client=client,\n        ^^^^^^^^^^^^^^\n    )\n    ^\n  File \"[...]/site-packages/litellm/llms/openai/openai.py\", line 368, in _get_openai_client\n    _new_client: Union[OpenAI, AsyncOpenAI] = AsyncOpenAI(\n                                              ~~~~~~~~~~~^\n        api_key=api_key,\n        ^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n        organization=organization,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"[...]/site-packages/openai/_client.py\", line 449, in __init__\n    raise OpenAIError(\n        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n    )\nopenai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"[...]/site-packages/litellm/main.py\", line 525, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"[...]/site-packages/litellm/llms/openai/openai.py\", line 1026, in async_streaming\n    raise OpenAIError(\n    ...<4 lines>...\n    )\nlitellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"[...]/site-packages/litellm/google_genai/adapters/handler.py\", line 59, in async_generate_content_handler\n    completion_response = await litellm.acompletion(**completion_kwargs)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"[...]/site-packages/litellm/utils.py\", line 1516, in wrapper_async\n    raise e\n  File \"[...]/site-packages/litellm/utils.py\", line 1374, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"[...]/site-packages/litellm/main.py\", line 544, in acompletion\n    raise exception_type(\n          ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"[...]/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2284, in exception_type\n    raise e\n  File \"[...]/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 414, in exception_type\n    raise AuthenticationError(\n    ...<5 lines>...\n    )\nlitellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"[...]/site-packages/litellm/google_genai/main.py\", line 376, in agenerate_content_stream\n    return await GenerateContentToCompletionHandler.async_generate_content_handler(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n  File \"[...]/site-packages/litellm/google_genai/adapters/handler.py\", line 77, in async_generate_content_handler\n    raise ValueError(\n        f\"Error calling litellm.acompletion for generate_content: {str(e)}\"\n    )\nValueError: Error calling litellm.acompletion for generate_content: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"[...]/site-packages/litellm/proxy/google_endpoints/endpoints.py\", line 118, in google_stream_generate_content\n    return await processor.base_process_llm_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<17 lines>...\n    )\n    ^\n  File \"[...]/site-packages/litellm/proxy/common_request_processing.py\", line 424, in base_process_llm_request\n    responses = await llm_responses\n                ^^^^^^^^^^^^^^^^^^^\n  File \"[...]/site-packages/litellm/router.py\", line 3336, in async_wrapper\n    return await self._ageneric_api_call_with_fallbacks(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n    )\n    ^\n  File \"[...]/site-packages/litellm/router.py\", line 2561, in _ageneric_api_call_with_fallbacks\n    raise e\n  File \"[...]/site-packages/litellm/router.py\", line 2548, in _ageneric_api_call_with_fallbacks\n    response = await response  # type: ignore\n               ^^^^^^^^^^^^^^\n  File \"[...]/site-packages/litellm/utils.py\", line 1516, in wrapper_async\n    raise e\n  File \"[...]/site-packages/litellm/utils.py\", line 1374, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"[...]/site-packages/litellm/google_genai/main.py\", line 404, in agenerate_content_stream\n    raise litellm.exception_type(\n          ~~~~~~~~~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<3 lines>...\n        extra_kwargs=kwargs,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"[...]/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2284, in exception_type\n    raise e\n  File \"[...]/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 414, in exception_type\n    raise AuthenticationError(\n    ...<5 lines>...\n    )\nlitellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - Error calling litellm.acompletion for generate_content: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable LiteLLM Retried: 2 times\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.73.6.rc.1\n\n### Twitter / LinkedIn details\n\n_No response_\n", "hints_text": "Fixed @neteroster thanks \n@ishaan-jaff Thanks for fixing that! \n\nIt seems there are still some problems with request format convertion (especially the tools). Here is the request body my openai backend received when using gemini-cli with litellm:\n\nthe `functionDeclarations` is not openai format, and litellm also passthrough some gemini format request params which are unnecessary; and the system instruction is not converted to `system` role message in `messages`; `temperature`, `top_p` is not converted \n\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"...\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"...\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"...\"\n    }\n  ],\n  \"model\": \"o4-mini\",\n  \"stream\": true,\n  \"tools\": [\n    {\n      \"functionDeclarations\": [\n        {\n          \"description\": \"...\",\n          \"name\": \"list_directory\",\n          \"parameters\": {\n            \"properties\": {\n              \"path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"ignore\": {\n                \"description\": \"...\",\n                \"items\": {\n                  \"type\": \"STRING\"\n                },\n                \"type\": \"ARRAY\"\n              },\n              \"respect_git_ignore\": {\n                \"description\": \"...\",\n                \"type\": \"BOOLEAN\"\n              }\n            },\n            \"required\": [\n              \"path\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"read_file\",\n          \"parameters\": {\n            \"properties\": {\n              \"absolute_path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\",\n                \"pattern\": \"^/\"\n              },\n              \"offset\": {\n                \"description\": \"..\",\n                \"type\": \"NUMBER\"\n              },\n              \"limit\": {\n                \"description\": \"...\",\n                \"type\": \"NUMBER\"\n              }\n            },\n            \"required\": [\n              \"absolute_path\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"search_file_content\",\n          \"parameters\": {\n            \"properties\": {\n              \"pattern\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"include\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              }\n            },\n            \"required\": [\n              \"pattern\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"glob\",\n          \"parameters\": {\n            \"properties\": {\n              \"pattern\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"case_sensitive\": {\n                \"description\": \"...\",\n                \"type\": \"BOOLEAN\"\n              },\n              \"respect_git_ignore\": {\n                \"description\": \"...\",\n                \"type\": \"BOOLEAN\"\n              }\n            },\n            \"required\": [\n              \"pattern\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"replace\",\n          \"parameters\": {\n            \"properties\": {\n              \"file_path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"old_string\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"new_string\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"expected_replacements\": {\n                \"type\": \"NUMBER\",\n                \"description\": \"...\",\n                \"minimum\": 1\n              }\n            },\n            \"required\": [\n              \"file_path\",\n              \"old_string\",\n              \"new_string\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"write_file\",\n          \"parameters\": {\n            \"properties\": {\n              \"file_path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"content\": {\n                \"description\": \"....\",\n                \"type\": \"STRING\"\n              }\n            },\n            \"required\": [\n              \"file_path\",\n              \"content\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"parameters\": {\n            \"properties\": {\n              \"prompt\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              }\n            },\n            \"required\": [\n              \"prompt\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"read_many_files\",\n          \"parameters\": {\n            \"type\": \"OBJECT\",\n            \"properties\": {\n              \"paths\": {\n                \"type\": \"ARRAY\",\n                \"items\": {\n                  \"type\": \"STRING\"\n                },\n                \"description\": \"...\"\n              },\n              \"include\": {\n                \"type\": \"ARRAY\",\n                \"items\": {\n                  \"type\": \"STRING\"\n                },\n                \"description\": \"...\",\n                \"default\": []\n              },\n              \"exclude\": {\n                \"type\": \"ARRAY\",\n                \"items\": {\n                  \"type\": \"STRING\"\n                },\n                \"description\": \"...\",\n                \"default\": []\n              },\n              \"recursive\": {\n                \"type\": \"BOOLEAN\",\n                \"description\": \"...\",\n                \"default\": true\n              },\n              \"useDefaultExcludes\": {\n                \"type\": \"BOOLEAN\",\n                \"description\": \"...\",\n                \"default\": true\n              },\n              \"respect_git_ignore\": {\n                \"type\": \"BOOLEAN\",\n                \"description\": \"...\",\n                \"default\": true\n              }\n            },\n            \"required\": [\n              \"paths\"\n            ]\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"run_shell_command\",\n          \"parameters\": {\n            \"type\": \"OBJECT\",\n            \"properties\": {\n              \"command\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"description\": {\n                \"description\": \"....\",\n                \"type\": \"STRING\"\n              },\n              \"directory\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              }\n            },\n            \"required\": [\n              \"command\"\n            ]\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"save_memory\",\n          \"parameters\": {\n            \"type\": \"OBJECT\",\n            \"properties\": {\n              \"fact\": {\n                \"type\": \"STRING\",\n                \"description\": \"....\"\n              }\n            },\n            \"required\": [\n              \"fact\"\n            ]\n          }\n        },\n        {\n          \"description\": \"....\",\n          \"name\": \"google_web_search\",\n          \"parameters\": {\n            \"type\": \"OBJECT\",\n            \"properties\": {\n              \"query\": {\n                \"type\": \"STRING\",\n                \"description\": \"...\"\n              }\n            },\n            \"required\": [\n              \"query\"\n            ]\n          }\n        }\n      ]\n    }\n  ],\n  \"systemInstruction\": {\n    \"parts\": [\n      {\n        \"text\": \"...\"\n      }\n    ],\n    \"role\": \"user\"\n  },\n  \"generationConfig\": {\n    \"temperature\": 0,\n    \"topP\": 1,\n    \"thinkingConfig\": {\n      \"includeThoughts\": true\n    }\n  },\n  \"agenerate_content_stream\": true\n}\n```\n\nCould you make a new issue for this ? do you use LiteLLM with gemini-cli ?\r\n\r\nOn Mon, Jun 30, 2025 at 7:39\u202fPM NeterOster ***@***.***> wrote:\r\n\r\n> *neteroster* left a comment (BerriAI/litellm#12174)\r\n> <https://github.com/BerriAI/litellm/issues/12174#issuecomment-3021532521>\r\n>\r\n> @ishaan-jaff <https://github.com/ishaan-jaff> Thanks for fixing that!\r\n>\r\n> It seems there are still some problems with request format convertion.\r\n> Here is the request my openai backend received:\r\n>\r\n> 127.0.0.1 - - [01/Jul/2025 10:23:12] \"POST /v1/chat/completions HTTP/1.1\" 200 -\r\n> Received POST data:\r\n> {\"messages\":[{\"role\":\"user\",\"content\":\"1\"}],\"model\":\"o4-mini\",\"stream\":true,\"tools\":[],\"systemInstruction\":{\"parts\":[{\"text\":\"\"}],\"role\":\"user\"},\"safetySettings\":[{\"category\":\"HARM_CATEGORY_HATE_SPEECH\",\"threshold\":\"OFF\"},{\"category\":\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\"threshold\":\"OFF\"},{\"category\":\"HARM_CATEGORY_HARASSMENT\",\"threshold\":\"OFF\"},{\"category\":\"HARM_CATEGORY_DANGEROUS_CONTENT\",\"threshold\":\"OFF\"},{\"category\":\"HARM_CATEGORY_CIVIC_INTEGRITY\",\"threshold\":\"BLOCK_NONE\"}],\"generationConfig\":{\"temperature\":0.6,\"topP\":1,\"thinkingConfig\":{\"includeThoughts\":false,\"thinkingBudget\":0}},\"agenerate_content_stream\":true}\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/BerriAI/litellm/issues/12174#issuecomment-3021532521>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AHASVM5CDLJKCFPG67RXP7L3GHYE5AVCNFSM6AAAAACAO2NUCOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZTAMRRGUZTENJSGE>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n\r\n\r\n-- \r\nIshaan Jaffer\r\nCo-Founder https://github.com/BerriAI/litellm\r\n\n@ishaan-jaff \n\nI have created a new issue for this: #12191 and mentioned more details there, thanks for your help!\n\n", "all_hints_text": "Fixed @neteroster thanks \n@ishaan-jaff Thanks for fixing that! \n\nIt seems there are still some problems with request format convertion (especially the tools). Here is the request body my openai backend received when using gemini-cli with litellm:\n\nthe `functionDeclarations` is not openai format, and litellm also passthrough some gemini format request params which are unnecessary; and the system instruction is not converted to `system` role message in `messages`; `temperature`, `top_p` is not converted \n\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"...\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"...\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"...\"\n    }\n  ],\n  \"model\": \"o4-mini\",\n  \"stream\": true,\n  \"tools\": [\n    {\n      \"functionDeclarations\": [\n        {\n          \"description\": \"...\",\n          \"name\": \"list_directory\",\n          \"parameters\": {\n            \"properties\": {\n              \"path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"ignore\": {\n                \"description\": \"...\",\n                \"items\": {\n                  \"type\": \"STRING\"\n                },\n                \"type\": \"ARRAY\"\n              },\n              \"respect_git_ignore\": {\n                \"description\": \"...\",\n                \"type\": \"BOOLEAN\"\n              }\n            },\n            \"required\": [\n              \"path\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"read_file\",\n          \"parameters\": {\n            \"properties\": {\n              \"absolute_path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\",\n                \"pattern\": \"^/\"\n              },\n              \"offset\": {\n                \"description\": \"..\",\n                \"type\": \"NUMBER\"\n              },\n              \"limit\": {\n                \"description\": \"...\",\n                \"type\": \"NUMBER\"\n              }\n            },\n            \"required\": [\n              \"absolute_path\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"search_file_content\",\n          \"parameters\": {\n            \"properties\": {\n              \"pattern\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"include\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              }\n            },\n            \"required\": [\n              \"pattern\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"glob\",\n          \"parameters\": {\n            \"properties\": {\n              \"pattern\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"case_sensitive\": {\n                \"description\": \"...\",\n                \"type\": \"BOOLEAN\"\n              },\n              \"respect_git_ignore\": {\n                \"description\": \"...\",\n                \"type\": \"BOOLEAN\"\n              }\n            },\n            \"required\": [\n              \"pattern\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"replace\",\n          \"parameters\": {\n            \"properties\": {\n              \"file_path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"old_string\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"new_string\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"expected_replacements\": {\n                \"type\": \"NUMBER\",\n                \"description\": \"...\",\n                \"minimum\": 1\n              }\n            },\n            \"required\": [\n              \"file_path\",\n              \"old_string\",\n              \"new_string\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"write_file\",\n          \"parameters\": {\n            \"properties\": {\n              \"file_path\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"content\": {\n                \"description\": \"....\",\n                \"type\": \"STRING\"\n              }\n            },\n            \"required\": [\n              \"file_path\",\n              \"content\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"parameters\": {\n            \"properties\": {\n              \"prompt\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              }\n            },\n            \"required\": [\n              \"prompt\"\n            ],\n            \"type\": \"OBJECT\"\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"read_many_files\",\n          \"parameters\": {\n            \"type\": \"OBJECT\",\n            \"properties\": {\n              \"paths\": {\n                \"type\": \"ARRAY\",\n                \"items\": {\n                  \"type\": \"STRING\"\n                },\n                \"description\": \"...\"\n              },\n              \"include\": {\n                \"type\": \"ARRAY\",\n                \"items\": {\n                  \"type\": \"STRING\"\n                },\n                \"description\": \"...\",\n                \"default\": []\n              },\n              \"exclude\": {\n                \"type\": \"ARRAY\",\n                \"items\": {\n                  \"type\": \"STRING\"\n                },\n                \"description\": \"...\",\n                \"default\": []\n              },\n              \"recursive\": {\n                \"type\": \"BOOLEAN\",\n                \"description\": \"...\",\n                \"default\": true\n              },\n              \"useDefaultExcludes\": {\n                \"type\": \"BOOLEAN\",\n                \"description\": \"...\",\n                \"default\": true\n              },\n              \"respect_git_ignore\": {\n                \"type\": \"BOOLEAN\",\n                \"description\": \"...\",\n                \"default\": true\n              }\n            },\n            \"required\": [\n              \"paths\"\n            ]\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"run_shell_command\",\n          \"parameters\": {\n            \"type\": \"OBJECT\",\n            \"properties\": {\n              \"command\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              },\n              \"description\": {\n                \"description\": \"....\",\n                \"type\": \"STRING\"\n              },\n              \"directory\": {\n                \"description\": \"...\",\n                \"type\": \"STRING\"\n              }\n            },\n            \"required\": [\n              \"command\"\n            ]\n          }\n        },\n        {\n          \"description\": \"...\",\n          \"name\": \"save_memory\",\n          \"parameters\": {\n            \"type\": \"OBJECT\",\n            \"properties\": {\n              \"fact\": {\n                \"type\": \"STRING\",\n                \"description\": \"....\"\n              }\n            },\n            \"required\": [\n              \"fact\"\n            ]\n          }\n        },\n        {\n          \"description\": \"....\",\n          \"name\": \"google_web_search\",\n          \"parameters\": {\n            \"type\": \"OBJECT\",\n            \"properties\": {\n              \"query\": {\n                \"type\": \"STRING\",\n                \"description\": \"...\"\n              }\n            },\n            \"required\": [\n              \"query\"\n            ]\n          }\n        }\n      ]\n    }\n  ],\n  \"systemInstruction\": {\n    \"parts\": [\n      {\n        \"text\": \"...\"\n      }\n    ],\n    \"role\": \"user\"\n  },\n  \"generationConfig\": {\n    \"temperature\": 0,\n    \"topP\": 1,\n    \"thinkingConfig\": {\n      \"includeThoughts\": true\n    }\n  },\n  \"agenerate_content_stream\": true\n}\n```\n\nCould you make a new issue for this ? do you use LiteLLM with gemini-cli ?\r\n\r\nOn Mon, Jun 30, 2025 at 7:39\u202fPM NeterOster ***@***.***> wrote:\r\n\r\n> *neteroster* left a comment (BerriAI/litellm#12174)\r\n> <https://github.com/BerriAI/litellm/issues/12174#issuecomment-3021532521>\r\n>\r\n> @ishaan-jaff <https://github.com/ishaan-jaff> Thanks for fixing that!\r\n>\r\n> It seems there are still some problems with request format convertion.\r\n> Here is the request my openai backend received:\r\n>\r\n> 127.0.0.1 - - [01/Jul/2025 10:23:12] \"POST /v1/chat/completions HTTP/1.1\" 200 -\r\n> Received POST data:\r\n> {\"messages\":[{\"role\":\"user\",\"content\":\"1\"}],\"model\":\"o4-mini\",\"stream\":true,\"tools\":[],\"systemInstruction\":{\"parts\":[{\"text\":\"\"}],\"role\":\"user\"},\"safetySettings\":[{\"category\":\"HARM_CATEGORY_HATE_SPEECH\",\"threshold\":\"OFF\"},{\"category\":\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\"threshold\":\"OFF\"},{\"category\":\"HARM_CATEGORY_HARASSMENT\",\"threshold\":\"OFF\"},{\"category\":\"HARM_CATEGORY_DANGEROUS_CONTENT\",\"threshold\":\"OFF\"},{\"category\":\"HARM_CATEGORY_CIVIC_INTEGRITY\",\"threshold\":\"BLOCK_NONE\"}],\"generationConfig\":{\"temperature\":0.6,\"topP\":1,\"thinkingConfig\":{\"includeThoughts\":false,\"thinkingBudget\":0}},\"agenerate_content_stream\":true}\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/BerriAI/litellm/issues/12174#issuecomment-3021532521>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AHASVM5CDLJKCFPG67RXP7L3GHYE5AVCNFSM6AAAAACAO2NUCOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZTAMRRGUZTENJSGE>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n\r\n\r\n-- \r\nIshaan Jaffer\r\nCo-Founder https://github.com/BerriAI/litellm\r\n\n@ishaan-jaff \n\nI have created a new issue for this: #12191 and mentioned more details there, thanks for your help!\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/62789e755727ad7072532f78374670b349e96bff", "https://github.com/BerriAI/litellm/commit/69e783140788e01d2fd29cc0a23dd455b465768e", "https://github.com/BerriAI/litellm/commit/9adbe131bf967fe9783c260f70bc4f420022b75f", "https://github.com/BerriAI/litellm/commit/23a6ffc51a4cec4ab9ef622391b040cafa6f5701", "https://github.com/BerriAI/litellm/commit/a481fa33a8a9bd2b2b1225980dc46abc689e1f91", "https://github.com/BerriAI/litellm/commit/d6a1fa52ac110116e4d088c47c7e86781831c68f", "https://github.com/BerriAI/litellm/commit/15a83bce164c34ea25320cb75cfd5e0479dc5c08"], "created_at": "2025-06-30T21:24:37Z", "classification": "Security"}
{"repo": "BerriAI/litellm", "pull_number": 12281, "instance_id": "BerriAI__litellm-12281", "issue_numbers": [12237], "base_commit": "4fc4304f37f3254e4412e125c4a5938d71c3d1e2", "patch": "diff --git a/docs/my-website/docs/guides/security_settings.md b/docs/my-website/docs/guides/security_settings.md\nindex 008e620c515b..7995f6c3c9c9 100644\n--- a/docs/my-website/docs/guides/security_settings.md\n+++ b/docs/my-website/docs/guides/security_settings.md\n@@ -3,12 +3,43 @@ import TabItem from '@theme/TabItem';\n \n # SSL, HTTP Proxy Security Settings\n \n-If you're in an environment using an older TTS bundle, with an older encryption, follow this guide. \n+If you're in an environment using an older TTS bundle, with an older encryption, follow this guide. By default\n+LiteLLM uses the certifi CA bundle for SSL verification, which is compatible with most modern servers.\n+ However, if you need to disable SSL verification or use a custom CA bundle, you can do so by following the steps below.\n \n+Be aware that environmental variables take precedence over the settings in the SDK.\n \n-LiteLLM uses HTTPX for network requests, unless otherwise specified. \n+LiteLLM uses HTTPX for network requests, unless otherwise specified.\n \n-## 1. Disable SSL verification\n+## 1. Custom CA Bundle\n+\n+You can set a custom CA bundle file path using the `SSL_CERT_FILE` environmental variable or passing a string to the the ssl_verify setting.\n+\n+<Tabs>\n+<TabItem value=\"sdk\" label=\"SDK\">\n+\n+```python\n+import litellm\n+litellm.ssl_verify = \"client.pem\"\n+```\n+</TabItem>\n+<TabItem value=\"proxy\" label=\"PROXY\">\n+\n+```yaml\n+litellm_settings:\n+  ssl_verify: \"client.pem\"\n+```\n+\n+</TabItem>  \n+<TabItem value=\"env_var\" label=\"Environment Variables\">\n+\n+```bash\n+export SSL_CERT_FILE=\"client.pem\"\n+```\n+</TabItem>\n+</Tabs>\n+\n+## 2. Disable SSL verification\n \n \n <Tabs>\n@@ -35,14 +66,42 @@ export SSL_VERIFY=\"False\"\n </TabItem>\n </Tabs>\n \n-## 2. Lower security settings\n+## 3. Lower security settings\n+\n+The `ssl_security_level` allows setting a lower security level for SSL connections.\n+\n+<Tabs>\n+<TabItem value=\"sdk\" label=\"SDK\">\n+\n+```python\n+import litellm\n+litellm.ssl_security_level = \"DEFAULT@SECLEVEL=1\"\n+```\n+</TabItem>\n+<TabItem value=\"proxy\" label=\"PROXY\">\n+\n+```yaml\n+litellm_settings:\n+  ssl_security_level: \"DEFAULT@SECLEVEL=1\"\n+```\n+</TabItem>\n+<TabItem value=\"env_var\" label=\"Environment Variables\">\n+\n+```bash\n+export SSL_SECURITY_LEVEL=\"DEFAULT@SECLEVEL=1\"\n+```\n+</TabItem>\n+</Tabs>\n+\n+## 4. Certificate authentication\n+\n+The `SSL_CERTIFICATE` environmental variable or `ssl_certificate` attribute allows setting a client side certificate to authenticate the client to the server.\n \n <Tabs>\n <TabItem value=\"sdk\" label=\"SDK\">\n \n ```python\n import litellm\n-litellm.ssl_security_level = 1\n litellm.ssl_certificate = \"/path/to/certificate.pem\"\n ```\n </TabItem>\n@@ -50,20 +109,18 @@ litellm.ssl_certificate = \"/path/to/certificate.pem\"\n \n ```yaml\n litellm_settings:\n-  ssl_security_level: 1\n   ssl_certificate: \"/path/to/certificate.pem\"\n ```\n </TabItem>\n <TabItem value=\"env_var\" label=\"Environment Variables\">\n \n ```bash\n-export SSL_SECURITY_LEVEL=\"1\"\n export SSL_CERTIFICATE=\"/path/to/certificate.pem\"\n ```\n </TabItem>\n </Tabs>\n \n-## 3. Use HTTP_PROXY environment variable\n+## 5. Use HTTP_PROXY environment variable\n \n Both httpx and aiohttp libraries use `urllib.request.getproxies` from environment variables. Before client initialization, you may set proxy (and optional SSL_CERT_FILE) by setting the environment variables:\n \ndiff --git a/litellm/__init__.py b/litellm/__init__.py\nindex 77b3bcd87997..946ba0336278 100644\n--- a/litellm/__init__.py\n+++ b/litellm/__init__.py\n@@ -214,6 +214,7 @@\n )\n use_client: bool = False\n ssl_verify: Union[str, bool] = True\n+ssl_security_level: Optional[str] = None\n ssl_certificate: Optional[str] = None\n disable_streaming_logging: bool = False\n disable_token_counter: bool = False\ndiff --git a/litellm/llms/custom_httpx/http_handler.py b/litellm/llms/custom_httpx/http_handler.py\nindex 34968a63aeea..201874603e9e 100644\n--- a/litellm/llms/custom_httpx/http_handler.py\n+++ b/litellm/llms/custom_httpx/http_handler.py\n@@ -5,6 +5,7 @@\n from typing import TYPE_CHECKING, Any, Callable, Dict, List, Mapping, Optional, Union\n \n import httpx\n+import certifi\n from aiohttp import ClientSession, TCPConnector\n from httpx import USE_CLIENT_DEFAULT, AsyncHTTPTransport, HTTPTransport\n from httpx._types import RequestFiles\n@@ -39,6 +40,72 @@\n _DEFAULT_TIMEOUT = httpx.Timeout(timeout=5.0, connect=5.0)\n \n \n+def get_ssl_configuration(ssl_verify: Optional[VerifyTypes] = None) -> Union[bool, str, ssl.SSLContext]:\n+    \"\"\"\n+    Unified SSL configuration function that handles ssl_context and ssl_verify logic.\n+\n+    SSL Configuration Priority:\n+    1. If ssl_verify is provided -> is a SSL context use the custom SSL context\n+    2. If ssl_verify is False -> disable SSL verification (ssl=False)\n+    3. If ssl_verify is a string -> use it as a path to CA bundle file\n+    4. If SSL_CERT_FILE environment variable is set and exists -> use it as CA bundle file\n+    5. Else will use default SSL context with certifi CA bundle\n+\n+    If ssl_security_level is set, it will apply the security level to the SSL context.\n+\n+    Args:\n+        ssl_verify: SSL verification setting. Can be:\n+            - None: Use default from environment/litellm settings\n+            - False: Disable SSL verification\n+            - True: Enable SSL verification\n+            - str: Path to CA bundle file\n+    \n+    Returns:\n+        Union[bool, str, ssl.SSLContext]: Appropriate SSL configuration\n+    \"\"\"\n+    from litellm.secret_managers.main import str_to_bool\n+\n+    if isinstance(ssl_verify, ssl.SSLContext):\n+        # If ssl_verify is already an SSLContext, return it directly\n+        return ssl_verify\n+\n+    # Get ssl_verify from environment or litellm settings if not provided\n+    if ssl_verify is None:\n+        ssl_verify = os.getenv(\"SSL_VERIFY\", litellm.ssl_verify)\n+        ssl_verify_bool = str_to_bool(ssl_verify) if isinstance(ssl_verify, str) else ssl_verify\n+        if ssl_verify_bool is not None:\n+            ssl_verify = ssl_verify_bool\n+\n+    ssl_security_level = os.getenv(\"SSL_SECURITY_LEVEL\", litellm.ssl_security_level)\n+\n+    cafile = None\n+    if isinstance(ssl_verify, str) and os.path.exists(ssl_verify):\n+        cafile = ssl_verify\n+    if not cafile:\n+        ssl_cert_file = os.getenv(\"SSL_CERT_FILE\")\n+        if ssl_cert_file and os.path.exists(ssl_cert_file):\n+            cafile = ssl_cert_file\n+        else:\n+            cafile = certifi.where()\n+\n+    if ssl_verify is not False:\n+        custom_ssl_context = ssl.create_default_context(\n+            cafile=cafile\n+        )\n+        # If security level is set, apply it to the SSL context\n+        if (\n+            ssl_security_level\n+            and isinstance(ssl_security_level, str)\n+        ):\n+            # Create a custom SSL context with reduced security level\n+            custom_ssl_context.set_ciphers(ssl_security_level)\n+\n+        # Use our custom SSL context instead of the original ssl_verify value\n+        return custom_ssl_context\n+\n+    return ssl_verify\n+\n+\n def mask_sensitive_info(error_message):\n     # Find the start of the key parameter\n     if isinstance(error_message, str):\n@@ -119,29 +186,8 @@ def create_client(\n         event_hooks: Optional[Mapping[str, List[Callable[..., Any]]]],\n         ssl_verify: Optional[VerifyTypes] = None,\n     ) -> httpx.AsyncClient:\n-        # SSL certificates (a.k.a CA bundle) used to verify the identity of requested hosts.\n-        # /path/to/certificate.pem\n-        if ssl_verify is None:\n-            ssl_verify = os.getenv(\"SSL_VERIFY\", litellm.ssl_verify)\n-\n-        ssl_security_level = os.getenv(\"SSL_SECURITY_LEVEL\")\n-\n-        # If ssl_verify is not False and we need a lower security level\n-        if (\n-            not ssl_verify\n-            and ssl_security_level\n-            and isinstance(ssl_security_level, str)\n-        ):\n-            # Create a custom SSL context with reduced security level\n-            custom_ssl_context = ssl.create_default_context()\n-            custom_ssl_context.set_ciphers(ssl_security_level)\n-\n-            # If ssl_verify is a path to a CA bundle, load it into our custom context\n-            if isinstance(ssl_verify, str) and os.path.exists(ssl_verify):\n-                custom_ssl_context.load_verify_locations(cafile=ssl_verify)\n-\n-            # Use our custom SSL context instead of the original ssl_verify value\n-            ssl_verify = custom_ssl_context\n+        # Get unified SSL configuration\n+        ssl_config = get_ssl_configuration(ssl_verify)\n \n         # An SSL certificate used by the requested host to authenticate the client.\n         # /path/to/client.pem\n@@ -152,8 +198,8 @@ def create_client(\n         # Create a client with a connection pool\n \n         transport = AsyncHTTPHandler._create_async_transport(\n-            ssl_context=ssl_verify if isinstance(ssl_verify, ssl.SSLContext) else None,\n-            ssl_verify=ssl_verify if isinstance(ssl_verify, bool) else None,\n+            ssl_context=ssl_config if isinstance(ssl_config, ssl.SSLContext) else None,\n+            ssl_verify=ssl_config if isinstance(ssl_config, bool) else None,\n         )\n \n         return httpx.AsyncClient(\n@@ -164,7 +210,7 @@ def create_client(\n                 max_connections=concurrent_limit,\n                 max_keepalive_connections=concurrent_limit,\n             ),\n-            verify=ssl_verify,\n+            verify=ssl_config,\n             cert=cert,\n             headers=headers,\n         )\n@@ -544,7 +590,6 @@ def _get_ssl_connector_kwargs(\n         SSL Configuration Priority:\n         1. If ssl_context is provided -> use the custom SSL context\n         2. If ssl_verify is False -> disable SSL verification (ssl=False)\n-        3. If ssl_verify is True/None -> use default SSL context with certifi CA bundle\n \n         Returns:\n             Dict with appropriate SSL configuration for TCPConnector\n@@ -559,10 +604,6 @@ def _get_ssl_connector_kwargs(\n         elif ssl_verify is False:\n             # Priority 2: Explicitly disable SSL verification\n             connector_kwargs[\"verify_ssl\"] = False\n-        else:\n-            # Priority 3: Use our default SSL context with certifi CA bundle\n-            # This covers ssl_verify=True and ssl_verify=None cases\n-            connector_kwargs[\"ssl\"] = AsyncHTTPHandler._get_ssl_context()\n         \n         return connector_kwargs\n \n@@ -577,7 +618,6 @@ def _create_aiohttp_transport(\n         Note: aiohttp TCPConnector ssl parameter accepts:\n         - SSLContext: custom SSL context\n         - False: disable SSL verification\n-        - True: use default SSL verification (equivalent to ssl.create_default_context())\n         \"\"\"\n         from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n         from litellm.secret_managers.main import str_to_bool\n@@ -600,17 +640,6 @@ def _create_aiohttp_transport(\n                 trust_env=trust_env,\n             ),\n         )\n-    \n-\n-    @staticmethod\n-    def _get_ssl_context() -> ssl.SSLContext:\n-        \"\"\"\n-        Get the SSL context for the AiohttpTransport\n-        \"\"\"\n-        import certifi\n-        return ssl.create_default_context(\n-            cafile=certifi.where()\n-        )\n \n     @staticmethod\n     def _create_httpx_transport() -> Optional[AsyncHTTPTransport]:\n@@ -637,11 +666,8 @@ def __init__(\n         if timeout is None:\n             timeout = _DEFAULT_TIMEOUT\n \n-        # SSL certificates (a.k.a CA bundle) used to verify the identity of requested hosts.\n-        # /path/to/certificate.pem\n-\n-        if ssl_verify is None:\n-            ssl_verify = os.getenv(\"SSL_VERIFY\", litellm.ssl_verify)\n+        # Get unified SSL configuration\n+        ssl_config = get_ssl_configuration(ssl_verify)\n \n         # An SSL certificate used by the requested host to authenticate the client.\n         # /path/to/client.pem\n@@ -658,7 +684,7 @@ def __init__(\n                     max_connections=concurrent_limit,\n                     max_keepalive_connections=concurrent_limit,\n                 ),\n-                verify=ssl_verify,\n+                verify=ssl_config,\n                 cert=cert,\n                 headers=headers,\n             )\ndiff --git a/litellm/llms/openai/common_utils.py b/litellm/llms/openai/common_utils.py\nindex 853efb784663..aa670df0531a 100644\n--- a/litellm/llms/openai/common_utils.py\n+++ b/litellm/llms/openai/common_utils.py\n@@ -4,6 +4,7 @@\n \n import hashlib\n import json\n+import ssl\n from typing import Any, Dict, List, Literal, Optional, Union\n \n import httpx\n@@ -15,6 +16,7 @@\n from litellm.llms.custom_httpx.http_handler import (\n     _DEFAULT_TTL_FOR_HTTPX_CLIENTS,\n     AsyncHTTPHandler,\n+    get_ssl_configuration,\n )\n \n \n@@ -196,10 +198,16 @@ def _get_async_http_client() -> Optional[httpx.AsyncClient]:\n         if litellm.aclient_session is not None:\n             return litellm.aclient_session\n \n+        # Get unified SSL configuration\n+        ssl_config = get_ssl_configuration()\n+\n         return httpx.AsyncClient(\n             limits=httpx.Limits(max_connections=1000, max_keepalive_connections=100),\n-            verify=litellm.ssl_verify,\n-            transport=AsyncHTTPHandler._create_async_transport(),\n+            verify=ssl_config,\n+            transport=AsyncHTTPHandler._create_async_transport(\n+                ssl_context=ssl_config if isinstance(ssl_config, ssl.SSLContext) else None,\n+                ssl_verify=ssl_config if isinstance(ssl_config, bool) else None,\n+            ),\n             follow_redirects=True,\n         )\n \n@@ -207,8 +215,12 @@ def _get_async_http_client() -> Optional[httpx.AsyncClient]:\n     def _get_sync_http_client() -> Optional[httpx.Client]:\n         if litellm.client_session is not None:\n             return litellm.client_session\n+        \n+        # Get unified SSL configuration\n+        ssl_config = get_ssl_configuration()\n+        \n         return httpx.Client(\n             limits=httpx.Limits(max_connections=1000, max_keepalive_connections=100),\n-            verify=litellm.ssl_verify,\n+            verify=ssl_config,\n             follow_redirects=True,\n         )\n", "test_patch": "diff --git a/tests/test_litellm/llms/custom_httpx/test_http_handler.py b/tests/test_litellm/llms/custom_httpx/test_http_handler.py\nindex 6f66bf20a0a6..a649e9b6b9cb 100644\n--- a/tests/test_litellm/llms/custom_httpx/test_http_handler.py\n+++ b/tests/test_litellm/llms/custom_httpx/test_http_handler.py\n@@ -15,34 +15,35 @@\n )  # Adds the parent directory to the system path\n import litellm\n from litellm.llms.custom_httpx.aiohttp_transport import LiteLLMAiohttpTransport\n-from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler, HTTPHandler\n+from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler, get_ssl_configuration\n \n \n @pytest.mark.asyncio\n async def test_ssl_security_level(monkeypatch):\n-    # Set environment variable for SSL security level\n-    monkeypatch.setenv(\"SSL_SECURITY_LEVEL\", \"DEFAULT@SECLEVEL=1\")\n+    with patch.dict(os.environ, clear=True):\n+        # Set environment variable for SSL security level\n+        monkeypatch.setenv(\"SSL_SECURITY_LEVEL\", \"DEFAULT@SECLEVEL=1\")\n \n-    # Create async client with SSL verification disabled to isolate SSL context testing\n-    client = AsyncHTTPHandler(ssl_verify=False)\n+        # Create async client with SSL verification disabled to isolate SSL context testing\n+        client = AsyncHTTPHandler()\n \n-    # Get the transport (should be LiteLLMAiohttpTransport)\n-    transport = client.client._transport\n+        # Get the transport (should be LiteLLMAiohttpTransport)\n+        transport = client.client._transport\n \n-    # Get the aiohttp ClientSession\n-    client_session = transport._get_valid_client_session()\n+        # Get the aiohttp ClientSession\n+        client_session = transport._get_valid_client_session()\n \n-    # Get the connector from the session\n-    connector = client_session.connector\n+        # Get the connector from the session\n+        connector = client_session.connector\n \n-    # Get the SSL context from the connector\n-    ssl_context = connector._ssl\n-    print(\"ssl_context\", ssl_context)\n+        # Get the SSL context from the connector\n+        ssl_context = connector._ssl\n+        print(\"ssl_context\", ssl_context)\n \n-    # Verify that the SSL context exists and has the correct cipher string\n-    assert isinstance(ssl_context, ssl.SSLContext)\n-    # Optionally, check the ciphers string if needed\n-    # assert \"DEFAULT@SECLEVEL=1\" in ssl_context.get_ciphers()\n+        # Verify that the SSL context exists and has the correct cipher string\n+        assert isinstance(ssl_context, ssl.SSLContext)\n+        # Optionally, check the ciphers string if needed\n+        # assert \"DEFAULT@SECLEVEL=1\" in ssl_context.get_ciphers()\n \n \n @pytest.mark.asyncio\n@@ -151,28 +152,30 @@ async def test_aiohttp_transport_trust_env_setting(monkeypatch):\n     assert client_session_with_false_env._trust_env == default_trust_env\n \n \n-def test_get_ssl_context():\n-    \"\"\"Test that _get_ssl_context() returns a proper SSL context with certifi CA bundle\"\"\"\n-    with patch('ssl.create_default_context') as mock_create_context:\n-        # Mock the return value\n-        mock_ssl_context = MagicMock(spec=ssl.SSLContext)\n-        mock_create_context.return_value = mock_ssl_context\n-        \n-        # Call the static method\n-        result = AsyncHTTPHandler._get_ssl_context()\n-        \n-        # Verify ssl.create_default_context was called with certifi's CA file\n-        expected_ca_file = certifi.where()\n-        mock_create_context.assert_called_once_with(cafile=expected_ca_file)\n-        \n-        # Verify it returns the mocked SSL context\n-        assert result == mock_ssl_context\n-\n-\n-def test_get_ssl_context_integration():\n+def test_get_ssl_configuration():\n+    \"\"\"Test that get_ssl_configuration() returns a proper SSL context with certifi CA bundle\n+    when no environment variables are set.\"\"\"\n+    with patch.dict(os.environ, clear=True):\n+        with patch('ssl.create_default_context') as mock_create_context:\n+            # Mock the return value\n+            mock_ssl_context = MagicMock(spec=ssl.SSLContext)\n+            mock_create_context.return_value = mock_ssl_context\n+            \n+            # Call the static method\n+            result = get_ssl_configuration()\n+            \n+            # Verify ssl.create_default_context was called with certifi's CA file\n+            expected_ca_file = certifi.where()\n+            mock_create_context.assert_called_once_with(cafile=expected_ca_file)\n+            \n+            # Verify it returns the mocked SSL context\n+            assert result == mock_ssl_context\n+\n+\n+def test_get_ssl_configuration_integration():\n     \"\"\"Integration test that _get_ssl_context() returns a working SSL context\"\"\"\n     # Call the static method without mocking\n-    ssl_context = AsyncHTTPHandler._get_ssl_context()\n+    ssl_context = get_ssl_configuration()\n     \n     # Verify it returns an SSLContext instance\n     assert isinstance(ssl_context, ssl.SSLContext)\n", "problem_statement": "[Bug]: aiohttp_transport doesn't consider ssl settings\n### What happened?\n\nThe settings in https://docs.litellm.ai/docs/guides/security_settings don't seem to be respected for aiohttp_transport. This applies to `ssl_verify`, `ssl_security_level`, and `ssl_certificate` when set through the `litellm` module. In addition when setting `SSL_VERIFY=False`, `SSL_SECURITY_LEVEL=\"1\"` or `SSL_CERTIFICATE`, I also get errors that are different from the ones encountered when running acompletion.\n\nE.g. in this request I set ssl_verify, but ssl_verify is not respected. This code will lead to a SSLCertVerificationError if the endpoint is self signed.\n\n```python\nimport litellm\nimport asyncio\nlitellm.ssl_verify = False\nresponse = asyncio.run(litellm.acompletion(\n    model=\"openai/qwen_qwen3_235b_a22b\",\n    api_key=\"placeholder\",\n    messages=[{\"role\": \"user\", \"content\": \"what is custom llama?\"}],\n    api_base=\"https://private_endpoint_self_signed.com\"))\n```\n\nAs an aside, for me it was unexpected that SSL_CERT_FILE is not respected by this library due to the choice of setting the cabundle to `certifi.where()`.\n\n**Possible solution**\nPossible solution in line 202 it should pass the relevant variables if these are set, e.g. for ssl_verify it could be:\n```python\ntransport=AsyncHTTPHandler._create_async_transport(ssl_verify=litellm.ssl_verify), \n```\nhttps://github.com/BerriAI/litellm/blob/042f6b187e08425503b1f69eb33da404a635034b/litellm/llms/openai/common_utils.py#L199-L204\n\nThis is passed to the aiohttp handler here:\nhttps://github.com/BerriAI/litellm/blob/042f6b187e08425503b1f69eb33da404a635034b/litellm/llms/custom_httpx/http_handler.py#L585-L587\n\n\n### Relevant log output\n\n```shell\n[SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')]\n```\n\n### Are you a ML Ops Team?\n\nNo\n\n### What LiteLLM version are you on ?\n\nv1.73.6\n\n### Twitter / LinkedIn details\n\n_No response_\n", "hints_text": "@JoostvDoorn can you help with a PR ? \nMade an attempt in #12281 mostly tried to keep it aligned as much with what is documented and make sure that functionality is also supported for aiohttp.\n\n", "all_hints_text": "@JoostvDoorn can you help with a PR ? \nMade an attempt in #12281 mostly tried to keep it aligned as much with what is documented and make sure that functionality is also supported for aiohttp.\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/514190eb735c7cd66463b715dd3e879831c8058c", "https://github.com/BerriAI/litellm/commit/f1e34398fce96e2c90b070555f5d4618c6b4e275"], "created_at": "2025-07-03T11:18:37Z", "classification": "Security"}
{"repo": "BerriAI/litellm", "pull_number": 12268, "instance_id": "BerriAI__litellm-12268", "issue_numbers": [12254], "base_commit": "a198d4a39fd6c386a4c755c212a7f3b4efa4e6e0", "patch": "diff --git a/litellm/integrations/custom_guardrail.py b/litellm/integrations/custom_guardrail.py\nindex a82eed8eb8fc..2cb80b3d6981 100644\n--- a/litellm/integrations/custom_guardrail.py\n+++ b/litellm/integrations/custom_guardrail.py\n@@ -46,6 +46,7 @@ def __init__(\n         self.mask_response_content: bool = mask_response_content\n \n         if supported_event_hooks:\n+\n             ## validate event_hook is in supported_event_hooks\n             self._validate_event_hook(event_hook, supported_event_hooks)\n         super().__init__(**kwargs)\n@@ -55,10 +56,15 @@ def _validate_event_hook(\n         event_hook: Optional[Union[GuardrailEventHooks, List[GuardrailEventHooks]]],\n         supported_event_hooks: List[GuardrailEventHooks],\n     ) -> None:\n+\n         if event_hook is None:\n             return\n+        if isinstance(event_hook, str):\n+            event_hook = GuardrailEventHooks(event_hook)\n         if isinstance(event_hook, list):\n             for hook in event_hook:\n+                if isinstance(hook, str):\n+                    hook = GuardrailEventHooks(hook)\n                 if hook not in supported_event_hooks:\n                     raise ValueError(\n                         f\"Event hook {hook} is not in the supported event hooks {supported_event_hooks}\"\ndiff --git a/litellm/litellm_core_utils/exception_mapping_utils.py b/litellm/litellm_core_utils/exception_mapping_utils.py\nindex ad5060c533b0..5d1d5c712767 100644\n--- a/litellm/litellm_core_utils/exception_mapping_utils.py\n+++ b/litellm/litellm_core_utils/exception_mapping_utils.py\n@@ -28,23 +28,22 @@ class ExceptionCheckers:\n     \"\"\"\n     Helper class for checking various error conditions in exception strings.\n     \"\"\"\n-    \n+\n     @staticmethod\n     def is_error_str_rate_limit(error_str: str) -> bool:\n         \"\"\"\n         Check if an error string indicates a rate limit error.\n-        \n+\n         Args:\n             error_str: The error string to check\n-            \n+\n         Returns:\n             True if the error indicates a rate limit, False otherwise\n         \"\"\"\n         if not isinstance(error_str, str):\n             return False\n-            \n+\n         return \"429\" in error_str or \"rate limit\" in error_str.lower()\n-    \n \n     @staticmethod\n     def is_error_str_context_window_exceeded(error_str: str) -> bool:\n@@ -289,6 +288,7 @@ def exception_type(  # type: ignore  # noqa: PLR0915\n                 or custom_llm_provider == \"text-completion-openai\"\n                 or custom_llm_provider == \"custom_openai\"\n                 or custom_llm_provider in litellm.openai_compatible_providers\n+                or custom_llm_provider == \"mistral\"\n             ):\n                 # custom_llm_provider is openai, make it OpenAI\n                 message = get_error_message(error_obj=original_exception)\ndiff --git a/litellm/proxy/_new_secret_config.yaml b/litellm/proxy/_new_secret_config.yaml\nindex 6ad4c7788889..8be231a1fe5f 100644\n--- a/litellm/proxy/_new_secret_config.yaml\n+++ b/litellm/proxy/_new_secret_config.yaml\n@@ -1,11 +1,8 @@\n model_list:\n-  - model_name: \"gpt-4o\"\n+  - model_name: gpt-3.5-turbo\n     litellm_params:\n-      model: \"gpt-4o\"\n+      model: openai/gpt-3.5-turbo\n       api_key: os.environ/OPENAI_API_KEY\n-<<<<<<< HEAD\n-  - model_name: \"gemini-2.0-flash\"\n-=======\n \n guardrails:\n   - guardrail_name: azure-prompt-shield\n@@ -13,10 +10,4 @@ guardrails:\n       guardrail: azure/prompt_shield\n       mode: pre_call # \"During_call\" is also available\n       api_key: os.environ/AZURE_GUARDRAIL_API_KEY\n-      api_base: os.environ/AZURE_GUARDRAIL_API_BASE \n-  - guardrail_name: azure-text-moderation\n->>>>>>> 6ebc8d473 (test: change mistral model)\n-    litellm_params:\n-      model: \"gemini/gemini-2.0-flash\"\n-      api_key: os.environ/GEMINI_API_KEY\n-      stream: True\n+      api_base: os.environ/AZURE_GUARDRAIL_API_BASE \n\\ No newline at end of file\ndiff --git a/litellm/proxy/guardrails/guardrail_hooks/azure/__init__.py b/litellm/proxy/guardrails/guardrail_hooks/azure/__init__.py\nnew file mode 100644\nindex 000000000000..39501ca470db\n--- /dev/null\n+++ b/litellm/proxy/guardrails/guardrail_hooks/azure/__init__.py\n@@ -0,0 +1,59 @@\n+from typing import TYPE_CHECKING, Union\n+\n+from litellm.types.guardrails import SupportedGuardrailIntegrations\n+\n+if TYPE_CHECKING:\n+    from litellm.types.guardrails import Guardrail, LitellmParams\n+\n+\n+def initialize_guardrail(litellm_params: \"LitellmParams\", guardrail: \"Guardrail\"):\n+    import litellm\n+\n+    from .prompt_shield import AzureContentSafetyPromptShieldGuardrail\n+    from .text_moderation import AzureContentSafetyTextModerationGuardrail\n+\n+    if not litellm_params.api_key:\n+        raise ValueError(\"Azure Content Safety: api_key is required\")\n+    if not litellm_params.api_base:\n+        raise ValueError(\"Azure Content Safety: api_base is required\")\n+\n+    azure_guardrail = litellm_params.guardrail.split(\"/\")[1]\n+\n+    guardrail_name = guardrail.get(\"guardrail_name\")\n+    if not guardrail_name:\n+        raise ValueError(\"Azure Content Safety: guardrail_name is required\")\n+\n+    if azure_guardrail == \"prompt_shield\":\n+        azure_content_safety_guardrail: Union[\n+            AzureContentSafetyPromptShieldGuardrail,\n+            AzureContentSafetyTextModerationGuardrail,\n+        ] = AzureContentSafetyPromptShieldGuardrail(\n+            guardrail_name=guardrail_name,\n+            api_key=litellm_params.api_key,\n+            api_base=litellm_params.api_base,\n+            default_on=litellm_params.default_on,\n+            event_hook=litellm_params.mode,\n+        )\n+    elif azure_guardrail == \"text_moderations\":\n+        azure_content_safety_guardrail = AzureContentSafetyTextModerationGuardrail(\n+            guardrail_name=guardrail_name,\n+            api_key=litellm_params.api_key,\n+            api_base=litellm_params.api_base,\n+            default_on=litellm_params.default_on,\n+            event_hook=litellm_params.mode,\n+        )\n+    else:\n+        raise ValueError(\n+            f\"Azure Content Safety: {azure_guardrail} is not a valid guardrail\"\n+        )\n+\n+    litellm.logging_callback_manager.add_litellm_callback(\n+        azure_content_safety_guardrail\n+    )\n+    return azure_content_safety_guardrail\n+\n+\n+guardrail_initializer_registry = {\n+    SupportedGuardrailIntegrations.AZURE_PROMPT_SHIELD.value: initialize_guardrail,\n+    SupportedGuardrailIntegrations.AZURE_TEXT_MODERATIONS.value: initialize_guardrail,\n+}\ndiff --git a/litellm/proxy/guardrails/guardrail_hooks/azure/base.py b/litellm/proxy/guardrails/guardrail_hooks/azure/base.py\nnew file mode 100644\nindex 000000000000..7903cd8bc0d6\n--- /dev/null\n+++ b/litellm/proxy/guardrails/guardrail_hooks/azure/base.py\n@@ -0,0 +1,52 @@\n+from typing import TYPE_CHECKING, List, Optional\n+\n+if TYPE_CHECKING:\n+    from litellm.types.llms.openai import AllMessageValues\n+\n+\n+class AzureGuardrailBase:\n+    \"\"\"\n+    Base class for Azure guardrails.\n+    \"\"\"\n+\n+    def get_user_prompt(self, messages: List[\"AllMessageValues\"]) -> Optional[str]:\n+        \"\"\"\n+        Get the last consecutive block of messages from the user.\n+\n+        Example:\n+        messages = [\n+            {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n+            {\"role\": \"assistant\", \"content\": \"I'm good, thank you!\"},\n+            {\"role\": \"user\", \"content\": \"What is the weather in Tokyo?\"},\n+        ]\n+        get_user_prompt(messages) -> \"What is the weather in Tokyo?\"\n+        \"\"\"\n+        from litellm.litellm_core_utils.prompt_templates.common_utils import (\n+            convert_content_list_to_str,\n+        )\n+\n+        if not messages:\n+            return None\n+\n+        # Iterate from the end to find the last consecutive block of user messages\n+        user_messages = []\n+        for message in reversed(messages):\n+            if message.get(\"role\") == \"user\":\n+                user_messages.append(message)\n+            else:\n+                # Stop when we hit a non-user message\n+                break\n+\n+        if not user_messages:\n+            return None\n+\n+        # Reverse to get the messages in chronological order\n+        user_messages.reverse()\n+\n+        user_prompt = \"\"\n+        for message in user_messages:\n+            text_content = convert_content_list_to_str(message)\n+            user_prompt += text_content + \"\\n\"\n+\n+        result = user_prompt.strip()\n+        return result if result else None\ndiff --git a/litellm/proxy/guardrails/guardrail_hooks/azure/prompt_shield.py b/litellm/proxy/guardrails/guardrail_hooks/azure/prompt_shield.py\nnew file mode 100644\nindex 000000000000..7624a963b776\n--- /dev/null\n+++ b/litellm/proxy/guardrails/guardrail_hooks/azure/prompt_shield.py\n@@ -0,0 +1,185 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Azure Prompt Shield Native Guardrail Integrationfor LiteLLM\n+\"\"\"\n+\n+from typing import TYPE_CHECKING, Any, Dict, List, Literal, Optional, cast\n+\n+from fastapi import HTTPException\n+\n+from litellm._logging import verbose_proxy_logger\n+from litellm.integrations.custom_guardrail import (\n+    CustomGuardrail,\n+    log_guardrail_information,\n+)\n+from litellm.llms.custom_httpx.http_handler import (\n+    get_async_httpx_client,\n+    httpxSpecialProvider,\n+)\n+\n+from .base import AzureGuardrailBase\n+\n+if TYPE_CHECKING:\n+    from litellm.proxy._types import UserAPIKeyAuth\n+    from litellm.types.llms.openai import AllMessageValues\n+    from litellm.types.proxy.guardrails.guardrail_hooks.azure.azure_prompt_shield import (\n+        AzurePromptShieldGuardrailResponse,\n+    )\n+    from litellm.types.utils import ModelResponse\n+\n+\n+class AzureContentSafetyPromptShieldGuardrail(AzureGuardrailBase, CustomGuardrail):\n+    \"\"\"\n+    LiteLLM Built-in Guardrail for Azure Content Safety Guardrail (Prompt Shield).\n+\n+    This guardrail scans prompts and responses using the Azure Prompt Shield API to detect\n+    malicious content, injection attempts, and policy violations.\n+\n+    Configuration:\n+        guardrail_name: Name of the guardrail instance\n+        api_key: Azure Prompt Shield API key\n+        api_base: Azure Prompt Shield API endpoint\n+        default_on: Whether to enable by default\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        guardrail_name: str,\n+        api_key: str,\n+        api_base: str,\n+        **kwargs,\n+    ):\n+        \"\"\"Initialize Azure Prompt Shield guardrail handler.\"\"\"\n+        from litellm.types.guardrails import GuardrailEventHooks\n+\n+        # Initialize parent CustomGuardrail\n+\n+        supported_event_hooks = [\n+            GuardrailEventHooks.pre_call,\n+            GuardrailEventHooks.during_call,\n+        ]\n+        super().__init__(\n+            guardrail_name=guardrail_name,\n+            supported_event_hooks=supported_event_hooks,\n+            **kwargs,\n+        )\n+        self.async_handler = get_async_httpx_client(\n+            llm_provider=httpxSpecialProvider.GuardrailCallback\n+        )\n+\n+        # Store configuration\n+        self.api_key = api_key\n+        self.api_base = api_base\n+        self.api_version = kwargs.get(\"api_version\") or \"2024-09-01\"\n+\n+        verbose_proxy_logger.info(\n+            f\"Initialized Azure Prompt Shield Guardrail: {guardrail_name}\"\n+        )\n+\n+    async def async_make_request(\n+        self, user_prompt: str\n+    ) -> \"AzurePromptShieldGuardrailResponse\":\n+        \"\"\"\n+        Make a request to the Azure Prompt Shield API.\n+        \"\"\"\n+        from litellm.types.proxy.guardrails.guardrail_hooks.azure.azure_prompt_shield import (\n+            AzurePromptShieldGuardrailRequestBody,\n+            AzurePromptShieldGuardrailResponse,\n+        )\n+\n+        request_body = AzurePromptShieldGuardrailRequestBody(\n+            documents=[], userPrompt=user_prompt\n+        )\n+        verbose_proxy_logger.debug(\n+            \"Azure Prompt Shield guard request: %s\", request_body\n+        )\n+        response = await self.async_handler.post(\n+            url=f\"{self.api_base}/contentsafety/text:shieldPrompt?api-version={self.api_version}\",\n+            headers={\n+                \"Ocp-Apim-Subscription-Key\": self.api_key,\n+                \"Content-Type\": \"application/json\",\n+            },\n+            json=cast(dict, request_body),\n+        )\n+\n+        verbose_proxy_logger.debug(\n+            \"Azure Prompt Shield guard response: %s\", response.json()\n+        )\n+        return AzurePromptShieldGuardrailResponse(**response.json())  # type: ignore\n+\n+    @log_guardrail_information\n+    async def async_pre_call_hook(\n+        self,\n+        user_api_key_dict: \"UserAPIKeyAuth\",\n+        cache: Any,\n+        data: Dict[str, Any],\n+        call_type: Literal[\n+            \"completion\",\n+            \"text_completion\",\n+            \"embeddings\",\n+            \"image_generation\",\n+            \"moderation\",\n+            \"audio_transcription\",\n+            \"pass_through_endpoint\",\n+            \"rerank\",\n+        ],\n+    ) -> Optional[Dict[str, Any]]:\n+        \"\"\"\n+        Pre-call hook to scan user prompts before sending to LLM.\n+\n+        Raises HTTPException if content should be blocked.\n+        \"\"\"\n+        verbose_proxy_logger.info(\n+            \"Azure Prompt Shield: Running pre-call prompt scan, on call_type: %s\",\n+            call_type,\n+        )\n+        if call_type == \"acompletion\":\n+            new_messages: Optional[List[AllMessageValues]] = data.get(\"messages\")\n+            if new_messages is None:\n+                verbose_proxy_logger.warning(\n+                    \"Lakera AI: not running guardrail. No messages in data\"\n+                )\n+                return data\n+            user_prompt = self.get_user_prompt(new_messages)\n+\n+            if user_prompt:\n+                verbose_proxy_logger.info(\n+                    f\"Azure Prompt Shield: User prompt: {user_prompt}\"\n+                )\n+                azure_prompt_shield_response = await self.async_make_request(\n+                    user_prompt=user_prompt,\n+                )\n+                if azure_prompt_shield_response[\"userPromptAnalysis\"].get(\n+                    \"attackDetected\"\n+                ):\n+                    verbose_proxy_logger.warning(\"Azure Prompt Shield: Attack detected\")\n+                    raise HTTPException(\n+                        status_code=400,\n+                        detail={\n+                            \"error\": \"Violated Azure Prompt Shield guardrail policy\",\n+                            \"detection_message\": f\"Attack detected: {azure_prompt_shield_response['userPromptAnalysis']}\",\n+                        },\n+                    )\n+            else:\n+                verbose_proxy_logger.warning(\n+                    \"Azure Prompt Shield: No user prompt found\"\n+                )\n+        return None\n+\n+    @log_guardrail_information\n+    async def async_post_call_hook(\n+        self,\n+        data: Dict[str, Any],\n+        user_api_key_dict: \"UserAPIKeyAuth\",\n+        response: \"ModelResponse\",\n+    ) -> \"ModelResponse\":\n+        \"\"\"\n+        Post-call hook to scan LLM responses before returning to user.\n+\n+        Raises HTTPException if response should be blocked.\n+        \"\"\"\n+        verbose_proxy_logger.info(\n+            \"Azure Prompt Shield: Running post-call response scan\"\n+        )\n+\n+        return response\ndiff --git a/litellm/proxy/guardrails/guardrail_hooks/azure/text_moderation.py b/litellm/proxy/guardrails/guardrail_hooks/azure/text_moderation.py\nnew file mode 100644\nindex 000000000000..d1f154daeadc\n--- /dev/null\n+++ b/litellm/proxy/guardrails/guardrail_hooks/azure/text_moderation.py\n@@ -0,0 +1,273 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Azure Text Moderation Native Guardrail Integrationfor LiteLLM\n+\"\"\"\n+\n+from typing import TYPE_CHECKING, Any, Dict, List, Literal, Optional, Union, cast\n+\n+from fastapi import HTTPException\n+\n+from litellm._logging import verbose_proxy_logger\n+from litellm.integrations.custom_guardrail import (\n+    CustomGuardrail,\n+    log_guardrail_information,\n+)\n+from litellm.llms.custom_httpx.http_handler import (\n+    get_async_httpx_client,\n+    httpxSpecialProvider,\n+)\n+from litellm.proxy._types import UserAPIKeyAuth\n+\n+from .base import AzureGuardrailBase\n+\n+if TYPE_CHECKING:\n+    from litellm.proxy._types import UserAPIKeyAuth\n+    from litellm.types.llms.openai import AllMessageValues\n+    from litellm.types.proxy.guardrails.guardrail_hooks.azure.azure_text_moderation import (\n+        AzureTextModerationGuardrailResponse,\n+    )\n+    from litellm.types.utils import EmbeddingResponse, ImageResponse, ModelResponse\n+\n+\n+class AzureContentSafetyTextModerationGuardrail(AzureGuardrailBase, CustomGuardrail):\n+    \"\"\"\n+    LiteLLM Built-in Guardrail for Azure Content Safety Guardrail (Prompt Shield).\n+\n+    This guardrail scans prompts and responses using the Azure Prompt Shield API to detect\n+    malicious content, injection attempts, and policy violations.\n+\n+    Configuration:\n+        guardrail_name: Name of the guardrail instance\n+        api_key: Azure Prompt Shield API key\n+        api_base: Azure Prompt Shield API endpoint\n+        default_on: Whether to enable by default\n+    \"\"\"\n+\n+    default_severity_threshold: int = 2\n+\n+    def __init__(\n+        self,\n+        guardrail_name: str,\n+        api_key: str,\n+        api_base: str,\n+        severity_threshold: Optional[int] = None,\n+        severity_threshold_by_category: Optional[Dict[str, int]] = None,\n+        **kwargs,\n+    ):\n+        \"\"\"Initialize Azure Text Moderation guardrail handler.\"\"\"\n+        # Initialize parent CustomGuardrail\n+        from litellm.types.proxy.guardrails.guardrail_hooks.azure.azure_text_moderation import (\n+            AzureTextModerationRequestBodyOptionalParams,\n+        )\n+\n+        super().__init__(\n+            guardrail_name=guardrail_name,\n+            **kwargs,\n+        )\n+        self.async_handler = get_async_httpx_client(\n+            llm_provider=httpxSpecialProvider.GuardrailCallback\n+        )\n+\n+        # Store configuration\n+        self.api_key = api_key\n+        self.api_base = api_base\n+        self.api_version = kwargs.get(\"api_version\") or \"2024-09-01\"\n+        self.optional_params_request_body: (\n+            AzureTextModerationRequestBodyOptionalParams\n+        ) = {\n+            \"categories\": kwargs.get(\"categories\")\n+            or [\n+                \"Hate\",\n+                \"Sexual\",\n+                \"SelfHarm\",\n+                \"Violence\",\n+            ],\n+            \"blocklistNames\": cast(\n+                Optional[List[str]], kwargs.get(\"blocklistNames\") or None\n+            ),\n+            \"haltOnBlocklistHit\": kwargs.get(\"haltOnBlocklistHit\") or False,\n+            \"outputType\": kwargs.get(\"outputType\") or \"FourSeverityLevels\",\n+        }\n+\n+        self.severity_threshold = severity_threshold\n+        self.severity_threshold_by_category = severity_threshold_by_category\n+\n+        verbose_proxy_logger.info(\n+            f\"Initialized Azure Prompt Shield Guardrail: {guardrail_name}\"\n+        )\n+\n+    async def async_make_request(\n+        self, text: str\n+    ) -> \"AzureTextModerationGuardrailResponse\":\n+        \"\"\"\n+        Make a request to the Azure Prompt Shield API.\n+        \"\"\"\n+        from litellm.types.proxy.guardrails.guardrail_hooks.azure.azure_text_moderation import (\n+            AzureTextModerationGuardrailRequestBody,\n+            AzureTextModerationGuardrailResponse,\n+        )\n+\n+        request_body = AzureTextModerationGuardrailRequestBody(\n+            text=text,\n+            **self.optional_params_request_body,\n+        )\n+        verbose_proxy_logger.debug(\n+            \"Azure Text Moderation guard request: %s\", request_body\n+        )\n+        response = await self.async_handler.post(\n+            url=f\"{self.api_base}/contentsafety/text:analyze?api-version={self.api_version}\",\n+            headers={\n+                \"Ocp-Apim-Subscription-Key\": self.api_key,\n+                \"Content-Type\": \"application/json\",\n+            },\n+            json=cast(dict, request_body),\n+        )\n+\n+        verbose_proxy_logger.debug(\n+            \"Azure Text Moderation guard response: %s\", response.json()\n+        )\n+        return AzureTextModerationGuardrailResponse(**response.json())  # type: ignore\n+\n+    def check_severity_threshold(\n+        self, response: \"AzureTextModerationGuardrailResponse\"\n+    ) -> Literal[True]:\n+        \"\"\"\n+        - Check if threshold set by category\n+        - Check if general severity threshold set\n+        - If both none, use default_severity_threshold\n+        \"\"\"\n+        if self.severity_threshold_by_category:\n+            for category in response[\"categoriesAnalysis\"]:\n+                severity_category_threshold_item = (\n+                    self.severity_threshold_by_category.get(category[\"category\"])\n+                )\n+                if (\n+                    severity_category_threshold_item is not None\n+                    and severity_category_threshold_item >= category[\"severity\"]\n+                ):\n+                    raise HTTPException(\n+                        status_code=400,\n+                        detail={\n+                            \"error\": \"Azure Content Safety Guardrail: {} crossed severity {}, Got severity: {}\".format(\n+                                category[\"category\"],\n+                                self.severity_threshold_by_category.get(\n+                                    category[\"category\"]\n+                                ),\n+                                category[\"severity\"],\n+                            )\n+                        },\n+                    )\n+        if self.severity_threshold:\n+            for category in response[\"categoriesAnalysis\"]:\n+                if category[\"severity\"] >= self.severity_threshold:\n+                    raise HTTPException(\n+                        status_code=400,\n+                        detail={\n+                            \"error\": \"Azure Content Safety Guardrail: {} crossed severity {}, Got severity: {}\".format(\n+                                category[\"category\"],\n+                                self.severity_threshold,\n+                                category[\"severity\"],\n+                            )\n+                        },\n+                    )\n+        if (\n+            self.severity_threshold is None\n+            and self.severity_threshold_by_category is None\n+        ):\n+            for category in response[\"categoriesAnalysis\"]:\n+\n+                if category[\"severity\"] >= self.default_severity_threshold:\n+                    raise HTTPException(\n+                        status_code=400,\n+                        detail={\n+                            \"error\": \"Azure Content Safety Guardrail: {} crossed severity {}, Got severity: {}\".format(\n+                                category[\"category\"],\n+                                self.default_severity_threshold,\n+                                category[\"severity\"],\n+                            )\n+                        },\n+                    )\n+        return True\n+\n+    @log_guardrail_information\n+    async def async_pre_call_hook(\n+        self,\n+        user_api_key_dict: \"UserAPIKeyAuth\",\n+        cache: Any,\n+        data: Dict[str, Any],\n+        call_type: Literal[\n+            \"completion\",\n+            \"text_completion\",\n+            \"embeddings\",\n+            \"image_generation\",\n+            \"moderation\",\n+            \"audio_transcription\",\n+            \"pass_through_endpoint\",\n+            \"rerank\",\n+        ],\n+    ) -> Optional[Dict[str, Any]]:\n+        \"\"\"\n+        Pre-call hook to scan user prompts before sending to LLM.\n+\n+        Raises HTTPException if content should be blocked.\n+        \"\"\"\n+        verbose_proxy_logger.info(\n+            \"Azure Prompt Shield: Running pre-call prompt scan, on call_type: %s\",\n+            call_type,\n+        )\n+        if call_type == \"acompletion\":\n+            new_messages: Optional[List[AllMessageValues]] = data.get(\"messages\")\n+            if new_messages is None:\n+                verbose_proxy_logger.warning(\n+                    \"Lakera AI: not running guardrail. No messages in data\"\n+                )\n+                return data\n+            user_prompt = self.get_user_prompt(new_messages)\n+\n+            if user_prompt:\n+                verbose_proxy_logger.info(\n+                    f\"Azure Text Moderation: User prompt: {user_prompt}\"\n+                )\n+                azure_text_moderation_response = await self.async_make_request(\n+                    text=user_prompt,\n+                )\n+                self.check_severity_threshold(response=azure_text_moderation_response)\n+            else:\n+                verbose_proxy_logger.warning(\"Azure Text Moderation: No text found\")\n+        return None\n+\n+    async def async_post_call_success_hook(\n+        self,\n+        data: dict,\n+        user_api_key_dict: \"UserAPIKeyAuth\",\n+        response: Union[Any, \"ModelResponse\", \"EmbeddingResponse\", \"ImageResponse\"],\n+    ) -> Any:\n+        from litellm.types.utils import Choices, ModelResponse\n+\n+        if (\n+            isinstance(response, ModelResponse)\n+            and response.choices\n+            and isinstance(response.choices[0], Choices)\n+        ):\n+            content = response.choices[0].message.content or \"\"\n+            azure_text_moderation_response = await self.async_make_request(\n+                text=content,\n+            )\n+            self.check_severity_threshold(response=azure_text_moderation_response)\n+        return response\n+\n+    async def async_post_call_streaming_hook(\n+        self, user_api_key_dict: UserAPIKeyAuth, response: str\n+    ) -> Any:\n+        try:\n+            if response is not None and len(response) > 0:\n+                azure_text_moderation_response = await self.async_make_request(\n+                    text=response,\n+                )\n+                self.check_severity_threshold(response=azure_text_moderation_response)\n+            return response\n+        except HTTPException as e:\n+            import json\n+\n+            error_returned = json.dumps({\"error\": e.detail})\n+            return f\"data: {error_returned}\\n\\n\"\ndiff --git a/litellm/proxy/guardrails/guardrail_registry.py b/litellm/proxy/guardrails/guardrail_registry.py\nindex c7418116bc2f..fb6810a18937 100644\n--- a/litellm/proxy/guardrails/guardrail_registry.py\n+++ b/litellm/proxy/guardrails/guardrail_registry.py\n@@ -30,8 +30,8 @@\n     initialize_lakera_v2,\n     initialize_lasso,\n     initialize_pangea,\n-    initialize_presidio,\n     initialize_panw_prisma_airs,\n+    initialize_presidio,\n )\n \n guardrail_initializer_registry = {\n@@ -49,6 +49,88 @@\n }\n \n \n+def get_guardrail_initializer_from_hooks():\n+    \"\"\"\n+    Get guardrail initializers by discovering them from the guardrail_hooks directory structure.\n+\n+    Scans the guardrail_hooks directory for subdirectories containing __init__.py files\n+    with either guardrail_initializer_registry or initialize_guardrail functions.\n+\n+    Returns:\n+        Dict[str, Callable]: A dictionary mapping guardrail types to their initializer functions\n+    \"\"\"\n+    discovered_initializers = {}\n+\n+    try:\n+        # Get the path to the guardrail_hooks directory\n+        current_dir = os.path.dirname(__file__)\n+        hooks_dir = os.path.join(current_dir, \"guardrail_hooks\")\n+\n+        if not os.path.exists(hooks_dir):\n+            verbose_proxy_logger.debug(\"guardrail_hooks directory not found\")\n+            return discovered_initializers\n+\n+        # Scan each subdirectory in guardrail_hooks\n+        for item in os.listdir(hooks_dir):\n+            item_path = os.path.join(hooks_dir, item)\n+\n+            # Skip files and __pycache__ directories\n+            if not os.path.isdir(item_path) or item.startswith(\"__\"):\n+                continue\n+\n+            # Check if the directory has an __init__.py file\n+            init_file = os.path.join(item_path, \"__init__.py\")\n+            if not os.path.exists(init_file):\n+                continue\n+\n+            module_path = f\"litellm.proxy.guardrails.guardrail_hooks.{item}\"\n+            try:\n+                # Import the module\n+                verbose_proxy_logger.debug(f\"Discovering guardrails in: {module_path}\")\n+\n+                module = importlib.import_module(module_path)\n+\n+                # Check for guardrail_initializer_registry dictionary\n+                if hasattr(module, \"guardrail_initializer_registry\"):\n+                    registry = getattr(module, \"guardrail_initializer_registry\")\n+                    if isinstance(registry, dict):\n+                        discovered_initializers.update(registry)\n+                        verbose_proxy_logger.debug(\n+                            f\"Found guardrail_initializer_registry in {module_path}: {list(registry.keys())}\"\n+                        )\n+\n+                # Check for standalone initialize_guardrail function (fallback for directory-based guardrails)\n+                elif hasattr(module, \"initialize_guardrail\"):\n+                    # For directories with just initialize_guardrail, use the directory name as the key\n+                    initialize_fn = getattr(module, \"initialize_guardrail\")\n+                    discovered_initializers[item] = initialize_fn\n+                    verbose_proxy_logger.debug(\n+                        f\"Found initialize_guardrail function in {module_path}\"\n+                    )\n+\n+            except ImportError as e:\n+                verbose_proxy_logger.debug(f\"Could not import {module_path}: {e}\")\n+                continue\n+            except Exception as e:\n+                verbose_proxy_logger.error(f\"Error processing {module_path}: {e}\")\n+                continue\n+\n+        verbose_proxy_logger.debug(\n+            f\"Discovered {len(discovered_initializers)} guardrail initializers: {list(discovered_initializers.keys())}\"\n+        )\n+\n+    except Exception as e:\n+        verbose_proxy_logger.error(f\"Error discovering guardrail initializers: {e}\")\n+\n+    return discovered_initializers\n+\n+\n+# Merge with dynamically discovered guardrail initializers\n+_discovered_initializers = get_guardrail_initializer_from_hooks()\n+\n+guardrail_initializer_registry.update(_discovered_initializers)\n+\n+\n class GuardrailRegistry:\n     \"\"\"\n     Registry for guardrails\ndiff --git a/litellm/proxy/guardrails/init_guardrails.py b/litellm/proxy/guardrails/init_guardrails.py\nindex 70c6fe65ad59..6009fce31ea9 100644\n--- a/litellm/proxy/guardrails/init_guardrails.py\n+++ b/litellm/proxy/guardrails/init_guardrails.py\n@@ -89,4 +89,4 @@ def init_guardrails_v2(\n         if initialized_guardrail:\n             guardrail_list.append(initialized_guardrail)\n \n-    verbose_proxy_logger.info(f\"\\nGuardrail List:{guardrail_list}\\n\")\n+    verbose_proxy_logger.debug(f\"\\nGuardrail List:{guardrail_list}\\n\")\ndiff --git a/litellm/types/guardrails.py b/litellm/types/guardrails.py\nindex 615fd15839f5..742dcf82b5a3 100644\n--- a/litellm/types/guardrails.py\n+++ b/litellm/types/guardrails.py\n@@ -31,6 +31,8 @@ class SupportedGuardrailIntegrations(Enum):\n     PANGEA = \"pangea\"\n     LASSO = \"lasso\"\n     PANW_PRISMA_AIRS = \"panw_prisma_airs\"\n+    AZURE_PROMPT_SHIELD = \"azure/prompt_shield\"\n+    AZURE_TEXT_MODERATIONS = \"azure/text_moderations\"\n \n \n class Role(Enum):\ndiff --git a/litellm/types/proxy/guardrails/guardrail_hooks/azure/azure_prompt_shield.py b/litellm/types/proxy/guardrails/guardrail_hooks/azure/azure_prompt_shield.py\nnew file mode 100644\nindex 000000000000..6b7f1da76202\n--- /dev/null\n+++ b/litellm/types/proxy/guardrails/guardrail_hooks/azure/azure_prompt_shield.py\n@@ -0,0 +1,19 @@\n+from typing import Any, Dict, List, TypedDict\n+\n+\n+class AzurePromptShieldGuardrailRequestBody(TypedDict):\n+    \"\"\"Configuration parameters for the Azure Prompt Shield guardrail\"\"\"\n+\n+    userPrompt: str\n+    documents: List[str]\n+\n+\n+class UserPromptAnalysis(TypedDict, total=False):\n+    attackDetected: bool\n+\n+\n+class AzurePromptShieldGuardrailResponse(TypedDict):\n+    \"\"\"Configuration parameters for the Azure Prompt Shield guardrail\"\"\"\n+\n+    userPromptAnalysis: UserPromptAnalysis\n+    documentsAnalysis: List[Dict[str, Any]]\ndiff --git a/litellm/types/proxy/guardrails/guardrail_hooks/azure/azure_text_moderation.py b/litellm/types/proxy/guardrails/guardrail_hooks/azure/azure_text_moderation.py\nnew file mode 100644\nindex 000000000000..39fad0553b75\n--- /dev/null\n+++ b/litellm/types/proxy/guardrails/guardrail_hooks/azure/azure_text_moderation.py\n@@ -0,0 +1,32 @@\n+from typing import Any, Dict, List, Literal, Optional, Required, TypedDict\n+\n+\n+class AzureTextModerationRequestBodyOptionalParams(TypedDict, total=False):\n+    \"\"\"Optional parameters for the Azure Text Moderation guardrail\"\"\"\n+\n+    categories: Optional[List[str]]\n+    blocklistNames: Optional[List[str]]\n+    haltOnBlocklistHit: Optional[bool]\n+    outputType: Literal[\"FourSeverityLevels\", \"EightSeverityLevels\"]\n+\n+\n+class AzureTextModerationGuardrailRequestBody(\n+    AzureTextModerationRequestBodyOptionalParams\n+):\n+    \"\"\"Configuration parameters for the Azure Text Moderation guardrail\"\"\"\n+\n+    text: Required[str]\n+\n+\n+class AzureTextModerationGuardrailResponseCategoriesAnalysis(TypedDict):\n+    \"\"\"Response from the Azure Text Moderation guardrail\"\"\"\n+\n+    category: str\n+    severity: int\n+\n+\n+class AzureTextModerationGuardrailResponse(TypedDict):\n+    \"\"\"Response from the Azure Text Moderation guardrail\"\"\"\n+\n+    blocklistsMatch: List[Dict[str, Any]]\n+    categoriesAnalysis: List[AzureTextModerationGuardrailResponseCategoriesAnalysis]\n", "test_patch": "diff --git a/tests/test_litellm/proxy/guardrails/guardrail_hooks/azure/test_azure_prompt_shield.py b/tests/test_litellm/proxy/guardrails/guardrail_hooks/azure/test_azure_prompt_shield.py\nnew file mode 100644\nindex 000000000000..19c07d60e9d9\n--- /dev/null\n+++ b/tests/test_litellm/proxy/guardrails/guardrail_hooks/azure/test_azure_prompt_shield.py\n@@ -0,0 +1,48 @@\n+from unittest.mock import AsyncMock, patch\n+\n+import httpx\n+import pytest\n+from fastapi import HTTPException\n+\n+from litellm.proxy._types import UserAPIKeyAuth\n+from litellm.proxy.guardrails.guardrail_hooks.azure.prompt_shield import (\n+    AzureContentSafetyPromptShieldGuardrail,\n+)\n+from litellm.proxy.guardrails.init_guardrails import init_guardrails_v2\n+from litellm.types.utils import Choices, Message, ModelResponse\n+\n+\n+@pytest.mark.asyncio\n+async def test_azure_prompt_shield_guardrail_pre_call_hook():\n+\n+    azure_prompt_shield_guardrail = AzureContentSafetyPromptShieldGuardrail(\n+        guardrail_name=\"azure_prompt_shield\",\n+        api_key=\"azure_prompt_shield_api_key\",\n+        api_base=\"azure_prompt_shield_api_base\",\n+    )\n+    with patch.object(\n+        azure_prompt_shield_guardrail, \"async_make_request\"\n+    ) as mock_async_make_request:\n+        mock_async_make_request.return_value = {\n+            \"userPromptAnalysis\": {\"attackDetected\": False},\n+            \"documentsAnalysis\": [],\n+        }\n+        await azure_prompt_shield_guardrail.async_pre_call_hook(\n+            user_api_key_dict=UserAPIKeyAuth(api_key=\"azure_prompt_shield_api_key\"),\n+            cache=None,\n+            data={\n+                \"messages\": [\n+                    {\n+                        \"role\": \"user\",\n+                        \"content\": \"Hello, how are you?\",\n+                    }\n+                ]\n+            },\n+            call_type=\"acompletion\",\n+        )\n+\n+        mock_async_make_request.assert_called_once()\n+        assert (\n+            mock_async_make_request.call_args.kwargs[\"user_prompt\"]\n+            == \"Hello, how are you?\"\n+        )\ndiff --git a/tests/test_litellm/proxy/guardrails/guardrail_hooks/azure/test_azure_text_moderation.py b/tests/test_litellm/proxy/guardrails/guardrail_hooks/azure/test_azure_text_moderation.py\nnew file mode 100644\nindex 000000000000..6fc70560d47f\n--- /dev/null\n+++ b/tests/test_litellm/proxy/guardrails/guardrail_hooks/azure/test_azure_text_moderation.py\n@@ -0,0 +1,87 @@\n+from unittest.mock import AsyncMock, patch\n+\n+import httpx\n+import pytest\n+from fastapi import HTTPException\n+\n+from litellm.proxy._types import UserAPIKeyAuth\n+from litellm.proxy.guardrails.guardrail_hooks.azure.text_moderation import (\n+    AzureContentSafetyTextModerationGuardrail,\n+)\n+from litellm.proxy.guardrails.init_guardrails import init_guardrails_v2\n+from litellm.types.utils import Choices, Message, ModelResponse\n+\n+\n+@pytest.mark.asyncio\n+async def test_azure_text_moderation_guardrail_pre_call_hook():\n+\n+    azure_text_moderation_guardrail = AzureContentSafetyTextModerationGuardrail(\n+        guardrail_name=\"azure_text_moderation\",\n+        api_key=\"azure_text_moderation_api_key\",\n+        api_base=\"azure_text_moderation_api_base\",\n+    )\n+    with patch.object(\n+        azure_text_moderation_guardrail, \"async_make_request\"\n+    ) as mock_async_make_request:\n+        mock_async_make_request.return_value = {\n+            \"blocklistsMatch\": [],\n+            \"categoriesAnalysis\": [\n+                {\"category\": \"Hate\", \"severity\": 2},\n+            ],\n+        }\n+        with pytest.raises(HTTPException):\n+            await azure_text_moderation_guardrail.async_pre_call_hook(\n+                user_api_key_dict=UserAPIKeyAuth(\n+                    api_key=\"azure_text_moderation_api_key\"\n+                ),\n+                cache=None,\n+                data={\n+                    \"messages\": [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": \"I hate you!\",\n+                        }\n+                    ]\n+                },\n+                call_type=\"acompletion\",\n+            )\n+\n+        mock_async_make_request.assert_called_once()\n+        assert mock_async_make_request.call_args.kwargs[\"text\"] == \"I hate you!\"\n+\n+\n+@pytest.mark.asyncio\n+async def test_azure_text_moderation_guardrail_post_call_success_hook():\n+\n+    azure_text_moderation_guardrail = AzureContentSafetyTextModerationGuardrail(\n+        guardrail_name=\"azure_text_moderation\",\n+        api_key=\"azure_text_moderation_api_key\",\n+        api_base=\"azure_text_moderation_api_base\",\n+    )\n+    with patch.object(\n+        azure_text_moderation_guardrail, \"async_make_request\"\n+    ) as mock_async_make_request:\n+        mock_async_make_request.return_value = {\n+            \"blocklistsMatch\": [],\n+            \"categoriesAnalysis\": [\n+                {\"category\": \"Hate\", \"severity\": 2},\n+            ],\n+        }\n+        with pytest.raises(HTTPException):\n+            result = await azure_text_moderation_guardrail.async_post_call_success_hook(\n+                data={},\n+                user_api_key_dict=UserAPIKeyAuth(\n+                    api_key=\"azure_text_moderation_api_key\"\n+                ),\n+                response=ModelResponse(\n+                    choices=[\n+                        Choices(\n+                            index=0,\n+                            message=Message(content=\"I hate you!\"),\n+                        )\n+                    ]\n+                ),\n+            )\n+\n+        mock_async_make_request.assert_called_once()\n+        mock_async_make_request.call_args.kwargs[\"text\"] == \"I hate you!\"\n", "problem_statement": "[Feature]: Add azure content safety guardrails on litellm proxy\n### The Feature\n\nPrompt Shields and Analyze Text are the two that are most important to us. Groundedness would be the next one we're interested in. Then protected material text detection. We may have a use case for custom guardrails in the future, but really the ones we know people would use today are Prompt Shields and Analyze text.\n\n\n\n\n### Motivation, pitch\n\nWe have an internal ChatGPT clone. We want to use Azure Content Safety Prompt Shields to detect if any of the users are trying to perform a jailbreak or prompt injection. In this scenario the user's prompt would first be sent to our internal ChatGPT and that app would pass the prompt over to LiteLLM proxy. At the proxy level we want to enable the prompt shield check before or in parallel to the LLM generation request.\n\n\n### LiteLLM is hiring a founding backend engineer, are you interested in joining us and shipping to all our users?\n\nNo\n\n### Twitter / LinkedIn details\n\n_No response_\n", "hints_text": "It looks like Prompt Shields and Analyze Text (now: text moderations) are 2 different API's. \n\nwe can either have these be 2 different guardrails \n\n```yaml\nmodel_list:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: openai/gpt-3.5-turbo\n      api_key: os.environ/OPENAI_API_KEY\n\nguardrails:\n  - guardrail_name: azure-prompt-shield\n    litellm_params:\n      guardrail: azure/prompt_shield\n      mode: [pre_call, post_call] # \"During_call\" is also available\n      api_key: os.environ/AZURE_API_KEY\n      api_base: os.environ/AZURE_API_BASE \n  - guardrail_name: azure-text-moderation\n      litellm_params:\n        guardrail: azure/text_moderations\n        mode: [pre_call, post_call] # \"During_call\" is also available\n        api_key: os.environ/AZURE_API_KEY\n        api_base: os.environ/AZURE_API_BASE \n```\nOr have it be 1 guardrail, with the user specifying which azure guardrail they want to run \n\n```yaml\nmodel_list:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: openai/gpt-3.5-turbo\n      api_key: os.environ/OPENAI_API_KEY\n\nguardrails:\n  - guardrail_name: azure-prompt-shield\n    litellm_params:\n      guardrail: azure\n      mode: [pre_call, post_call] # \"During_call\" is also available\n      api_key: os.environ/AZURE_API_KEY\n      api_base: os.environ/AZURE_API_BASE \n      azure_guardrail: \"prompt_shield\"\n```\n\n", "all_hints_text": "It looks like Prompt Shields and Analyze Text (now: text moderations) are 2 different API's. \n\nwe can either have these be 2 different guardrails \n\n```yaml\nmodel_list:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: openai/gpt-3.5-turbo\n      api_key: os.environ/OPENAI_API_KEY\n\nguardrails:\n  - guardrail_name: azure-prompt-shield\n    litellm_params:\n      guardrail: azure/prompt_shield\n      mode: [pre_call, post_call] # \"During_call\" is also available\n      api_key: os.environ/AZURE_API_KEY\n      api_base: os.environ/AZURE_API_BASE \n  - guardrail_name: azure-text-moderation\n      litellm_params:\n        guardrail: azure/text_moderations\n        mode: [pre_call, post_call] # \"During_call\" is also available\n        api_key: os.environ/AZURE_API_KEY\n        api_base: os.environ/AZURE_API_BASE \n```\nOr have it be 1 guardrail, with the user specifying which azure guardrail they want to run \n\n```yaml\nmodel_list:\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: openai/gpt-3.5-turbo\n      api_key: os.environ/OPENAI_API_KEY\n\nguardrails:\n  - guardrail_name: azure-prompt-shield\n    litellm_params:\n      guardrail: azure\n      mode: [pre_call, post_call] # \"During_call\" is also available\n      api_key: os.environ/AZURE_API_KEY\n      api_base: os.environ/AZURE_API_BASE \n      azure_guardrail: \"prompt_shield\"\n```\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/baff00e26be81264819602a3f9108907bf5eeb0d", "https://github.com/BerriAI/litellm/commit/0b5abefde57575379509f007a4e68920bc760aba", "https://github.com/BerriAI/litellm/commit/782e00ad20506c750340e0d4b0d4caaeef87e54d", "https://github.com/BerriAI/litellm/commit/366a8f51ebd27212fe5db9d6f332c0871342328d", "https://github.com/BerriAI/litellm/commit/e1488df2d22f2b278cfdbbda1cdac16ca2fbf308", "https://github.com/BerriAI/litellm/commit/4baf13159cd1b3dea459bf6a78031a5763c5fe0e", "https://github.com/BerriAI/litellm/commit/9013240e63bae6bd891d8af4540d1d8b58183720", "https://github.com/BerriAI/litellm/commit/2fc3eec6c9aabd8ea8a3c651a9d108fcb1bb9743", "https://github.com/BerriAI/litellm/commit/14b5acf96241c26a9f23fdd97ec1b669020cf1df", "https://github.com/BerriAI/litellm/commit/317bdbcaece0a124437d43f61667c919239b4dba", "https://github.com/BerriAI/litellm/commit/8956426fb88bcd9525a065f05463522dec97e9fc", "https://github.com/BerriAI/litellm/commit/6ebc8d473f382a34c44a14c0bf3955ebcca5efe7", "https://github.com/BerriAI/litellm/commit/8a2d6e8af08d5aaea3f2ce3e05238c277eeafe5d", "https://github.com/BerriAI/litellm/commit/eb829d286db56d6c7bf04ec6e442d2b8effdb985"], "created_at": "2025-07-03T03:30:59Z", "classification": "Security"}
{"repo": "BerriAI/litellm", "pull_number": 12840, "instance_id": "BerriAI__litellm-12840", "issue_numbers": [12402], "base_commit": "27c9be67ba5c303ee3ffb7c3a781363da0f45acc", "patch": "diff --git a/litellm/proxy/management_endpoints/key_management_endpoints.py b/litellm/proxy/management_endpoints/key_management_endpoints.py\nindex 8998d96e48cb..e0432405cec4 100644\n--- a/litellm/proxy/management_endpoints/key_management_endpoints.py\n+++ b/litellm/proxy/management_endpoints/key_management_endpoints.py\n@@ -59,6 +59,7 @@\n     _hash_token_if_needed,\n     handle_exception_on_proxy,\n     jsonify_object,\n+    is_valid_api_key,\n )\n from litellm.router import Router\n from litellm.secret_managers.main import get_secret\n@@ -2667,10 +2668,14 @@ async def block_key(\n     if prisma_client is None:\n         raise Exception(\"{}\".format(CommonProxyErrors.db_not_connected_error.value))\n \n-    if data.key.startswith(\"sk-\"):\n-        hashed_token = hash_token(token=data.key)\n-    else:\n-        hashed_token = data.key\n+    if not is_valid_api_key(data.key):\n+        raise ProxyException(\n+            message=\"Invalid key format.\",\n+            type=ProxyErrorTypes.bad_request_error,\n+            param=\"key\",\n+            code=status.HTTP_400_BAD_REQUEST,\n+        )\n+    hashed_token = hash_token(token=data.key)\n \n     if litellm.store_audit_logs is True:\n         # make an audit log for key update\n@@ -2774,10 +2779,14 @@ async def unblock_key(\n     if prisma_client is None:\n         raise Exception(\"{}\".format(CommonProxyErrors.db_not_connected_error.value))\n \n-    if data.key.startswith(\"sk-\"):\n-        hashed_token = hash_token(token=data.key)\n-    else:\n-        hashed_token = data.key\n+    if not is_valid_api_key(data.key):\n+        raise ProxyException(\n+            message=\"Invalid key format.\",\n+            type=ProxyErrorTypes.bad_request_error,\n+            param=\"key\",\n+            code=status.HTTP_400_BAD_REQUEST,\n+        )\n+    hashed_token = hash_token(token=data.key)\n \n     if litellm.store_audit_logs is True:\n         # make an audit log for key update\ndiff --git a/litellm/proxy/utils.py b/litellm/proxy/utils.py\nindex 008b151afce8..655b50cfc200 100644\n--- a/litellm/proxy/utils.py\n+++ b/litellm/proxy/utils.py\n@@ -3184,3 +3184,21 @@ def get_prisma_client_or_throw(message: str):\n             detail={\"error\": message},\n         )\n     return prisma_client\n+\n+\n+def is_valid_api_key(key: str) -> bool:\n+    \"\"\"\n+    Validates API key format:\n+    - sk- keys: must match ^sk-[A-Za-z0-9_-]+$\n+    - hashed keys: must match ^[a-fA-F0-9]{64}$\n+    - Length between 20 and 100 characters\n+    \"\"\"\n+    import re\n+    if not isinstance(key, str):\n+        return False\n+    if 3 <= len(key) <= 100:\n+        if re.match(r\"^sk-[A-Za-z0-9_-]+$\", key):\n+            return True\n+        if re.match(r\"^[a-fA-F0-9]{64}$\", key):\n+            return True\n+    return False\n", "test_patch": "diff --git a/tests/test_litellm/test_utils.py b/tests/test_litellm/test_utils.py\nindex 0ec39e32b977..8f5971387df5 100644\n--- a/tests/test_litellm/test_utils.py\n+++ b/tests/test_litellm/test_utils.py\n@@ -23,6 +23,7 @@\n     get_llm_provider,\n     get_optional_params_image_gen,\n )\n+from litellm.proxy.utils import is_valid_api_key\n \n # Adds the parent directory to the system path\n \n@@ -2167,6 +2168,35 @@ def test_image_response_utils():\n     image_response = ImageResponse(**result)\n \n \n+def test_is_valid_api_key():\n+    import hashlib\n+    # Valid sk- keys\n+    assert is_valid_api_key(\"sk-abc123\")\n+    assert is_valid_api_key(\"sk-ABC_123-xyz\")\n+    # Valid hashed key (64 hex chars)\n+    assert is_valid_api_key(\"a\" * 64)\n+    assert is_valid_api_key(\"0123456789abcdef\" * 4)  # 16*4 = 64\n+    # Real SHA-256 hash\n+    real_hash = hashlib.sha256(b\"my_secret_key\").hexdigest()\n+    assert len(real_hash) == 64\n+    assert is_valid_api_key(real_hash)\n+    # Invalid: too short\n+    assert not is_valid_api_key(\"sk-\")\n+    assert not is_valid_api_key(\"\")\n+    # Invalid: too long\n+    assert not is_valid_api_key(\"sk-\" + \"a\" * 200)\n+    # Invalid: wrong prefix\n+    assert not is_valid_api_key(\"pk-abc123\")\n+    # Invalid: wrong chars in sk- key\n+    assert not is_valid_api_key(\"sk-abc$%#@!\")\n+    # Invalid: not a string\n+    assert not is_valid_api_key(None)\n+    assert not is_valid_api_key(12345)\n+    # Invalid: wrong length for hash\n+    assert not is_valid_api_key(\"a\" * 63)\n+    assert not is_valid_api_key(\"a\" * 65)\n+\n+\n if __name__ == \"__main__\":\n     # Allow running this test file directly for debugging\n     pytest.main([__file__, \"-v\"])\n", "problem_statement": "[Bug]: CVE SQL Injection\n### What happened?\n\nSee this Medium CVE reported here:\n\nhttps://security.snyk.io/vuln/SNYK-PYTHON-LITELLM-10598343\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes - but this is non blocking\n\n### What LiteLLM version are you on ?\n\nv1.72.1\n\n### Twitter / LinkedIn details\n\n_No response_\n", "hints_text": "@ishaan-jaff @krrishdholakia \nHi @ishaan-jaff , @krrishdholakia , we are trying to deploy our app to production and we hit this issue raised by Snyk. \nIs it possible to help to fix this SQL injection issue? \nI think the issue is there: https://github.com/BerriAI/litellm/blob/3a0ea80c757f47309c221222261c57b60e941a44/litellm/proxy/management_endpoints/key_management_endpoints.py#L2673\n\nBasically the token is never sanitized....\n\nLooking at that file https://github.com/BerriAI/litellm/blob/3a0ea80c757f47309c221222261c57b60e941a44/litellm/proxy/management_endpoints/key_management_endpoints.py#L2673 seems that that value is never sanitized.\n\nAnyway looking at the CVE, suggests to use the orm and on https://github.com/BerriAI/litellm/blob/3a0ea80c757f47309c221222261c57b60e941a44/litellm/proxy/management_endpoints/key_management_endpoints.py#L2677 it is used for that value.\nAlso https://github.com/BerriAI/litellm/blob/3a0ea80c757f47309c221222261c57b60e941a44/litellm/proxy/management_endpoints/key_management_endpoints.py#L2705 it is used the orm.\n\nOn https://github.com/BerriAI/litellm/blob/3a0ea80c757f47309c221222261c57b60e941a44/litellm/proxy/management_endpoints/key_management_endpoints.py#L2688 and the function https://github.com/BerriAI/litellm/blob/main/litellm/proxy/management_helpers/audit_logs.py#L67 it is used https://github.com/BerriAI/litellm/blob/main/litellm/proxy/management_helpers/audit_logs.py#L98\n\nSo I am thinking what is of the 3 DB calls creating the problem, maybe @shadia0 the poc author can give more info https://github.com/shadia0/Patienc/blob/main/litellm/SQL_injection.md\n\nI am worried that the issue is on the Prisma ORM is deprecated and litellm uses not pretty well everywhere in the codebase, (there are raw queries in some cases) and the litellm devs are not focusing on replacing it https://github.com/BerriAI/litellm/issues/9753\n\n", "all_hints_text": "@ishaan-jaff @krrishdholakia \nHi @ishaan-jaff , @krrishdholakia , we are trying to deploy our app to production and we hit this issue raised by Snyk. \nIs it possible to help to fix this SQL injection issue? \nI think the issue is there: https://github.com/BerriAI/litellm/blob/3a0ea80c757f47309c221222261c57b60e941a44/litellm/proxy/management_endpoints/key_management_endpoints.py#L2673\n\nBasically the token is never sanitized....\n\nLooking at that file https://github.com/BerriAI/litellm/blob/3a0ea80c757f47309c221222261c57b60e941a44/litellm/proxy/management_endpoints/key_management_endpoints.py#L2673 seems that that value is never sanitized.\n\nAnyway looking at the CVE, suggests to use the orm and on https://github.com/BerriAI/litellm/blob/3a0ea80c757f47309c221222261c57b60e941a44/litellm/proxy/management_endpoints/key_management_endpoints.py#L2677 it is used for that value.\nAlso https://github.com/BerriAI/litellm/blob/3a0ea80c757f47309c221222261c57b60e941a44/litellm/proxy/management_endpoints/key_management_endpoints.py#L2705 it is used the orm.\n\nOn https://github.com/BerriAI/litellm/blob/3a0ea80c757f47309c221222261c57b60e941a44/litellm/proxy/management_endpoints/key_management_endpoints.py#L2688 and the function https://github.com/BerriAI/litellm/blob/main/litellm/proxy/management_helpers/audit_logs.py#L67 it is used https://github.com/BerriAI/litellm/blob/main/litellm/proxy/management_helpers/audit_logs.py#L98\n\nSo I am thinking what is of the 3 DB calls creating the problem, maybe @shadia0 the poc author can give more info https://github.com/shadia0/Patienc/blob/main/litellm/SQL_injection.md\n\nI am worried that the issue is on the Prisma ORM is deprecated and litellm uses not pretty well everywhere in the codebase, (there are raw queries in some cases) and the litellm devs are not focusing on replacing it https://github.com/BerriAI/litellm/issues/9753\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/7d34f36d7b40b8fb2fb9ce6601c66420d23ae868", "https://github.com/BerriAI/litellm/commit/9e339a1b3bf2ecff706ba970a6e089a6d9b5f761", "https://github.com/BerriAI/litellm/commit/9632a8325965dcb7866f4f35998d0221131f39f3"], "created_at": "2025-07-21T23:47:06Z", "classification": "Security"}
{"repo": "BerriAI/litellm", "pull_number": 12607, "instance_id": "BerriAI__litellm-12607", "issue_numbers": [12583], "base_commit": "1b52815b70864aa6d7f5b2379af55441eb5ee3ec", "patch": "diff --git a/litellm/llms/bedrock/base_aws_llm.py b/litellm/llms/bedrock/base_aws_llm.py\nindex df6e0d19f7f6..cc205e62dc99 100644\n--- a/litellm/llms/bedrock/base_aws_llm.py\n+++ b/litellm/llms/bedrock/base_aws_llm.py\n@@ -178,7 +178,10 @@ def get_credentials(\n                 aws_region_name=aws_region_name,\n                 aws_sts_endpoint=aws_sts_endpoint,\n             )\n-        elif aws_role_name is not None and aws_session_name is not None:\n+        elif aws_role_name is not None:\n+            # If aws_session_name is not provided, generate a default one\n+            if aws_session_name is None:\n+                aws_session_name = f\"litellm-session-{int(datetime.now().timestamp())}\"\n             credentials, _cache_ttl = self._auth_with_aws_role(\n                 aws_access_key_id=aws_access_key_id,\n                 aws_secret_access_key=aws_secret_access_key,\n", "test_patch": "diff --git a/tests/test_litellm/llms/bedrock/test_base_aws_llm.py b/tests/test_litellm/llms/bedrock/test_base_aws_llm.py\nindex 20abe8ae6109..a2bbaa620b28 100644\n--- a/tests/test_litellm/llms/bedrock/test_base_aws_llm.py\n+++ b/tests/test_litellm/llms/bedrock/test_base_aws_llm.py\n@@ -22,6 +22,7 @@\n     BaseAWSLLM,\n     Boto3CredentialsInfo,\n )\n+from litellm.caching.caching import DualCache\n \n # Global variable for the base_aws_llm.py file path\n \n@@ -176,7 +177,8 @@ def side_effect(key, default=None):\n \n             assert result == \"ap-southeast-1\"\n             mock_boto3_session.assert_not_called()\n-            \n+\n+\n def test_sign_request_with_env_var_bearer_token():\n     # Create instance of actual class\n     llm = BaseAWSLLM()\n@@ -189,14 +191,14 @@ def test_sign_request_with_env_var_bearer_token():\n     api_base = \"https://api.example.com\"\n \n     # Mock environment variable\n-    with patch.dict(os.environ, {'AWS_BEARER_TOKEN_BEDROCK': 'test_token'}):\n+    with patch.dict(os.environ, {\"AWS_BEARER_TOKEN_BEDROCK\": \"test_token\"}):\n         # Execute\n         result_headers, result_body = llm._sign_request(\n             service_name=service_name,\n             headers=headers,\n             optional_params=optional_params,\n             request_data=request_data,\n-            api_base=api_base\n+            api_base=api_base,\n         )\n \n         # Assert\n@@ -215,7 +217,7 @@ def test_sign_request_with_sigv4():\n     mock_request = MagicMock()\n     mock_request.headers = {\n         \"Authorization\": \"AWS4-HMAC-SHA256 Credential=test\",\n-        \"Content-Type\": \"application/json\"\n+        \"Content-Type\": \"application/json\",\n     }\n     mock_request.body = b'{\"prompt\": \"test\"}'\n \n@@ -225,23 +227,25 @@ def test_sign_request_with_sigv4():\n     optional_params = {\n         \"aws_access_key_id\": \"test_key\",\n         \"aws_secret_access_key\": \"test_secret\",\n-        \"aws_region_name\": \"us-west-2\"\n+        \"aws_region_name\": \"us-west-2\",\n     }\n     request_data = {\"prompt\": \"test\"}\n     api_base = \"https://api.example.com\"\n \n     # Mock the necessary components\n-    with patch('botocore.auth.SigV4Auth', return_value=mock_sigv4), \\\n-            patch('botocore.awsrequest.AWSRequest', return_value=mock_request), \\\n-            patch.object(llm, 'get_credentials', return_value=mock_credentials), \\\n-            patch.object(llm, '_get_aws_region_name', return_value=\"us-west-2\"):\n-\n+    with patch(\"botocore.auth.SigV4Auth\", return_value=mock_sigv4), patch(\n+        \"botocore.awsrequest.AWSRequest\", return_value=mock_request\n+    ), patch.object(\n+        llm, \"get_credentials\", return_value=mock_credentials\n+    ), patch.object(\n+        llm, \"_get_aws_region_name\", return_value=\"us-west-2\"\n+    ):\n         result_headers, result_body = llm._sign_request(\n             service_name=service_name,\n             headers=headers,\n             optional_params=optional_params,\n             request_data=request_data,\n-            api_base=api_base\n+            api_base=api_base,\n         )\n \n         # Assert\n@@ -272,7 +276,7 @@ def test_sign_request_with_api_key_bearer_token():\n         optional_params=optional_params,\n         request_data=request_data,\n         api_base=api_base,\n-        api_key=api_key\n+        api_key=api_key,\n     )\n \n     # Assert\n@@ -300,16 +304,16 @@ def mock_aws_request_init(method, url, data, headers):\n         return mock_request\n \n     # Test with bearer token\n-    with patch.dict(os.environ, {'AWS_BEARER_TOKEN_BEDROCK': 'test_token'}), \\\n-            patch('botocore.awsrequest.AWSRequest', side_effect=mock_aws_request_init):\n-\n+    with patch.dict(os.environ, {\"AWS_BEARER_TOKEN_BEDROCK\": \"test_token\"}), patch(\n+        \"botocore.awsrequest.AWSRequest\", side_effect=mock_aws_request_init\n+    ):\n         result = llm.get_request_headers(\n             credentials=credentials,\n             aws_region_name=\"us-west-2\",\n             extra_headers=None,\n             endpoint_url=\"https://api.example.com\",\n             data='{\"prompt\": \"test\"}',\n-            headers=headers_dict\n+            headers=headers_dict,\n         )\n \n         # Assert\n@@ -331,17 +335,18 @@ def test_get_request_headers_with_sigv4():\n     mock_sigv4 = MagicMock()\n \n     # Test without bearer token (should use SigV4)\n-    with patch.dict(os.environ, {}, clear=True), \\\n-            patch('botocore.auth.SigV4Auth', return_value=mock_sigv4) as mock_sigv4_class, \\\n-            patch('botocore.awsrequest.AWSRequest', return_value=mock_request):\n-\n+    with patch.dict(os.environ, {}, clear=True), patch(\n+        \"botocore.auth.SigV4Auth\", return_value=mock_sigv4\n+    ) as mock_sigv4_class, patch(\n+        \"botocore.awsrequest.AWSRequest\", return_value=mock_request\n+    ):\n         result = llm.get_request_headers(\n             credentials=credentials,\n             aws_region_name=\"us-west-2\",\n             extra_headers=None,\n             endpoint_url=\"https://api.example.com\",\n             data='{\"prompt\": \"test\"}',\n-            headers=headers\n+            headers=headers,\n         )\n \n         # Verify SigV4 authentication and result\n@@ -372,9 +377,9 @@ def mock_aws_request_init(method, url, data, headers):\n         return mock_request\n \n     # Test with api_key parameter\n-    with patch.dict(os.environ, {}, clear=True), \\\n-            patch('botocore.awsrequest.AWSRequest', side_effect=mock_aws_request_init):\n-\n+    with patch.dict(os.environ, {}, clear=True), patch(\n+        \"botocore.awsrequest.AWSRequest\", side_effect=mock_aws_request_init\n+    ):\n         result = llm.get_request_headers(\n             credentials=credentials,\n             aws_region_name=\"us-west-2\",\n@@ -382,9 +387,95 @@ def mock_aws_request_init(method, url, data, headers):\n             endpoint_url=\"https://api.example.com\",\n             data='{\"prompt\": \"test\"}',\n             headers=headers_dict,\n-            api_key=api_key\n+            api_key=api_key,\n         )\n \n         # Assert\n         assert mock_request.headers[\"Authorization\"] == f\"Bearer {api_key}\"\n         assert result == mock_prepared_request\n+\n+\n+def test_role_assumption_without_session_name():\n+    \"\"\"\n+    Test for issue 12583: Role assumption should work when only aws_role_name is provided\n+    without aws_session_name. The system should auto-generate a session name.\n+    \"\"\"\n+    base_aws_llm = BaseAWSLLM()\n+\n+    # Mock the boto3 STS client\n+    mock_sts_client = MagicMock()\n+\n+    # Mock the STS response with proper expiration handling\n+    mock_expiry = MagicMock()\n+    mock_expiry.tzinfo = timezone.utc\n+    current_time = datetime.now(timezone.utc)\n+    # Create a timedelta object that returns 3600 when total_seconds() is called\n+    time_diff = MagicMock()\n+    time_diff.total_seconds.return_value = 3600\n+    mock_expiry.__sub__ = MagicMock(return_value=time_diff)\n+\n+    mock_sts_response = {\n+        \"Credentials\": {\n+            \"AccessKeyId\": \"assumed-access-key\",\n+            \"SecretAccessKey\": \"assumed-secret-key\",\n+            \"SessionToken\": \"assumed-session-token\",\n+            \"Expiration\": mock_expiry,\n+        }\n+    }\n+    mock_sts_client.assume_role.return_value = mock_sts_response\n+\n+    # Test case 1: aws_role_name provided without aws_session_name\n+    with patch(\"boto3.client\", return_value=mock_sts_client):\n+        credentials = base_aws_llm.get_credentials(\n+            aws_role_name=\"arn:aws:iam::2222222222222:role/LitellmEvalBedrockRole\"\n+        )\n+\n+        # Verify assume_role was called\n+        mock_sts_client.assume_role.assert_called_once()\n+\n+        # Check the call arguments\n+        call_args = mock_sts_client.assume_role.call_args\n+        assert (\n+            call_args[1][\"RoleArn\"]\n+            == \"arn:aws:iam::2222222222222:role/LitellmEvalBedrockRole\"\n+        )\n+        # Session name should be auto-generated with format \"litellm-session-{timestamp}\"\n+        assert call_args[1][\"RoleSessionName\"].startswith(\"litellm-session-\")\n+\n+        # Verify credentials are returned correctly\n+        assert isinstance(credentials, Credentials)\n+        assert credentials.access_key == \"assumed-access-key\"\n+        assert credentials.secret_key == \"assumed-secret-key\"\n+        assert credentials.token == \"assumed-session-token\"\n+\n+    # Test case 2: Both aws_role_name and aws_session_name provided (existing behavior)\n+    mock_sts_client.reset_mock()\n+    with patch(\"boto3.client\", return_value=mock_sts_client):\n+        credentials = base_aws_llm.get_credentials(\n+            aws_role_name=\"arn:aws:iam::2222222222222:role/LitellmEvalBedrockRole\",\n+            aws_session_name=\"my-custom-session\",\n+        )\n+\n+        # Verify assume_role was called with custom session name\n+        mock_sts_client.assume_role.assert_called_once()\n+        call_args = mock_sts_client.assume_role.call_args\n+        assert call_args[1][\"RoleSessionName\"] == \"my-custom-session\"\n+\n+    # Test case 3: Verify caching works with auto-generated session names\n+    # Clear the cache first\n+    base_aws_llm.iam_cache = DualCache()\n+\n+    mock_sts_client.reset_mock()\n+    with patch(\"boto3.client\", return_value=mock_sts_client):\n+        # First call\n+        credentials1 = base_aws_llm.get_credentials(\n+            aws_role_name=\"arn:aws:iam::2222222222222:role/LitellmEvalBedrockRole\"\n+        )\n+\n+        # Second call with same role should use cache (not call assume_role again)\n+        credentials2 = base_aws_llm.get_credentials(\n+            aws_role_name=\"arn:aws:iam::2222222222222:role/LitellmEvalBedrockRole\"\n+        )\n+\n+        # Should only be called once due to caching\n+        assert mock_sts_client.assume_role.call_count == 1\n", "problem_statement": "[Bug]: Cross account webauthn not working\n### What happened?\n\nI have litellm running on eks, using https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html where I've allowed litellm to assume a role `arn:aws:iam::1111111111111:role/LitellmRole` which is allowed to use bedrock models via an annotation on the attached kubernetes service account, as well as assuming the role `arn:aws:iam::2222222222222:role/LitellmEvalBedrockRole `which is also allowed to use bedrock models but in another account\n\n```yaml\n\nproxy_config:\n    model_list: \n      - model_name: \"bedrock/*\"\n        litellm_params:\n          model: \"bedrock/*\"\n          aws_region_name: us-east-1\n          aws_role_name: arn:aws:iam::1111111111111:role/LitellmRole\n      - model_name: \"evals-bedrock/*\"\n        litellm_params:\n          model: \"bedrock/*\"\n          aws_region_name: us-east-1\n          aws_role_name: arn:aws:iam::2222222222222:role/LitellmEvalBedrockRole \n\n```\n\nThis setup is so that evals use the rate limits of account 2222222222222 and other usage counts against account 1111111111111\n\nIn the langfuse traces, it claims (via aws_role_name on the trace) that the following api call used role `arn:aws:iam::2222222222222:role/LitellmEvalBedrockRole` but checking the AWS cloudtrail logs, it actually used `arn:aws:iam::1111111111111:role/LitellmRole`\n\n```\ncurl -X POST https://my-api/chat/completions -H \"Content-Type: application/json\" -H \"Authorization: Bearer sk-some-high-entropy-key \" -d '{                                                                                                     \n    \"model\": \"evals-bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Hello there3!\"\n        }\n    ]\n}'\n```\n\n\nInstead, what I expected was for litellm as a proxy to assume that second role, and then use that for doing that bedrock call.\n\n### Relevant log output\n\n```shell\nN/A\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nv1.72.2\n\n### Twitter / LinkedIn details\n\n_No response_\n", "hints_text": "@omrishiv can you help here? \nLooks like @RichardoC beat me to a PR\n\n", "all_hints_text": "@omrishiv can you help here? \nLooks like @RichardoC beat me to a PR\nI think my previous PR broke the bedrock usage in this scenario. Continuing to investigate\n\nHealthcheck for `bedrock` in the example fails with  \n\n```\nlitellm.APIConnectionError: An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:sts::1111111111111:assumed-role/LitellmRole/botocore-session-1752765804 is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::1111111111111:role/LitellmRole\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 2913, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<13 li\n```\n\nHealthcheck for `evals-bedrock` fails with the following, when I also set `aws_session_name: \"evals-bedrock\"`\n\n```\n\nFull Error Details:\n\nlitellm.APIConnectionError: An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:sts::1111111111111:assumed-role/LitellmRole/botocore-session-1752765804 is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::2222222222222:role/LitellmEvalBedrockRole\nTraceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 2913, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<13 lines>...\n        api_key=api_key\n    )\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/bedrock/chat/converse_handler.py\", line 308, in completion\n    credentials: Credentials = self.get_credentials(\n                               ~~~~~~~~~~~~~~~~~~~~^\n        aws_access_key_id=aws_access_key_id,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<7 lines>...\n        aws_sts_endpoint=aws_sts_endpoint,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/bedrock/base_aws_llm.py\", line 185, in get_credentials\n    credentials, _cache_ttl = self._auth_with_aws_role(\n                              ~~~~~~~~~~~~~~~~~~~~~~~~^\n        aws_access_key_id=aws_access_key_id,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n        aws_session_name=aws_session_name,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/llms/bedrock/base_aws_llm.py\", line 470, in _auth_with_aws_role\n    sts_response = sts_client.assume_role(\n        RoleArn=aws_role_name, RoleSessionName=aws_session_name\n    )\n  File \"/usr/lib/python3.13/site-packages/botocore/client.py\", line 553, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/botocore/client.py\", line 1009, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:sts::1111111111111:assumed-role/LitellmRole/botocore-session-1752765804 is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::2222222222222:role/LitellmEvalBedrockRole\n. All fallback attempts failed. Enable verbose logging with `litellm.set_verbose=True` for details.\nstack trace: Traceback (most recent call last):\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 5521, in ahealth_check\n    return await ahealth_check_wildcard_models(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 5461, in ahealth_check_wildcard_models\n    await acompletion(**model_params)\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1553, in wrapper_async\n    raise e\n  File \"/usr/lib/python3.13/site-packages/litellm/utils.py\", line 1411, in wrapper_async\n    result = await original_function(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.13/site-packages/litellm/main.py\", line 505, in acompletion\n    response = await async_completion_with_fallbacks(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n```\n\n\n\nRunning container tag\nmain-v1.74.4-nightly\n> litellm.APIConnectionError: An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:sts::1111111111111:assumed-role/LitellmRole/botocore-session-1752765804 is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::1111111111111:role/LitellmRole\n\n\n@RichardoC does this user even exist? the `1111111111111` seems odd \n> > litellm.APIConnectionError: An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:sts::1111111111111:assumed-role/LitellmRole/botocore-session-1752765804 is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::1111111111111:role/LitellmRole\n> \n> [@RichardoC](https://github.com/RichardoC) does this user even exist? the `1111111111111` seems odd\n\nI replaced the account names with 1111111111111 and 2222222222222 in order to prevent leaking details about the AWS accounts I'm using. These are consistent with the original configuration in the issue.\n\nHope this helps\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/6f2f5bd623f4f4d09f380eaa18af566442ae9d2c", "https://github.com/BerriAI/litellm/commit/11af4e308704bf6a94439b70d0acd39e84d72daf"], "created_at": "2025-07-15T12:36:58Z", "classification": "Security"}
{"repo": "BerriAI/litellm", "pull_number": 13098, "instance_id": "BerriAI__litellm-13098", "issue_numbers": [13074], "base_commit": "ae9a02d7d5579eded58795177e1fa6c48143de5c", "patch": "diff --git a/litellm/llms/gemini/google_genai/transformation.py b/litellm/llms/gemini/google_genai/transformation.py\nindex 9910e478063f..0de6ef9043d9 100644\n--- a/litellm/llms/gemini/google_genai/transformation.py\n+++ b/litellm/llms/gemini/google_genai/transformation.py\n@@ -30,6 +30,12 @@ class GoogleGenAIConfig(BaseGoogleGenAIGenerateContentConfig, VertexLLM):\n     \"\"\"\n     Configuration for calling Google models in their native format.\n     \"\"\"\n+    ##############################\n+    # Constants\n+    ##############################\n+    XGOOGLE_API_KEY = \"x-goog-api-key\"\n+    ##############################\n+    \n     @property\n     def custom_llm_provider(self) -> Literal[\"gemini\", \"vertex_ai\"]:\n         return \"gemini\"\n@@ -113,8 +119,9 @@ def validate_environment(\n         default_headers = {\n             \"Content-Type\": \"application/json\",\n         }\n-        if api_key is not None:\n-            default_headers[\"Authorization\"] = f\"Bearer {api_key}\"\n+        gemini_api_key = self._get_google_ai_studio_api_key(dict(litellm_params or {}))\n+        if gemini_api_key is not None:\n+            default_headers[self.XGOOGLE_API_KEY] = gemini_api_key\n         if headers is not None:\n             default_headers.update(headers)\n \ndiff --git a/litellm/proxy/proxy_config.yaml b/litellm/proxy/proxy_config.yaml\nindex f1cfe75c7ac6..9ee8e675c380 100644\n--- a/litellm/proxy/proxy_config.yaml\n+++ b/litellm/proxy/proxy_config.yaml\n@@ -1,19 +1,5 @@\n model_list:\n-  - model_name: anthropic/*\n+  - model_name: vertex_ai/*\n     litellm_params:\n-      model: anthropic/*\n-  - model_name: openai/*\n-    litellm_params:\n-      model: openai/*\n-\n-  - model_name: vertex_ai/gemini-2.5-pro # Vertex AI Gemini\n-    litellm_params:\n-      model: vertex_ai/gemini-2.5-pro\n-      thinking: {\"type\": \"enabled\", \"budget_tokens\": 1024}\n-      merge_reasoning_content_in_choices: true\n-\n-\n-litellm_settings:\n-  callbacks: [\"datadog_llm_observability\"]\n-  cache: true\n+      model: gemini/*\n \n", "test_patch": "diff --git a/tests/test_litellm/google_genai/test_google_genai_adapter.py b/tests/test_litellm/google_genai/test_google_genai_adapter.py\nindex bf2977588881..69ab677e86a7 100644\n--- a/tests/test_litellm/google_genai/test_google_genai_adapter.py\n+++ b/tests/test_litellm/google_genai/test_google_genai_adapter.py\n@@ -1125,4 +1125,79 @@ async def test_google_generate_content_with_openai():\n         passed_fields = set(call_kwargs.keys())\n         # remove any GenericLiteLLMParams fields\n         passed_fields = passed_fields - set(GenericLiteLLMParams.model_fields.keys())\n-        assert passed_fields == set([\"model\", \"messages\"]), f\"Expected only model, contents, systemInstruction, and safetySettings to be passed through, got {passed_fields}\"\n\\ No newline at end of file\n+        assert passed_fields == set([\"model\", \"messages\"]), f\"Expected only model, contents, systemInstruction, and safetySettings to be passed through, got {passed_fields}\"\n+\n+@pytest.mark.asyncio\n+async def test_agenerate_content_x_goog_api_key_header():\n+    \"\"\"\n+    Test that agenerate_content passes x-goog-api-key header correctly.\n+    \n+    This test verifies that when calling agenerate_content with a Google GenAI model,\n+    the HTTP request includes the x-goog-api-key header with the correct API key value.\n+    \"\"\"\n+    import os\n+    import unittest.mock\n+\n+    import httpx\n+    \n+    test_api_key = \"test-gemini-api-key-123\"\n+    \n+    # Mock environment to ensure we use our test API key\n+    with unittest.mock.patch.dict(os.environ, {\"GEMINI_API_KEY\": test_api_key}, clear=False):\n+        # Mock the AsyncHTTPHandler's post method to capture headers\n+        with unittest.mock.patch(\"litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler.post\", new_callable=unittest.mock.AsyncMock) as mock_post:\n+            # Mock a successful response\n+            mock_response = unittest.mock.MagicMock()\n+            mock_response.json.return_value = {\n+                \"candidates\": [\n+                    {\n+                        \"content\": {\n+                            \"parts\": [{\"text\": \"Hello! How can I help you today?\"}],\n+                            \"role\": \"model\"\n+                        },\n+                        \"finishReason\": \"STOP\",\n+                        \"index\": 0\n+                    }\n+                ],\n+                \"usageMetadata\": {\n+                    \"promptTokenCount\": 5,\n+                    \"candidatesTokenCount\": 10,\n+                    \"totalTokenCount\": 15\n+                }\n+            }\n+            mock_response.status_code = 200\n+            mock_response.headers = {}\n+            mock_post.return_value = mock_response\n+            \n+            # Call agenerate_content with Google AI Studio model\n+            try:\n+                response = await agenerate_content(\n+                    model=\"gemini/gemini-1.5-flash\",\n+                    contents=[\n+                        {\"role\": \"user\", \"parts\": [{\"text\": \"Hello, world!\"}]}\n+                    ],\n+                    api_key=test_api_key\n+                )\n+            except Exception:\n+                # Ignore any response processing errors, we just want to check the headers\n+                pass\n+            \n+            # Verify that AsyncHTTPHandler.post was called\n+            mock_post.assert_called_once()\n+            \n+            # Get the arguments passed to the post call\n+            call_args, call_kwargs = mock_post.call_args\n+            \n+            # Verify that headers contain x-goog-api-key\n+            headers = call_kwargs.get(\"headers\", {})\n+            assert \"x-goog-api-key\" in headers, f\"x-goog-api-key header not found in headers: {list(headers.keys())}\"\n+            \n+            # Verify the API key is set (could be our test key or from api_key parameter)\n+            api_key_value = headers[\"x-goog-api-key\"]\n+            assert api_key_value == test_api_key, f\"Expected x-goog-api-key to be {test_api_key}, got {api_key_value}\"\n+            \n+            # Verify other expected headers\n+            assert headers.get(\"Content-Type\") == \"application/json\", f\"Expected Content-Type application/json, got {headers.get('Content-Type')}\"\n+            \n+            print(f\"\u2713 Test passed: x-goog-api-key header correctly set to {api_key_value}\")\n+            print(f\"\u2713 All headers: {list(headers.keys())}\")\n\\ No newline at end of file\n", "problem_statement": "[Bug]: The Gemini Custom API request has an incorrect authorization format\n### What happened?\n\nI followed the documentation and set up a custom api\n\n<img width=\"1175\" height=\"456\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f96b18f8-3c83-4a17-b581-9fe033c3e87c\" />\n\nUse `sudo docker run --rm -it --network host mitmproxy/mitmproxy mitmweb --web-host 0.0.0.0 --mode reverse:https://one-hub.sunwen.eu.org@8080` to capture and analyze packets\n\n<img width=\"2175\" height=\"1390\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1e0df010-46e4-43c7-98d3-1d9495f6c2b0\" />\n\nAccording to google's gemini documentation: https://ai.google.dev/gemini-api/docs/google-search?hl=zh-cn#rest\n\n<img width=\"1303\" height=\"730\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/36984346-6958-4f0b-8fbb-4c4331560aec\" />\n\nThe header should be `x-goog-api-key`, not `authorization`.\n\n### Relevant log output\n\n```shell\n\n```\n\n### Are you a ML Ops Team?\n\nYes\n\n### What LiteLLM version are you on ?\n\nmain-v1.74.9.rc.1\n\n### Twitter / LinkedIn details\n\n_No response_\n", "hints_text": "\n\n", "all_hints_text": "\n\n", "commit_urls": ["https://github.com/BerriAI/litellm/commit/b4b25b28f45087c27d5a709fdaeee54b5599c6c8", "https://github.com/BerriAI/litellm/commit/962bab9cc3fb92e586de26cbbb1ba07ca367c568", "https://github.com/BerriAI/litellm/commit/9d463d51add125bdf712637cbd05c9fe60bc21a5"], "created_at": "2025-07-29T20:14:30Z", "classification": "Security"}
{"repo": "Textualize/textual", "pull_number": 5851, "instance_id": "Textualize__textual-5851", "issue_numbers": [5768], "base_commit": "29297c4b52f0c0dcbc8c97309ef86a5c2c2bf874", "patch": "diff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 190c9923fb..5c7debf71d 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -17,6 +17,10 @@ and this project adheres to [Semantic Versioning](http://semver.org/).\n \n - Added experimental opt-in support for https://github.com/willmcgugan/textual-speedups\n \n+### Changed\n+\n+- Content markup is now more lenient; if a 'tag' doesn't contain a valid style it will be included verbatim. https://github.com/Textualize/textual/pull/5851\n+\n ## [3.3.0] - 2025-06-01\n \n ### Fixed\ndiff --git a/src/textual/markup.py b/src/textual/markup.py\nindex c2a4571bde..60abb9f5cb 100644\n--- a/src/textual/markup.py\n+++ b/src/textual/markup.py\n@@ -48,8 +48,9 @@ class MarkupError(Exception):\n         variable_ref=VARIABLE_REF,\n         whitespace=r\"\\s+\",\n     )\n-    .expect_eof(False)\n+    .expect_eof(True)\n     .expect_semicolon(False)\n+    .extract_text(True)\n )\n \n expect_markup = Expect(\n@@ -374,14 +375,36 @@ def process_text(template_text: str, /) -> str:\n         elif token_name == \"open_tag\":\n             tag_text = []\n \n+            eof = False\n+            contains_text = False\n             for token in iter_tokens:\n                 if token.name == \"end_tag\":\n                     break\n+                elif token.name == \"text\":\n+                    contains_text = True\n+                elif token.name == \"eof\":\n+                    eof = True\n                 tag_text.append(token.value)\n-            opening_tag = \"\".join(tag_text).strip()\n-            style_stack.append(\n-                (position, opening_tag, normalize_markup_tag(opening_tag))\n-            )\n+            if contains_text or eof:\n+                # \"tag\" was unparsable\n+                text_content = f\"[{''.join(tag_text)}\" + (\"\" if eof else \"]\")\n+                text_append(text_content)\n+                position += len(text_content)\n+            else:\n+                opening_tag = \"\".join(tag_text)\n+\n+                if not opening_tag.strip():\n+                    blank_tag = f\"[{opening_tag}]\"\n+                    text_append(blank_tag)\n+                    position += len(blank_tag)\n+                else:\n+                    style_stack.append(\n+                        (\n+                            position,\n+                            opening_tag,\n+                            normalize_markup_tag(opening_tag.strip()),\n+                        )\n+                    )\n \n         elif token_name == \"open_closing_tag\":\n             tag_text = []\n", "test_patch": "diff --git a/tests/test_content.py b/tests/test_content.py\nindex 9de62d21e4..a5c459f3f6 100644\n--- a/tests/test_content.py\n+++ b/tests/test_content.py\n@@ -229,6 +229,7 @@ def test_assemble():\n         (\"\\\\[/foo\", \"[/foo\"),\n         (\"\\\\[/foo]\", \"[/foo]\"),\n         (\"\\\\[]\", \"[]\"),\n+        (\"\\\\[0]\", \"[0]\"),\n     ],\n )\n def test_escape(markup: str, plain: str) -> None:\n@@ -273,14 +274,3 @@ def test_first_line():\n     first_line = content.first_line\n     assert first_line.plain == \"foo\"\n     assert first_line.spans == [Span(0, 3, \"red\")]\n-\n-\n-def test_errors():\n-    with pytest.raises(Exception):\n-        Content.from_markup(\"[\")\n-\n-    with pytest.raises(Exception):\n-        Content.from_markup(\"[:\")\n-\n-    with pytest.raises(Exception):\n-        Content.from_markup(\"[foo\")\ndiff --git a/tests/test_markup.py b/tests/test_markup.py\nindex 93e2a37b36..346b0a903c 100644\n--- a/tests/test_markup.py\n+++ b/tests/test_markup.py\n@@ -10,6 +10,15 @@\n     [\"markup\", \"content\"],\n     [\n         (\"\", Content(\"\")),\n+        (\"[\", Content(\"[\")),\n+        (\"[]\", Content(\"[]\")),\n+        (\"[ \", Content(\"[ \")),\n+        (\"[  \", Content(\"[  \")),\n+        (\"[  ]\", Content(\"[  ]\")),\n+        (\"[0\", Content(\"[0\")),\n+        (\"[0]\", Content(\"[0]\")),\n+        (\"[red\", Content(\"[red\")),\n+        (\"[red]\", Content(\"\")),\n         (\"foo\", Content(\"foo\")),\n         (\"foo\\n\", Content(\"foo\\n\")),\n         (\"foo\\nbar\", Content(\"foo\\nbar\")),\n@@ -152,8 +161,6 @@ def test_to_content(markup: str, content: Content):\n \n \n def test_content_parse_fail() -> None:\n-    with pytest.raises(MarkupError):\n-        to_content(\"[rgb(1,2,3,4)]foo\")\n     with pytest.raises(MarkupError):\n         to_content(\"[foo]foo[/bar]\")\n     with pytest.raises(MarkupError):\n", "problem_statement": "[0] is being considered a markup tag\nHave you checked closed issues? (https://github.com/Textualize/textual/issues?q=is%3Aissue+is%3Aclosed) *Yes*\n\nHave you checked against the most recent version of Textual? (https://pypi.org/search/?q=textual) *Yes*\n\n## The bug\n\n* Trying to create a Static with markup=True with predefined text \"[0] Hello World\".\n=> Crash\n\n* Trying to escape the text results in a non-escaped string. \n=> Crash\n\nAccording to McGugan:\n> `[0]` shouldn't be considered a tag and shouldn't need escaping. If it is throwing a markup error, that is a bug. Please file an issue.\n\nError:\n```\nMarkupError: Expected markup style value (found '0] Hello World!').\n```\n\nMRE:\n```python\nfrom textual.app import App\nfrom textual.widgets import Footer, Static\nfrom textual.markup import escape\nclass TextualApp(App):\n\n    DEFAULT_CSS = \"\"\"\n    /* any needed CSS  here */\n    \"\"\"\n\n    def compose(self):\n        yield Static(escape(\"[0] Hello World!\"))\n        yield Footer()\n\n    # code that causes a bug here\n```\n\ntextual diagnose:\n```\n<!-- This is valid Markdown, please paste the following directly in to a GitHub issue -->\n# Textual Diagnostics\n\n## Versions\n\n| Name    | Value  |\n|---------|--------|\n| Textual | 3.1.1  |\n| Rich    | 14.0.0 |\n\n## Python\n\n| Name           | Value                                |\n|----------------|--------------------------------------|\n| Version        | 3.12.9                               |\n| Implementation | CPython                              |\n| Compiler       | GCC 13.2.1 20240309                  |\n| Executable     | /home/kuhnchris/test/venv/bin/python |\n\n## Operating System\n\n| Name    | Value                                             |\n|---------|---------------------------------------------------|\n| System  | Linux                                             |\n| Release | 6.12.21-0-lts                                     |\n| Version | #1-Alpine SMP PREEMPT_DYNAMIC 2025-04-01 11:09:17 |\n\n## Terminal\n\n| Name                 | Value          |\n|----------------------|----------------|\n| Terminal Application | *Unknown*      |\n| TERM                 | xterm-256color |\n| COLORTERM            | truecolor      |\n| FORCE_COLOR          | *Not set*      |\n| NO_COLOR             | *Not set*      |\n\n## Rich Console options\n\n| Name           | Value                |\n|----------------|----------------------|\n| size           | width=289, height=98 |\n| legacy_windows | False                |\n| min_width      | 1                    |\n| max_width      | 289                  |\n| is_terminal    | True                 |\n| encoding       | utf-8                |\n| max_height     | 98                   |\n| justify        | None                 |\n| overflow       | None                 |\n| no_wrap        | False                |\n| highlight      | None                 |\n| markup         | None                 |\n| height         | None                 |\n\n```\n\nTechnical workaround:\n(by David Fokkema)\n```\nimport re\n\nescape(\"[1]\", _escape=re.compile(r\"(\\\\*)(\\[[a-z0-9#/@][^[]*?])\").sub)\n```\n\n(by kuhnchris)\n```\noutputText.replace(\"[\", \"\\\\[\")\n```\n\nFull Stack log (textual run --dev):\n\n[fullStackTrace.txt](https://github.com/user-attachments/files/19912727/fullStackTrace.txt)\n", "hints_text": "We found the following entries in the [FAQ](https://textual.textualize.io/FAQ/) which you may find helpful:\n\n\n- [Does Textual support images?](https://textual.textualize.io/FAQ/#does-textual-support-images)\n- [Why doesn't Textual look good on macOS?](https://textual.textualize.io/FAQ/#why-doesnt-textual-look-good-on-macos)\n\nFeel free to close this issue if you found an answer in the FAQ. Otherwise, please give us a little time to review.\n\nThis is an automated reply, generated by [FAQtory](https://github.com/willmcgugan/faqtory)\nThe uppercase also should be considered:\n```python3\nfrom textual.app import App\nfrom textual.widgets import Static\nfrom textual.markup import escape\nimport re\n\nclass MyApp(App):\n    def on_mount(self):\n        self.widgets = [ \n                Static(escape('[0 W] Hello World!',re.compile(r'(\\\\*)(\\[[0-9a-zA-Z#/@][^[]*?])').sub)),\n            ]\n        self.mount_all(self.widgets)\n\nif __name__ == '__main__':\n    app = MyApp()\n    app.run()\n```\nNot sure if I misunderstood how to use `escape()`, but it seems to not work properly on many strings.\nBelow are some strings I tested that cause issues:\n\n```python\nfrom __future__ import annotations\n\nfrom typing import Final\n\nfrom textual.app import App, ComposeResult\nfrom textual.markup import escape\nfrom textual.widgets import Static\n\nTEST_STRINGS: Final = [\n    'foo [123] bar',  # MarkupError\n    'foo [12,345 items] bar',  # MarkupError\n    'foo [ word ] bar',  # incorrectly remove \"[ word ]\" from the text\n    'foo [ Checked , Alpha 20 ] bar',  # MarkupError\n    'foo [Super] bar',  # incorrectly remove \"[Super]\" from the text\n    'foo [\u4e2d\u6587] bar',  # MarkupError\n    'path\\\\[123]\\\\to\\\\file',  # incorrectly remove the first backslash from the text\n    'path\\\\[xyz]\\\\to\\\\file',  # incorrectly add an extra backslash to the text\n    '[123][456]',  # MarkupError\n    '[123][456][/123]',  # MarkupError\n    '\\\\[yes]/',  # incorrectly add an extra backslash to the text\n    '\\\\\\\\\\\\[yes] \\\\\\\\ \\\\[no]',  # incorrectly add extra backslashes to the text\n    ]\n\n\nclass TestEscapeApp(App):\n    def compose(self) -> ComposeResult:\n        for text in TEST_STRINGS:\n            yield Static(escape(text))\n\n\nif __name__ == '__main__':\n    app = TestEscapeApp()\n    app.run()\n\n```\n\nTested on the following version:\n\n```\nTextual 3.3.0\nRich    14.0.0\n```\n\nThe workaround provided by kuhnchris appears to work (though not thoroughly tested):\n\n```python\ntext.replace('[', '\\\\[')\n```\n\n\n", "all_hints_text": "We found the following entries in the [FAQ](https://textual.textualize.io/FAQ/) which you may find helpful:\n\n\n- [Does Textual support images?](https://textual.textualize.io/FAQ/#does-textual-support-images)\n- [Why doesn't Textual look good on macOS?](https://textual.textualize.io/FAQ/#why-doesnt-textual-look-good-on-macos)\n\nFeel free to close this issue if you found an answer in the FAQ. Otherwise, please give us a little time to review.\n\nThis is an automated reply, generated by [FAQtory](https://github.com/willmcgugan/faqtory)\nThe uppercase also should be considered:\n```python3\nfrom textual.app import App\nfrom textual.widgets import Static\nfrom textual.markup import escape\nimport re\n\nclass MyApp(App):\n    def on_mount(self):\n        self.widgets = [ \n                Static(escape('[0 W] Hello World!',re.compile(r'(\\\\*)(\\[[0-9a-zA-Z#/@][^[]*?])').sub)),\n            ]\n        self.mount_all(self.widgets)\n\nif __name__ == '__main__':\n    app = MyApp()\n    app.run()\n```\nNot sure if I misunderstood how to use `escape()`, but it seems to not work properly on many strings.\nBelow are some strings I tested that cause issues:\n\n```python\nfrom __future__ import annotations\n\nfrom typing import Final\n\nfrom textual.app import App, ComposeResult\nfrom textual.markup import escape\nfrom textual.widgets import Static\n\nTEST_STRINGS: Final = [\n    'foo [123] bar',  # MarkupError\n    'foo [12,345 items] bar',  # MarkupError\n    'foo [ word ] bar',  # incorrectly remove \"[ word ]\" from the text\n    'foo [ Checked , Alpha 20 ] bar',  # MarkupError\n    'foo [Super] bar',  # incorrectly remove \"[Super]\" from the text\n    'foo [\u4e2d\u6587] bar',  # MarkupError\n    'path\\\\[123]\\\\to\\\\file',  # incorrectly remove the first backslash from the text\n    'path\\\\[xyz]\\\\to\\\\file',  # incorrectly add an extra backslash to the text\n    '[123][456]',  # MarkupError\n    '[123][456][/123]',  # MarkupError\n    '\\\\[yes]/',  # incorrectly add an extra backslash to the text\n    '\\\\\\\\\\\\[yes] \\\\\\\\ \\\\[no]',  # incorrectly add extra backslashes to the text\n    ]\n\n\nclass TestEscapeApp(App):\n    def compose(self) -> ComposeResult:\n        for text in TEST_STRINGS:\n            yield Static(escape(text))\n\n\nif __name__ == '__main__':\n    app = TestEscapeApp()\n    app.run()\n\n```\n\nTested on the following version:\n\n```\nTextual 3.3.0\nRich    14.0.0\n```\n\nThe workaround provided by kuhnchris appears to work (though not thoroughly tested):\n\n```python\ntext.replace('[', '\\\\[')\n```\n\nDon't forget to [star](https://github.com/Textualize/textual) the repository!\n\nFollow [@textualizeio](https://twitter.com/textualizeio) for Textual updates.\n\n", "commit_urls": ["https://github.com/Textualize/textual/commit/9b72dc9719cb369b03d0f453e2a4cc67da561a42", "https://github.com/Textualize/textual/commit/5ee3c6a4a988eec1834143e73cca083451916e20", "https://github.com/Textualize/textual/commit/ffd644972acf46f2ada215878eef562f5db804a1", "https://github.com/Textualize/textual/commit/2f8f5b9dbb695b489ab14c78f70b9f9f57139969"], "created_at": "2025-06-08T15:54:19Z", "classification": "Security"}
{"repo": "tornadoweb/tornado", "pull_number": 3521, "instance_id": "tornadoweb__tornado-3521", "issue_numbers": [3510], "base_commit": "d3bf1ee425a6d44f9e57c21a3763860f60babd8c", "patch": "diff --git a/tornado/http1connection.py b/tornado/http1connection.py\nindex 8dd0c9b6e..5b0dd1b8d 100644\n--- a/tornado/http1connection.py\n+++ b/tornado/http1connection.py\n@@ -66,6 +66,9 @@ def __exit__(\n     ) -> None:\n         if value is not None:\n             assert typ is not None\n+            # Let HTTPInputError pass through to higher-level handler\n+            if isinstance(value, httputil.HTTPInputError):\n+                return None\n             self.logger.error(\"Uncaught exception\", exc_info=(typ, value, tb))\n             raise _QuietException\n \ndiff --git a/tornado/routing.py b/tornado/routing.py\nindex 245070c79..ff833d5e7 100644\n--- a/tornado/routing.py\n+++ b/tornado/routing.py\n@@ -279,8 +279,8 @@ def finish(self) -> None:\n         self.delegate.finish()\n \n     def on_connection_close(self) -> None:\n-        assert self.delegate is not None\n-        self.delegate.on_connection_close()\n+        if self.delegate is not None:\n+            self.delegate.on_connection_close()\n \n \n class _DefaultMessageDelegate(httputil.HTTPMessageDelegate):\n", "test_patch": "diff --git a/tornado/test/httpclient_test.py b/tornado/test/httpclient_test.py\nindex 08c809a2f..77c0d6eb9 100644\n--- a/tornado/test/httpclient_test.py\n+++ b/tornado/test/httpclient_test.py\n@@ -442,7 +442,7 @@ def test_invalid_gzip(self):\n         # test if client hangs on tricky invalid gzip\n         # curl/simple httpclient have different behavior (exception, logging)\n         with ExpectLog(\n-            app_log, \"(Uncaught exception|Exception in callback)\", required=False\n+            gen_log, \".*Malformed HTTP message.*unconsumed gzip data\", required=False\n         ):\n             try:\n                 response = self.fetch(\"/invalid_gzip\")\ndiff --git a/tornado/test/httpserver_test.py b/tornado/test/httpserver_test.py\nindex f197cfef8..008078ed9 100644\n--- a/tornado/test/httpserver_test.py\n+++ b/tornado/test/httpserver_test.py\n@@ -462,6 +462,18 @@ def test_malformed_headers(self):\n             self.io_loop.add_timeout(datetime.timedelta(seconds=0.05), self.stop)\n             self.wait()\n \n+    def test_invalid_host_header_with_whitespace(self):\n+        with ExpectLog(\n+            gen_log, \".*Malformed HTTP message.*Invalid Host header\", level=logging.INFO\n+        ):\n+            self.stream.write(b\"GET / HTTP/1.0\\r\\nHost: foo bar\\r\\n\\r\\n\")\n+            start_line, headers, response = self.io_loop.run_sync(\n+                lambda: read_stream_body(self.stream)\n+            )\n+            self.assertEqual(\"HTTP/1.1\", start_line.version)\n+            self.assertEqual(400, start_line.code)\n+            self.assertEqual(\"Bad Request\", start_line.reason)\n+\n     def test_chunked_request_body(self):\n         # Chunked requests are not widely supported and we don't have a way\n         # to generate them in AsyncHTTPClient, but HTTPServer will read them.\n", "problem_statement": "500 error if non-RFC Host is being passed in headers.\nWith 6.5.1 (and 6.5) I am getting 500 error if I add `Host: http//1.1.1.1:80` or similar non-RFC values.\n\n```\n2025-06-11 23:12:48,612 - tornado.application - ERROR - Uncaught exception\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/tornado/http1connection.py\", line 222, in _read_message\n    header_recv_future = delegate.headers_received(start_line, headers)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/tornado/routing.py\", line 255, in headers_received\n    request = httputil.HTTPServerRequest(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/tornado/httputil.py\", line 506, in __init__\n    raise HTTPInputError(\"Invalid Host header: %r\" % self.host)\ntornado.httputil.HTTPInputError: Invalid Host header: 'http//1.1.1.1:80'\n2025-06-11 23:12:48,614 - tornado.application - ERROR - Uncaught exception\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/tornado/http1connection.py\", line 222, in _read_message\n    header_recv_future = delegate.headers_received(start_line, headers)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/tornado/routing.py\", line 255, in headers_received\n    request = httputil.HTTPServerRequest(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/tornado/httputil.py\", line 506, in __init__\n    raise HTTPInputError(\"Invalid Host header: %r\" % self.host)\ntornado.httputil.HTTPInputError: Invalid Host header: 'http//1.1.1.1:80'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/tornado/http1connection.py\", line 221, in _read_message\n    with _ExceptionLoggingContext(app_log):\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/tornado/http1connection.py\", line 70, in __exit__\n    raise _QuietException\ntornado.http1connection._QuietException\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/tornado/http1connection.py\", line 302, in _read_message\n    delegate.on_connection_close()\n  File \"/usr/local/lib/python3.12/site-packages/tornado/routing.py\", line 282, in on_connection_close\n    assert self.delegate is not None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n```\nTo reproduce:\n`Dockerfile`\n```\nFROM python:3.12-slim\nWORKDIR /app\nRUN apt-get update && apt-get install -y \\\n    gcc \\\n    && rm -rf /var/lib/apt/lists/*\nRUN pip install --no-cache-dir tornado==6.5.1 requests\nCOPY . .\nEXPOSE 8888\nCMD [\"python\", \"server.py\"]\n```\n`server.py`\n```\n#!/usr/bin/env python3\n\nimport asyncio\nimport tornado.web\n\n\nclass MainHandler(tornado.web.RequestHandler):\n\n    def get(self):\n        self.write({\"message\": \"Hello from Tornado 6.5.1\", \"status\": \"running\"})\n\n\ndef make_app():\n    return tornado.web.Application([\n        (r\"/\", MainHandler),\n    ], debug=True)\n\n\nasync def main():\n    app = make_app()\n    app.listen(8888)\n\n    print(\"Tornado server started on http://0.0.0.0:8888\")\n    print(\"Tornado version: 6.5.1\")\n\n    await asyncio.Event().wait()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n`client.py`\n```\n#!/usr/bin/env python3\n\nimport requests\n\n\ndef main() -> None:\n    session = requests.Session()\n    session.headers.update({\"Host\": \"http//1.1.1.1:80\"})\n\n    try:\n        response = session.get(\"http://localhost:8888/\")\n        print(f\"Status: {response.status_code}\")\n        print(f\"Response: {response.json()}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n    finally:\n        session.close()\n\n\nif __name__ == \"__main__\":\n    main()\n```\nTo run:\n```\npodman build -t tornado-bug . && podman run -it --rm -p 8888:8888 tornado-bug\n```\nand from another terminal: `python client.py`\n\nAlso I think this line can be removed \ud83d\ude04 \nhttps://github.com/tornadoweb/tornado/blob/bb07ce9d1f0e9529b6fe9c2cebe78cfb238b4a0c/tornado/httputil.py#L505\n", "hints_text": "Oh dear, that `print` was a mistake. But otherwise, this was a deliberate change. Why should requests that fail to conform to the RFCs be accepted? Sometimes there's a good reason, like if there's a major piece of software that sends this format and we're matching widely-supported behavior, but this isn't one that I've seen before. \n\nNote that even in older versions of Tornado when this format did not raise a 500 error, it wouldn't work properly if you had any routing rules that depended on the `Host` header. \n> Why should requests that fail to conform to the RFCs be accepted?\n\nI agree it should not be accepted, but I (personally) would not expect 500 and stacktrace in logs. It not the _server_ error, but _client_ messing up with request. So some 4xx would look as a better fit.\nAh, yes, it should be a 400 instead of 500. The try/except that turns HTTPInputError into a 400 is not in the right place.\n\n", "all_hints_text": "Oh dear, that `print` was a mistake. But otherwise, this was a deliberate change. Why should requests that fail to conform to the RFCs be accepted? Sometimes there's a good reason, like if there's a major piece of software that sends this format and we're matching widely-supported behavior, but this isn't one that I've seen before. \n\nNote that even in older versions of Tornado when this format did not raise a 500 error, it wouldn't work properly if you had any routing rules that depended on the `Host` header. \n> Why should requests that fail to conform to the RFCs be accepted?\n\nI agree it should not be accepted, but I (personally) would not expect 500 and stacktrace in logs. It not the _server_ error, but _client_ messing up with request. So some 4xx would look as a better fit.\nAh, yes, it should be a 400 instead of 500. The try/except that turns HTTPInputError into a 400 is not in the right place.\n\n", "commit_urls": ["https://github.com/tornadoweb/tornado/commit/44aac917728caf855d6acbb42f8001ca03810509"], "created_at": "2025-07-22T19:09:40Z", "classification": "Security"}
